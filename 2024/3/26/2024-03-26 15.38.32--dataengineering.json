{"kind": "Listing", "data": {"after": "t3_1bo5yzw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, so Parquet is famouse for being more efficient than csv in terms of storage sapce and query times. I wanted to check that, so I created a dummy csv file with 1 million records. It takes up 4.34 MB. I then converted it to Parquet using pandas and the newly cerated file takes up  35.9 MB! Almost 10 times more! Why? I expected the opposite.\n\n&amp;#x200B;\n\nCode for reference:\n\n    import csv\n    import random\n    from faker import Faker\n    \n    # Initialize Faker to generate fake data\n    fake = Faker()\n    \n    # Define the number of rows to generate\n    num_rows = 1000000\n    \n    # Define field names for the CSV\n    field_names = ['ID', 'Name', 'Age', 'Email', 'Address']\n    \n    # Function to generate random data for a row\n    def generate_row():\n        return {\n            'ID': random.randint(1, 1000000),\n            'Name': fake.name(),\n            'Age': random.randint(18, 80),\n            'Email': fake.email(),\n            'Address': fake.address()\n        }\n    \n    # Generate and write data to CSV file\n    with open('dummy_data.csv', 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n        writer.writeheader()\n        for _ in range(num_rows):\n            writer.writerow(generate_row())\n    \n    print(\"CSV file generation complete.\")\n    \n    ##########################################\n    \n    import pandas as pd\n    \n    # Read CSV file into pandas DataFrame\n    df = pd.read_csv('dummy_data.csv')\n    \n    # Write DataFrame to Parquet file\n    df.to_parquet('dummy_data.parquet', index=False)\n    \n    print(\"Parquet file generation complete.\")\n    \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My parquet file takes up MORE space than original csv. Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnps6k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711402498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, so Parquet is famouse for being more efficient than csv in terms of storage sapce and query times. I wanted to check that, so I created a dummy csv file with 1 million records. It takes up 4.34 MB. I then converted it to Parquet using pandas and the newly cerated file takes up  35.9 MB! Almost 10 times more! Why? I expected the opposite.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Code for reference:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import csv\nimport random\nfrom faker import Faker\n\n# Initialize Faker to generate fake data\nfake = Faker()\n\n# Define the number of rows to generate\nnum_rows = 1000000\n\n# Define field names for the CSV\nfield_names = [&amp;#39;ID&amp;#39;, &amp;#39;Name&amp;#39;, &amp;#39;Age&amp;#39;, &amp;#39;Email&amp;#39;, &amp;#39;Address&amp;#39;]\n\n# Function to generate random data for a row\ndef generate_row():\n    return {\n        &amp;#39;ID&amp;#39;: random.randint(1, 1000000),\n        &amp;#39;Name&amp;#39;: fake.name(),\n        &amp;#39;Age&amp;#39;: random.randint(18, 80),\n        &amp;#39;Email&amp;#39;: fake.email(),\n        &amp;#39;Address&amp;#39;: fake.address()\n    }\n\n# Generate and write data to CSV file\nwith open(&amp;#39;dummy_data.csv&amp;#39;, &amp;#39;w&amp;#39;, newline=&amp;#39;&amp;#39;) as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n    writer.writeheader()\n    for _ in range(num_rows):\n        writer.writerow(generate_row())\n\nprint(&amp;quot;CSV file generation complete.&amp;quot;)\n\n##########################################\n\nimport pandas as pd\n\n# Read CSV file into pandas DataFrame\ndf = pd.read_csv(&amp;#39;dummy_data.csv&amp;#39;)\n\n# Write DataFrame to Parquet file\ndf.to_parquet(&amp;#39;dummy_data.parquet&amp;#39;, index=False)\n\nprint(&amp;quot;Parquet file generation complete.&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnps6k", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnps6k/my_parquet_file_takes_up_more_space_than_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnps6k/my_parquet_file_takes_up_more_space_than_original/", "subreddit_subscribers": 171875, "created_utc": 1711402498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys after finishing a contract in a company I\u2019m searching for another opportunity in Europe based remotely and what I see in the job descriptions in LinkedIn are 27 technologies needed for the position and you have to be an expert, even not a senior position (I have 3.5 years of experience), what is happening here?\n\nYou need to know: python, pyspark, scala , JavaScript, java, azure, aws, gcp (and all the the technologies), databricks, airflow, Kafka, sql, no sql, data lakes, dwh, oracle, ETL\u2019s, terraform, Jenkins, kubernetes\u2026 and more \n\nOfc all of this fluent and proficient, lol\n\n\n\nAnd not even senior positions\u2026 what would you recommend, guys?\nI\u2019ve been working with azure data factory/synapse/Databricks with python/pyspark and sql, doing etl/elt pipelines from on-premise ddbb or simple excels or cloud ddbb, or api\u2019s.\n\n", "author_fullname": "t2_6bblasam", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding a new job, ridiculous ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo4nne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711449324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys after finishing a contract in a company I\u2019m searching for another opportunity in Europe based remotely and what I see in the job descriptions in LinkedIn are 27 technologies needed for the position and you have to be an expert, even not a senior position (I have 3.5 years of experience), what is happening here?&lt;/p&gt;\n\n&lt;p&gt;You need to know: python, pyspark, scala , JavaScript, java, azure, aws, gcp (and all the the technologies), databricks, airflow, Kafka, sql, no sql, data lakes, dwh, oracle, ETL\u2019s, terraform, Jenkins, kubernetes\u2026 and more &lt;/p&gt;\n\n&lt;p&gt;Ofc all of this fluent and proficient, lol&lt;/p&gt;\n\n&lt;p&gt;And not even senior positions\u2026 what would you recommend, guys?\nI\u2019ve been working with azure data factory/synapse/Databricks with python/pyspark and sql, doing etl/elt pipelines from on-premise ddbb or simple excels or cloud ddbb, or api\u2019s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bo4nne", "is_robot_indexable": true, "report_reasons": null, "author": "Irachar", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo4nne/finding_a_new_job_ridiculous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo4nne/finding_a_new_job_ridiculous/", "subreddit_subscribers": 171875, "created_utc": 1711449324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I probably just need to vent. I have taken on so many analytic/ds type tasks (beginner to intermediate ml/stats type stuff) at this point that Im essentially the lead analyst of my small team... as well as the de, swe, infra guy etc (de is official role - those tasks involve managing our airflow/infra and ETL/db modeling, probably on a smaller scale than most des).\n\nLet me get this out of the way: every job is like this to a degree, at the end of the day you do what youre told and get paid. And some of it can even be enjoyable. \n\nBut at what point do you just nope out of there? At this point my team depends on me for everything - engineering, analysis, and any kind of soft skills task (presentations to stakeholders). Its exhausting and starting to feel ridiculous. Other analysts on the team were hired primarily for sql queries and dashboards, as we were told our team would be streamlining to mostly do reporting and the etl that drives it. Of course, that never happened. This leaves me with \"everything else\". I dont think mgmt realizes how screwed they could be re biz continuity, because they dont really understand any of the work involved. Not to brag that Im irreplacable, Im sure they could find some other sap to do the same.\n\nAnyone else been or currently in this kinda situation? What did you do?", "author_fullname": "t2_v3k0dc9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "where do you draw the line at the nebulous job title?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnqqvl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711404953.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711404685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I probably just need to vent. I have taken on so many analytic/ds type tasks (beginner to intermediate ml/stats type stuff) at this point that Im essentially the lead analyst of my small team... as well as the de, swe, infra guy etc (de is official role - those tasks involve managing our airflow/infra and ETL/db modeling, probably on a smaller scale than most des).&lt;/p&gt;\n\n&lt;p&gt;Let me get this out of the way: every job is like this to a degree, at the end of the day you do what youre told and get paid. And some of it can even be enjoyable. &lt;/p&gt;\n\n&lt;p&gt;But at what point do you just nope out of there? At this point my team depends on me for everything - engineering, analysis, and any kind of soft skills task (presentations to stakeholders). Its exhausting and starting to feel ridiculous. Other analysts on the team were hired primarily for sql queries and dashboards, as we were told our team would be streamlining to mostly do reporting and the etl that drives it. Of course, that never happened. This leaves me with &amp;quot;everything else&amp;quot;. I dont think mgmt realizes how screwed they could be re biz continuity, because they dont really understand any of the work involved. Not to brag that Im irreplacable, Im sure they could find some other sap to do the same.&lt;/p&gt;\n\n&lt;p&gt;Anyone else been or currently in this kinda situation? What did you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnqqvl", "is_robot_indexable": true, "report_reasons": null, "author": "zazzersmel", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnqqvl/where_do_you_draw_the_line_at_the_nebulous_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnqqvl/where_do_you_draw_the_line_at_the_nebulous_job/", "subreddit_subscribers": 171875, "created_utc": 1711404685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm mostly a python guy but at one point I wrote c#. Lately I've been enjoying scala. Seems like a nice abstraction over java and has a much more declarative structure than python. \n\nMy job is mostly writing backend APIs and automation in fastapi. I've done a few wildly basic akka endpoints over the years and really like that syntax. \n\nThing is.. scala is pretty much not used by our industry. At prior shops they were either straight java / python or c# if they were Microsoft shop. I have yet to find a shop that embraced scala. I love the live execution debugger, such a nice feature. Feels like pdb. \n\nIt's fair to say that I personally am not benifiting from any speed differential between python and scala Im preferential because I enjoy the syntax. My python is purely a wrapper over compiled binaries. \n\nWhy don't YOU personally use scala?", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on scala? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnupia", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m mostly a python guy but at one point I wrote c#. Lately I&amp;#39;ve been enjoying scala. Seems like a nice abstraction over java and has a much more declarative structure than python. &lt;/p&gt;\n\n&lt;p&gt;My job is mostly writing backend APIs and automation in fastapi. I&amp;#39;ve done a few wildly basic akka endpoints over the years and really like that syntax. &lt;/p&gt;\n\n&lt;p&gt;Thing is.. scala is pretty much not used by our industry. At prior shops they were either straight java / python or c# if they were Microsoft shop. I have yet to find a shop that embraced scala. I love the live execution debugger, such a nice feature. Feels like pdb. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fair to say that I personally am not benifiting from any speed differential between python and scala Im preferential because I enjoy the syntax. My python is purely a wrapper over compiled binaries. &lt;/p&gt;\n\n&lt;p&gt;Why don&amp;#39;t YOU personally use scala?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnupia", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnupia/thoughts_on_scala/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnupia/thoughts_on_scala/", "subreddit_subscribers": 171875, "created_utc": 1711414574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to get some career advice. I've been working as a Data Engineer for almost 5 years, but a huge part of this job was just Python development (APIs, libs, microservices...) and another part was building pipelines, using spark, Kafka, SQL, NoSQL, data bricks,  Airflow and others. Currently, I am working on another project in Hadoop on-prem where I'm building spark streaming apps - but I don't like Hadoop and the on-prem ecosystem, and I think also that it's legacy stack.   \nBecause of these above, I have very poor, almost 0 experience and knowledge in Cloud.\n\nThat's why I wanted to ask you for advice. I got an offer for a Data Engineer on AWS - the stack there is very simple like s3, kinesis, glue, Athena, Ecs, Rds, and Redshift - and the team has only one Data Engineer who basically is also on his road with AWS, but already passed some certificates.  \nIn this role there is a very small part of coding - like 10-15% of the job. But they are okay, that I don't know AWS and they will give me time and space to learn it and to pass certificates.  \nAnd I wonder if I should take it, on one side I will learn AWS, probably a couple of DE-related services, and I will pass the exam. On the other side, I won't code too much, and probably everything will be pretty low quality, as there is no collaboration with DevOps, SWEs, and other DEs into good swe practices (testing, high quality of code, design etc).   \n\n\nI'm having a bit of a headache with this. Do you think this is a good idea and a step forward in your career - I think it's a position for at least a year. But will it be a step backward - especially since I have been working more in programming like data engineering (writing code 80% of the time) for the last 5 years?  \n\n\n  \n\n\n  \n\n\n&amp;#x200B;", "author_fullname": "t2_2llofc3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice regarding new offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo1qwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711437436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to get some career advice. I&amp;#39;ve been working as a Data Engineer for almost 5 years, but a huge part of this job was just Python development (APIs, libs, microservices...) and another part was building pipelines, using spark, Kafka, SQL, NoSQL, data bricks,  Airflow and others. Currently, I am working on another project in Hadoop on-prem where I&amp;#39;m building spark streaming apps - but I don&amp;#39;t like Hadoop and the on-prem ecosystem, and I think also that it&amp;#39;s legacy stack.&lt;br/&gt;\nBecause of these above, I have very poor, almost 0 experience and knowledge in Cloud.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why I wanted to ask you for advice. I got an offer for a Data Engineer on AWS - the stack there is very simple like s3, kinesis, glue, Athena, Ecs, Rds, and Redshift - and the team has only one Data Engineer who basically is also on his road with AWS, but already passed some certificates.&lt;br/&gt;\nIn this role there is a very small part of coding - like 10-15% of the job. But they are okay, that I don&amp;#39;t know AWS and they will give me time and space to learn it and to pass certificates.&lt;br/&gt;\nAnd I wonder if I should take it, on one side I will learn AWS, probably a couple of DE-related services, and I will pass the exam. On the other side, I won&amp;#39;t code too much, and probably everything will be pretty low quality, as there is no collaboration with DevOps, SWEs, and other DEs into good swe practices (testing, high quality of code, design etc).   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a bit of a headache with this. Do you think this is a good idea and a step forward in your career - I think it&amp;#39;s a position for at least a year. But will it be a step backward - especially since I have been working more in programming like data engineering (writing code 80% of the time) for the last 5 years?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bo1qwr", "is_robot_indexable": true, "report_reasons": null, "author": "masek94", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo1qwr/career_advice_regarding_new_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo1qwr/career_advice_regarding_new_offer/", "subreddit_subscribers": 171875, "created_utc": 1711437436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHello everyone,\n\nI have just completed learning Snowflake and dbt. Is building a project the only way to become proficient?", "author_fullname": "t2_ic15ya1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "snowflake+dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnkubv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711391040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I have just completed learning Snowflake and dbt. Is building a project the only way to become proficient?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnkubv", "is_robot_indexable": true, "report_reasons": null, "author": "satwikchandra", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnkubv/snowflakedbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnkubv/snowflakedbt/", "subreddit_subscribers": 171875, "created_utc": 1711391040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I've been working hard to learn these, but I'm stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven't seen many jobs on Upwork. Is full-time freelancing even a possibility in this field? ", "author_fullname": "t2_w54oah5ij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Guidance on Advancing Skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnnfq8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711397087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I&amp;#39;ve been working hard to learn these, but I&amp;#39;m stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven&amp;#39;t seen many jobs on Upwork. Is full-time freelancing even a possibility in this field? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnnfq8", "is_robot_indexable": true, "report_reasons": null, "author": "abhannan980", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnnfq8/seeking_guidance_on_advancing_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnnfq8/seeking_guidance_on_advancing_skills/", "subreddit_subscribers": 171875, "created_utc": 1711397087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an etl pipeline that ingests IOT data hourly. The storage account, bronze, and silver tables are hive partitioned by the year month day hour of when my system received the data. This works well for some use cases.\n\n I have another use case where having the same data but partitioned by observation date would be way more efficient. Observation date is when my device made the reading, and the partition is based on when my system received the data. Sometimes these are different depending on late data. \n\nCurrently I insert all the data into both tables, one partitioned by y/m/d/h and the other partitioned by observation date. Is there  more efficient way to accomplish this? I can only think of switching to structured streaming with CDC all the way through my pipeline instead of some spots batch based on y/m/d/h, and some spots already structured streaming. Then I could scrap the y/m/d/h table and only rely on date partitioned everywhere. ", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Different partitions of same table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnuiuv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an etl pipeline that ingests IOT data hourly. The storage account, bronze, and silver tables are hive partitioned by the year month day hour of when my system received the data. This works well for some use cases.&lt;/p&gt;\n\n&lt;p&gt;I have another use case where having the same data but partitioned by observation date would be way more efficient. Observation date is when my device made the reading, and the partition is based on when my system received the data. Sometimes these are different depending on late data. &lt;/p&gt;\n\n&lt;p&gt;Currently I insert all the data into both tables, one partitioned by y/m/d/h and the other partitioned by observation date. Is there  more efficient way to accomplish this? I can only think of switching to structured streaming with CDC all the way through my pipeline instead of some spots batch based on y/m/d/h, and some spots already structured streaming. Then I could scrap the y/m/d/h table and only rely on date partitioned everywhere. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnuiuv", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnuiuv/different_partitions_of_same_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnuiuv/different_partitions_of_same_table/", "subreddit_subscribers": 171875, "created_utc": 1711414068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've had pretty good success with Copilot with things like Python (Airflow), Bash and powershell but not much with SQL. \n\nIt hasn't done a good job learning how the different tables join or what I'm most likely to select or fields I'm returning. Has anyone used this with Visual Studio Code or Azure Data Studio?", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with Github Copilot for SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnstzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711409739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had pretty good success with Copilot with things like Python (Airflow), Bash and powershell but not much with SQL. &lt;/p&gt;\n\n&lt;p&gt;It hasn&amp;#39;t done a good job learning how the different tables join or what I&amp;#39;m most likely to select or fields I&amp;#39;m returning. Has anyone used this with Visual Studio Code or Azure Data Studio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnstzw", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnstzw/experience_with_github_copilot_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnstzw/experience_with_github_copilot_for_sql/", "subreddit_subscribers": 171875, "created_utc": 1711409739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dqm6u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "42.parquet \u2013 A Zip Bomb for the Big Data Age", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo81xh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/54pHuKgLKELSMqBOoDh9A6a-NlBNVHNc9rZ_W7SH9gA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711460104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "duckdb.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://duckdb.org/2024/03/26/42-parquet-a-zip-bomb-for-the-big-data-age.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?auto=webp&amp;s=a3ce4d9713e9b21d12f203bb1557511dedc29060", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b86b56641acbba7143cf836e76a6ed127d0cdc7c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f7911437d91a92bdb5eaa0846b1481429176330", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=18a3e761649359ef6f006465b214153f3ee22b64", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d6d7436d826b7e957b7a49ae82dbd0475e0b149", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a85eb7536b7a9cb175fd04a6e566a36738718f7f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73e36fb4e5a8b625ec054bc33b25149637525dd2", "width": 1080, "height": 567}], "variants": {}, "id": "jWyiaF4Jb7ULQyU8SCl75THeEbJM9dbSQ9YXdauXufk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bo81xh", "is_robot_indexable": true, "report_reasons": null, "author": "commandlineluser", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo81xh/42parquet_a_zip_bomb_for_the_big_data_age/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://duckdb.org/2024/03/26/42-parquet-a-zip-bomb-for-the-big-data-age.html", "subreddit_subscribers": 171875, "created_utc": 1711460104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, moving from basically a completely SQL based analytics engineer type role to a technical big data engineer role utilising Scala and Spark.\n\nThere\u2019s a lot more experienced engineers here than me so from your own personal experiences or that of your colleagues:\n\n- What will be the biggest shock/change?\n- How long did it take to feel comfortable in the new role?\n- What did/could you have done that made the transition smoother? \n\n", "author_fullname": "t2_2o4st1yo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics to Big Data Engineer Tips", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnt8o9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711410774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, moving from basically a completely SQL based analytics engineer type role to a technical big data engineer role utilising Scala and Spark.&lt;/p&gt;\n\n&lt;p&gt;There\u2019s a lot more experienced engineers here than me so from your own personal experiences or that of your colleagues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What will be the biggest shock/change?&lt;/li&gt;\n&lt;li&gt;How long did it take to feel comfortable in the new role?&lt;/li&gt;\n&lt;li&gt;What did/could you have done that made the transition smoother? &lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnt8o9", "is_robot_indexable": true, "report_reasons": null, "author": "el527", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnt8o9/analytics_to_big_data_engineer_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnt8o9/analytics_to_big_data_engineer_tips/", "subreddit_subscribers": 171875, "created_utc": 1711410774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. \n\nOne of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. \n\nThe prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don't want to move anything to a new storage. So I'm a little worried if we are biting more than we can chew, but I don't want to lose a business that is already at the doorsteps. \n\nWhat are some implementation techniques you guys used during virtualization to address speed and performance issues?", "author_fullname": "t2_a1jxqx9hs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Potential Issues with Data Virtualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnis0n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711386134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. &lt;/p&gt;\n\n&lt;p&gt;One of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. &lt;/p&gt;\n\n&lt;p&gt;The prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don&amp;#39;t want to move anything to a new storage. So I&amp;#39;m a little worried if we are biting more than we can chew, but I don&amp;#39;t want to lose a business that is already at the doorsteps. &lt;/p&gt;\n\n&lt;p&gt;What are some implementation techniques you guys used during virtualization to address speed and performance issues?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnis0n", "is_robot_indexable": true, "report_reasons": null, "author": "turboline-ai", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnis0n/potential_issues_with_data_virtualization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnis0n/potential_issues_with_data_virtualization/", "subreddit_subscribers": 171875, "created_utc": 1711386134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apparently, the decision to go for superset is final.\n\nthe company is around 200.\n\ndata size around 10s of millions.\n\nSo far i've noted the below -\n\n**Managed**\n\n1. [Preset.io](http://preset.io/) -\n\n* from founders of superset\n* 20$ per User (14D free trial)\n* additional benefits: RBAC, Slack reporting\n\n1. AWS based [alternative](https://aws.amazon.com/marketplace/pp/prodview-c3evrh2ho3fn4#pdp-pricing)\n\n**Self**\n\n1. AWS fargate based solution - \ud83d\udcf7[Apache Superset on AWS\u2014Partner Solution](https://aws.amazon.com/solutions/implementations/apache-superset/)\n\n* scalable\n* cost friendliness TBD - [https://github.com/aws-ia/cfn-ps-apache-superset/issues/14](https://github.com/aws-ia/cfn-ps-apache-superset/issues/14)\n\n1. Self-managed deployment on EC2 instances\n\nRealistically the cost efficiency is what would be latched on to even if the dashboards aren't instantly loaded.\n\nWould the deployment on EC2 \"win\" then?\n\nThe fargate based solution appears to cost 1k$ pm [just for setup](https://github.com/aws-ia/cfn-ps-apache-superset/issues/14) so i'm skeptical on how it can explode when viz work starts.\n\nAre there any other solutions missed? Did anyone deploy via ec2? any helpful guides/tips?\n\nTIA.\n\nRegards", "author_fullname": "t2_v5qxw89ws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting superset for redshift viz", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo3ryk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711445929.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apparently, the decision to go for superset is final.&lt;/p&gt;\n\n&lt;p&gt;the company is around 200.&lt;/p&gt;\n\n&lt;p&gt;data size around 10s of millions.&lt;/p&gt;\n\n&lt;p&gt;So far i&amp;#39;ve noted the below -&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Managed&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"http://preset.io/\"&gt;Preset.io&lt;/a&gt; -&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;from founders of superset&lt;/li&gt;\n&lt;li&gt;20$ per User (14D free trial)&lt;/li&gt;\n&lt;li&gt;additional benefits: RBAC, Slack reporting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;AWS based &lt;a href=\"https://aws.amazon.com/marketplace/pp/prodview-c3evrh2ho3fn4#pdp-pricing\"&gt;alternative&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Self&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;AWS fargate based solution - \ud83d\udcf7&lt;a href=\"https://aws.amazon.com/solutions/implementations/apache-superset/\"&gt;Apache Superset on AWS\u2014Partner Solution&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;scalable&lt;/li&gt;\n&lt;li&gt;cost friendliness TBD - &lt;a href=\"https://github.com/aws-ia/cfn-ps-apache-superset/issues/14\"&gt;https://github.com/aws-ia/cfn-ps-apache-superset/issues/14&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Self-managed deployment on EC2 instances&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Realistically the cost efficiency is what would be latched on to even if the dashboards aren&amp;#39;t instantly loaded.&lt;/p&gt;\n\n&lt;p&gt;Would the deployment on EC2 &amp;quot;win&amp;quot; then?&lt;/p&gt;\n\n&lt;p&gt;The fargate based solution appears to cost 1k$ pm &lt;a href=\"https://github.com/aws-ia/cfn-ps-apache-superset/issues/14\"&gt;just for setup&lt;/a&gt; so i&amp;#39;m skeptical on how it can explode when viz work starts.&lt;/p&gt;\n\n&lt;p&gt;Are there any other solutions missed? Did anyone deploy via ec2? any helpful guides/tips?&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n\n&lt;p&gt;Regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?auto=webp&amp;s=6f144a743f642dbb0357469ecaf0480fc2ff97a3", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5342c40a4000b37f44701465aca07efecd6c4ed3", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f749eacffd6ff6649fe936bac84b95926b602183", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c5410b1771ce6abd360b4d964bc3affac537919", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=434343bf29189b3ab61f86510f92847920e9d0d2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9f65aa9a38e0e88e52ace516b9b9042b65832ff", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e124d5e1fd582bd2073aa42a4e8b7f04200f294", "width": 1080, "height": 607}], "variants": {}, "id": "__YqDw1NGDgPEwaBIWRMCVKQmM6RcFOpg1dAbXa6-Ns"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo3ryk", "is_robot_indexable": true, "report_reasons": null, "author": "what-you-need-is-you", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo3ryk/hosting_superset_for_redshift_viz/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo3ryk/hosting_superset_for_redshift_viz/", "subreddit_subscribers": 171875, "created_utc": 1711445929.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, so i\u00b4m facing some troubleshooting when i make a dimensional model. The data that i collect have some geographic fields lat-lon that is the exact point of a listing property, that makes that if i convert to a dim table with exact adress have similar size of my fact table eg my fact table has 172k rows and the dim table for locations have 150k rows (because one apartment have the same location as others)\n\nIt is OK if i treat as a degenerated dimension and put into my fact table that is a factless table?\n\nIf i do that my dim table has only the city and  neighborhood granularity and have around 10k rows\n\n&amp;#x200B;\n\nThanks for your time", "author_fullname": "t2_wgg22vwi4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Geographic info is stored in FACT or DIM table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnunsa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, so i\u00b4m facing some troubleshooting when i make a dimensional model. The data that i collect have some geographic fields lat-lon that is the exact point of a listing property, that makes that if i convert to a dim table with exact adress have similar size of my fact table eg my fact table has 172k rows and the dim table for locations have 150k rows (because one apartment have the same location as others)&lt;/p&gt;\n\n&lt;p&gt;It is OK if i treat as a degenerated dimension and put into my fact table that is a factless table?&lt;/p&gt;\n\n&lt;p&gt;If i do that my dim table has only the city and  neighborhood granularity and have around 10k rows&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for your time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnunsa", "is_robot_indexable": true, "report_reasons": null, "author": "fr-profile1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnunsa/geographic_info_is_stored_in_fact_or_dim_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnunsa/geographic_info_is_stored_in_fact_or_dim_table/", "subreddit_subscribers": 171875, "created_utc": 1711414443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you call the curated dataset that you use to validate or unit test your transformation?\n\nExample, say you have a manual process that you are automating. The users have their way of extracting the raw data, and doing their thing in Excel with procV's galore, or something like that and you are automating all the business rules and making it run daily. Now, you're going to validate if your pipeline recreates exactly what the users used to do in Excel.\n\nHow do you call the dataset that you have to mirror? baseline, ground truth, gold standard, template?", "author_fullname": "t2_rpj2htsu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ground Truth vs Gold Standard vs Baseline vs ???", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnm4w0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711394092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you call the curated dataset that you use to validate or unit test your transformation?&lt;/p&gt;\n\n&lt;p&gt;Example, say you have a manual process that you are automating. The users have their way of extracting the raw data, and doing their thing in Excel with procV&amp;#39;s galore, or something like that and you are automating all the business rules and making it run daily. Now, you&amp;#39;re going to validate if your pipeline recreates exactly what the users used to do in Excel.&lt;/p&gt;\n\n&lt;p&gt;How do you call the dataset that you have to mirror? baseline, ground truth, gold standard, template?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnm4w0", "is_robot_indexable": true, "report_reasons": null, "author": "taciom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnm4w0/ground_truth_vs_gold_standard_vs_baseline_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnm4w0/ground_truth_vs_gold_standard_vs_baseline_vs/", "subreddit_subscribers": 171875, "created_utc": 1711394092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working with a transactional database that authors records that change retrospectively.\nEG a supplier may have an attribute that changes each day (as an example).\nThis means a record with the same supplier ID is changing each day, but I dont want this duplicated in my final suppliers table.\nIs there any way to approach this without doing a window function and hence reading/shuffling my entire dataset each time? I'm finding compute limits on my dataset (approx 30gb) because i am deduping and then partitioning by date (resulting in a massivle shuffle) and disk spillage.", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing Duplicates with PySpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo5a7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711451570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working with a transactional database that authors records that change retrospectively.\nEG a supplier may have an attribute that changes each day (as an example).\nThis means a record with the same supplier ID is changing each day, but I dont want this duplicated in my final suppliers table.\nIs there any way to approach this without doing a window function and hence reading/shuffling my entire dataset each time? I&amp;#39;m finding compute limits on my dataset (approx 30gb) because i am deduping and then partitioning by date (resulting in a massivle shuffle) and disk spillage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo5a7j", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo5a7j/processing_duplicates_with_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo5a7j/processing_duplicates_with_pyspark/", "subreddit_subscribers": 171875, "created_utc": 1711451570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. So a quick back story- I am a self taught data engineer currently working at a small company as a junior data engineer. I\u2019ve been here nearly a year now and I don\u2019t think I\u2019ve developed as quickly as I\u2019d hoped. I like where I work and they\u2019ve been very patient with me but I don\u2019t think it\u2019s the right place for me to progress. I am proficient in SQL, Excel, Power Bi and a little bit of Python (still learning). \n\nJust wanted to see how you guys have navigated your career journey from where you started and the skills you had back then to where you are and the skills you have now.\n\nWhat do I need to do to become a senior or better my self? Skills ? Experience ? \n\nThanks in advance.", "author_fullname": "t2_6l52lijc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnskfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711409103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. So a quick back story- I am a self taught data engineer currently working at a small company as a junior data engineer. I\u2019ve been here nearly a year now and I don\u2019t think I\u2019ve developed as quickly as I\u2019d hoped. I like where I work and they\u2019ve been very patient with me but I don\u2019t think it\u2019s the right place for me to progress. I am proficient in SQL, Excel, Power Bi and a little bit of Python (still learning). &lt;/p&gt;\n\n&lt;p&gt;Just wanted to see how you guys have navigated your career journey from where you started and the skills you had back then to where you are and the skills you have now.&lt;/p&gt;\n\n&lt;p&gt;What do I need to do to become a senior or better my self? Skills ? Experience ? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnskfq", "is_robot_indexable": true, "report_reasons": null, "author": "Tookie2x", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnskfq/career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnskfq/career_advice/", "subreddit_subscribers": 171875, "created_utc": 1711409103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey All,\n\nI have launched my own channel where I'll be talking about DE related stuff, if you guys found the previous blogs to be useful please do consider visiting and subbing to my channel. \n\nThis is the link to it: [https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ](https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ)", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Continuation to structured blogs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnpwgz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711402774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;I have launched my own channel where I&amp;#39;ll be talking about DE related stuff, if you guys found the previous blogs to be useful please do consider visiting and subbing to my channel. &lt;/p&gt;\n\n&lt;p&gt;This is the link to it: &lt;a href=\"https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ\"&gt;https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?auto=webp&amp;s=a4be3076916d800bb3a8eb096c5789d1eaa97d1c", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66404e8501d319d0e15d2f3a1659a2786a8f9643", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38673cf42a869d7ded60f2972a3855ad6aecdd79", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e3e7065e11363a6d0505cfc4be54b168c8dce12", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f9b6bf60319f1d3533cc3da27c216d9a465de96", "width": 640, "height": 640}], "variants": {}, "id": "Kcq4v7vShDHigIbd-Y5Fpp501_MvubPvf7chWti_PUg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bnpwgz", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnpwgz/continuation_to_structured_blogs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnpwgz/continuation_to_structured_blogs/", "subreddit_subscribers": 171875, "created_utc": 1711402774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Parquet is better for read operations, while avro is more efficient for write operations. Do you ever  decide to save some data as avro because you think it will be written to more often in your data lake or do you prefer to have all files in one file format like parquet? What are your experiences here?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet for reads, avro for writes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnlwjj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711393542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Parquet is better for read operations, while avro is more efficient for write operations. Do you ever  decide to save some data as avro because you think it will be written to more often in your data lake or do you prefer to have all files in one file format like parquet? What are your experiences here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnlwjj", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnlwjj/parquet_for_reads_avro_for_writes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnlwjj/parquet_for_reads_avro_for_writes/", "subreddit_subscribers": 171875, "created_utc": 1711393542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Too specific of a question but would like to know the nuances", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the difference between exadata database vs exadata warehouse? Also, can we migrate both of them to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnljan", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Too specific of a question but would like to know the nuances&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnljan", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnljan/what_is_the_difference_between_exadata_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnljan/what_is_the_difference_between_exadata_database/", "subreddit_subscribers": 171875, "created_utc": 1711392678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the prime use case behind migrating oltp/ods on prem systems to databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnl9gw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnl9gw", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnl9gw/what_is_the_prime_use_case_behind_migrating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnl9gw/what_is_the_prime_use_case_behind_migrating/", "subreddit_subscribers": 171875, "created_utc": 1711392036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nAs the hype around AI and LLM continues to grow, I'm genuinely curious about how you are currently utilizing LLM and whether it truly helps in enabling self-service BI.\n\nI want to hear from you about your experience / opinion with LLM. Are you still planning to implement it or have you already implemented it? Does it meet your expectations or have you encountered any challenges?\n\nThank you.\n\n[View Poll](https://www.reddit.com/poll/1bo9na7)", "author_fullname": "t2_133sq5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you leverage LLM in your Self-service BI stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bo9na7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711464286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;As the hype around AI and LLM continues to grow, I&amp;#39;m genuinely curious about how you are currently utilizing LLM and whether it truly helps in enabling self-service BI.&lt;/p&gt;\n\n&lt;p&gt;I want to hear from you about your experience / opinion with LLM. Are you still planning to implement it or have you already implemented it? Does it meet your expectations or have you encountered any challenges?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1bo9na7\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bo9na7", "is_robot_indexable": true, "report_reasons": null, "author": "anhthong00", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1712069086279, "options": [{"text": "Haven't implemented, want to give it a try", "id": "27541963"}, {"text": "Haven't implemented, don't believe it works", "id": "27541964"}, {"text": "Have implemented, doesn't work as expected", "id": "27541965"}, {"text": "Have implemented, it works sometimes", "id": "27541966"}, {"text": "Have implemented, and it's been a game-changer", "id": "27541967"}, {"text": "Other (please specify)", "id": "27541968"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 7, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo9na7/how_do_you_leverage_llm_in_your_selfservice_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1bo9na7/how_do_you_leverage_llm_in_your_selfservice_bi/", "subreddit_subscribers": 171875, "created_utc": 1711464286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working with snowflake, dbt, dagster stack. Didn't find such a tag in the data engineering wiki. Could you suggest something?", "author_fullname": "t2_v3orytfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could you recommend learning materials about Data Cleaning/Cleansing best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bo9ghz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711463793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with snowflake, dbt, dagster stack. Didn&amp;#39;t find such a tag in the data engineering wiki. Could you suggest something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo9ghz", "is_robot_indexable": true, "report_reasons": null, "author": "awkward_period", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo9ghz/could_you_recommend_learning_materials_about_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo9ghz/could_you_recommend_learning_materials_about_data/", "subreddit_subscribers": 171875, "created_utc": 1711463793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm trying to benchmark the TPCx-BB on 30TB scale factor. Any recommended type of nodes and amount of nodes on Google Cloud? I have no experience in this cloud provider.", "author_fullname": "t2_v22qxq2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks TPCx-BB 30TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bo85gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711460382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to benchmark the TPCx-BB on 30TB scale factor. Any recommended type of nodes and amount of nodes on Google Cloud? I have no experience in this cloud provider.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo85gj", "is_robot_indexable": true, "report_reasons": null, "author": "All-is-data3891", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo85gj/databricks_tpcxbb_30tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo85gj/databricks_tpcxbb_30tb/", "subreddit_subscribers": 171875, "created_utc": 1711460382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Currently on the market there are integration tools with different capabilities. If we deep dive more into it is that they are tools suitable for application integration ( APIs, event-driven, real time) and data integration (batch, big amount of data). Now, for example, for application integration the suggested tools are Mulesoft or boomi, while for ETL is in general informatica or talend. \n\nCan someone educate me why the design architecture of tool like boomi is not suitable for ETL but informatica is? I guess there must be something, just was not able to find anything concrete.\n", "author_fullname": "t2_3otium74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Application vs Data integration tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo5yzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711453920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently on the market there are integration tools with different capabilities. If we deep dive more into it is that they are tools suitable for application integration ( APIs, event-driven, real time) and data integration (batch, big amount of data). Now, for example, for application integration the suggested tools are Mulesoft or boomi, while for ETL is in general informatica or talend. &lt;/p&gt;\n\n&lt;p&gt;Can someone educate me why the design architecture of tool like boomi is not suitable for ETL but informatica is? I guess there must be something, just was not able to find anything concrete.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bo5yzw", "is_robot_indexable": true, "report_reasons": null, "author": "Legal_Explanation_59", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo5yzw/application_vs_data_integration_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo5yzw/application_vs_data_integration_tools/", "subreddit_subscribers": 171875, "created_utc": 1711453920.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}