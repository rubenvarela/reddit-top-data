{"kind": "Listing", "data": {"after": "t3_1boc1tt", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys after finishing a contract in a company I\u2019m searching for another opportunity in Europe based remotely and what I see in the job descriptions in LinkedIn are 27 technologies needed for the position and you have to be an expert, even not a senior position (I have 3.5 years of experience), what is happening here?\n\nYou need to know: python, pyspark, scala , JavaScript, java, azure, aws, gcp (and all the the technologies), databricks, airflow, Kafka, sql, no sql, data lakes, dwh, oracle, ETL\u2019s, terraform, Jenkins, kubernetes\u2026 and more \n\nOfc all of this fluent and proficient, lol\n\n\n\nAnd not even senior positions\u2026 what would you recommend, guys?\nI\u2019ve been working with azure data factory/synapse/Databricks with python/pyspark and sql, doing etl/elt pipelines from on-premise ddbb or simple excels or cloud ddbb, or api\u2019s.\n\n", "author_fullname": "t2_6bblasam", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding a new job, ridiculous ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo4nne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 99, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 99, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711449324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys after finishing a contract in a company I\u2019m searching for another opportunity in Europe based remotely and what I see in the job descriptions in LinkedIn are 27 technologies needed for the position and you have to be an expert, even not a senior position (I have 3.5 years of experience), what is happening here?&lt;/p&gt;\n\n&lt;p&gt;You need to know: python, pyspark, scala , JavaScript, java, azure, aws, gcp (and all the the technologies), databricks, airflow, Kafka, sql, no sql, data lakes, dwh, oracle, ETL\u2019s, terraform, Jenkins, kubernetes\u2026 and more &lt;/p&gt;\n\n&lt;p&gt;Ofc all of this fluent and proficient, lol&lt;/p&gt;\n\n&lt;p&gt;And not even senior positions\u2026 what would you recommend, guys?\nI\u2019ve been working with azure data factory/synapse/Databricks with python/pyspark and sql, doing etl/elt pipelines from on-premise ddbb or simple excels or cloud ddbb, or api\u2019s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bo4nne", "is_robot_indexable": true, "report_reasons": null, "author": "Irachar", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo4nne/finding_a_new_job_ridiculous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo4nne/finding_a_new_job_ridiculous/", "subreddit_subscribers": 171970, "created_utc": 1711449324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I probably just need to vent. I have taken on so many analytic/ds type tasks (beginner to intermediate ml/stats type stuff) at this point that Im essentially the lead analyst of my small team... as well as the de, swe, infra guy etc (de is official role - those tasks involve managing our airflow/infra and ETL/db modeling, probably on a smaller scale than most des).\n\nLet me get this out of the way: every job is like this to a degree, at the end of the day you do what youre told and get paid. And some of it can even be enjoyable. \n\nBut at what point do you just nope out of there? At this point my team depends on me for everything - engineering, analysis, and any kind of soft skills task (presentations to stakeholders). Its exhausting and starting to feel ridiculous. Other analysts on the team were hired primarily for sql queries and dashboards, as we were told our team would be streamlining to mostly do reporting and the etl that drives it. Of course, that never happened. This leaves me with \"everything else\". I dont think mgmt realizes how screwed they could be re biz continuity, because they dont really understand any of the work involved. Not to brag that Im irreplacable, Im sure they could find some other sap to do the same.\n\nAnyone else been or currently in this kinda situation? What did you do?", "author_fullname": "t2_v3k0dc9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "where do you draw the line at the nebulous job title?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnqqvl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711404953.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711404685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I probably just need to vent. I have taken on so many analytic/ds type tasks (beginner to intermediate ml/stats type stuff) at this point that Im essentially the lead analyst of my small team... as well as the de, swe, infra guy etc (de is official role - those tasks involve managing our airflow/infra and ETL/db modeling, probably on a smaller scale than most des).&lt;/p&gt;\n\n&lt;p&gt;Let me get this out of the way: every job is like this to a degree, at the end of the day you do what youre told and get paid. And some of it can even be enjoyable. &lt;/p&gt;\n\n&lt;p&gt;But at what point do you just nope out of there? At this point my team depends on me for everything - engineering, analysis, and any kind of soft skills task (presentations to stakeholders). Its exhausting and starting to feel ridiculous. Other analysts on the team were hired primarily for sql queries and dashboards, as we were told our team would be streamlining to mostly do reporting and the etl that drives it. Of course, that never happened. This leaves me with &amp;quot;everything else&amp;quot;. I dont think mgmt realizes how screwed they could be re biz continuity, because they dont really understand any of the work involved. Not to brag that Im irreplacable, Im sure they could find some other sap to do the same.&lt;/p&gt;\n\n&lt;p&gt;Anyone else been or currently in this kinda situation? What did you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnqqvl", "is_robot_indexable": true, "report_reasons": null, "author": "zazzersmel", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnqqvl/where_do_you_draw_the_line_at_the_nebulous_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnqqvl/where_do_you_draw_the_line_at_the_nebulous_job/", "subreddit_subscribers": 171970, "created_utc": 1711404685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dqm6u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "42.parquet \u2013 A Zip Bomb for the Big Data Age", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo81xh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/54pHuKgLKELSMqBOoDh9A6a-NlBNVHNc9rZ_W7SH9gA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711460104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "duckdb.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://duckdb.org/2024/03/26/42-parquet-a-zip-bomb-for-the-big-data-age.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?auto=webp&amp;s=a3ce4d9713e9b21d12f203bb1557511dedc29060", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b86b56641acbba7143cf836e76a6ed127d0cdc7c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f7911437d91a92bdb5eaa0846b1481429176330", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=18a3e761649359ef6f006465b214153f3ee22b64", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d6d7436d826b7e957b7a49ae82dbd0475e0b149", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a85eb7536b7a9cb175fd04a6e566a36738718f7f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73e36fb4e5a8b625ec054bc33b25149637525dd2", "width": 1080, "height": 567}], "variants": {}, "id": "jWyiaF4Jb7ULQyU8SCl75THeEbJM9dbSQ9YXdauXufk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bo81xh", "is_robot_indexable": true, "report_reasons": null, "author": "commandlineluser", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo81xh/42parquet_a_zip_bomb_for_the_big_data_age/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://duckdb.org/2024/03/26/42-parquet-a-zip-bomb-for-the-big-data-age.html", "subreddit_subscribers": 171970, "created_utc": 1711460104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm mostly a python guy but at one point I wrote c#. Lately I've been enjoying scala. Seems like a nice abstraction over java and has a much more declarative structure than python. \n\nMy job is mostly writing backend APIs and automation in fastapi. I've done a few wildly basic akka endpoints over the years and really like that syntax. \n\nThing is.. scala is pretty much not used by our industry. At prior shops they were either straight java / python or c# if they were Microsoft shop. I have yet to find a shop that embraced scala. I love the live execution debugger, such a nice feature. Feels like pdb. \n\nIt's fair to say that I personally am not benifiting from any speed differential between python and scala Im preferential because I enjoy the syntax. My python is purely a wrapper over compiled binaries. \n\nWhy don't YOU personally use scala?", "author_fullname": "t2_706trkkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on scala? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnupia", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m mostly a python guy but at one point I wrote c#. Lately I&amp;#39;ve been enjoying scala. Seems like a nice abstraction over java and has a much more declarative structure than python. &lt;/p&gt;\n\n&lt;p&gt;My job is mostly writing backend APIs and automation in fastapi. I&amp;#39;ve done a few wildly basic akka endpoints over the years and really like that syntax. &lt;/p&gt;\n\n&lt;p&gt;Thing is.. scala is pretty much not used by our industry. At prior shops they were either straight java / python or c# if they were Microsoft shop. I have yet to find a shop that embraced scala. I love the live execution debugger, such a nice feature. Feels like pdb. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fair to say that I personally am not benifiting from any speed differential between python and scala Im preferential because I enjoy the syntax. My python is purely a wrapper over compiled binaries. &lt;/p&gt;\n\n&lt;p&gt;Why don&amp;#39;t YOU personally use scala?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnupia", "is_robot_indexable": true, "report_reasons": null, "author": "Foodwithfloyd", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnupia/thoughts_on_scala/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnupia/thoughts_on_scala/", "subreddit_subscribers": 171970, "created_utc": 1711414574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "?", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle NULLs when cleaning data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bod2zr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711472850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bod2zr", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bod2zr/how_do_you_handle_nulls_when_cleaning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bod2zr/how_do_you_handle_nulls_when_cleaning_data/", "subreddit_subscribers": 171970, "created_utc": 1711472850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to get some career advice. I've been working as a Data Engineer for almost 5 years, but a huge part of this job was just Python development (APIs, libs, microservices...) and another part was building pipelines, using spark, Kafka, SQL, NoSQL, data bricks,  Airflow and others. Currently, I am working on another project in Hadoop on-prem where I'm building spark streaming apps - but I don't like Hadoop and the on-prem ecosystem, and I think also that it's legacy stack.   \nBecause of these above, I have very poor, almost 0 experience and knowledge in Cloud.\n\nThat's why I wanted to ask you for advice. I got an offer for a Data Engineer on AWS - the stack there is very simple like s3, kinesis, glue, Athena, Ecs, Rds, and Redshift - and the team has only one Data Engineer who basically is also on his road with AWS, but already passed some certificates.  \nIn this role there is a very small part of coding - like 10-15% of the job. But they are okay, that I don't know AWS and they will give me time and space to learn it and to pass certificates.  \nAnd I wonder if I should take it, on one side I will learn AWS, probably a couple of DE-related services, and I will pass the exam. On the other side, I won't code too much, and probably everything will be pretty low quality, as there is no collaboration with DevOps, SWEs, and other DEs into good swe practices (testing, high quality of code, design etc).   \n\n\nI'm having a bit of a headache with this. Do you think this is a good idea and a step forward in your career - I think it's a position for at least a year. But will it be a step backward - especially since I have been working more in programming like data engineering (writing code 80% of the time) for the last 5 years?  \n\n\n  \n\n\n  \n\n\n&amp;#x200B;", "author_fullname": "t2_2llofc3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice regarding new offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo1qwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711437436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to get some career advice. I&amp;#39;ve been working as a Data Engineer for almost 5 years, but a huge part of this job was just Python development (APIs, libs, microservices...) and another part was building pipelines, using spark, Kafka, SQL, NoSQL, data bricks,  Airflow and others. Currently, I am working on another project in Hadoop on-prem where I&amp;#39;m building spark streaming apps - but I don&amp;#39;t like Hadoop and the on-prem ecosystem, and I think also that it&amp;#39;s legacy stack.&lt;br/&gt;\nBecause of these above, I have very poor, almost 0 experience and knowledge in Cloud.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why I wanted to ask you for advice. I got an offer for a Data Engineer on AWS - the stack there is very simple like s3, kinesis, glue, Athena, Ecs, Rds, and Redshift - and the team has only one Data Engineer who basically is also on his road with AWS, but already passed some certificates.&lt;br/&gt;\nIn this role there is a very small part of coding - like 10-15% of the job. But they are okay, that I don&amp;#39;t know AWS and they will give me time and space to learn it and to pass certificates.&lt;br/&gt;\nAnd I wonder if I should take it, on one side I will learn AWS, probably a couple of DE-related services, and I will pass the exam. On the other side, I won&amp;#39;t code too much, and probably everything will be pretty low quality, as there is no collaboration with DevOps, SWEs, and other DEs into good swe practices (testing, high quality of code, design etc).   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a bit of a headache with this. Do you think this is a good idea and a step forward in your career - I think it&amp;#39;s a position for at least a year. But will it be a step backward - especially since I have been working more in programming like data engineering (writing code 80% of the time) for the last 5 years?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bo1qwr", "is_robot_indexable": true, "report_reasons": null, "author": "masek94", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo1qwr/career_advice_regarding_new_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo1qwr/career_advice_regarding_new_offer/", "subreddit_subscribers": 171970, "created_utc": 1711437436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everybody,\n\nI have been in the field for some time and still unsure about optimal solution for analytical database of 100GiB-10TiB range. \n\nIf you have less, you just go with PostgreSQL or some other conventional database with reasonable level of support of table scans +dbt. If you have more, you go with Spark/Athena. \n\nBut that range in the middle\u2026 You cannot put it into a reasonably priced db server. A proper host would cost me around 10K/month. That\u2019s roughly the same amount I pay for 50 servers Spark cluster. \n\nBut that amount of data does not need massive parallelism for ETL processing and associated Spark complexities. I probably need 10 process running in parallel to convert json to parquet (oversimplification here). \n\nWhat technologies/products would you use for this sort of ETL/reporting tasks? \n\nThank you\n", "author_fullname": "t2_gi05k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good solution for 100GiB-10TiB analytical DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bobzzj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711470216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everybody,&lt;/p&gt;\n\n&lt;p&gt;I have been in the field for some time and still unsure about optimal solution for analytical database of 100GiB-10TiB range. &lt;/p&gt;\n\n&lt;p&gt;If you have less, you just go with PostgreSQL or some other conventional database with reasonable level of support of table scans +dbt. If you have more, you go with Spark/Athena. &lt;/p&gt;\n\n&lt;p&gt;But that range in the middle\u2026 You cannot put it into a reasonably priced db server. A proper host would cost me around 10K/month. That\u2019s roughly the same amount I pay for 50 servers Spark cluster. &lt;/p&gt;\n\n&lt;p&gt;But that amount of data does not need massive parallelism for ETL processing and associated Spark complexities. I probably need 10 process running in parallel to convert json to parquet (oversimplification here). &lt;/p&gt;\n\n&lt;p&gt;What technologies/products would you use for this sort of ETL/reporting tasks? &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bobzzj", "is_robot_indexable": true, "report_reasons": null, "author": "aih1013", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bobzzj/good_solution_for_100gib10tib_analytical_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bobzzj/good_solution_for_100gib10tib_analytical_db/", "subreddit_subscribers": 171970, "created_utc": 1711470216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apparently, the decision to go for superset is final.\n\nthe company is around 200.\n\ndata size around 10s of millions.\n\nSo far i've noted the below -\n\n**Managed**\n\n1. [Preset.io](http://preset.io/) -\n\n* from founders of superset\n* 20$ per User (14D free trial)\n* additional benefits: RBAC, Slack reporting\n\n1. AWS based [alternative](https://aws.amazon.com/marketplace/pp/prodview-c3evrh2ho3fn4#pdp-pricing)\n\n**Self**\n\n1. AWS fargate based solution - \ud83d\udcf7[Apache Superset on AWS\u2014Partner Solution](https://aws.amazon.com/solutions/implementations/apache-superset/)\n\n* scalable\n* cost friendliness TBD - [https://github.com/aws-ia/cfn-ps-apache-superset/issues/14](https://github.com/aws-ia/cfn-ps-apache-superset/issues/14)\n\n1. Self-managed deployment on EC2 instances\n\nRealistically the cost efficiency is what would be latched on to even if the dashboards aren't instantly loaded.\n\nWould the deployment on EC2 \"win\" then?\n\nThe fargate based solution appears to cost 1k$ pm [just for setup](https://github.com/aws-ia/cfn-ps-apache-superset/issues/14) so i'm skeptical on how it can explode when viz work starts.\n\nAre there any other solutions missed? Did anyone deploy via ec2? any helpful guides/tips?\n\nTIA.\n\nRegards", "author_fullname": "t2_v5qxw89ws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting superset for redshift viz", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo3ryk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711445929.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apparently, the decision to go for superset is final.&lt;/p&gt;\n\n&lt;p&gt;the company is around 200.&lt;/p&gt;\n\n&lt;p&gt;data size around 10s of millions.&lt;/p&gt;\n\n&lt;p&gt;So far i&amp;#39;ve noted the below -&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Managed&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"http://preset.io/\"&gt;Preset.io&lt;/a&gt; -&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;from founders of superset&lt;/li&gt;\n&lt;li&gt;20$ per User (14D free trial)&lt;/li&gt;\n&lt;li&gt;additional benefits: RBAC, Slack reporting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;AWS based &lt;a href=\"https://aws.amazon.com/marketplace/pp/prodview-c3evrh2ho3fn4#pdp-pricing\"&gt;alternative&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Self&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;AWS fargate based solution - \ud83d\udcf7&lt;a href=\"https://aws.amazon.com/solutions/implementations/apache-superset/\"&gt;Apache Superset on AWS\u2014Partner Solution&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;scalable&lt;/li&gt;\n&lt;li&gt;cost friendliness TBD - &lt;a href=\"https://github.com/aws-ia/cfn-ps-apache-superset/issues/14\"&gt;https://github.com/aws-ia/cfn-ps-apache-superset/issues/14&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Self-managed deployment on EC2 instances&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Realistically the cost efficiency is what would be latched on to even if the dashboards aren&amp;#39;t instantly loaded.&lt;/p&gt;\n\n&lt;p&gt;Would the deployment on EC2 &amp;quot;win&amp;quot; then?&lt;/p&gt;\n\n&lt;p&gt;The fargate based solution appears to cost 1k$ pm &lt;a href=\"https://github.com/aws-ia/cfn-ps-apache-superset/issues/14\"&gt;just for setup&lt;/a&gt; so i&amp;#39;m skeptical on how it can explode when viz work starts.&lt;/p&gt;\n\n&lt;p&gt;Are there any other solutions missed? Did anyone deploy via ec2? any helpful guides/tips?&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n\n&lt;p&gt;Regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?auto=webp&amp;s=6f144a743f642dbb0357469ecaf0480fc2ff97a3", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5342c40a4000b37f44701465aca07efecd6c4ed3", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f749eacffd6ff6649fe936bac84b95926b602183", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c5410b1771ce6abd360b4d964bc3affac537919", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=434343bf29189b3ab61f86510f92847920e9d0d2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9f65aa9a38e0e88e52ace516b9b9042b65832ff", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e124d5e1fd582bd2073aa42a4e8b7f04200f294", "width": 1080, "height": 607}], "variants": {}, "id": "__YqDw1NGDgPEwaBIWRMCVKQmM6RcFOpg1dAbXa6-Ns"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo3ryk", "is_robot_indexable": true, "report_reasons": null, "author": "what-you-need-is-you", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo3ryk/hosting_superset_for_redshift_viz/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo3ryk/hosting_superset_for_redshift_viz/", "subreddit_subscribers": 171970, "created_utc": 1711445929.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an etl pipeline that ingests IOT data hourly. The storage account, bronze, and silver tables are hive partitioned by the year month day hour of when my system received the data. This works well for some use cases.\n\n I have another use case where having the same data but partitioned by observation date would be way more efficient. Observation date is when my device made the reading, and the partition is based on when my system received the data. Sometimes these are different depending on late data. \n\nCurrently I insert all the data into both tables, one partitioned by y/m/d/h and the other partitioned by observation date. Is there  more efficient way to accomplish this? I can only think of switching to structured streaming with CDC all the way through my pipeline instead of some spots batch based on y/m/d/h, and some spots already structured streaming. Then I could scrap the y/m/d/h table and only rely on date partitioned everywhere. ", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Different partitions of same table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnuiuv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an etl pipeline that ingests IOT data hourly. The storage account, bronze, and silver tables are hive partitioned by the year month day hour of when my system received the data. This works well for some use cases.&lt;/p&gt;\n\n&lt;p&gt;I have another use case where having the same data but partitioned by observation date would be way more efficient. Observation date is when my device made the reading, and the partition is based on when my system received the data. Sometimes these are different depending on late data. &lt;/p&gt;\n\n&lt;p&gt;Currently I insert all the data into both tables, one partitioned by y/m/d/h and the other partitioned by observation date. Is there  more efficient way to accomplish this? I can only think of switching to structured streaming with CDC all the way through my pipeline instead of some spots batch based on y/m/d/h, and some spots already structured streaming. Then I could scrap the y/m/d/h table and only rely on date partitioned everywhere. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnuiuv", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnuiuv/different_partitions_of_same_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnuiuv/different_partitions_of_same_table/", "subreddit_subscribers": 171970, "created_utc": 1711414068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've had pretty good success with Copilot with things like Python (Airflow), Bash and powershell but not much with SQL. \n\nIt hasn't done a good job learning how the different tables join or what I'm most likely to select or fields I'm returning. Has anyone used this with Visual Studio Code or Azure Data Studio?", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with Github Copilot for SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnstzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711409739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had pretty good success with Copilot with things like Python (Airflow), Bash and powershell but not much with SQL. &lt;/p&gt;\n\n&lt;p&gt;It hasn&amp;#39;t done a good job learning how the different tables join or what I&amp;#39;m most likely to select or fields I&amp;#39;m returning. Has anyone used this with Visual Studio Code or Azure Data Studio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnstzw", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnstzw/experience_with_github_copilot_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnstzw/experience_with_github_copilot_for_sql/", "subreddit_subscribers": 171970, "created_utc": 1711409739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, moving from basically a completely SQL based analytics engineer type role to a technical big data engineer role utilising Scala and Spark.\n\nThere\u2019s a lot more experienced engineers here than me so from your own personal experiences or that of your colleagues:\n\n- What will be the biggest shock/change?\n- How long did it take to feel comfortable in the new role?\n- What did/could you have done that made the transition smoother? \n\n", "author_fullname": "t2_2o4st1yo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics to Big Data Engineer Tips", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnt8o9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711410774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, moving from basically a completely SQL based analytics engineer type role to a technical big data engineer role utilising Scala and Spark.&lt;/p&gt;\n\n&lt;p&gt;There\u2019s a lot more experienced engineers here than me so from your own personal experiences or that of your colleagues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What will be the biggest shock/change?&lt;/li&gt;\n&lt;li&gt;How long did it take to feel comfortable in the new role?&lt;/li&gt;\n&lt;li&gt;What did/could you have done that made the transition smoother? &lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnt8o9", "is_robot_indexable": true, "report_reasons": null, "author": "el527", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnt8o9/analytics_to_big_data_engineer_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnt8o9/analytics_to_big_data_engineer_tips/", "subreddit_subscribers": 171970, "created_utc": 1711410774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As part of my job I have to VPN into customer databases and extract the data that we need. Every customer has a custom table structure and there are often 300-400 tables. I need to find the 5 or so tables that have the specific data I am after.\n\nIs there an easy way to figure out which tables reference other tables and what the keys are?\n\nI\u2019ve tried putting the table, column, and data type into a data frame then searching for words that might be what I am looking for, opening those tables and looking at the data but that\u2019s rather time consuming and they do a lot of weird stuff, for example one of the things I look for is a column called \u2018batch\u2019 or \u2018batch_number\u2019 that has an integer data type but a few dbs I have looked at store batch numbers as strings for some reason", "author_fullname": "t2_cvnuc9q27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Figuring out what tables are what in a database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1boj2rh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711487086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As part of my job I have to VPN into customer databases and extract the data that we need. Every customer has a custom table structure and there are often 300-400 tables. I need to find the 5 or so tables that have the specific data I am after.&lt;/p&gt;\n\n&lt;p&gt;Is there an easy way to figure out which tables reference other tables and what the keys are?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried putting the table, column, and data type into a data frame then searching for words that might be what I am looking for, opening those tables and looking at the data but that\u2019s rather time consuming and they do a lot of weird stuff, for example one of the things I look for is a column called \u2018batch\u2019 or \u2018batch_number\u2019 that has an integer data type but a few dbs I have looked at store batch numbers as strings for some reason&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1boj2rh", "is_robot_indexable": true, "report_reasons": null, "author": "big_data_mike", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boj2rh/figuring_out_what_tables_are_what_in_a_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boj2rh/figuring_out_what_tables_are_what_in_a_database/", "subreddit_subscribers": 171970, "created_utc": 1711487086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, so i\u00b4m facing some troubleshooting when i make a dimensional model. The data that i collect have some geographic fields lat-lon that is the exact point of a listing property, that makes that if i convert to a dim table with exact adress have similar size of my fact table eg my fact table has 172k rows and the dim table for locations have 150k rows (because one apartment have the same location as others)\n\nIt is OK if i treat as a degenerated dimension and put into my fact table that is a factless table?\n\nIf i do that my dim table has only the city and  neighborhood granularity and have around 10k rows\n\n&amp;#x200B;\n\nThanks for your time", "author_fullname": "t2_wgg22vwi4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Geographic info is stored in FACT or DIM table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnunsa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711414443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, so i\u00b4m facing some troubleshooting when i make a dimensional model. The data that i collect have some geographic fields lat-lon that is the exact point of a listing property, that makes that if i convert to a dim table with exact adress have similar size of my fact table eg my fact table has 172k rows and the dim table for locations have 150k rows (because one apartment have the same location as others)&lt;/p&gt;\n\n&lt;p&gt;It is OK if i treat as a degenerated dimension and put into my fact table that is a factless table?&lt;/p&gt;\n\n&lt;p&gt;If i do that my dim table has only the city and  neighborhood granularity and have around 10k rows&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for your time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnunsa", "is_robot_indexable": true, "report_reasons": null, "author": "fr-profile1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnunsa/geographic_info_is_stored_in_fact_or_dim_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnunsa/geographic_info_is_stored_in_fact_or_dim_table/", "subreddit_subscribers": 171970, "created_utc": 1711414443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wozut7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage ELT pipelines with code using Airbyte\u2019s Terraform provider", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_1bojruu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Df8VcbWW_BM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Manage ELT pipelines with code using Airbyte\u2019s Terraform provider\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"oembed": {"provider_url": "https://www.youtube.com/", "title": "Manage ELT pipelines with code using Airbyte\u2019s Terraform provider", "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Df8VcbWW_BM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Manage ELT pipelines with code using Airbyte\u2019s Terraform provider\"&gt;&lt;/iframe&gt;", "thumbnail_width": 480, "height": 200, "width": 356, "version": "1.0", "author_name": "Teradata", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Df8VcbWW_BM/hqdefault.jpg", "type": "video", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@teradata"}, "type": "youtube.com"}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Df8VcbWW_BM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Manage ELT pipelines with code using Airbyte\u2019s Terraform provider\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1bojruu", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BKfAdRE--N6kwz9ml2s0mEGBS5UeDcsqhwmCbg5t_ws.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711488699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=Df8VcbWW_BM&amp;t", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GmhVgAPei2l09KAByjepvSPY-sf2ZTZ9QO5nRLk1I1M.jpg?auto=webp&amp;s=e80deddbab565910d1b2ead814abc55081aeb434", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GmhVgAPei2l09KAByjepvSPY-sf2ZTZ9QO5nRLk1I1M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c5665fa2a63f09086d451a8eaa7bf382cc7bfb27", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GmhVgAPei2l09KAByjepvSPY-sf2ZTZ9QO5nRLk1I1M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f8609b27e7b1793f6e7111cb1a2ecb3f2f3145d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GmhVgAPei2l09KAByjepvSPY-sf2ZTZ9QO5nRLk1I1M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=863eaf9894fc8ebea413529f0d6e5c39bd140307", "width": 320, "height": 240}], "variants": {}, "id": "0c0qDs6NmT_Db-fklM9DKXUzX9NEsTwro3f7VNE9YPU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bojruu", "is_robot_indexable": true, "report_reasons": null, "author": "JanethL", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bojruu/manage_elt_pipelines_with_code_using_airbytes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=Df8VcbWW_BM&amp;t", "subreddit_subscribers": 171970, "created_utc": 1711488699.0, "num_crossposts": 0, "media": {"oembed": {"provider_url": "https://www.youtube.com/", "title": "Manage ELT pipelines with code using Airbyte\u2019s Terraform provider", "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Df8VcbWW_BM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Manage ELT pipelines with code using Airbyte\u2019s Terraform provider\"&gt;&lt;/iframe&gt;", "thumbnail_width": 480, "height": 200, "width": 356, "version": "1.0", "author_name": "Teradata", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Df8VcbWW_BM/hqdefault.jpg", "type": "video", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@teradata"}, "type": "youtube.com"}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit,\n\nI'm part of a team that's been grappling with a challenge I think many of you can relate to: the inefficiencies and security concerns of traditional ETL processes that rely heavily on cloud transit. It led us to ask, \"What if there's a better way?\"\n\nSo, we built Mycelial - an open-source platform designed from the ground up to enable direct, point-to-point ETL. This means your data moves from A to B without being forced to detour through a central cloud, eliminating unnecessary latency, reducing costs, and enhancing data security.\n\n**How Mycelial Works:**\n\nCrafted with love in 100% Rust, Mycelial leverages local daemons to execute data workflows, guaranteeing fast and secure data movement. It features a universal interface between data sources and destinations, meaning once you're connected to the Mycelial network, the possibilities for your data are endless.\n\n**Why we think it's a game changer:**\n\n* **Security**: With data breaches on the rise, keeping your data off public cloud networks reduces exposure.\n* **Efficiency**: Direct transfers mean quicker ETL processes, getting your data where it needs to be faster.\n* **Cost-effective**: Less reliance on cloud infrastructure translates to lower operational costs.\n* **Control**: You maintain complete control over your data's journey, end-to-end.\n\nWe're thrilled (and a tad nervous) to introduce Mycelial to the Reddit community. Your feedback, whether it's praise, criticism, or anything in between, is invaluable to us. We're committed to transparency and community-driven development, so we're all ears for your thoughts, concerns, and questions.\n\n**Be Part of the Beta Test:**\n\nMycelial is in beta, and we're eager for testers to push our platform to its limits. Your insights will directly influence the evolution of Mycelial, helping us refine and perfect the platform. Visit [www.mycelial.com](http://www.mycelial.com/) to sign up and start exploring the possibilities.\n\nFeel free to AMA about the technology, our development journey, or any burning questions you might have. Your input is crucial in our mission to make Mycelial the best it can be for the community.", "author_fullname": "t2_fu5zl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\ude80 Launching Mycelial: A New Way to Handle ETL Without the Cloud - Seeking Your Feedback!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bogg92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711480871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m part of a team that&amp;#39;s been grappling with a challenge I think many of you can relate to: the inefficiencies and security concerns of traditional ETL processes that rely heavily on cloud transit. It led us to ask, &amp;quot;What if there&amp;#39;s a better way?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;So, we built Mycelial - an open-source platform designed from the ground up to enable direct, point-to-point ETL. This means your data moves from A to B without being forced to detour through a central cloud, eliminating unnecessary latency, reducing costs, and enhancing data security.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How Mycelial Works:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Crafted with love in 100% Rust, Mycelial leverages local daemons to execute data workflows, guaranteeing fast and secure data movement. It features a universal interface between data sources and destinations, meaning once you&amp;#39;re connected to the Mycelial network, the possibilities for your data are endless.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why we think it&amp;#39;s a game changer:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: With data breaches on the rise, keeping your data off public cloud networks reduces exposure.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Direct transfers mean quicker ETL processes, getting your data where it needs to be faster.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cost-effective&lt;/strong&gt;: Less reliance on cloud infrastructure translates to lower operational costs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: You maintain complete control over your data&amp;#39;s journey, end-to-end.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;re thrilled (and a tad nervous) to introduce Mycelial to the Reddit community. Your feedback, whether it&amp;#39;s praise, criticism, or anything in between, is invaluable to us. We&amp;#39;re committed to transparency and community-driven development, so we&amp;#39;re all ears for your thoughts, concerns, and questions.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Be Part of the Beta Test:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Mycelial is in beta, and we&amp;#39;re eager for testers to push our platform to its limits. Your insights will directly influence the evolution of Mycelial, helping us refine and perfect the platform. Visit &lt;a href=\"http://www.mycelial.com/\"&gt;www.mycelial.com&lt;/a&gt; to sign up and start exploring the possibilities.&lt;/p&gt;\n\n&lt;p&gt;Feel free to AMA about the technology, our development journey, or any burning questions you might have. Your input is crucial in our mission to make Mycelial the best it can be for the community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bogg92", "is_robot_indexable": true, "report_reasons": null, "author": "jofus101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bogg92/launching_mycelial_a_new_way_to_handle_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bogg92/launching_mycelial_a_new_way_to_handle_etl/", "subreddit_subscribers": 171970, "created_utc": 1711480871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is in cost-cutting mode, but we have some little-used servers on-prem. I'm hoping to create a more modern ELT stack than what we have, which is basically separate extract scripts run through a custom scheduler into a relational database. Don't get me started.\n\nI'm currently thinking something like the below, but would be very happy for some advice. Nobody on our team has any experience with any of them, so we're (a) open to new, but (b) wary of steep learning curves:\n\n\\[Sources\\] (many, sql/nosql/flat) -&gt; \\[Flink\\] -&gt; \\[doris\\] -&gt; \\[dbt\\] -&gt; \\[doris\\]\n\nCurrently approx 5TB of data, will probably double this year as more is added.", "author_fullname": "t2_8ha5wvt6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to use for an open source ETL/ELT stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1boc91z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711486876.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711470827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is in cost-cutting mode, but we have some little-used servers on-prem. I&amp;#39;m hoping to create a more modern ELT stack than what we have, which is basically separate extract scripts run through a custom scheduler into a relational database. Don&amp;#39;t get me started.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently thinking something like the below, but would be very happy for some advice. Nobody on our team has any experience with any of them, so we&amp;#39;re (a) open to new, but (b) wary of steep learning curves:&lt;/p&gt;\n\n&lt;p&gt;[Sources] (many, sql/nosql/flat) -&amp;gt; [Flink] -&amp;gt; [doris] -&amp;gt; [dbt] -&amp;gt; [doris]&lt;/p&gt;\n\n&lt;p&gt;Currently approx 5TB of data, will probably double this year as more is added.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1boc91z", "is_robot_indexable": true, "report_reasons": null, "author": "Melodic_One4333", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boc91z/what_to_use_for_an_open_source_etlelt_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boc91z/what_to_use_for_an_open_source_etlelt_stack/", "subreddit_subscribers": 171970, "created_utc": 1711470827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I\u2019m doing a project for work and needs to be resolved asap. \n\nUsing talent 7.1 open studio for reference \n\nSteps done so far: \n\n*Extraction*\n\n1. Python script to get some data from API into a .json file \n\n2. Using talend: TSystem -&gt;tFileInputDelimited. I created a schema and mapped out the fields accordingly and it generated a csv file \n\n\n* load*\n\n3 . Connected tFileInputDelimited-&gt;tDBInput_1 (snowflake) . Mapped out data base and connected the schema\n\n1st QUESTION: how do I do these steps using airflow? 1stly having an issue installing airflow in my local system. So should I use EC2 instance or lambda? If not please help provide some resources. IF not, what orchestration  tool is recommended to capture metrics?!? \n\n\n\n2nd question ( dependent on 1st): if you\u2019re able to solve the 1st, then I want to do some transformations in Snowflake but right code for that so I can capture metrics.\n", "author_fullname": "t2_hkw0w6ao", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Urgent deadline : resources requested-Using Airflow (local or AWS connect Talend &amp; Snowflake to build ELT pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo056j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711431054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I\u2019m doing a project for work and needs to be resolved asap. &lt;/p&gt;\n\n&lt;p&gt;Using talent 7.1 open studio for reference &lt;/p&gt;\n\n&lt;p&gt;Steps done so far: &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Extraction&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Python script to get some data from API into a .json file &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using talend: TSystem -&amp;gt;tFileInputDelimited. I created a schema and mapped out the fields accordingly and it generated a csv file &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;load*&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;3 . Connected tFileInputDelimited-&amp;gt;tDBInput_1 (snowflake) . Mapped out data base and connected the schema&lt;/p&gt;\n\n&lt;p&gt;1st QUESTION: how do I do these steps using airflow? 1stly having an issue installing airflow in my local system. So should I use EC2 instance or lambda? If not please help provide some resources. IF not, what orchestration  tool is recommended to capture metrics?!? &lt;/p&gt;\n\n&lt;p&gt;2nd question ( dependent on 1st): if you\u2019re able to solve the 1st, then I want to do some transformations in Snowflake but right code for that so I can capture metrics.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo056j", "is_robot_indexable": true, "report_reasons": null, "author": "DotMurky2910", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo056j/urgent_deadline_resources_requestedusing_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo056j/urgent_deadline_resources_requestedusing_airflow/", "subreddit_subscribers": 171970, "created_utc": 1711431054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. So a quick back story- I am a self taught data engineer currently working at a small company as a junior data engineer. I\u2019ve been here nearly a year now and I don\u2019t think I\u2019ve developed as quickly as I\u2019d hoped. I like where I work and they\u2019ve been very patient with me but I don\u2019t think it\u2019s the right place for me to progress. I am proficient in SQL, Excel, Power Bi and a little bit of Python (still learning). \n\nJust wanted to see how you guys have navigated your career journey from where you started and the skills you had back then to where you are and the skills you have now.\n\nWhat do I need to do to become a senior or better my self? Skills ? Experience ? \n\nThanks in advance.", "author_fullname": "t2_6l52lijc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnskfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711409103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. So a quick back story- I am a self taught data engineer currently working at a small company as a junior data engineer. I\u2019ve been here nearly a year now and I don\u2019t think I\u2019ve developed as quickly as I\u2019d hoped. I like where I work and they\u2019ve been very patient with me but I don\u2019t think it\u2019s the right place for me to progress. I am proficient in SQL, Excel, Power Bi and a little bit of Python (still learning). &lt;/p&gt;\n\n&lt;p&gt;Just wanted to see how you guys have navigated your career journey from where you started and the skills you had back then to where you are and the skills you have now.&lt;/p&gt;\n\n&lt;p&gt;What do I need to do to become a senior or better my self? Skills ? Experience ? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnskfq", "is_robot_indexable": true, "report_reasons": null, "author": "Tookie2x", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnskfq/career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnskfq/career_advice/", "subreddit_subscribers": 171970, "created_utc": 1711409103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! I am in the middle of ingesting some excel files and i need an opinion on the architecture.\n\nSo i have the excel files in one drive. My ideia would be to either ingest the files pass them to a database, then from that use airbyte to retrieve the data put it into snowflake using dbt, and make some dimensional modelling and then visualize.\n\n&amp;#x200B;\n\nNow this seems kinda stupid to have a database in the middle right, as well as having the files only in one drive?\n\n&amp;#x200B;\n\nBut i dont see the use of making the transition from one drive to azure storage for example.\n\n&amp;#x200B;\n\nI need help xd", "author_fullname": "t2_ud8fo6hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingestion of Excel files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bok0e9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711489270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I am in the middle of ingesting some excel files and i need an opinion on the architecture.&lt;/p&gt;\n\n&lt;p&gt;So i have the excel files in one drive. My ideia would be to either ingest the files pass them to a database, then from that use airbyte to retrieve the data put it into snowflake using dbt, and make some dimensional modelling and then visualize.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now this seems kinda stupid to have a database in the middle right, as well as having the files only in one drive?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But i dont see the use of making the transition from one drive to azure storage for example.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need help xd&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bok0e9", "is_robot_indexable": true, "report_reasons": null, "author": "PoundPotential3062", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bok0e9/ingestion_of_excel_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bok0e9/ingestion_of_excel_files/", "subreddit_subscribers": 171970, "created_utc": 1711489270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I everyone! I am new to this community. I just got my first intern in DE and if I do well the company might offer a job.\n\nIt is a scale-up that wants to build the first DE/DS team and the first step is building the data wherehousing infrastracture (AWS cloud - the data are timeseries with customer data like service usage , subscriptions  and so on ) and I have some freedom to propose the technologies I prefer. \n\nI am wondering what are the best practice I should be aware of and best tools for the job .\n\nI come from a physics, DA/HPC background (Python and I started learning SQL - came across C, Matlab, Mathematica, R in the past) , so Just want to hear opinions from experts in using databases and cloud.\n\nThey do not need at the moment Big Data.(but happy to hear on this if you want speak)", "author_fullname": "t2_u7fsj6m26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to DE: any advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1boika2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711486119.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711485886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I everyone! I am new to this community. I just got my first intern in DE and if I do well the company might offer a job.&lt;/p&gt;\n\n&lt;p&gt;It is a scale-up that wants to build the first DE/DS team and the first step is building the data wherehousing infrastracture (AWS cloud - the data are timeseries with customer data like service usage , subscriptions  and so on ) and I have some freedom to propose the technologies I prefer. &lt;/p&gt;\n\n&lt;p&gt;I am wondering what are the best practice I should be aware of and best tools for the job .&lt;/p&gt;\n\n&lt;p&gt;I come from a physics, DA/HPC background (Python and I started learning SQL - came across C, Matlab, Mathematica, R in the past) , so Just want to hear opinions from experts in using databases and cloud.&lt;/p&gt;\n\n&lt;p&gt;They do not need at the moment Big Data.(but happy to hear on this if you want speak)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1boika2", "is_robot_indexable": true, "report_reasons": null, "author": "ubiond", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boika2/new_to_de_any_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boika2/new_to_de_any_advice/", "subreddit_subscribers": 171970, "created_utc": 1711485886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In AWS Redshift data warehouse it costs 0.0256 $ per gb to store data and in S3 (data lake) it costs 0.0018$. So, 0.0018/0.0256 = 18 times cheaper? Does this estimate allign with your experiences?\n\n  \n ", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "By how much is storage in data lakes cheaper compared to data warehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1boia8q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711485229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In AWS Redshift data warehouse it costs 0.0256 $ per gb to store data and in S3 (data lake) it costs 0.0018$. So, 0.0018/0.0256 = 18 times cheaper? Does this estimate allign with your experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1boia8q", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boia8q/by_how_much_is_storage_in_data_lakes_cheaper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boia8q/by_how_much_is_storage_in_data_lakes_cheaper/", "subreddit_subscribers": 171970, "created_utc": 1711485229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer with 2 years of experience. I\u2019ve joined a fairly well established company but found it to be completely different to my last (and only other) workplace. Is this normal?\n\n- Single Terraformed GCP project hosting\n  - Airbyte Open Source\n  - Airflow\n  - DBT\n- Single Snowflake Account with DBT\n\nThere is no permanent Sandbox / Dev GCP project or testing procedure. Anything to do with Airbyte and Airflow gets done directly in production which is making me age twice as fast. If you need to configure a new VM you are free to create a new GCP project to give it a go but as only a production GCP account is Terraformed there is no environment parity. \n\nThe infrastructure supports analytics so it\u2019s not business critical, but it still feels like an uncommon way to do things. The Head of Data is onboard with my suggestion to deploy our IaC to a second GCP project but the Senior Data Engineer is less enthusiastic. The whole situation has gotten me wondering whether I\u2019m right, or whether it should even be my job as a mid to do what a senior data engineer doesn\u2019t feel the need to do.", "author_fullname": "t2_827pnc17k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workplace has 0 procedures. Time to look for a new job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bohwi7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711484676.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711484322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer with 2 years of experience. I\u2019ve joined a fairly well established company but found it to be completely different to my last (and only other) workplace. Is this normal?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Single Terraformed GCP project hosting\n\n&lt;ul&gt;\n&lt;li&gt;Airbyte Open Source&lt;/li&gt;\n&lt;li&gt;Airflow&lt;/li&gt;\n&lt;li&gt;DBT&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Single Snowflake Account with DBT&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;There is no permanent Sandbox / Dev GCP project or testing procedure. Anything to do with Airbyte and Airflow gets done directly in production which is making me age twice as fast. If you need to configure a new VM you are free to create a new GCP project to give it a go but as only a production GCP account is Terraformed there is no environment parity. &lt;/p&gt;\n\n&lt;p&gt;The infrastructure supports analytics so it\u2019s not business critical, but it still feels like an uncommon way to do things. The Head of Data is onboard with my suggestion to deploy our IaC to a second GCP project but the Senior Data Engineer is less enthusiastic. The whole situation has gotten me wondering whether I\u2019m right, or whether it should even be my job as a mid to do what a senior data engineer doesn\u2019t feel the need to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bohwi7", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished_Cup_392", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bohwi7/workplace_has_0_procedures_time_to_look_for_a_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bohwi7/workplace_has_0_procedures_time_to_look_for_a_new/", "subreddit_subscribers": 171970, "created_utc": 1711484322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Something I have been wondering for a while. As long as I have been at my current job we replicate all the data in every table for a set of schemas from our production cluster to a development cluster to facilitate development work. This seems unnecessary to me though. \n\nI feel like we could implement a tiered structure where we keep all data from the current year, half the data from last year, a quarter of the data from the year before, etc. in order to shorten our dev runs while retaining enough information to ensure that we are consistent with historical data as much as current data. \n\nCurious if anyone has implemented something similar.", "author_fullname": "t2_6nf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle replication of data into development environments? Do you replicate everything or do you implement some sort or reduction strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bogde0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711480694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Something I have been wondering for a while. As long as I have been at my current job we replicate all the data in every table for a set of schemas from our production cluster to a development cluster to facilitate development work. This seems unnecessary to me though. &lt;/p&gt;\n\n&lt;p&gt;I feel like we could implement a tiered structure where we keep all data from the current year, half the data from last year, a quarter of the data from the year before, etc. in order to shorten our dev runs while retaining enough information to ensure that we are consistent with historical data as much as current data. &lt;/p&gt;\n\n&lt;p&gt;Curious if anyone has implemented something similar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bogde0", "is_robot_indexable": true, "report_reasons": null, "author": "radil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bogde0/how_do_you_handle_replication_of_data_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bogde0/how_do_you_handle_replication_of_data_into/", "subreddit_subscribers": 171970, "created_utc": 1711480694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been asked by one of my connections to build a web application for them. The purpose is to eliminate traditional paper format and implement online reporting system. It will be useful to track missing or old data whenever required. I am fine with understanding requirements, storyboarding, wireframing, prototype building and having resource who knows front end and backend. \n\nI need your help in one part where I am clueless which is data collection. If someone uses the reporting system that we built, how is the data that they enter in the application be saved, where is the data saved, is it cloud or any other technologies. Which is the best platform for this? And what would be the pricing model?", "author_fullname": "t2_dibw6jed", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help on my knowledge gap", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bofyeu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711479718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been asked by one of my connections to build a web application for them. The purpose is to eliminate traditional paper format and implement online reporting system. It will be useful to track missing or old data whenever required. I am fine with understanding requirements, storyboarding, wireframing, prototype building and having resource who knows front end and backend. &lt;/p&gt;\n\n&lt;p&gt;I need your help in one part where I am clueless which is data collection. If someone uses the reporting system that we built, how is the data that they enter in the application be saved, where is the data saved, is it cloud or any other technologies. Which is the best platform for this? And what would be the pricing model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bofyeu", "is_robot_indexable": true, "report_reasons": null, "author": "hemanathkumarj", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bofyeu/need_help_on_my_knowledge_gap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bofyeu/need_help_on_my_knowledge_gap/", "subreddit_subscribers": 171970, "created_utc": 1711479718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have overall 2.3 years of experience, I have worked with Pyspark, Hive and SQL,  for some months , currently in a project where I have to debug the SQL Server procs and PySpark scripts and find why data is missing or why the data is not flowing to reporting side and all that issues....I am learning DataBricks right now....Need some career advice from you all.", "author_fullname": "t2_rtrjd5uk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1boc1tt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711470343.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have overall 2.3 years of experience, I have worked with Pyspark, Hive and SQL,  for some months , currently in a project where I have to debug the SQL Server procs and PySpark scripts and find why data is missing or why the data is not flowing to reporting side and all that issues....I am learning DataBricks right now....Need some career advice from you all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1boc1tt", "is_robot_indexable": true, "report_reasons": null, "author": "Sea_Draft_4623", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boc1tt/career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boc1tt/career_advice/", "subreddit_subscribers": 171970, "created_utc": 1711470343.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}