{"kind": "Listing", "data": {"after": "t3_1b8rtol", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "An analysis of DOIs suggests that digital preservation is not keeping up with burgeoning scholarly knowledge.", "author_fullname": "t2_8ztqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Millions of research papers at risk of disappearing from the Internet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1b97hzi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 302, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 302, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/t-GLhfzbts7OaxBsljTaZ98mZClXnwwC7PSqRUkRVas.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709850587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "nature.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;An analysis of DOIs suggests that digital preservation is not keeping up with burgeoning scholarly knowledge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.nature.com/articles/d41586-024-00616-5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?auto=webp&amp;s=25f1969367754f109a0ebe67ee18cd4c19137292", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e9918ad1ad9123bb023b1fa35eff194fa99f2a5", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7488d4727a0f46e003cab13d90a19f5c5e6c65e2", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2848d1ca7d59eb85c96c6458b3805d33f52a021b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7102bef0fa982c27c1d084e5ddfd50fe02cf553f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef1c943f15c8f896808c0a1703b7e0a2c143513e", "width": 960, "height": 540}], "variants": {}, "id": "9CymYjs6R6CONIR46BpXETBmjrlmCu5Zhwaui5rEl_g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b97hzi", "is_robot_indexable": true, "report_reasons": null, "author": "peliciego", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b97hzi/millions_of_research_papers_at_risk_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.nature.com/articles/d41586-024-00616-5", "subreddit_subscribers": 737145, "created_utc": 1709850587.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have nearly 5 terabytes of games, websites, music, documents and software on my Linux RAID. What is a good way to efficiently search through this mess? Simple file search (say via \\`find\\`) takes forever and isn't very fast when I enable regexes. Another type of query I want to perform is about the content of the files, I use \\`silversearcher-ag\\` for this and its reasonably fast but still a pain to use and sometimes very slow.\n\nIs there any tool that can index this properly and be better at searching both files with certain name and/or files with certain data. Bonus points if it has some web ui so that I can make it available over the network.", "author_fullname": "t2_bhm4t9j3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you efficiently search through your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8wqyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709823646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have nearly 5 terabytes of games, websites, music, documents and software on my Linux RAID. What is a good way to efficiently search through this mess? Simple file search (say via `find`) takes forever and isn&amp;#39;t very fast when I enable regexes. Another type of query I want to perform is about the content of the files, I use `silversearcher-ag` for this and its reasonably fast but still a pain to use and sometimes very slow.&lt;/p&gt;\n\n&lt;p&gt;Is there any tool that can index this properly and be better at searching both files with certain name and/or files with certain data. Bonus points if it has some web ui so that I can make it available over the network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8wqyd", "is_robot_indexable": true, "report_reasons": null, "author": "Emergency_Apricot_77", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8wqyd/how_do_you_efficiently_search_through_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8wqyd/how_do_you_efficiently_search_through_your_data/", "subreddit_subscribers": 737145, "created_utc": 1709823646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello. I received this hard drive from a friend who bought it on ebay. The drive states it's a pre-production sample, but googling the model number doesn't provide any results. Does anyone know what model of hard drive this is a pre prod sample too? I've yet to get it to power on, but may be due to the 3.3 volt rail, still troubleshooting", "author_fullname": "t2_5vaqz7s0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 22TB \"Pre-production sample\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b95zv5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 32, "domain": "i.redd.it", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/pucK1rJJsVfk4jWEpSrd0bP7gCJNr8-0IQCUuiIBbz0.jpg", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709847002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I received this hard drive from a friend who bought it on ebay. The drive states it&amp;#39;s a pre-production sample, but googling the model number doesn&amp;#39;t provide any results. Does anyone know what model of hard drive this is a pre prod sample too? I&amp;#39;ve yet to get it to power on, but may be due to the 3.3 volt rail, still troubleshooting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/sxqkvx00dzmc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?auto=webp&amp;s=78871d0ada3111f7f622711c5c6faa35c90920db", "width": 2252, "height": 3776}, "resolutions": [{"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fbcee879ef51bf4f8b2f76c7ca7162890eb4566", "width": 108, "height": 181}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=248c073ecf7aa30cd1a6fbaec999a4663bf993c4", "width": 216, "height": 362}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=834e3354b38bb170fd0c77993ed23d84a157c3e7", "width": 320, "height": 536}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5efd78384da521ab375e112103deb8b9496ef9a", "width": 640, "height": 1073}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ceb69cb323788e00244b6374eb6f66c5114ee2c6", "width": 960, "height": 1609}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49fe940f22b766c3fc08aef6f1efc412ca523d15", "width": 1080, "height": 1810}], "variants": {}, "id": "su-VYCZDt-hjhfHiJZ2juCLVAreXI3sNd1RI15mur8I"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b95zv5", "is_robot_indexable": true, "report_reasons": null, "author": "saints0963", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b95zv5/wd_22tb_preproduction_sample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/sxqkvx00dzmc1.jpeg", "subreddit_subscribers": 737145, "created_utc": 1709847002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In my university class we are doing a time capsule for our university to be opened 50 years from now. It is my job to figure out how to have data last that long. So far I've determined the best way to go about it would be to print our presentations on paper designed to last a long time. But in the event other presenters would like to have music or videos in their presentation I'd also like to prepare for a digital method that could last 50 years in a time capsule.\n\nFeedback would be greatly appreciated, thank you.", "author_fullname": "t2_2sjzxx2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ensure digital data lasts 50 years without interaction or additional cost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b963ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709847234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my university class we are doing a time capsule for our university to be opened 50 years from now. It is my job to figure out how to have data last that long. So far I&amp;#39;ve determined the best way to go about it would be to print our presentations on paper designed to last a long time. But in the event other presenters would like to have music or videos in their presentation I&amp;#39;d also like to prepare for a digital method that could last 50 years in a time capsule.&lt;/p&gt;\n\n&lt;p&gt;Feedback would be greatly appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b963ig", "is_robot_indexable": true, "report_reasons": null, "author": "PiggiesGoSqueal", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b963ig/how_to_ensure_digital_data_lasts_50_years_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b963ig/how_to_ensure_digital_data_lasts_50_years_without/", "subreddit_subscribers": 737145, "created_utc": 1709847234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_bsklr77o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "StableBid Cloud will stop working with Google on 5/15/2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b98xzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CjmjH1RgR0J44awrLRlz3knRb-kQrsze6G_Bp33h9oo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709854504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/q9wzxxabyzmc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/q9wzxxabyzmc1.png?auto=webp&amp;s=95feda381ce100f6f1e17de4005faf2372098e3c", "width": 632, "height": 698}, "resolutions": [{"url": "https://preview.redd.it/q9wzxxabyzmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=625968463c9c7d41f9c3052da90ea51f08e7858e", "width": 108, "height": 119}, {"url": "https://preview.redd.it/q9wzxxabyzmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa86d65ac1b20bf51489a6c78a1a785966b631ee", "width": 216, "height": 238}, {"url": "https://preview.redd.it/q9wzxxabyzmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e3a6c3a6da8e0a8920b97f5eec8e3b493d4df8e", "width": 320, "height": 353}], "variants": {}, "id": "sY3hqrrMf8DdQDDatMtm3eSSQriHOHIzO0tmRqUzjMk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b98xzl", "is_robot_indexable": true, "report_reasons": null, "author": "Fish_Fellatio", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b98xzl/stablebid_cloud_will_stop_working_with_google_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/q9wzxxabyzmc1.png", "subreddit_subscribers": 737145, "created_utc": 1709854504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_bdveu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "After a week of running checmsum verifications and fixing any errors to migrate to a new server I can finally start..... building another checksum!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 33, "top_awarded_type": null, "hide_score": false, "name": "t3_1b98uyg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/freF4W-Dru1WOVKzKLzjh-eYVh_u330YRBeWelWTXrI.jpg", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709854254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ua2qn1bjyzmc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?auto=webp&amp;s=55506dbcbc69c6a671af9bb7345e83fcc8e5f109", "width": 1917, "height": 461}, "resolutions": [{"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7be9143b0398b9c9f9d344428a3a4b5bb313dad0", "width": 108, "height": 25}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=05dc4177640de76517ff5a67b459fea181cfc6f5", "width": 216, "height": 51}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=00705a436bddbc222b547a2e54e7d8570f13628b", "width": 320, "height": 76}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec844c37fe4d6fe6dbe7432fc9163e1dbcb54a9b", "width": 640, "height": 153}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e8f5a94db611b96ab6aaaea7dd707e8b3b31342", "width": 960, "height": 230}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f27b438bbc182d535a544d1f3526616f969591ec", "width": 1080, "height": 259}], "variants": {}, "id": "n-eRrMpUyqGRTL2tXaGB_7fCg4VPbvbeqiIi0eKRfgA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "My backups are on floppies.", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b98uyg", "is_robot_indexable": true, "report_reasons": null, "author": "XOIIO", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b98uyg/after_a_week_of_running_checmsum_verifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ua2qn1bjyzmc1.jpeg", "subreddit_subscribers": 737145, "created_utc": 1709854254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First of all let me say that I've spent weeks digging through this sub's valuable information in terms of optical media archiving, and stumbled upon many of the Verbatim-CMC-HTL-LTH comments that were very insightful. However, I wasn't be able to read a definitive answer to the actual state of archiving data on off-the-shelf media in 2023.\n\nFirst and foremost, with all the *Blu-ray M-Disc not being real M-Discs anymore* things going on, what is a realistic expectation of data longevity, when discs are stored under normal everyday conditions (e.g. outside of the 10-25*C temperature and 40-60% humidity range)? Is it still worth considering optical media to get at least a decade of stress free archiving or does media produced in the recent years show significant fallout rates even after short periods of time? What's the actual return on invest on today's media, even speaking about regular \"cheap\" Blu-ray discs and what would be the recommended re-burn interval based on the little experience with post-2019 media that there might be?\n\nThank you!", "author_fullname": "t2_kigcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realistic expectation of Blu-ray M-Disc life in 2023 under everyday conditions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8xe10", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709825215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all let me say that I&amp;#39;ve spent weeks digging through this sub&amp;#39;s valuable information in terms of optical media archiving, and stumbled upon many of the Verbatim-CMC-HTL-LTH comments that were very insightful. However, I wasn&amp;#39;t be able to read a definitive answer to the actual state of archiving data on off-the-shelf media in 2023.&lt;/p&gt;\n\n&lt;p&gt;First and foremost, with all the &lt;em&gt;Blu-ray M-Disc not being real M-Discs anymore&lt;/em&gt; things going on, what is a realistic expectation of data longevity, when discs are stored under normal everyday conditions (e.g. outside of the 10-25*C temperature and 40-60% humidity range)? Is it still worth considering optical media to get at least a decade of stress free archiving or does media produced in the recent years show significant fallout rates even after short periods of time? What&amp;#39;s the actual return on invest on today&amp;#39;s media, even speaking about regular &amp;quot;cheap&amp;quot; Blu-ray discs and what would be the recommended re-burn interval based on the little experience with post-2019 media that there might be?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8xe10", "is_robot_indexable": true, "report_reasons": null, "author": "mrusme", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8xe10/realistic_expectation_of_bluray_mdisc_life_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8xe10/realistic_expectation_of_bluray_mdisc_life_in/", "subreddit_subscribers": 737145, "created_utc": 1709825215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I need to recover some data from a very old HDD that i don't feel super safe just plugging in my PC.\n\nI thought of live booting Ubuntu of a usb stick, but have also read about kali live with forensic mode and stuff, keep in mind i'm not very familiar with linux, which is why i'm looking for suggestions.\n\nI would transfer that data to a usb stick, how should i tackle scanning it after?", "author_fullname": "t2_b5suwht0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to retrieve data from a potentially infected HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b96hef", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709848142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I need to recover some data from a very old HDD that i don&amp;#39;t feel super safe just plugging in my PC.&lt;/p&gt;\n\n&lt;p&gt;I thought of live booting Ubuntu of a usb stick, but have also read about kali live with forensic mode and stuff, keep in mind i&amp;#39;m not very familiar with linux, which is why i&amp;#39;m looking for suggestions.&lt;/p&gt;\n\n&lt;p&gt;I would transfer that data to a usb stick, how should i tackle scanning it after?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b96hef", "is_robot_indexable": true, "report_reasons": null, "author": "banekal", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b96hef/best_way_to_retrieve_data_from_a_potentially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b96hef/best_way_to_retrieve_data_from_a_potentially/", "subreddit_subscribers": 737145, "created_utc": 1709848142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Need to create a storage solution at remote sites. These will reside in awkward and/or smaller spaces without racks. A good-sized shed that's protected from the elements but no HVAC, for example. It will store surveillance footage, so 24/7 writing and occasional reading. One other caveat is that it has to run windows. Our VMS recording application is windows only. A pre-built NAS would obviously work but then it turns into two devices to manage instead of one. Since I'd still have a PC running the software. Because of that, I'd rather stick to a PC-only solution, but not married to it.\n\nOne option is simply building a PC with a bunch of spinners. But it needs some resiliency. I manage them, but I\u2019ve never built out a RAID device myself from scratch. I\u2019ve read not to use diskmgmt raid or storage spaces. Though some say if you're just running a mirror, no parity, storage spaces can be fine. This doesn't need to be fancy. A RAID 1 would satisfy the requirements. Maybe storage spaces is fine? Any other decent software raid options out there? Thoughts on Stablebit Drivepool? I would need the storage to show up in Windows as one drive.\n\nAnother option is to run a hardware raid card in the build. I've read that using some consumer level card like startech is basically useless. It looks like a decent LSI enterprise card with caching and BBU runs \\~$1,000? At that price, it may be better to get smaller Dell or HP server tower instead of trying to piece something together from scratch.\n\nAnd of course, the \"easiest\" solution is to just get a pre-built nas and call it day.\n\nKind of looking for a conversation here. Thoughts on the some of the assumptions made due to what I've read so far. Some real-world experience with any of the outlined options. Again, it's going to live in outdoor temperatures. Around here anywhere from 80-100 in the summer, down to 10-30 in the winter. Not really sure if any options are going to hold up better than others given that.", "author_fullname": "t2_qacpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CCTV Storage Solutions. Outside?!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b903d7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709838870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709832051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need to create a storage solution at remote sites. These will reside in awkward and/or smaller spaces without racks. A good-sized shed that&amp;#39;s protected from the elements but no HVAC, for example. It will store surveillance footage, so 24/7 writing and occasional reading. One other caveat is that it has to run windows. Our VMS recording application is windows only. A pre-built NAS would obviously work but then it turns into two devices to manage instead of one. Since I&amp;#39;d still have a PC running the software. Because of that, I&amp;#39;d rather stick to a PC-only solution, but not married to it.&lt;/p&gt;\n\n&lt;p&gt;One option is simply building a PC with a bunch of spinners. But it needs some resiliency. I manage them, but I\u2019ve never built out a RAID device myself from scratch. I\u2019ve read not to use diskmgmt raid or storage spaces. Though some say if you&amp;#39;re just running a mirror, no parity, storage spaces can be fine. This doesn&amp;#39;t need to be fancy. A RAID 1 would satisfy the requirements. Maybe storage spaces is fine? Any other decent software raid options out there? Thoughts on Stablebit Drivepool? I would need the storage to show up in Windows as one drive.&lt;/p&gt;\n\n&lt;p&gt;Another option is to run a hardware raid card in the build. I&amp;#39;ve read that using some consumer level card like startech is basically useless. It looks like a decent LSI enterprise card with caching and BBU runs ~$1,000? At that price, it may be better to get smaller Dell or HP server tower instead of trying to piece something together from scratch.&lt;/p&gt;\n\n&lt;p&gt;And of course, the &amp;quot;easiest&amp;quot; solution is to just get a pre-built nas and call it day.&lt;/p&gt;\n\n&lt;p&gt;Kind of looking for a conversation here. Thoughts on the some of the assumptions made due to what I&amp;#39;ve read so far. Some real-world experience with any of the outlined options. Again, it&amp;#39;s going to live in outdoor temperatures. Around here anywhere from 80-100 in the summer, down to 10-30 in the winter. Not really sure if any options are going to hold up better than others given that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b903d7", "is_robot_indexable": true, "report_reasons": null, "author": "sageRJ", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b903d7/cctv_storage_solutions_outside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b903d7/cctv_storage_solutions_outside/", "subreddit_subscribers": 737145, "created_utc": 1709832051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I have an James Bond Volume 2 set which has a infinite loop in the menus, HOW DO I JUST RIP THE DVD!", "author_fullname": "t2_ah8awy9z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HELP BYPASS DRM INFINITE MENU LOOP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b9g3lw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709875163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an James Bond Volume 2 set which has a infinite loop in the menus, HOW DO I JUST RIP THE DVD!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9g3lw", "is_robot_indexable": true, "report_reasons": null, "author": "mindeloo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9g3lw/help_bypass_drm_infinite_menu_loop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9g3lw/help_bypass_drm_infinite_menu_loop/", "subreddit_subscribers": 737145, "created_utc": 1709875163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi storage geeks. I currently use Snapraid on my data drives with 2 drives dedicated to parity. I'm thinking about ditching this approach and moving to a RAID setup with btrfs.\n\nI have about 68.4TB usable spread over 7 disks and am using about 20.4TB so plenty of space for parity and snaps.\n\nI do backup my essential data with borg to 6TB of offsite VPS.\n\nThis is what I have to play with:\n\n    \u2502 MOUNTED ON  \u2502    SIZE \u2502   USED \u2502  AVAIL \u2502  USE% \u2502 TYPE \u2502\n    \u2502 /           \u2502  116.0G \u2502  61.9G \u2502  54.1G \u2502 53.3% \u2502 xfs  \u2502\n    \u2502             \u2502         \u2502        \u2502        \u2502       \u2502      \u2502\n    \u2502 /boot       \u2502 1014.0M \u2502 400.7M \u2502 613.3M \u2502 39.5% \u2502 xfs  \u2502\n    \u2502 /boot/efi   \u2502  598.8M \u2502   7.4M \u2502 591.4M \u2502  1.2% \u2502 vfat \u2502\n    \u2502 /mnt/data1  \u2502   10.8T \u2502   7.6T \u2502   2.7T \u2502 69.8% \u2502 ext4 \u2502\n    \u2502 /mnt/data2  \u2502   10.8T \u2502   4.6T \u2502   5.7T \u2502 42.5% \u2502 ext4 \u2502\n    \u2502 /mnt/data3  \u2502   10.8T \u2502   8.0T \u2502   2.3T \u2502 73.9% \u2502 ext4 \u2502\n    \u2502 /mnt/data4  \u2502    3.6T \u2502 176.6M \u2502   3.4T \u2502  0.0% \u2502 ext4 \u2502\n    \u2502 /mnt/diskp1 \u2502   10.8T \u2502   8.2T \u2502   2.0T \u2502 76.2% \u2502 ext4 \u2502\n    \u2502 /mnt/diskp2 \u2502   10.8T \u2502   8.2T \u2502   2.0T \u2502 76.2% \u2502 ext4 \u2502\n    \n    + not yet installed another 10.8T\n\nI was thinking maybe 2 x RAID5 groups with 2D+1P 10.8TB drives or should I RAID 6 the whole lot of large drives in one large group?\n\nAppreciate any opinions on a set up that provides some level of protection from drive failure. I'm going round in circles trying to decide on an optimum layout.", "author_fullname": "t2_mo11s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pls recommend a storage layout", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9exds", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709871561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi storage geeks. I currently use Snapraid on my data drives with 2 drives dedicated to parity. I&amp;#39;m thinking about ditching this approach and moving to a RAID setup with btrfs.&lt;/p&gt;\n\n&lt;p&gt;I have about 68.4TB usable spread over 7 disks and am using about 20.4TB so plenty of space for parity and snaps.&lt;/p&gt;\n\n&lt;p&gt;I do backup my essential data with borg to 6TB of offsite VPS.&lt;/p&gt;\n\n&lt;p&gt;This is what I have to play with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\u2502 MOUNTED ON  \u2502    SIZE \u2502   USED \u2502  AVAIL \u2502  USE% \u2502 TYPE \u2502\n\u2502 /           \u2502  116.0G \u2502  61.9G \u2502  54.1G \u2502 53.3% \u2502 xfs  \u2502\n\u2502             \u2502         \u2502        \u2502        \u2502       \u2502      \u2502\n\u2502 /boot       \u2502 1014.0M \u2502 400.7M \u2502 613.3M \u2502 39.5% \u2502 xfs  \u2502\n\u2502 /boot/efi   \u2502  598.8M \u2502   7.4M \u2502 591.4M \u2502  1.2% \u2502 vfat \u2502\n\u2502 /mnt/data1  \u2502   10.8T \u2502   7.6T \u2502   2.7T \u2502 69.8% \u2502 ext4 \u2502\n\u2502 /mnt/data2  \u2502   10.8T \u2502   4.6T \u2502   5.7T \u2502 42.5% \u2502 ext4 \u2502\n\u2502 /mnt/data3  \u2502   10.8T \u2502   8.0T \u2502   2.3T \u2502 73.9% \u2502 ext4 \u2502\n\u2502 /mnt/data4  \u2502    3.6T \u2502 176.6M \u2502   3.4T \u2502  0.0% \u2502 ext4 \u2502\n\u2502 /mnt/diskp1 \u2502   10.8T \u2502   8.2T \u2502   2.0T \u2502 76.2% \u2502 ext4 \u2502\n\u2502 /mnt/diskp2 \u2502   10.8T \u2502   8.2T \u2502   2.0T \u2502 76.2% \u2502 ext4 \u2502\n\n+ not yet installed another 10.8T\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I was thinking maybe 2 x RAID5 groups with 2D+1P 10.8TB drives or should I RAID 6 the whole lot of large drives in one large group?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any opinions on a set up that provides some level of protection from drive failure. I&amp;#39;m going round in circles trying to decide on an optimum layout.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9exds", "is_robot_indexable": true, "report_reasons": null, "author": "Finno_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9exds/pls_recommend_a_storage_layout/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9exds/pls_recommend_a_storage_layout/", "subreddit_subscribers": 737145, "created_utc": 1709871561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I wanted to start a thread on this topic because, even though it comes up frequently, there isn't a list of recommended cloud storage providers in the wiki which feels like a bit of a gap currently.\n\nMy use case: I run a Synology NAS at home to backup family photos/videos/files/media and always use an offsite cloud backup of my NAS for just-in-case retrieval. Over the years I've experimented with a ton of different options:\n\n* Google Drive Unlimited (RIP)\n* Amazon S3 (reliable, but expensive)\n* Amazon Glacier (reliable, cheap, but hard to use and extremely expensive if you need to actually retrieve your data)\n* iDrive (looks relatively inexpensive but have heard terrible things)\n* Backblaze B2 (good cost, but recently got more expensive)\n* Storj (decentralized, durable (11 9's), S3-compatible APIs, and extremely inexpensive -- $4/TB/mo)\n\nI've recently switched over to use Storj as my cloud backup provider, simply because they have 11 9's of durability, the lowest cost per TB I've been able to find, and an S3-compatible API so they work with pretty much every tool.\n\nWhat are you using?", "author_fullname": "t2_4g95l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest Cloud Storage Providers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9czns", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709865971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to start a thread on this topic because, even though it comes up frequently, there isn&amp;#39;t a list of recommended cloud storage providers in the wiki which feels like a bit of a gap currently.&lt;/p&gt;\n\n&lt;p&gt;My use case: I run a Synology NAS at home to backup family photos/videos/files/media and always use an offsite cloud backup of my NAS for just-in-case retrieval. Over the years I&amp;#39;ve experimented with a ton of different options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Google Drive Unlimited (RIP)&lt;/li&gt;\n&lt;li&gt;Amazon S3 (reliable, but expensive)&lt;/li&gt;\n&lt;li&gt;Amazon Glacier (reliable, cheap, but hard to use and extremely expensive if you need to actually retrieve your data)&lt;/li&gt;\n&lt;li&gt;iDrive (looks relatively inexpensive but have heard terrible things)&lt;/li&gt;\n&lt;li&gt;Backblaze B2 (good cost, but recently got more expensive)&lt;/li&gt;\n&lt;li&gt;Storj (decentralized, durable (11 9&amp;#39;s), S3-compatible APIs, and extremely inexpensive -- $4/TB/mo)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve recently switched over to use Storj as my cloud backup provider, simply because they have 11 9&amp;#39;s of durability, the lowest cost per TB I&amp;#39;ve been able to find, and an S3-compatible API so they work with pretty much every tool.&lt;/p&gt;\n\n&lt;p&gt;What are you using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9czns", "is_robot_indexable": true, "report_reasons": null, "author": "rdegges", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9czns/cheapest_cloud_storage_providers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9czns/cheapest_cloud_storage_providers/", "subreddit_subscribers": 737145, "created_utc": 1709865971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have onedrive student account with 5TB space. As onedrive is going to remove the A1 Plus license, my university has to downgrade the plan and limits the storage to 5GB per student starting 1st April. Yes, from 5TB to 5GB. Currently I'm having 2.1TB in the cloud and planning to move all the files to a selfhosted cloud storage. I'm familiar with docker and linux but still a newbie. So, here I am kindly ask for help and advice to setup a selfhosted cloud storage. \n\nUse cases: \n1. Multiplatform - ipad, mac, windows, android\n2. Upload files to cloud \n3. Download on-demand and view file locally \n3. Delete that locally save on-demand went needed\n4. Offline editing &amp; sync to cloud when online\n\nMy questions:\n1. What is the best multiplatform selfhosted cloud storage that suits my use cases? Nextcloud? Seafile?\n2. What is the best filesystem to use? Zfs? Btrfs? \n3. Where should I host the app? NAS? Docker?\n4. What is the best storage disk? HDD 3.5 or 2.5? SMR or CMR?\n5. Other things to consider? NAS? RAID? SMB? Webdav? ", "author_fullname": "t2_8fiv28qr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Onedrive to selfhosted cloud storage migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9abgr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709858680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have onedrive student account with 5TB space. As onedrive is going to remove the A1 Plus license, my university has to downgrade the plan and limits the storage to 5GB per student starting 1st April. Yes, from 5TB to 5GB. Currently I&amp;#39;m having 2.1TB in the cloud and planning to move all the files to a selfhosted cloud storage. I&amp;#39;m familiar with docker and linux but still a newbie. So, here I am kindly ask for help and advice to setup a selfhosted cloud storage. &lt;/p&gt;\n\n&lt;p&gt;Use cases: \n1. Multiplatform - ipad, mac, windows, android\n2. Upload files to cloud \n3. Download on-demand and view file locally \n3. Delete that locally save on-demand went needed\n4. Offline editing &amp;amp; sync to cloud when online&lt;/p&gt;\n\n&lt;p&gt;My questions:\n1. What is the best multiplatform selfhosted cloud storage that suits my use cases? Nextcloud? Seafile?\n2. What is the best filesystem to use? Zfs? Btrfs? \n3. Where should I host the app? NAS? Docker?\n4. What is the best storage disk? HDD 3.5 or 2.5? SMR or CMR?\n5. Other things to consider? NAS? RAID? SMB? Webdav? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9abgr", "is_robot_indexable": true, "report_reasons": null, "author": "Retiary_Lime", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9abgr/onedrive_to_selfhosted_cloud_storage_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9abgr/onedrive_to_selfhosted_cloud_storage_migration/", "subreddit_subscribers": 737145, "created_utc": 1709858680.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a NAS Linux backup server that I have setup smartd to send an email should it find a smart error on a drive.\n\nMy server only runs long enough to execute an incremental backup 3x a week. This backup takes 5-10min on average.\n\nI have not changed the default smart check time period from 30 min, so I was wondering if smartd ran a check at boot or does it only start checking at boot+30min.\n\nIf my server normally runs less than 30min; will smartd never check the drives and send a warning  as it never hits that 30min window for the first check?", "author_fullname": "t2_cskhbtwdp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smartd - When is the first smart check?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b98r7t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709853936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a NAS Linux backup server that I have setup smartd to send an email should it find a smart error on a drive.&lt;/p&gt;\n\n&lt;p&gt;My server only runs long enough to execute an incremental backup 3x a week. This backup takes 5-10min on average.&lt;/p&gt;\n\n&lt;p&gt;I have not changed the default smart check time period from 30 min, so I was wondering if smartd ran a check at boot or does it only start checking at boot+30min.&lt;/p&gt;\n\n&lt;p&gt;If my server normally runs less than 30min; will smartd never check the drives and send a warning  as it never hits that 30min window for the first check?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b98r7t", "is_robot_indexable": true, "report_reasons": null, "author": "Beaver-on-fire", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b98r7t/smartd_when_is_the_first_smart_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b98r7t/smartd_when_is_the_first_smart_check/", "subreddit_subscribers": 737145, "created_utc": 1709853936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm in the process of rebuilding a media server. The common Plex, ARR stack with qBittorent. So seeding is also relevant, not just hardlinking. I will be using Docker for the stack. Full disclosure, I am new to pretty much all of it, Linux, mergerfs, etc. I have been reading docs extensively but am still confused on a couple concepts/config settings.\n\nThe point in the setup process I am at now is taking 2 storage HDDs with media and pooling them with mergerfs. Most mergerfs Google results seem to be from this sub so here I am!\n\nFor context, my mergerfs config looks like this:\n\n`/mnt/hdd1:/mnt/hdd2 /merge mergerfs cache.files=partial,dropcacheonclose=true,category.create=mfs`\n\n* /mnt/hdd1 - already has TV and Movies on it\n* /mnt/hdd2 - empty, but have set the same high level directory structure on it:\n   * /torrents\n      * /movies\n      * /tv\n   * /library\n      * /movies\n      * /tv\n\nThe part in the docs that made me start second guessing was reference to \"hardlinks do not work across branches in a pool\". I started thinking about what happens if mergerfs effectively puts the torrent file on one branch, but the Sonarr/Radarr hardlink on another branch (if that's even what it's doing)? *If this is completely irrelevant by just setting Sonarr/Radarr root to /merge then let me know.*\n\nI also want to avoid running into the issue mentioned in docs of \"all my files ending up on 1 filesystem?!\" since I have one drive that already has a bunch of files and directories on it, and one empty drive.\n\nSo, now I'm wondering if I need to modify my mergerfs options from the default to make sure hardlinks and seeding work.\n\n* Do I need to change to create policy to `epmfs`?\n* Do I need to create specific policies for something like tv series' to keep all seasons/episodes related to a show on the same branch?\n* Do I need to enable `moveonenospc`?\n* Would any of the other extensive options be relevant to this topic?\n\nAs far as Docker is concerned, my understanding is I need to mount the containers at the root /merge directory. *It's entirely possible that's all I need to do and I'm overthinking this.*", "author_fullname": "t2_1ajq44ub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make sure hardlinks and seeding work with mergerfs and Radarr/Sonarr?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b96ke4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709848329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in the process of rebuilding a media server. The common Plex, ARR stack with qBittorent. So seeding is also relevant, not just hardlinking. I will be using Docker for the stack. Full disclosure, I am new to pretty much all of it, Linux, mergerfs, etc. I have been reading docs extensively but am still confused on a couple concepts/config settings.&lt;/p&gt;\n\n&lt;p&gt;The point in the setup process I am at now is taking 2 storage HDDs with media and pooling them with mergerfs. Most mergerfs Google results seem to be from this sub so here I am!&lt;/p&gt;\n\n&lt;p&gt;For context, my mergerfs config looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;/mnt/hdd1:/mnt/hdd2 /merge mergerfs cache.files=partial,dropcacheonclose=true,category.create=mfs&lt;/code&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;/mnt/hdd1 - already has TV and Movies on it&lt;/li&gt;\n&lt;li&gt;/mnt/hdd2 - empty, but have set the same high level directory structure on it:\n\n&lt;ul&gt;\n&lt;li&gt;/torrents\n\n&lt;ul&gt;\n&lt;li&gt;/movies&lt;/li&gt;\n&lt;li&gt;/tv&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;/library\n\n&lt;ul&gt;\n&lt;li&gt;/movies&lt;/li&gt;\n&lt;li&gt;/tv&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The part in the docs that made me start second guessing was reference to &amp;quot;hardlinks do not work across branches in a pool&amp;quot;. I started thinking about what happens if mergerfs effectively puts the torrent file on one branch, but the Sonarr/Radarr hardlink on another branch (if that&amp;#39;s even what it&amp;#39;s doing)? &lt;em&gt;If this is completely irrelevant by just setting Sonarr/Radarr root to /merge then let me know.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I also want to avoid running into the issue mentioned in docs of &amp;quot;all my files ending up on 1 filesystem?!&amp;quot; since I have one drive that already has a bunch of files and directories on it, and one empty drive.&lt;/p&gt;\n\n&lt;p&gt;So, now I&amp;#39;m wondering if I need to modify my mergerfs options from the default to make sure hardlinks and seeding work.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do I need to change to create policy to &lt;code&gt;epmfs&lt;/code&gt;?&lt;/li&gt;\n&lt;li&gt;Do I need to create specific policies for something like tv series&amp;#39; to keep all seasons/episodes related to a show on the same branch?&lt;/li&gt;\n&lt;li&gt;Do I need to enable &lt;code&gt;moveonenospc&lt;/code&gt;?&lt;/li&gt;\n&lt;li&gt;Would any of the other extensive options be relevant to this topic?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As far as Docker is concerned, my understanding is I need to mount the containers at the root /merge directory. &lt;em&gt;It&amp;#39;s entirely possible that&amp;#39;s all I need to do and I&amp;#39;m overthinking this.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b96ke4", "is_robot_indexable": true, "report_reasons": null, "author": "GooeyDuck1", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b96ke4/how_to_make_sure_hardlinks_and_seeding_work_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b96ke4/how_to_make_sure_hardlinks_and_seeding_work_with/", "subreddit_subscribers": 737145, "created_utc": 1709848329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Many years ago i manually copied all the photos and videos i had to a HDD. Ever since then i just cloned my hard drives with a clone dock and called it a day. Recently i found a external hard drive from my dad (which still works 12+ years later) which has a lot of pictures / videos i don't have already. So how would i go about copying those files onto my HDD?\n\nI thought doing the same thing i did before. Putting my drive into my drive bay and connecting the external drive through usb, then the old copy and paste. But is there a better option? A more secure option? A hopefully not expensive option? ", "author_fullname": "t2_kht4ovrc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying external HDD onto internal HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8w9nv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709822412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many years ago i manually copied all the photos and videos i had to a HDD. Ever since then i just cloned my hard drives with a clone dock and called it a day. Recently i found a external hard drive from my dad (which still works 12+ years later) which has a lot of pictures / videos i don&amp;#39;t have already. So how would i go about copying those files onto my HDD?&lt;/p&gt;\n\n&lt;p&gt;I thought doing the same thing i did before. Putting my drive into my drive bay and connecting the external drive through usb, then the old copy and paste. But is there a better option? A more secure option? A hopefully not expensive option? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8w9nv", "is_robot_indexable": true, "report_reasons": null, "author": "TengokuDaimakyo", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8w9nv/copying_external_hdd_onto_internal_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8w9nv/copying_external_hdd_onto_internal_hdd/", "subreddit_subscribers": 737145, "created_utc": 1709822412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Basic question but am I covered in terms of the hardware I need to run a basic media server (either Plex or Jellyfin) ?\n\nBeen planning to start this project for a while, have a bunch of saved resources to get through, but I some what instantaneously decided to start this project *now*.\n\nI live in a country where it's difficult and expensive to buy electronic hardware and I'm currently in the U.S. - want to make sure I have everything I might need before I fly back this weekend.\n\nAny tips or advice would be greatly appreciated.\n\nThanks!", "author_fullname": "t2_1llv04y0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just bought a DS224+ and two WD Red Plus HDD - is there anything else I need to set up my first NAS with the intention of serving media files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9bu7i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709862772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basic question but am I covered in terms of the hardware I need to run a basic media server (either Plex or Jellyfin) ?&lt;/p&gt;\n\n&lt;p&gt;Been planning to start this project for a while, have a bunch of saved resources to get through, but I some what instantaneously decided to start this project &lt;em&gt;now&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;I live in a country where it&amp;#39;s difficult and expensive to buy electronic hardware and I&amp;#39;m currently in the U.S. - want to make sure I have everything I might need before I fly back this weekend.&lt;/p&gt;\n\n&lt;p&gt;Any tips or advice would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9bu7i", "is_robot_indexable": true, "report_reasons": null, "author": "lovely_trequartista", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9bu7i/just_bought_a_ds224_and_two_wd_red_plus_hdd_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9bu7i/just_bought_a_ds224_and_two_wd_red_plus_hdd_is/", "subreddit_subscribers": 737145, "created_utc": 1709862772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, just upgraded my NAS with bigger drives, now I found myself with 2 spare old disks. I would securely wipe and forget about them thinking about give them away or drop them to some local store for a bunch of candies. I have an USB SATA 3.5\" adapter, what's the proper way to decomission and safely wipe everything? \nShred and wipe commands from my Linux machine?", "author_fullname": "t2_h89ybm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS disk decomissioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b98gez", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709853005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, just upgraded my NAS with bigger drives, now I found myself with 2 spare old disks. I would securely wipe and forget about them thinking about give them away or drop them to some local store for a bunch of candies. I have an USB SATA 3.5&amp;quot; adapter, what&amp;#39;s the proper way to decomission and safely wipe everything? \nShred and wipe commands from my Linux machine?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b98gez", "is_robot_indexable": true, "report_reasons": null, "author": "kinghino", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b98gez/nas_disk_decomissioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b98gez/nas_disk_decomissioning/", "subreddit_subscribers": 737145, "created_utc": 1709853005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, as far as I've understood it, this site was captured on [archive.org](https://archive.org). but I've never been able to access it. The entire screen just whites out, what could be causing this? I've never been able to acccess any of the things archived on this blog,  and when you look there are supposed to be several.\n\n[https://web.archive.org/web/20151201000000\\*/http://foolsforluv.blogspot.com/2013/02/BLUDDYVALENTINE.html](https://web.archive.org/web/20151201000000*/http://foolsforluv.blogspot.com/2013/02/BLUDDYVALENTINE.html)\n\n&amp;#x200B;\n\nwhenever I try to click any of these links, it never works.  \n[https://web.archive.org/web/\\*/http://foolsforluv.blogspot.com/\\*](https://web.archive.org/web/*/http://foolsforluv.blogspot.com/*)", "author_fullname": "t2_j3lajj2up", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can't access Archive.org archived site.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b96pbc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709848656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, as far as I&amp;#39;ve understood it, this site was captured on &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;. but I&amp;#39;ve never been able to access it. The entire screen just whites out, what could be causing this? I&amp;#39;ve never been able to acccess any of the things archived on this blog,  and when you look there are supposed to be several.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20151201000000*/http://foolsforluv.blogspot.com/2013/02/BLUDDYVALENTINE.html\"&gt;https://web.archive.org/web/20151201000000*/http://foolsforluv.blogspot.com/2013/02/BLUDDYVALENTINE.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;whenever I try to click any of these links, it never works.&lt;br/&gt;\n&lt;a href=\"https://web.archive.org/web/*/http://foolsforluv.blogspot.com/*\"&gt;https://web.archive.org/web/*/http://foolsforluv.blogspot.com/*&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b96pbc", "is_robot_indexable": true, "report_reasons": null, "author": "BedroomDependent8152", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b96pbc/cant_access_archiveorg_archived_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b96pbc/cant_access_archiveorg_archived_site/", "subreddit_subscribers": 737145, "created_utc": 1709848656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many pics to scan. I had a software called I think HP easy scan that was very good by splitting one scan to several pics.\n\nNow I have a new HP printer (516) and the software I'm using doesn't split them. This is big downside and I don't know what to do. I can't go picture by picture, it's too much. If I can put 2-4 pictures in one scan I can handle it hahaha.\n\nThank you so much for the help!", "author_fullname": "t2_4xllun6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanning old pics to drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b90cvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709832667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many pics to scan. I had a software called I think HP easy scan that was very good by splitting one scan to several pics.&lt;/p&gt;\n\n&lt;p&gt;Now I have a new HP printer (516) and the software I&amp;#39;m using doesn&amp;#39;t split them. This is big downside and I don&amp;#39;t know what to do. I can&amp;#39;t go picture by picture, it&amp;#39;s too much. If I can put 2-4 pictures in one scan I can handle it hahaha.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b90cvg", "is_robot_indexable": true, "report_reasons": null, "author": "Direct_Check_3366", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b90cvg/scanning_old_pics_to_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b90cvg/scanning_old_pics_to_drive/", "subreddit_subscribers": 737145, "created_utc": 1709832667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I run RAID1 on Linux mdadm with spinning HDDs.\n\nI have started getting these warnings from mdstat:\n\n    mdstat mismatch cnt = 128 unsynchronized blocks\n\nDoes this mean that my disks/arrays are damaged for certain?\n\nI previously had this issue before. I could not figure out what was wrong but I ended up replacing the disks. \n\nNow after a short while, this happens again...\n\nI have tried initiating a resin. But this counter increases again after a while. Note that the value is a very round number of 128. It seems to be always a power of 2 value. On the previous disks, I think it was 256 or 512.\n\nSMART does not report anything of note. Neither on the old nor the new disks.", "author_fullname": "t2_nlx42kb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mdstat mismatch cnt: are my disks/data damaged?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8z4jy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709829765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run RAID1 on Linux mdadm with spinning HDDs.&lt;/p&gt;\n\n&lt;p&gt;I have started getting these warnings from mdstat:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;mdstat mismatch cnt = 128 unsynchronized blocks\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Does this mean that my disks/arrays are damaged for certain?&lt;/p&gt;\n\n&lt;p&gt;I previously had this issue before. I could not figure out what was wrong but I ended up replacing the disks. &lt;/p&gt;\n\n&lt;p&gt;Now after a short while, this happens again...&lt;/p&gt;\n\n&lt;p&gt;I have tried initiating a resin. But this counter increases again after a while. Note that the value is a very round number of 128. It seems to be always a power of 2 value. On the previous disks, I think it was 256 or 512.&lt;/p&gt;\n\n&lt;p&gt;SMART does not report anything of note. Neither on the old nor the new disks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8z4jy", "is_robot_indexable": true, "report_reasons": null, "author": "PM_Me_Food_Pics_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8z4jy/mdstat_mismatch_cnt_are_my_disksdata_damaged/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8z4jy/mdstat_mismatch_cnt_are_my_disksdata_damaged/", "subreddit_subscribers": 737145, "created_utc": 1709829765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 4TB Seagate drive with a multitude of issues and very conflicting test results. Basically, I have experienced data corruption, data loss, unexpected noises, and other issues that started out happening often and seriously enough to deem the HDD too unreliable for data storage, and yet, after completing the backup process and giving the HDD some time off (not using it at all, but keeping it turned on), the issues mysteriously improved dramatically, but the drive still feels off, as copied files were still corrupted during the copying process recently.  The SMART logs, Windows Dsk Chk, HD tune, are pretty much all green, as far as I can tell, and I was told these results seemed good on the TechSupport subreddit as well.\n\nBecause of these conflicting results, I would prefer to exchange my drive for a brand new one through Seagate's RMA process before the warranty expires, but I am worried if these conflicting results will cause issues with the RMA process. Does anyone have any experience with sending out a Seagate HDD to RMA, especially with more difficult to detect issues, or with less clear-cut HDD problems? If so, did they fix or send you a new HDD, or did they just returned it back to you without doing anything to fix, or resulted in having trouble/difficulties with Seagate due to them not detecting hardware defects?\n\nDue to the data loss (although only contained to newly-written files past the first 2TB at the time of the data writing) and all the issues experienced, I just can't trust this drive at all until I can find out what caused it to misbehave, so I vastly prefer for Seagate to send me a new one, since this is still covered by the warranty, but I have never RMA'ed a drive before, and since this isn't a clear-cut case, I'm worried if they will really come through, so it would be great to know if based on previous RMA experiences in this subreddit, if my attempt to RMA this untrustworthy and/or faulty drive will likely be successful or not.\n\nThanks!", "author_fullname": "t2_17ez6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Regarding Seagate's RMA process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8ywgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709835965.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709829019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 4TB Seagate drive with a multitude of issues and very conflicting test results. Basically, I have experienced data corruption, data loss, unexpected noises, and other issues that started out happening often and seriously enough to deem the HDD too unreliable for data storage, and yet, after completing the backup process and giving the HDD some time off (not using it at all, but keeping it turned on), the issues mysteriously improved dramatically, but the drive still feels off, as copied files were still corrupted during the copying process recently.  The SMART logs, Windows Dsk Chk, HD tune, are pretty much all green, as far as I can tell, and I was told these results seemed good on the TechSupport subreddit as well.&lt;/p&gt;\n\n&lt;p&gt;Because of these conflicting results, I would prefer to exchange my drive for a brand new one through Seagate&amp;#39;s RMA process before the warranty expires, but I am worried if these conflicting results will cause issues with the RMA process. Does anyone have any experience with sending out a Seagate HDD to RMA, especially with more difficult to detect issues, or with less clear-cut HDD problems? If so, did they fix or send you a new HDD, or did they just returned it back to you without doing anything to fix, or resulted in having trouble/difficulties with Seagate due to them not detecting hardware defects?&lt;/p&gt;\n\n&lt;p&gt;Due to the data loss (although only contained to newly-written files past the first 2TB at the time of the data writing) and all the issues experienced, I just can&amp;#39;t trust this drive at all until I can find out what caused it to misbehave, so I vastly prefer for Seagate to send me a new one, since this is still covered by the warranty, but I have never RMA&amp;#39;ed a drive before, and since this isn&amp;#39;t a clear-cut case, I&amp;#39;m worried if they will really come through, so it would be great to know if based on previous RMA experiences in this subreddit, if my attempt to RMA this untrustworthy and/or faulty drive will likely be successful or not.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8ywgm", "is_robot_indexable": true, "report_reasons": null, "author": "Xenofan23", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8ywgm/regarding_seagates_rma_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8ywgm/regarding_seagates_rma_process/", "subreddit_subscribers": 737145, "created_utc": 1709829019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a bunch of high capacity samsung 512gb sd for a ridicoulsly low price (2\u20ac/tb), i would like to merge them with something like a multiple reader, like a USB hub but for sd, on the Internet I could not find anything, is there something like this or a DIY project, I have some soldering skills I could try to do something?", "author_fullname": "t2_eoa4e7zl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple sd reader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8xxn1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709826505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a bunch of high capacity samsung 512gb sd for a ridicoulsly low price (2\u20ac/tb), i would like to merge them with something like a multiple reader, like a USB hub but for sd, on the Internet I could not find anything, is there something like this or a DIY project, I have some soldering skills I could try to do something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8xxn1", "is_robot_indexable": true, "report_reasons": null, "author": "Loitering14", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8xxn1/multiple_sd_reader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8xxn1/multiple_sd_reader/", "subreddit_subscribers": 737145, "created_utc": 1709826505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On the tuto it says $ ia upload &lt;identifier&gt; file1 file2 --metadata=\"mediatype:texts\" --metadata=\"blah:arg\"\n\nBut does that upload the file to an existing archive?\n\nCan someone please show me how that line looks like normally? Because everything I try gives me an error. I know what the identifier is. I don't know how to write the file. Do I write the entire path? Do I include the .mp3? When I use &lt; and &gt; it says it was unexpected or something like that. So can you show me the line you would use to upload a file, or multiple, to an existing archive?\n\nEdit : And I can't (on the site itself) change the name of a file. It doesn't work. I change it, click on \"done editing\", and it doesn't change. What am I doing wrong?", "author_fullname": "t2_ymgr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Internet Archive) How to use the python to upload to existing archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8sakj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709810570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the tuto it says $ ia upload &amp;lt;identifier&amp;gt; file1 file2 --metadata=&amp;quot;mediatype:texts&amp;quot; --metadata=&amp;quot;blah:arg&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But does that upload the file to an existing archive?&lt;/p&gt;\n\n&lt;p&gt;Can someone please show me how that line looks like normally? Because everything I try gives me an error. I know what the identifier is. I don&amp;#39;t know how to write the file. Do I write the entire path? Do I include the .mp3? When I use &amp;lt; and &amp;gt; it says it was unexpected or something like that. So can you show me the line you would use to upload a file, or multiple, to an existing archive?&lt;/p&gt;\n\n&lt;p&gt;Edit : And I can&amp;#39;t (on the site itself) change the name of a file. It doesn&amp;#39;t work. I change it, click on &amp;quot;done editing&amp;quot;, and it doesn&amp;#39;t change. What am I doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8sakj", "is_robot_indexable": true, "report_reasons": null, "author": "PouffieEdc", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8sakj/internet_archive_how_to_use_the_python_to_upload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8sakj/internet_archive_how_to_use_the_python_to_upload/", "subreddit_subscribers": 737145, "created_utc": 1709810570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellow hoarders, long time lurker and first-time question asker here.\n\nI currently have \\~8tb of photos and video files from various travels, which I've been storing across a separate 2-bay NAS and a couple of RAID-1 drives on my PC. Once I've edited/printed/published the stuff that I want to keep, the rest is really just left in storage 'just in case', as I'm sure most of you understand!\n\nAnyway, storage space on these devices is now running low, and I probably need about \\~1.5TB/year, so I was looking at buying a shiny new DS923+ to keep my original solution going. However, this got me thinking: **instead, could I just buy a new PC and then fill my old PC with hard drives in RAID as a sort of cold storage backup, only powering it on when I need to backup a new set of photos/videos?** I don't need 24/7 access to the data over the network, and I could run data integrity checks every few months when I actually power it on (...couldn't I? Is there any software that does this in this way?)\n\nAm I missing something here, or would a new NAS (or something else) be the better solution? My thinking is that \u00a31k spent on a new NAS and HDDs is half way to a brand new computer, and I already have an existing NAS for films, music etc that I *do* want 24/7 access to. It wouldn't be the end of the world if the files were lost, so I'm not worried about a full 3/2/1 backup solution.\n\nSomewhat surprisingly, I just can't seem to find an answer online that covers my particular use-case, so any views/suggestions would be gratefully appreciated!", "author_fullname": "t2_v25tbbcqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using an old PC for cold(ish) backup storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8rtol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709808900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow hoarders, long time lurker and first-time question asker here.&lt;/p&gt;\n\n&lt;p&gt;I currently have ~8tb of photos and video files from various travels, which I&amp;#39;ve been storing across a separate 2-bay NAS and a couple of RAID-1 drives on my PC. Once I&amp;#39;ve edited/printed/published the stuff that I want to keep, the rest is really just left in storage &amp;#39;just in case&amp;#39;, as I&amp;#39;m sure most of you understand!&lt;/p&gt;\n\n&lt;p&gt;Anyway, storage space on these devices is now running low, and I probably need about ~1.5TB/year, so I was looking at buying a shiny new DS923+ to keep my original solution going. However, this got me thinking: &lt;strong&gt;instead, could I just buy a new PC and then fill my old PC with hard drives in RAID as a sort of cold storage backup, only powering it on when I need to backup a new set of photos/videos?&lt;/strong&gt; I don&amp;#39;t need 24/7 access to the data over the network, and I could run data integrity checks every few months when I actually power it on (...couldn&amp;#39;t I? Is there any software that does this in this way?)&lt;/p&gt;\n\n&lt;p&gt;Am I missing something here, or would a new NAS (or something else) be the better solution? My thinking is that \u00a31k spent on a new NAS and HDDs is half way to a brand new computer, and I already have an existing NAS for films, music etc that I &lt;em&gt;do&lt;/em&gt; want 24/7 access to. It wouldn&amp;#39;t be the end of the world if the files were lost, so I&amp;#39;m not worried about a full 3/2/1 backup solution.&lt;/p&gt;\n\n&lt;p&gt;Somewhat surprisingly, I just can&amp;#39;t seem to find an answer online that covers my particular use-case, so any views/suggestions would be gratefully appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8rtol", "is_robot_indexable": true, "report_reasons": null, "author": "toasty_trifle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8rtol/using_an_old_pc_for_coldish_backup_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8rtol/using_an_old_pc_for_coldish_backup_storage/", "subreddit_subscribers": 737145, "created_utc": 1709808900.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}