{"kind": "Listing", "data": {"after": "t3_1b8ywgm", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "An analysis of DOIs suggests that digital preservation is not keeping up with burgeoning scholarly knowledge.", "author_fullname": "t2_8ztqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Millions of research papers at risk of disappearing from the Internet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1b97hzi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 392, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 392, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/t-GLhfzbts7OaxBsljTaZ98mZClXnwwC7PSqRUkRVas.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709850587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "nature.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;An analysis of DOIs suggests that digital preservation is not keeping up with burgeoning scholarly knowledge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.nature.com/articles/d41586-024-00616-5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?auto=webp&amp;s=25f1969367754f109a0ebe67ee18cd4c19137292", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e9918ad1ad9123bb023b1fa35eff194fa99f2a5", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7488d4727a0f46e003cab13d90a19f5c5e6c65e2", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2848d1ca7d59eb85c96c6458b3805d33f52a021b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7102bef0fa982c27c1d084e5ddfd50fe02cf553f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Zolv08WE_4M5Gzvysp-THa_cws0Qn74AJ0ZLVw9RgpA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef1c943f15c8f896808c0a1703b7e0a2c143513e", "width": 960, "height": 540}], "variants": {}, "id": "9CymYjs6R6CONIR46BpXETBmjrlmCu5Zhwaui5rEl_g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b97hzi", "is_robot_indexable": true, "report_reasons": null, "author": "peliciego", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b97hzi/millions_of_research_papers_at_risk_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.nature.com/articles/d41586-024-00616-5", "subreddit_subscribers": 737168, "created_utc": 1709850587.0, "num_crossposts": 3, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have nearly 5 terabytes of games, websites, music, documents and software on my Linux RAID. What is a good way to efficiently search through this mess? Simple file search (say via \\`find\\`) takes forever and isn't very fast when I enable regexes. Another type of query I want to perform is about the content of the files, I use \\`silversearcher-ag\\` for this and its reasonably fast but still a pain to use and sometimes very slow.\n\nIs there any tool that can index this properly and be better at searching both files with certain name and/or files with certain data. Bonus points if it has some web ui so that I can make it available over the network.", "author_fullname": "t2_bhm4t9j3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you efficiently search through your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8wqyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709823646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have nearly 5 terabytes of games, websites, music, documents and software on my Linux RAID. What is a good way to efficiently search through this mess? Simple file search (say via `find`) takes forever and isn&amp;#39;t very fast when I enable regexes. Another type of query I want to perform is about the content of the files, I use `silversearcher-ag` for this and its reasonably fast but still a pain to use and sometimes very slow.&lt;/p&gt;\n\n&lt;p&gt;Is there any tool that can index this properly and be better at searching both files with certain name and/or files with certain data. Bonus points if it has some web ui so that I can make it available over the network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8wqyd", "is_robot_indexable": true, "report_reasons": null, "author": "Emergency_Apricot_77", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8wqyd/how_do_you_efficiently_search_through_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8wqyd/how_do_you_efficiently_search_through_your_data/", "subreddit_subscribers": 737168, "created_utc": 1709823646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello. I received this hard drive from a friend who bought it on ebay. The drive states it's a pre-production sample, but googling the model number doesn't provide any results. Does anyone know what model of hard drive this is a pre prod sample too? I've yet to get it to power on, but may be due to the 3.3 volt rail, still troubleshooting", "author_fullname": "t2_5vaqz7s0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 22TB \"Pre-production sample\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b95zv5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 40, "domain": "i.redd.it", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/pucK1rJJsVfk4jWEpSrd0bP7gCJNr8-0IQCUuiIBbz0.jpg", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709847002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I received this hard drive from a friend who bought it on ebay. The drive states it&amp;#39;s a pre-production sample, but googling the model number doesn&amp;#39;t provide any results. Does anyone know what model of hard drive this is a pre prod sample too? I&amp;#39;ve yet to get it to power on, but may be due to the 3.3 volt rail, still troubleshooting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/sxqkvx00dzmc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?auto=webp&amp;s=78871d0ada3111f7f622711c5c6faa35c90920db", "width": 2252, "height": 3776}, "resolutions": [{"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fbcee879ef51bf4f8b2f76c7ca7162890eb4566", "width": 108, "height": 181}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=248c073ecf7aa30cd1a6fbaec999a4663bf993c4", "width": 216, "height": 362}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=834e3354b38bb170fd0c77993ed23d84a157c3e7", "width": 320, "height": 536}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5efd78384da521ab375e112103deb8b9496ef9a", "width": 640, "height": 1073}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ceb69cb323788e00244b6374eb6f66c5114ee2c6", "width": 960, "height": 1609}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49fe940f22b766c3fc08aef6f1efc412ca523d15", "width": 1080, "height": 1810}], "variants": {}, "id": "su-VYCZDt-hjhfHiJZ2juCLVAreXI3sNd1RI15mur8I"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b95zv5", "is_robot_indexable": true, "report_reasons": null, "author": "saints0963", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b95zv5/wd_22tb_preproduction_sample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/sxqkvx00dzmc1.jpeg", "subreddit_subscribers": 737168, "created_utc": 1709847002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys. Thanks in advance for any advice.\n\nI am one of a team of moderators for a large pop music forum. We have tens of thousands of threads going back to October 2011 on our forums.\n\nApproximately 1 month ago, our Admin disappeared completely off the web. We have no way to contact him IRL, and he controlled all of the technical aspects of the website, and all socials,  etc relating to it. \n\nWe are very concerned for his safety as he lived and worked in a country that is currently at war and has recently banned all things related to LGBTQ. \n\n(The website is indirectly related to the LGBTQ community.)\n\nHe was a protester against this country and its dictator and we are very nervous since he has been arrested more than once before , that this time it may have been for good. \n\nMy question is, how can I train myself in a limited amount of time to back up a full Invision Power Boards forum? How big a harddrive should I buy? I know nothing of coding.  (We all have little to no money to spend on this, unless we made a payment plan with a company like Apify for example.)\n\nWe want to make a full copy of every forum, every thread/ topic,  and all the user profiles and all posts. We have several hidden sections only mods can see or only certain community members can see as well, so I can't hand over my log in info to someone, I'd need to run the crawler myself. Especially since there is sensitive data such as user ip addresses,  etc. \n\nBack in 2020, we had a moderator who made perfect backups of this site herself, that she ran on her own server, but her health has declined to the point she can no longer do it. When she did, It took her 2 to 3 months of running the crawler day and night to archive all posts from 2020 to 2011. \n\nWith the disappearance of our Admin, we are very concerned as we have no idea how the payments system for our forum went, or when the plug could be pulled. He handled all glitches or downtime or DDOS / server problems  himself and was always very accessible. We are now vulnerable and worried for his safety and our forums' future. \n\nA full backup of our content, even if only public facing, is valuable to the public fans of this pop musician. \n\nAny help or advice you can offer would be fantastic. \n\nThe website is:\n\nwww.gagadaily.com\n\nOur archives are here (link pending) and to give you an idea of what they once looked like when we did crawl them:\n\nThis is the perfect-ish copy our fellow mod made in 2020. You can get an idea from this as to what we want to achieve as far as a Read Only record of our forum: \n\nhttps://web.archive.org/web/20221012192112/http://gagaverse.com/archives/GagaDaily3_2020_03_07/Forums/\n\n\nAs you can see, the \"tree\" and \"branches\" of the forum went pretty deep, sometimes with hundreds of pages per thread:\n\nhttps://web.archive.org/web/20221011211452/http://gagaverse.com/archives/GagaDaily3_2020_03_07/Forums/Lady_Gaga/Gaga_Thoughts/index.html\n\n\n\n", "author_fullname": "t2_d8mr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Our forum owner may have been imprisoned / drafted /killed in a wartorn country.They have left no backup plans to us. What is the best webcrawler to preserve our massive forum if the payments should lapse? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9h9sd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709879701.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709879046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. Thanks in advance for any advice.&lt;/p&gt;\n\n&lt;p&gt;I am one of a team of moderators for a large pop music forum. We have tens of thousands of threads going back to October 2011 on our forums.&lt;/p&gt;\n\n&lt;p&gt;Approximately 1 month ago, our Admin disappeared completely off the web. We have no way to contact him IRL, and he controlled all of the technical aspects of the website, and all socials,  etc relating to it. &lt;/p&gt;\n\n&lt;p&gt;We are very concerned for his safety as he lived and worked in a country that is currently at war and has recently banned all things related to LGBTQ. &lt;/p&gt;\n\n&lt;p&gt;(The website is indirectly related to the LGBTQ community.)&lt;/p&gt;\n\n&lt;p&gt;He was a protester against this country and its dictator and we are very nervous since he has been arrested more than once before , that this time it may have been for good. &lt;/p&gt;\n\n&lt;p&gt;My question is, how can I train myself in a limited amount of time to back up a full Invision Power Boards forum? How big a harddrive should I buy? I know nothing of coding.  (We all have little to no money to spend on this, unless we made a payment plan with a company like Apify for example.)&lt;/p&gt;\n\n&lt;p&gt;We want to make a full copy of every forum, every thread/ topic,  and all the user profiles and all posts. We have several hidden sections only mods can see or only certain community members can see as well, so I can&amp;#39;t hand over my log in info to someone, I&amp;#39;d need to run the crawler myself. Especially since there is sensitive data such as user ip addresses,  etc. &lt;/p&gt;\n\n&lt;p&gt;Back in 2020, we had a moderator who made perfect backups of this site herself, that she ran on her own server, but her health has declined to the point she can no longer do it. When she did, It took her 2 to 3 months of running the crawler day and night to archive all posts from 2020 to 2011. &lt;/p&gt;\n\n&lt;p&gt;With the disappearance of our Admin, we are very concerned as we have no idea how the payments system for our forum went, or when the plug could be pulled. He handled all glitches or downtime or DDOS / server problems  himself and was always very accessible. We are now vulnerable and worried for his safety and our forums&amp;#39; future. &lt;/p&gt;\n\n&lt;p&gt;A full backup of our content, even if only public facing, is valuable to the public fans of this pop musician. &lt;/p&gt;\n\n&lt;p&gt;Any help or advice you can offer would be fantastic. &lt;/p&gt;\n\n&lt;p&gt;The website is:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.gagadaily.com\"&gt;www.gagadaily.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Our archives are here (link pending) and to give you an idea of what they once looked like when we did crawl them:&lt;/p&gt;\n\n&lt;p&gt;This is the perfect-ish copy our fellow mod made in 2020. You can get an idea from this as to what we want to achieve as far as a Read Only record of our forum: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20221012192112/http://gagaverse.com/archives/GagaDaily3_2020_03_07/Forums/\"&gt;https://web.archive.org/web/20221012192112/http://gagaverse.com/archives/GagaDaily3_2020_03_07/Forums/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As you can see, the &amp;quot;tree&amp;quot; and &amp;quot;branches&amp;quot; of the forum went pretty deep, sometimes with hundreds of pages per thread:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20221011211452/http://gagaverse.com/archives/GagaDaily3_2020_03_07/Forums/Lady_Gaga/Gaga_Thoughts/index.html\"&gt;https://web.archive.org/web/20221011211452/http://gagaverse.com/archives/GagaDaily3_2020_03_07/Forums/Lady_Gaga/Gaga_Thoughts/index.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9h9sd", "is_robot_indexable": true, "report_reasons": null, "author": "ChicaSkas", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9h9sd/our_forum_owner_may_have_been_imprisoned_drafted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9h9sd/our_forum_owner_may_have_been_imprisoned_drafted/", "subreddit_subscribers": 737168, "created_utc": 1709879046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In my university class we are doing a time capsule for our university to be opened 50 years from now. It is my job to figure out how to have data last that long. So far I've determined the best way to go about it would be to print our presentations on paper designed to last a long time. But in the event other presenters would like to have music or videos in their presentation I'd also like to prepare for a digital method that could last 50 years in a time capsule.\n\nFeedback would be greatly appreciated, thank you.", "author_fullname": "t2_2sjzxx2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ensure digital data lasts 50 years without interaction or additional cost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b963ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709847234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my university class we are doing a time capsule for our university to be opened 50 years from now. It is my job to figure out how to have data last that long. So far I&amp;#39;ve determined the best way to go about it would be to print our presentations on paper designed to last a long time. But in the event other presenters would like to have music or videos in their presentation I&amp;#39;d also like to prepare for a digital method that could last 50 years in a time capsule.&lt;/p&gt;\n\n&lt;p&gt;Feedback would be greatly appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b963ig", "is_robot_indexable": true, "report_reasons": null, "author": "PiggiesGoSqueal", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b963ig/how_to_ensure_digital_data_lasts_50_years_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b963ig/how_to_ensure_digital_data_lasts_50_years_without/", "subreddit_subscribers": 737168, "created_utc": 1709847234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_bdveu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "After a week of running checmsum verifications and fixing any errors to migrate to a new server I can finally start..... building another checksum!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 33, "top_awarded_type": null, "hide_score": false, "name": "t3_1b98uyg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/freF4W-Dru1WOVKzKLzjh-eYVh_u330YRBeWelWTXrI.jpg", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709854254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ua2qn1bjyzmc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?auto=webp&amp;s=55506dbcbc69c6a671af9bb7345e83fcc8e5f109", "width": 1917, "height": 461}, "resolutions": [{"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7be9143b0398b9c9f9d344428a3a4b5bb313dad0", "width": 108, "height": 25}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=05dc4177640de76517ff5a67b459fea181cfc6f5", "width": 216, "height": 51}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=00705a436bddbc222b547a2e54e7d8570f13628b", "width": 320, "height": 76}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec844c37fe4d6fe6dbe7432fc9163e1dbcb54a9b", "width": 640, "height": 153}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e8f5a94db611b96ab6aaaea7dd707e8b3b31342", "width": 960, "height": 230}, {"url": "https://preview.redd.it/ua2qn1bjyzmc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f27b438bbc182d535a544d1f3526616f969591ec", "width": 1080, "height": 259}], "variants": {}, "id": "n-eRrMpUyqGRTL2tXaGB_7fCg4VPbvbeqiIi0eKRfgA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "My backups are on floppies.", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b98uyg", "is_robot_indexable": true, "report_reasons": null, "author": "XOIIO", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b98uyg/after_a_week_of_running_checmsum_verifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ua2qn1bjyzmc1.jpeg", "subreddit_subscribers": 737168, "created_utc": 1709854254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_bsklr77o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "StableBid Cloud will stop working with Google on 5/15/2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b98xzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CjmjH1RgR0J44awrLRlz3knRb-kQrsze6G_Bp33h9oo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709854504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/q9wzxxabyzmc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/q9wzxxabyzmc1.png?auto=webp&amp;s=95feda381ce100f6f1e17de4005faf2372098e3c", "width": 632, "height": 698}, "resolutions": [{"url": "https://preview.redd.it/q9wzxxabyzmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=625968463c9c7d41f9c3052da90ea51f08e7858e", "width": 108, "height": 119}, {"url": "https://preview.redd.it/q9wzxxabyzmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa86d65ac1b20bf51489a6c78a1a785966b631ee", "width": 216, "height": 238}, {"url": "https://preview.redd.it/q9wzxxabyzmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e3a6c3a6da8e0a8920b97f5eec8e3b493d4df8e", "width": 320, "height": 353}], "variants": {}, "id": "sY3hqrrMf8DdQDDatMtm3eSSQriHOHIzO0tmRqUzjMk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b98xzl", "is_robot_indexable": true, "report_reasons": null, "author": "Fish_Fellatio", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b98xzl/stablebid_cloud_will_stop_working_with_google_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/q9wzxxabyzmc1.png", "subreddit_subscribers": 737168, "created_utc": 1709854504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First of all let me say that I've spent weeks digging through this sub's valuable information in terms of optical media archiving, and stumbled upon many of the Verbatim-CMC-HTL-LTH comments that were very insightful. However, I wasn't be able to read a definitive answer to the actual state of archiving data on off-the-shelf media in 2023.\n\nFirst and foremost, with all the *Blu-ray M-Disc not being real M-Discs anymore* things going on, what is a realistic expectation of data longevity, when discs are stored under normal everyday conditions (e.g. outside of the 10-25*C temperature and 40-60% humidity range)? Is it still worth considering optical media to get at least a decade of stress free archiving or does media produced in the recent years show significant fallout rates even after short periods of time? What's the actual return on invest on today's media, even speaking about regular \"cheap\" Blu-ray discs and what would be the recommended re-burn interval based on the little experience with post-2019 media that there might be?\n\nThank you!", "author_fullname": "t2_kigcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realistic expectation of Blu-ray M-Disc life in 2023 under everyday conditions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8xe10", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709825215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all let me say that I&amp;#39;ve spent weeks digging through this sub&amp;#39;s valuable information in terms of optical media archiving, and stumbled upon many of the Verbatim-CMC-HTL-LTH comments that were very insightful. However, I wasn&amp;#39;t be able to read a definitive answer to the actual state of archiving data on off-the-shelf media in 2023.&lt;/p&gt;\n\n&lt;p&gt;First and foremost, with all the &lt;em&gt;Blu-ray M-Disc not being real M-Discs anymore&lt;/em&gt; things going on, what is a realistic expectation of data longevity, when discs are stored under normal everyday conditions (e.g. outside of the 10-25*C temperature and 40-60% humidity range)? Is it still worth considering optical media to get at least a decade of stress free archiving or does media produced in the recent years show significant fallout rates even after short periods of time? What&amp;#39;s the actual return on invest on today&amp;#39;s media, even speaking about regular &amp;quot;cheap&amp;quot; Blu-ray discs and what would be the recommended re-burn interval based on the little experience with post-2019 media that there might be?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8xe10", "is_robot_indexable": true, "report_reasons": null, "author": "mrusme", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8xe10/realistic_expectation_of_bluray_mdisc_life_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8xe10/realistic_expectation_of_bluray_mdisc_life_in/", "subreddit_subscribers": 737168, "created_utc": 1709825215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I have an James Bond Volume 2 set which has a infinite loop in the menus, HOW DO I JUST RIP THE DVD!", "author_fullname": "t2_ah8awy9z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HELP BYPASS DRM INFINITE MENU LOOP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9g3lw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709875163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an James Bond Volume 2 set which has a infinite loop in the menus, HOW DO I JUST RIP THE DVD!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9g3lw", "is_robot_indexable": true, "report_reasons": null, "author": "mindeloo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9g3lw/help_bypass_drm_infinite_menu_loop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9g3lw/help_bypass_drm_infinite_menu_loop/", "subreddit_subscribers": 737168, "created_utc": 1709875163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I need to recover some data from a very old HDD that i don't feel super safe just plugging in my PC.\n\nI thought of live booting Ubuntu of a usb stick, but have also read about kali live with forensic mode and stuff, keep in mind i'm not very familiar with linux, which is why i'm looking for suggestions.\n\nI would transfer that data to a usb stick, how should i tackle scanning it after?", "author_fullname": "t2_b5suwht0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to retrieve data from a potentially infected HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b96hef", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709848142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I need to recover some data from a very old HDD that i don&amp;#39;t feel super safe just plugging in my PC.&lt;/p&gt;\n\n&lt;p&gt;I thought of live booting Ubuntu of a usb stick, but have also read about kali live with forensic mode and stuff, keep in mind i&amp;#39;m not very familiar with linux, which is why i&amp;#39;m looking for suggestions.&lt;/p&gt;\n\n&lt;p&gt;I would transfer that data to a usb stick, how should i tackle scanning it after?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b96hef", "is_robot_indexable": true, "report_reasons": null, "author": "banekal", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b96hef/best_way_to_retrieve_data_from_a_potentially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b96hef/best_way_to_retrieve_data_from_a_potentially/", "subreddit_subscribers": 737168, "created_utc": 1709848142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Need to create a storage solution at remote sites. These will reside in awkward and/or smaller spaces without racks. A good-sized shed that's protected from the elements but no HVAC, for example. It will store surveillance footage, so 24/7 writing and occasional reading. One other caveat is that it has to run windows. Our VMS recording application is windows only. A pre-built NAS would obviously work but then it turns into two devices to manage instead of one. Since I'd still have a PC running the software. Because of that, I'd rather stick to a PC-only solution, but not married to it.\n\nOne option is simply building a PC with a bunch of spinners. But it needs some resiliency. I manage them, but I\u2019ve never built out a RAID device myself from scratch. I\u2019ve read not to use diskmgmt raid or storage spaces. Though some say if you're just running a mirror, no parity, storage spaces can be fine. This doesn't need to be fancy. A RAID 1 would satisfy the requirements. Maybe storage spaces is fine? Any other decent software raid options out there? Thoughts on Stablebit Drivepool? I would need the storage to show up in Windows as one drive.\n\nAnother option is to run a hardware raid card in the build. I've read that using some consumer level card like startech is basically useless. It looks like a decent LSI enterprise card with caching and BBU runs \\~$1,000? At that price, it may be better to get smaller Dell or HP server tower instead of trying to piece something together from scratch.\n\nAnd of course, the \"easiest\" solution is to just get a pre-built nas and call it day.\n\nKind of looking for a conversation here. Thoughts on the some of the assumptions made due to what I've read so far. Some real-world experience with any of the outlined options. Again, it's going to live in outdoor temperatures. Around here anywhere from 80-100 in the summer, down to 10-30 in the winter. Not really sure if any options are going to hold up better than others given that.", "author_fullname": "t2_qacpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CCTV Storage Solutions. Outside?!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b903d7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709838870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709832051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need to create a storage solution at remote sites. These will reside in awkward and/or smaller spaces without racks. A good-sized shed that&amp;#39;s protected from the elements but no HVAC, for example. It will store surveillance footage, so 24/7 writing and occasional reading. One other caveat is that it has to run windows. Our VMS recording application is windows only. A pre-built NAS would obviously work but then it turns into two devices to manage instead of one. Since I&amp;#39;d still have a PC running the software. Because of that, I&amp;#39;d rather stick to a PC-only solution, but not married to it.&lt;/p&gt;\n\n&lt;p&gt;One option is simply building a PC with a bunch of spinners. But it needs some resiliency. I manage them, but I\u2019ve never built out a RAID device myself from scratch. I\u2019ve read not to use diskmgmt raid or storage spaces. Though some say if you&amp;#39;re just running a mirror, no parity, storage spaces can be fine. This doesn&amp;#39;t need to be fancy. A RAID 1 would satisfy the requirements. Maybe storage spaces is fine? Any other decent software raid options out there? Thoughts on Stablebit Drivepool? I would need the storage to show up in Windows as one drive.&lt;/p&gt;\n\n&lt;p&gt;Another option is to run a hardware raid card in the build. I&amp;#39;ve read that using some consumer level card like startech is basically useless. It looks like a decent LSI enterprise card with caching and BBU runs ~$1,000? At that price, it may be better to get smaller Dell or HP server tower instead of trying to piece something together from scratch.&lt;/p&gt;\n\n&lt;p&gt;And of course, the &amp;quot;easiest&amp;quot; solution is to just get a pre-built nas and call it day.&lt;/p&gt;\n\n&lt;p&gt;Kind of looking for a conversation here. Thoughts on the some of the assumptions made due to what I&amp;#39;ve read so far. Some real-world experience with any of the outlined options. Again, it&amp;#39;s going to live in outdoor temperatures. Around here anywhere from 80-100 in the summer, down to 10-30 in the winter. Not really sure if any options are going to hold up better than others given that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b903d7", "is_robot_indexable": true, "report_reasons": null, "author": "sageRJ", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b903d7/cctv_storage_solutions_outside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b903d7/cctv_storage_solutions_outside/", "subreddit_subscribers": 737168, "created_utc": 1709832051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In 2022, I somehow converted my entire collection at the time into a txt file, and I've been wanting to update it for so long but couldn't remember how I did that. I'm not exactly technology illiterate, but I'm not well versed in a lot of computer things. I figured out today that the original copies are in .wpl format, meaning I clearly used Windows Media Player to do the conversion but couldn't get it to stop crashing when trying to do what I did previously. \n\nSo I used CMD prompt, edited the stuff I didn't need in Notepad++, and now despite it being messy as *fuck* due to me having to omit a lot of characters that were also in song names, I'm thankful to finally have the txt files in case I lose my collection for some reason!! \n\nNow my next hurdle- sorting my photo collection from 2009-present *(shudders)*", "author_fullname": "t2_7l2p5s3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just updated my music collection into txt file format- had to use a less clean method, but the txt files of all my music from 2010-present are now fully updated!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9hj3m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Show 'n Tell", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709879953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In 2022, I somehow converted my entire collection at the time into a txt file, and I&amp;#39;ve been wanting to update it for so long but couldn&amp;#39;t remember how I did that. I&amp;#39;m not exactly technology illiterate, but I&amp;#39;m not well versed in a lot of computer things. I figured out today that the original copies are in .wpl format, meaning I clearly used Windows Media Player to do the conversion but couldn&amp;#39;t get it to stop crashing when trying to do what I did previously. &lt;/p&gt;\n\n&lt;p&gt;So I used CMD prompt, edited the stuff I didn&amp;#39;t need in Notepad++, and now despite it being messy as &lt;em&gt;fuck&lt;/em&gt; due to me having to omit a lot of characters that were also in song names, I&amp;#39;m thankful to finally have the txt files in case I lose my collection for some reason!! &lt;/p&gt;\n\n&lt;p&gt;Now my next hurdle- sorting my photo collection from 2009-present &lt;em&gt;(shudders)&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b9hj3m", "is_robot_indexable": true, "report_reasons": null, "author": "shinnith", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9hj3m/just_updated_my_music_collection_into_txt_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9hj3m/just_updated_my_music_collection_into_txt_file/", "subreddit_subscribers": 737168, "created_utc": 1709879953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi storage geeks. I currently use Snapraid on my data drives with 2 drives dedicated to parity. I'm thinking about ditching this approach and moving to a RAID setup with btrfs.\n\nI have about 68.4TB usable spread over 7 disks and am using about 20.4TB so plenty of space for parity and snaps.\n\nI do backup my essential data with borg to 6TB of offsite VPS.\n\nThis is what I have to play with:\n\n    \u2502 MOUNTED ON  \u2502    SIZE \u2502   USED \u2502  AVAIL \u2502  USE% \u2502 TYPE \u2502\n    \u2502 /           \u2502  116.0G \u2502  61.9G \u2502  54.1G \u2502 53.3% \u2502 xfs  \u2502\n    \u2502             \u2502         \u2502        \u2502        \u2502       \u2502      \u2502\n    \u2502 /boot       \u2502 1014.0M \u2502 400.7M \u2502 613.3M \u2502 39.5% \u2502 xfs  \u2502\n    \u2502 /boot/efi   \u2502  598.8M \u2502   7.4M \u2502 591.4M \u2502  1.2% \u2502 vfat \u2502\n    \u2502 /mnt/data1  \u2502   10.8T \u2502   7.6T \u2502   2.7T \u2502 69.8% \u2502 ext4 \u2502\n    \u2502 /mnt/data2  \u2502   10.8T \u2502   4.6T \u2502   5.7T \u2502 42.5% \u2502 ext4 \u2502\n    \u2502 /mnt/data3  \u2502   10.8T \u2502   8.0T \u2502   2.3T \u2502 73.9% \u2502 ext4 \u2502\n    \u2502 /mnt/data4  \u2502    3.6T \u2502 176.6M \u2502   3.4T \u2502  0.0% \u2502 ext4 \u2502\n    \u2502 /mnt/diskp1 \u2502   10.8T \u2502   8.2T \u2502   2.0T \u2502 76.2% \u2502 ext4 \u2502\n    \u2502 /mnt/diskp2 \u2502   10.8T \u2502   8.2T \u2502   2.0T \u2502 76.2% \u2502 ext4 \u2502\n    \n    + not yet installed another 10.8T\n\nI was thinking maybe 2 x RAID5 groups with 2D+1P 10.8TB drives or should I RAID 6 the whole lot of large drives in one large group?\n\nAppreciate any opinions on a set up that provides some level of protection from drive failure. I'm going round in circles trying to decide on an optimum layout.", "author_fullname": "t2_mo11s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pls recommend a storage layout", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9exds", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709871561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi storage geeks. I currently use Snapraid on my data drives with 2 drives dedicated to parity. I&amp;#39;m thinking about ditching this approach and moving to a RAID setup with btrfs.&lt;/p&gt;\n\n&lt;p&gt;I have about 68.4TB usable spread over 7 disks and am using about 20.4TB so plenty of space for parity and snaps.&lt;/p&gt;\n\n&lt;p&gt;I do backup my essential data with borg to 6TB of offsite VPS.&lt;/p&gt;\n\n&lt;p&gt;This is what I have to play with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\u2502 MOUNTED ON  \u2502    SIZE \u2502   USED \u2502  AVAIL \u2502  USE% \u2502 TYPE \u2502\n\u2502 /           \u2502  116.0G \u2502  61.9G \u2502  54.1G \u2502 53.3% \u2502 xfs  \u2502\n\u2502             \u2502         \u2502        \u2502        \u2502       \u2502      \u2502\n\u2502 /boot       \u2502 1014.0M \u2502 400.7M \u2502 613.3M \u2502 39.5% \u2502 xfs  \u2502\n\u2502 /boot/efi   \u2502  598.8M \u2502   7.4M \u2502 591.4M \u2502  1.2% \u2502 vfat \u2502\n\u2502 /mnt/data1  \u2502   10.8T \u2502   7.6T \u2502   2.7T \u2502 69.8% \u2502 ext4 \u2502\n\u2502 /mnt/data2  \u2502   10.8T \u2502   4.6T \u2502   5.7T \u2502 42.5% \u2502 ext4 \u2502\n\u2502 /mnt/data3  \u2502   10.8T \u2502   8.0T \u2502   2.3T \u2502 73.9% \u2502 ext4 \u2502\n\u2502 /mnt/data4  \u2502    3.6T \u2502 176.6M \u2502   3.4T \u2502  0.0% \u2502 ext4 \u2502\n\u2502 /mnt/diskp1 \u2502   10.8T \u2502   8.2T \u2502   2.0T \u2502 76.2% \u2502 ext4 \u2502\n\u2502 /mnt/diskp2 \u2502   10.8T \u2502   8.2T \u2502   2.0T \u2502 76.2% \u2502 ext4 \u2502\n\n+ not yet installed another 10.8T\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I was thinking maybe 2 x RAID5 groups with 2D+1P 10.8TB drives or should I RAID 6 the whole lot of large drives in one large group?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any opinions on a set up that provides some level of protection from drive failure. I&amp;#39;m going round in circles trying to decide on an optimum layout.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9exds", "is_robot_indexable": true, "report_reasons": null, "author": "Finno_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9exds/pls_recommend_a_storage_layout/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9exds/pls_recommend_a_storage_layout/", "subreddit_subscribers": 737168, "created_utc": 1709871561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I wanted to start a thread on this topic because, even though it comes up frequently, there isn't a list of recommended cloud storage providers in the wiki which feels like a bit of a gap currently.\n\nMy use case: I run a Synology NAS at home to backup family photos/videos/files/media and always use an offsite cloud backup of my NAS for just-in-case retrieval. Over the years I've experimented with a ton of different options:\n\n* Google Drive Unlimited (RIP)\n* Amazon S3 (reliable, but expensive)\n* Amazon Glacier (reliable, cheap, but hard to use and extremely expensive if you need to actually retrieve your data)\n* iDrive (looks relatively inexpensive but have heard terrible things)\n* Backblaze B2 (good cost, but recently got more expensive)\n* Storj (decentralized, durable (11 9's), S3-compatible APIs, and extremely inexpensive -- $4/TB/mo)\n\nI've recently switched over to use Storj as my cloud backup provider, simply because they have 11 9's of durability, the lowest cost per TB I've been able to find, and an S3-compatible API so they work with pretty much every tool.\n\nWhat are you using?", "author_fullname": "t2_4g95l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest Cloud Storage Providers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9czns", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709865971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to start a thread on this topic because, even though it comes up frequently, there isn&amp;#39;t a list of recommended cloud storage providers in the wiki which feels like a bit of a gap currently.&lt;/p&gt;\n\n&lt;p&gt;My use case: I run a Synology NAS at home to backup family photos/videos/files/media and always use an offsite cloud backup of my NAS for just-in-case retrieval. Over the years I&amp;#39;ve experimented with a ton of different options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Google Drive Unlimited (RIP)&lt;/li&gt;\n&lt;li&gt;Amazon S3 (reliable, but expensive)&lt;/li&gt;\n&lt;li&gt;Amazon Glacier (reliable, cheap, but hard to use and extremely expensive if you need to actually retrieve your data)&lt;/li&gt;\n&lt;li&gt;iDrive (looks relatively inexpensive but have heard terrible things)&lt;/li&gt;\n&lt;li&gt;Backblaze B2 (good cost, but recently got more expensive)&lt;/li&gt;\n&lt;li&gt;Storj (decentralized, durable (11 9&amp;#39;s), S3-compatible APIs, and extremely inexpensive -- $4/TB/mo)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve recently switched over to use Storj as my cloud backup provider, simply because they have 11 9&amp;#39;s of durability, the lowest cost per TB I&amp;#39;ve been able to find, and an S3-compatible API so they work with pretty much every tool.&lt;/p&gt;\n\n&lt;p&gt;What are you using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9czns", "is_robot_indexable": true, "report_reasons": null, "author": "rdegges", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9czns/cheapest_cloud_storage_providers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9czns/cheapest_cloud_storage_providers/", "subreddit_subscribers": 737168, "created_utc": 1709865971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Basic question but am I covered in terms of the hardware I need to run a basic media server (either Plex or Jellyfin) ?\n\nBeen planning to start this project for a while, have a bunch of saved resources to get through, but I some what instantaneously decided to start this project *now*.\n\nI live in a country where it's difficult and expensive to buy electronic hardware and I'm currently in the U.S. - want to make sure I have everything I might need before I fly back this weekend.\n\nAny tips or advice would be greatly appreciated.\n\nThanks!", "author_fullname": "t2_1llv04y0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just bought a DS224+ and two WD Red Plus HDD - is there anything else I need to set up my first NAS with the intention of serving media files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9bu7i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709862772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basic question but am I covered in terms of the hardware I need to run a basic media server (either Plex or Jellyfin) ?&lt;/p&gt;\n\n&lt;p&gt;Been planning to start this project for a while, have a bunch of saved resources to get through, but I some what instantaneously decided to start this project &lt;em&gt;now&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;I live in a country where it&amp;#39;s difficult and expensive to buy electronic hardware and I&amp;#39;m currently in the U.S. - want to make sure I have everything I might need before I fly back this weekend.&lt;/p&gt;\n\n&lt;p&gt;Any tips or advice would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9bu7i", "is_robot_indexable": true, "report_reasons": null, "author": "lovely_trequartista", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9bu7i/just_bought_a_ds224_and_two_wd_red_plus_hdd_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9bu7i/just_bought_a_ds224_and_two_wd_red_plus_hdd_is/", "subreddit_subscribers": 737168, "created_utc": 1709862772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have onedrive student account with 5TB space. As onedrive is going to remove the A1 Plus license, my university has to downgrade the plan and limits the storage to 5GB per student starting 1st April. Yes, from 5TB to 5GB. Currently I'm having 2.1TB in the cloud and planning to move all the files to a selfhosted cloud storage. I'm familiar with docker and linux but still a newbie. So, here I am kindly ask for help and advice to setup a selfhosted cloud storage. \n\nUse cases: \n1. Multiplatform - ipad, mac, windows, android\n2. Upload files to cloud \n3. Download on-demand and view file locally \n3. Delete that locally save on-demand went needed\n4. Offline editing &amp; sync to cloud when online\n\nMy questions:\n1. What is the best multiplatform selfhosted cloud storage that suits my use cases? Nextcloud? Seafile?\n2. What is the best filesystem to use? Zfs? Btrfs? \n3. Where should I host the app? NAS? Docker?\n4. What is the best storage disk? HDD 3.5 or 2.5? SMR or CMR?\n5. Other things to consider? NAS? RAID? SMB? Webdav? ", "author_fullname": "t2_8fiv28qr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Onedrive to selfhosted cloud storage migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9abgr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709858680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have onedrive student account with 5TB space. As onedrive is going to remove the A1 Plus license, my university has to downgrade the plan and limits the storage to 5GB per student starting 1st April. Yes, from 5TB to 5GB. Currently I&amp;#39;m having 2.1TB in the cloud and planning to move all the files to a selfhosted cloud storage. I&amp;#39;m familiar with docker and linux but still a newbie. So, here I am kindly ask for help and advice to setup a selfhosted cloud storage. &lt;/p&gt;\n\n&lt;p&gt;Use cases: \n1. Multiplatform - ipad, mac, windows, android\n2. Upload files to cloud \n3. Download on-demand and view file locally \n3. Delete that locally save on-demand went needed\n4. Offline editing &amp;amp; sync to cloud when online&lt;/p&gt;\n\n&lt;p&gt;My questions:\n1. What is the best multiplatform selfhosted cloud storage that suits my use cases? Nextcloud? Seafile?\n2. What is the best filesystem to use? Zfs? Btrfs? \n3. Where should I host the app? NAS? Docker?\n4. What is the best storage disk? HDD 3.5 or 2.5? SMR or CMR?\n5. Other things to consider? NAS? RAID? SMB? Webdav? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b9abgr", "is_robot_indexable": true, "report_reasons": null, "author": "Retiary_Lime", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b9abgr/onedrive_to_selfhosted_cloud_storage_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b9abgr/onedrive_to_selfhosted_cloud_storage_migration/", "subreddit_subscribers": 737168, "created_utc": 1709858680.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a NAS Linux backup server that I have setup smartd to send an email should it find a smart error on a drive.\n\nMy server only runs long enough to execute an incremental backup 3x a week. This backup takes 5-10min on average.\n\nI have not changed the default smart check time period from 30 min, so I was wondering if smartd ran a check at boot or does it only start checking at boot+30min.\n\nIf my server normally runs less than 30min; will smartd never check the drives and send a warning  as it never hits that 30min window for the first check?", "author_fullname": "t2_cskhbtwdp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smartd - When is the first smart check?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b98r7t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709853936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a NAS Linux backup server that I have setup smartd to send an email should it find a smart error on a drive.&lt;/p&gt;\n\n&lt;p&gt;My server only runs long enough to execute an incremental backup 3x a week. This backup takes 5-10min on average.&lt;/p&gt;\n\n&lt;p&gt;I have not changed the default smart check time period from 30 min, so I was wondering if smartd ran a check at boot or does it only start checking at boot+30min.&lt;/p&gt;\n\n&lt;p&gt;If my server normally runs less than 30min; will smartd never check the drives and send a warning  as it never hits that 30min window for the first check?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b98r7t", "is_robot_indexable": true, "report_reasons": null, "author": "Beaver-on-fire", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b98r7t/smartd_when_is_the_first_smart_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b98r7t/smartd_when_is_the_first_smart_check/", "subreddit_subscribers": 737168, "created_utc": 1709853936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm in the process of rebuilding a media server. The common Plex, ARR stack with qBittorent. So seeding is also relevant, not just hardlinking. I will be using Docker for the stack. Full disclosure, I am new to pretty much all of it, Linux, mergerfs, etc. I have been reading docs extensively but am still confused on a couple concepts/config settings.\n\nThe point in the setup process I am at now is taking 2 storage HDDs with media and pooling them with mergerfs. Most mergerfs Google results seem to be from this sub so here I am!\n\nFor context, my mergerfs config looks like this:\n\n`/mnt/hdd1:/mnt/hdd2 /merge mergerfs cache.files=partial,dropcacheonclose=true,category.create=mfs`\n\n* /mnt/hdd1 - already has TV and Movies on it\n* /mnt/hdd2 - empty, but have set the same high level directory structure on it:\n   * /torrents\n      * /movies\n      * /tv\n   * /library\n      * /movies\n      * /tv\n\nThe part in the docs that made me start second guessing was reference to \"hardlinks do not work across branches in a pool\". I started thinking about what happens if mergerfs effectively puts the torrent file on one branch, but the Sonarr/Radarr hardlink on another branch (if that's even what it's doing)? *If this is completely irrelevant by just setting Sonarr/Radarr root to /merge then let me know.*\n\nI also want to avoid running into the issue mentioned in docs of \"all my files ending up on 1 filesystem?!\" since I have one drive that already has a bunch of files and directories on it, and one empty drive.\n\nSo, now I'm wondering if I need to modify my mergerfs options from the default to make sure hardlinks and seeding work.\n\n* Do I need to change to create policy to `epmfs`?\n* Do I need to create specific policies for something like tv series' to keep all seasons/episodes related to a show on the same branch?\n* Do I need to enable `moveonenospc`?\n* Would any of the other extensive options be relevant to this topic?\n\nAs far as Docker is concerned, my understanding is I need to mount the containers at the root /merge directory. *It's entirely possible that's all I need to do and I'm overthinking this.*", "author_fullname": "t2_1ajq44ub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make sure hardlinks and seeding work with mergerfs and Radarr/Sonarr?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b96ke4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709848329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in the process of rebuilding a media server. The common Plex, ARR stack with qBittorent. So seeding is also relevant, not just hardlinking. I will be using Docker for the stack. Full disclosure, I am new to pretty much all of it, Linux, mergerfs, etc. I have been reading docs extensively but am still confused on a couple concepts/config settings.&lt;/p&gt;\n\n&lt;p&gt;The point in the setup process I am at now is taking 2 storage HDDs with media and pooling them with mergerfs. Most mergerfs Google results seem to be from this sub so here I am!&lt;/p&gt;\n\n&lt;p&gt;For context, my mergerfs config looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;/mnt/hdd1:/mnt/hdd2 /merge mergerfs cache.files=partial,dropcacheonclose=true,category.create=mfs&lt;/code&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;/mnt/hdd1 - already has TV and Movies on it&lt;/li&gt;\n&lt;li&gt;/mnt/hdd2 - empty, but have set the same high level directory structure on it:\n\n&lt;ul&gt;\n&lt;li&gt;/torrents\n\n&lt;ul&gt;\n&lt;li&gt;/movies&lt;/li&gt;\n&lt;li&gt;/tv&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;/library\n\n&lt;ul&gt;\n&lt;li&gt;/movies&lt;/li&gt;\n&lt;li&gt;/tv&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The part in the docs that made me start second guessing was reference to &amp;quot;hardlinks do not work across branches in a pool&amp;quot;. I started thinking about what happens if mergerfs effectively puts the torrent file on one branch, but the Sonarr/Radarr hardlink on another branch (if that&amp;#39;s even what it&amp;#39;s doing)? &lt;em&gt;If this is completely irrelevant by just setting Sonarr/Radarr root to /merge then let me know.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I also want to avoid running into the issue mentioned in docs of &amp;quot;all my files ending up on 1 filesystem?!&amp;quot; since I have one drive that already has a bunch of files and directories on it, and one empty drive.&lt;/p&gt;\n\n&lt;p&gt;So, now I&amp;#39;m wondering if I need to modify my mergerfs options from the default to make sure hardlinks and seeding work.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do I need to change to create policy to &lt;code&gt;epmfs&lt;/code&gt;?&lt;/li&gt;\n&lt;li&gt;Do I need to create specific policies for something like tv series&amp;#39; to keep all seasons/episodes related to a show on the same branch?&lt;/li&gt;\n&lt;li&gt;Do I need to enable &lt;code&gt;moveonenospc&lt;/code&gt;?&lt;/li&gt;\n&lt;li&gt;Would any of the other extensive options be relevant to this topic?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As far as Docker is concerned, my understanding is I need to mount the containers at the root /merge directory. &lt;em&gt;It&amp;#39;s entirely possible that&amp;#39;s all I need to do and I&amp;#39;m overthinking this.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b96ke4", "is_robot_indexable": true, "report_reasons": null, "author": "GooeyDuck1", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b96ke4/how_to_make_sure_hardlinks_and_seeding_work_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b96ke4/how_to_make_sure_hardlinks_and_seeding_work_with/", "subreddit_subscribers": 737168, "created_utc": 1709848329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Many years ago i manually copied all the photos and videos i had to a HDD. Ever since then i just cloned my hard drives with a clone dock and called it a day. Recently i found a external hard drive from my dad (which still works 12+ years later) which has a lot of pictures / videos i don't have already. So how would i go about copying those files onto my HDD?\n\nI thought doing the same thing i did before. Putting my drive into my drive bay and connecting the external drive through usb, then the old copy and paste. But is there a better option? A more secure option? A hopefully not expensive option? ", "author_fullname": "t2_kht4ovrc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying external HDD onto internal HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8w9nv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709822412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many years ago i manually copied all the photos and videos i had to a HDD. Ever since then i just cloned my hard drives with a clone dock and called it a day. Recently i found a external hard drive from my dad (which still works 12+ years later) which has a lot of pictures / videos i don&amp;#39;t have already. So how would i go about copying those files onto my HDD?&lt;/p&gt;\n\n&lt;p&gt;I thought doing the same thing i did before. Putting my drive into my drive bay and connecting the external drive through usb, then the old copy and paste. But is there a better option? A more secure option? A hopefully not expensive option? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8w9nv", "is_robot_indexable": true, "report_reasons": null, "author": "TengokuDaimakyo", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8w9nv/copying_external_hdd_onto_internal_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8w9nv/copying_external_hdd_onto_internal_hdd/", "subreddit_subscribers": 737168, "created_utc": 1709822412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I noticed my 8TB drive health was starting to go bad with uncorrectable sector count.  So I bought a drive from server part deals  (btw thanks for recommending that site, never would have known about them had it not been for this sub).  My original plan was to take the 3TB of movies I had from one drive and put it on another then replace the drive and just write them back.  But I had a dock so I figured why not just put the new drive in the PC and the old drive in the dock and copy that way.  Well the old drive came up in disk management with a GPT protected partition.  I read a guide online and used diskpart to clean it and I stupidly reformatted...bye bye movies.  I can get the movies back, it will just take a long time.  But I realized I had a drive from about a year ago with *most* of the movies.  I plugged that drive in the dock and it popped up with that same GPT protective partition.  So before I reformat that one too, I'd like to know how to remove that \"protection\" so I can get the movies off that drive so I don't have to redownload everything.", "author_fullname": "t2_umqki5a26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I got myself in a bit of a pickle", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b99kg5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709856499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I noticed my 8TB drive health was starting to go bad with uncorrectable sector count.  So I bought a drive from server part deals  (btw thanks for recommending that site, never would have known about them had it not been for this sub).  My original plan was to take the 3TB of movies I had from one drive and put it on another then replace the drive and just write them back.  But I had a dock so I figured why not just put the new drive in the PC and the old drive in the dock and copy that way.  Well the old drive came up in disk management with a GPT protected partition.  I read a guide online and used diskpart to clean it and I stupidly reformatted...bye bye movies.  I can get the movies back, it will just take a long time.  But I realized I had a drive from about a year ago with &lt;em&gt;most&lt;/em&gt; of the movies.  I plugged that drive in the dock and it popped up with that same GPT protective partition.  So before I reformat that one too, I&amp;#39;d like to know how to remove that &amp;quot;protection&amp;quot; so I can get the movies off that drive so I don&amp;#39;t have to redownload everything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b99kg5", "is_robot_indexable": true, "report_reasons": null, "author": "LightRyzen", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b99kg5/i_got_myself_in_a_bit_of_a_pickle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b99kg5/i_got_myself_in_a_bit_of_a_pickle/", "subreddit_subscribers": 737168, "created_utc": 1709856499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, just upgraded my NAS with bigger drives, now I found myself with 2 spare old disks. I would securely wipe and forget about them thinking about give them away or drop them to some local store for a bunch of candies. I have an USB SATA 3.5\" adapter, what's the proper way to decomission and safely wipe everything? \nShred and wipe commands from my Linux machine?", "author_fullname": "t2_h89ybm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS disk decomissioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b98gez", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709853005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, just upgraded my NAS with bigger drives, now I found myself with 2 spare old disks. I would securely wipe and forget about them thinking about give them away or drop them to some local store for a bunch of candies. I have an USB SATA 3.5&amp;quot; adapter, what&amp;#39;s the proper way to decomission and safely wipe everything? \nShred and wipe commands from my Linux machine?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b98gez", "is_robot_indexable": true, "report_reasons": null, "author": "kinghino", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b98gez/nas_disk_decomissioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b98gez/nas_disk_decomissioning/", "subreddit_subscribers": 737168, "created_utc": 1709853005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, as far as I've understood it, this site was captured on [archive.org](https://archive.org). but I've never been able to access it. The entire screen just whites out, what could be causing this? I've never been able to acccess any of the things archived on this blog,  and when you look there are supposed to be several.\n\n[https://web.archive.org/web/20151201000000\\*/http://foolsforluv.blogspot.com/2013/02/BLUDDYVALENTINE.html](https://web.archive.org/web/20151201000000*/http://foolsforluv.blogspot.com/2013/02/BLUDDYVALENTINE.html)\n\n&amp;#x200B;\n\nwhenever I try to click any of these links, it never works.  \n[https://web.archive.org/web/\\*/http://foolsforluv.blogspot.com/\\*](https://web.archive.org/web/*/http://foolsforluv.blogspot.com/*)", "author_fullname": "t2_j3lajj2up", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can't access Archive.org archived site.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b96pbc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709848656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, as far as I&amp;#39;ve understood it, this site was captured on &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;. but I&amp;#39;ve never been able to access it. The entire screen just whites out, what could be causing this? I&amp;#39;ve never been able to acccess any of the things archived on this blog,  and when you look there are supposed to be several.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20151201000000*/http://foolsforluv.blogspot.com/2013/02/BLUDDYVALENTINE.html\"&gt;https://web.archive.org/web/20151201000000*/http://foolsforluv.blogspot.com/2013/02/BLUDDYVALENTINE.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;whenever I try to click any of these links, it never works.&lt;br/&gt;\n&lt;a href=\"https://web.archive.org/web/*/http://foolsforluv.blogspot.com/*\"&gt;https://web.archive.org/web/*/http://foolsforluv.blogspot.com/*&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b96pbc", "is_robot_indexable": true, "report_reasons": null, "author": "BedroomDependent8152", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b96pbc/cant_access_archiveorg_archived_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b96pbc/cant_access_archiveorg_archived_site/", "subreddit_subscribers": 737168, "created_utc": 1709848656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many pics to scan. I had a software called I think HP easy scan that was very good by splitting one scan to several pics.\n\nNow I have a new HP printer (516) and the software I'm using doesn't split them. This is big downside and I don't know what to do. I can't go picture by picture, it's too much. If I can put 2-4 pictures in one scan I can handle it hahaha.\n\nThank you so much for the help!", "author_fullname": "t2_4xllun6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanning old pics to drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b90cvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709832667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many pics to scan. I had a software called I think HP easy scan that was very good by splitting one scan to several pics.&lt;/p&gt;\n\n&lt;p&gt;Now I have a new HP printer (516) and the software I&amp;#39;m using doesn&amp;#39;t split them. This is big downside and I don&amp;#39;t know what to do. I can&amp;#39;t go picture by picture, it&amp;#39;s too much. If I can put 2-4 pictures in one scan I can handle it hahaha.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b90cvg", "is_robot_indexable": true, "report_reasons": null, "author": "Direct_Check_3366", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b90cvg/scanning_old_pics_to_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b90cvg/scanning_old_pics_to_drive/", "subreddit_subscribers": 737168, "created_utc": 1709832667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I run RAID1 on Linux mdadm with spinning HDDs.\n\nI have started getting these warnings from mdstat:\n\n    mdstat mismatch cnt = 128 unsynchronized blocks\n\nDoes this mean that my disks/arrays are damaged for certain?\n\nI previously had this issue before. I could not figure out what was wrong but I ended up replacing the disks. \n\nNow after a short while, this happens again...\n\nI have tried initiating a resin. But this counter increases again after a while. Note that the value is a very round number of 128. It seems to be always a power of 2 value. On the previous disks, I think it was 256 or 512.\n\nSMART does not report anything of note. Neither on the old nor the new disks.", "author_fullname": "t2_nlx42kb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mdstat mismatch cnt: are my disks/data damaged?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8z4jy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709829765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run RAID1 on Linux mdadm with spinning HDDs.&lt;/p&gt;\n\n&lt;p&gt;I have started getting these warnings from mdstat:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;mdstat mismatch cnt = 128 unsynchronized blocks\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Does this mean that my disks/arrays are damaged for certain?&lt;/p&gt;\n\n&lt;p&gt;I previously had this issue before. I could not figure out what was wrong but I ended up replacing the disks. &lt;/p&gt;\n\n&lt;p&gt;Now after a short while, this happens again...&lt;/p&gt;\n\n&lt;p&gt;I have tried initiating a resin. But this counter increases again after a while. Note that the value is a very round number of 128. It seems to be always a power of 2 value. On the previous disks, I think it was 256 or 512.&lt;/p&gt;\n\n&lt;p&gt;SMART does not report anything of note. Neither on the old nor the new disks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8z4jy", "is_robot_indexable": true, "report_reasons": null, "author": "PM_Me_Food_Pics_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8z4jy/mdstat_mismatch_cnt_are_my_disksdata_damaged/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8z4jy/mdstat_mismatch_cnt_are_my_disksdata_damaged/", "subreddit_subscribers": 737168, "created_utc": 1709829765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 4TB Seagate drive with a multitude of issues and very conflicting test results. Basically, I have experienced data corruption, data loss, unexpected noises, and other issues that started out happening often and seriously enough to deem the HDD too unreliable for data storage, and yet, after completing the backup process and giving the HDD some time off (not using it at all, but keeping it turned on), the issues mysteriously improved dramatically, but the drive still feels off, as copied files were still corrupted during the copying process recently.  The SMART logs, Windows Dsk Chk, HD tune, are pretty much all green, as far as I can tell, and I was told these results seemed good on the TechSupport subreddit as well.\n\nBecause of these conflicting results, I would prefer to exchange my drive for a brand new one through Seagate's RMA process before the warranty expires, but I am worried if these conflicting results will cause issues with the RMA process. Does anyone have any experience with sending out a Seagate HDD to RMA, especially with more difficult to detect issues, or with less clear-cut HDD problems? If so, did they fix or send you a new HDD, or did they just returned it back to you without doing anything to fix, or resulted in having trouble/difficulties with Seagate due to them not detecting hardware defects?\n\nDue to the data loss (although only contained to newly-written files past the first 2TB at the time of the data writing) and all the issues experienced, I just can't trust this drive at all until I can find out what caused it to misbehave, so I vastly prefer for Seagate to send me a new one, since this is still covered by the warranty, but I have never RMA'ed a drive before, and since this isn't a clear-cut case, I'm worried if they will really come through, so it would be great to know if based on previous RMA experiences in this subreddit, if my attempt to RMA this untrustworthy and/or faulty drive will likely be successful or not.\n\nThanks!", "author_fullname": "t2_17ez6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Regarding Seagate's RMA process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8ywgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709835965.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709829019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 4TB Seagate drive with a multitude of issues and very conflicting test results. Basically, I have experienced data corruption, data loss, unexpected noises, and other issues that started out happening often and seriously enough to deem the HDD too unreliable for data storage, and yet, after completing the backup process and giving the HDD some time off (not using it at all, but keeping it turned on), the issues mysteriously improved dramatically, but the drive still feels off, as copied files were still corrupted during the copying process recently.  The SMART logs, Windows Dsk Chk, HD tune, are pretty much all green, as far as I can tell, and I was told these results seemed good on the TechSupport subreddit as well.&lt;/p&gt;\n\n&lt;p&gt;Because of these conflicting results, I would prefer to exchange my drive for a brand new one through Seagate&amp;#39;s RMA process before the warranty expires, but I am worried if these conflicting results will cause issues with the RMA process. Does anyone have any experience with sending out a Seagate HDD to RMA, especially with more difficult to detect issues, or with less clear-cut HDD problems? If so, did they fix or send you a new HDD, or did they just returned it back to you without doing anything to fix, or resulted in having trouble/difficulties with Seagate due to them not detecting hardware defects?&lt;/p&gt;\n\n&lt;p&gt;Due to the data loss (although only contained to newly-written files past the first 2TB at the time of the data writing) and all the issues experienced, I just can&amp;#39;t trust this drive at all until I can find out what caused it to misbehave, so I vastly prefer for Seagate to send me a new one, since this is still covered by the warranty, but I have never RMA&amp;#39;ed a drive before, and since this isn&amp;#39;t a clear-cut case, I&amp;#39;m worried if they will really come through, so it would be great to know if based on previous RMA experiences in this subreddit, if my attempt to RMA this untrustworthy and/or faulty drive will likely be successful or not.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8ywgm", "is_robot_indexable": true, "report_reasons": null, "author": "Xenofan23", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8ywgm/regarding_seagates_rma_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8ywgm/regarding_seagates_rma_process/", "subreddit_subscribers": 737168, "created_utc": 1709829019.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}