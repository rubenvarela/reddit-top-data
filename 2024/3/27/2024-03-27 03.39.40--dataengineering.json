{"kind": "Listing", "data": {"after": "t3_1boh9qk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys after finishing a contract in a company I\u2019m searching for another opportunity in Europe based remotely and what I see in the job descriptions in LinkedIn are 27 technologies needed for the position and you have to be an expert, even not a senior position (I have 3.5 years of experience), what is happening here?\n\nYou need to know: python, pyspark, scala , JavaScript, java, azure, aws, gcp (and all the the technologies), databricks, airflow, Kafka, sql, no sql, data lakes, dwh, oracle, ETL\u2019s, terraform, Jenkins, kubernetes\u2026 and more \n\nOfc all of this fluent and proficient, lol\n\n\n\nAnd not even senior positions\u2026 what would you recommend, guys?\nI\u2019ve been working with azure data factory/synapse/Databricks with python/pyspark and sql, doing etl/elt pipelines from on-premise ddbb or simple excels or cloud ddbb, or api\u2019s.\n\n", "author_fullname": "t2_6bblasam", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding a new job, ridiculous ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo4nne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 119, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 119, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711449324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys after finishing a contract in a company I\u2019m searching for another opportunity in Europe based remotely and what I see in the job descriptions in LinkedIn are 27 technologies needed for the position and you have to be an expert, even not a senior position (I have 3.5 years of experience), what is happening here?&lt;/p&gt;\n\n&lt;p&gt;You need to know: python, pyspark, scala , JavaScript, java, azure, aws, gcp (and all the the technologies), databricks, airflow, Kafka, sql, no sql, data lakes, dwh, oracle, ETL\u2019s, terraform, Jenkins, kubernetes\u2026 and more &lt;/p&gt;\n\n&lt;p&gt;Ofc all of this fluent and proficient, lol&lt;/p&gt;\n\n&lt;p&gt;And not even senior positions\u2026 what would you recommend, guys?\nI\u2019ve been working with azure data factory/synapse/Databricks with python/pyspark and sql, doing etl/elt pipelines from on-premise ddbb or simple excels or cloud ddbb, or api\u2019s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bo4nne", "is_robot_indexable": true, "report_reasons": null, "author": "Irachar", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo4nne/finding_a_new_job_ridiculous/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo4nne/finding_a_new_job_ridiculous/", "subreddit_subscribers": 172012, "created_utc": 1711449324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer with 2 years of experience. I\u2019ve joined a fairly well established company but found it to be completely different to my last (and only other) workplace. Is this normal?\n\n- Single Terraformed GCP project hosting\n  - Airbyte Open Source\n  - Airflow\n  - DBT\n- Single Snowflake Account with DBT\n\nThere is no permanent Sandbox / Dev GCP project or testing procedure. Anything to do with Airbyte and Airflow gets done directly in production which is making me age twice as fast. If you need to configure a new VM you are free to create a new GCP project to give it a go but as only a production GCP account is Terraformed there is no environment parity. \n\nThe infrastructure supports analytics so it\u2019s not business critical, but it still feels like an uncommon way to do things. The Head of Data is onboard with my suggestion to deploy our IaC to a second GCP project but the Senior Data Engineer is less enthusiastic. The whole situation has gotten me wondering whether I\u2019m right, or whether it should even be my job as a mid to do what a senior data engineer doesn\u2019t feel the need to do.", "author_fullname": "t2_827pnc17k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workplace has 0 procedures. Time to look for a new job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bohwi7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711484676.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711484322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer with 2 years of experience. I\u2019ve joined a fairly well established company but found it to be completely different to my last (and only other) workplace. Is this normal?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Single Terraformed GCP project hosting\n\n&lt;ul&gt;\n&lt;li&gt;Airbyte Open Source&lt;/li&gt;\n&lt;li&gt;Airflow&lt;/li&gt;\n&lt;li&gt;DBT&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Single Snowflake Account with DBT&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;There is no permanent Sandbox / Dev GCP project or testing procedure. Anything to do with Airbyte and Airflow gets done directly in production which is making me age twice as fast. If you need to configure a new VM you are free to create a new GCP project to give it a go but as only a production GCP account is Terraformed there is no environment parity. &lt;/p&gt;\n\n&lt;p&gt;The infrastructure supports analytics so it\u2019s not business critical, but it still feels like an uncommon way to do things. The Head of Data is onboard with my suggestion to deploy our IaC to a second GCP project but the Senior Data Engineer is less enthusiastic. The whole situation has gotten me wondering whether I\u2019m right, or whether it should even be my job as a mid to do what a senior data engineer doesn\u2019t feel the need to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bohwi7", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished_Cup_392", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bohwi7/workplace_has_0_procedures_time_to_look_for_a_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bohwi7/workplace_has_0_procedures_time_to_look_for_a_new/", "subreddit_subscribers": 172012, "created_utc": 1711484322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "?", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle NULLs when cleaning data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bod2zr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711472850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bod2zr", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bod2zr/how_do_you_handle_nulls_when_cleaning_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bod2zr/how_do_you_handle_nulls_when_cleaning_data/", "subreddit_subscribers": 172012, "created_utc": 1711472850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dqm6u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "42.parquet \u2013 A Zip Bomb for the Big Data Age", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo81xh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/54pHuKgLKELSMqBOoDh9A6a-NlBNVHNc9rZ_W7SH9gA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711460104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "duckdb.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://duckdb.org/2024/03/26/42-parquet-a-zip-bomb-for-the-big-data-age.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?auto=webp&amp;s=a3ce4d9713e9b21d12f203bb1557511dedc29060", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b86b56641acbba7143cf836e76a6ed127d0cdc7c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f7911437d91a92bdb5eaa0846b1481429176330", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=18a3e761649359ef6f006465b214153f3ee22b64", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d6d7436d826b7e957b7a49ae82dbd0475e0b149", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a85eb7536b7a9cb175fd04a6e566a36738718f7f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73e36fb4e5a8b625ec054bc33b25149637525dd2", "width": 1080, "height": 567}], "variants": {}, "id": "jWyiaF4Jb7ULQyU8SCl75THeEbJM9dbSQ9YXdauXufk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bo81xh", "is_robot_indexable": true, "report_reasons": null, "author": "commandlineluser", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo81xh/42parquet_a_zip_bomb_for_the_big_data_age/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://duckdb.org/2024/03/26/42-parquet-a-zip-bomb-for-the-big-data-age.html", "subreddit_subscribers": 172012, "created_utc": 1711460104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to get some career advice. I've been working as a Data Engineer for almost 5 years, but a huge part of this job was just Python development (APIs, libs, microservices...) and another part was building pipelines, using spark, Kafka, SQL, NoSQL, data bricks,  Airflow and others. Currently, I am working on another project in Hadoop on-prem where I'm building spark streaming apps - but I don't like Hadoop and the on-prem ecosystem, and I think also that it's legacy stack.   \nBecause of these above, I have very poor, almost 0 experience and knowledge in Cloud.\n\nThat's why I wanted to ask you for advice. I got an offer for a Data Engineer on AWS - the stack there is very simple like s3, kinesis, glue, Athena, Ecs, Rds, and Redshift - and the team has only one Data Engineer who basically is also on his road with AWS, but already passed some certificates.  \nIn this role there is a very small part of coding - like 10-15% of the job. But they are okay, that I don't know AWS and they will give me time and space to learn it and to pass certificates.  \nAnd I wonder if I should take it, on one side I will learn AWS, probably a couple of DE-related services, and I will pass the exam. On the other side, I won't code too much, and probably everything will be pretty low quality, as there is no collaboration with DevOps, SWEs, and other DEs into good swe practices (testing, high quality of code, design etc).   \n\n\nI'm having a bit of a headache with this. Do you think this is a good idea and a step forward in your career - I think it's a position for at least a year. But will it be a step backward - especially since I have been working more in programming like data engineering (writing code 80% of the time) for the last 5 years?  \n\n\n  \n\n\n  \n\n\n&amp;#x200B;", "author_fullname": "t2_2llofc3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice regarding new offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo1qwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711437436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to get some career advice. I&amp;#39;ve been working as a Data Engineer for almost 5 years, but a huge part of this job was just Python development (APIs, libs, microservices...) and another part was building pipelines, using spark, Kafka, SQL, NoSQL, data bricks,  Airflow and others. Currently, I am working on another project in Hadoop on-prem where I&amp;#39;m building spark streaming apps - but I don&amp;#39;t like Hadoop and the on-prem ecosystem, and I think also that it&amp;#39;s legacy stack.&lt;br/&gt;\nBecause of these above, I have very poor, almost 0 experience and knowledge in Cloud.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why I wanted to ask you for advice. I got an offer for a Data Engineer on AWS - the stack there is very simple like s3, kinesis, glue, Athena, Ecs, Rds, and Redshift - and the team has only one Data Engineer who basically is also on his road with AWS, but already passed some certificates.&lt;br/&gt;\nIn this role there is a very small part of coding - like 10-15% of the job. But they are okay, that I don&amp;#39;t know AWS and they will give me time and space to learn it and to pass certificates.&lt;br/&gt;\nAnd I wonder if I should take it, on one side I will learn AWS, probably a couple of DE-related services, and I will pass the exam. On the other side, I won&amp;#39;t code too much, and probably everything will be pretty low quality, as there is no collaboration with DevOps, SWEs, and other DEs into good swe practices (testing, high quality of code, design etc).   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a bit of a headache with this. Do you think this is a good idea and a step forward in your career - I think it&amp;#39;s a position for at least a year. But will it be a step backward - especially since I have been working more in programming like data engineering (writing code 80% of the time) for the last 5 years?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bo1qwr", "is_robot_indexable": true, "report_reasons": null, "author": "masek94", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo1qwr/career_advice_regarding_new_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo1qwr/career_advice_regarding_new_offer/", "subreddit_subscribers": 172012, "created_utc": 1711437436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everybody,\n\nI have been in the field for some time and still unsure about optimal solution for analytical database of 100GiB-10TiB range. \n\nIf you have less, you just go with PostgreSQL or some other conventional database with reasonable level of support of table scans +dbt. If you have more, you go with Spark/Athena. \n\nBut that range in the middle\u2026 You cannot put it into a reasonably priced db server. A proper host would cost me around 10K/month. That\u2019s roughly the same amount I pay for 50 servers Spark cluster. \n\nBut that amount of data does not need massive parallelism for ETL processing and associated Spark complexities. I probably need 10 process running in parallel to convert json to parquet (oversimplification here). \n\nWhat technologies/products would you use for this sort of ETL/reporting tasks? \n\nThank you\n", "author_fullname": "t2_gi05k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good solution for 100GiB-10TiB analytical DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bobzzj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711470216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everybody,&lt;/p&gt;\n\n&lt;p&gt;I have been in the field for some time and still unsure about optimal solution for analytical database of 100GiB-10TiB range. &lt;/p&gt;\n\n&lt;p&gt;If you have less, you just go with PostgreSQL or some other conventional database with reasonable level of support of table scans +dbt. If you have more, you go with Spark/Athena. &lt;/p&gt;\n\n&lt;p&gt;But that range in the middle\u2026 You cannot put it into a reasonably priced db server. A proper host would cost me around 10K/month. That\u2019s roughly the same amount I pay for 50 servers Spark cluster. &lt;/p&gt;\n\n&lt;p&gt;But that amount of data does not need massive parallelism for ETL processing and associated Spark complexities. I probably need 10 process running in parallel to convert json to parquet (oversimplification here). &lt;/p&gt;\n\n&lt;p&gt;What technologies/products would you use for this sort of ETL/reporting tasks? &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bobzzj", "is_robot_indexable": true, "report_reasons": null, "author": "aih1013", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bobzzj/good_solution_for_100gib10tib_analytical_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bobzzj/good_solution_for_100gib10tib_analytical_db/", "subreddit_subscribers": 172012, "created_utc": 1711470216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apparently, the decision to go for superset is final.\n\nthe company is around 200.\n\ndata size around 10s of millions.\n\nSo far i've noted the below -\n\n**Managed**\n\n1. [Preset.io](http://preset.io/) -\n\n* from founders of superset\n* 20$ per User (14D free trial)\n* additional benefits: RBAC, Slack reporting\n\n1. AWS based [alternative](https://aws.amazon.com/marketplace/pp/prodview-c3evrh2ho3fn4#pdp-pricing)\n\n**Self**\n\n1. AWS fargate based solution - \ud83d\udcf7[Apache Superset on AWS\u2014Partner Solution](https://aws.amazon.com/solutions/implementations/apache-superset/)\n\n* scalable\n* cost friendliness TBD - [https://github.com/aws-ia/cfn-ps-apache-superset/issues/14](https://github.com/aws-ia/cfn-ps-apache-superset/issues/14)\n\n1. Self-managed deployment on EC2 instances\n\nRealistically the cost efficiency is what would be latched on to even if the dashboards aren't instantly loaded.\n\nWould the deployment on EC2 \"win\" then?\n\nThe fargate based solution appears to cost 1k$ pm [just for setup](https://github.com/aws-ia/cfn-ps-apache-superset/issues/14) so i'm skeptical on how it can explode when viz work starts.\n\nAre there any other solutions missed? Did anyone deploy via ec2? any helpful guides/tips?\n\nTIA.\n\nRegards", "author_fullname": "t2_v5qxw89ws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting superset for redshift viz", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo3ryk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711445929.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apparently, the decision to go for superset is final.&lt;/p&gt;\n\n&lt;p&gt;the company is around 200.&lt;/p&gt;\n\n&lt;p&gt;data size around 10s of millions.&lt;/p&gt;\n\n&lt;p&gt;So far i&amp;#39;ve noted the below -&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Managed&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"http://preset.io/\"&gt;Preset.io&lt;/a&gt; -&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;from founders of superset&lt;/li&gt;\n&lt;li&gt;20$ per User (14D free trial)&lt;/li&gt;\n&lt;li&gt;additional benefits: RBAC, Slack reporting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;AWS based &lt;a href=\"https://aws.amazon.com/marketplace/pp/prodview-c3evrh2ho3fn4#pdp-pricing\"&gt;alternative&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Self&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;AWS fargate based solution - \ud83d\udcf7&lt;a href=\"https://aws.amazon.com/solutions/implementations/apache-superset/\"&gt;Apache Superset on AWS\u2014Partner Solution&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;scalable&lt;/li&gt;\n&lt;li&gt;cost friendliness TBD - &lt;a href=\"https://github.com/aws-ia/cfn-ps-apache-superset/issues/14\"&gt;https://github.com/aws-ia/cfn-ps-apache-superset/issues/14&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Self-managed deployment on EC2 instances&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Realistically the cost efficiency is what would be latched on to even if the dashboards aren&amp;#39;t instantly loaded.&lt;/p&gt;\n\n&lt;p&gt;Would the deployment on EC2 &amp;quot;win&amp;quot; then?&lt;/p&gt;\n\n&lt;p&gt;The fargate based solution appears to cost 1k$ pm &lt;a href=\"https://github.com/aws-ia/cfn-ps-apache-superset/issues/14\"&gt;just for setup&lt;/a&gt; so i&amp;#39;m skeptical on how it can explode when viz work starts.&lt;/p&gt;\n\n&lt;p&gt;Are there any other solutions missed? Did anyone deploy via ec2? any helpful guides/tips?&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n\n&lt;p&gt;Regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?auto=webp&amp;s=6f144a743f642dbb0357469ecaf0480fc2ff97a3", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5342c40a4000b37f44701465aca07efecd6c4ed3", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f749eacffd6ff6649fe936bac84b95926b602183", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c5410b1771ce6abd360b4d964bc3affac537919", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=434343bf29189b3ab61f86510f92847920e9d0d2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9f65aa9a38e0e88e52ace516b9b9042b65832ff", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/l3b2Wx-4DtC2vbj-HCY9jEX09IHdxiZao1U4Ahj0-tk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e124d5e1fd582bd2073aa42a4e8b7f04200f294", "width": 1080, "height": 607}], "variants": {}, "id": "__YqDw1NGDgPEwaBIWRMCVKQmM6RcFOpg1dAbXa6-Ns"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo3ryk", "is_robot_indexable": true, "report_reasons": null, "author": "what-you-need-is-you", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo3ryk/hosting_superset_for_redshift_viz/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo3ryk/hosting_superset_for_redshift_viz/", "subreddit_subscribers": 172012, "created_utc": 1711445929.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do any of you guys work in a niche that lets you play with programming / reading from RFIDs or using the NFC protocol in order to collect, transmit, or store data? How about physical sensors?\n\nI\u2019m curious what flavor of data engineering you typically enjoy on a dataly basis. I have been reading about these little things and they\u2019re pretty cool. I just bought a BuildYourOwn light sensor kit online, gonna play around with it here soon.\n\nWhat do you use them for, personally and professionally (if you\u2019re allowed to share)?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone do anything with NFC and/or RFID?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bonpws", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711498085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do any of you guys work in a niche that lets you play with programming / reading from RFIDs or using the NFC protocol in order to collect, transmit, or store data? How about physical sensors?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m curious what flavor of data engineering you typically enjoy on a dataly basis. I have been reading about these little things and they\u2019re pretty cool. I just bought a BuildYourOwn light sensor kit online, gonna play around with it here soon.&lt;/p&gt;\n\n&lt;p&gt;What do you use them for, personally and professionally (if you\u2019re allowed to share)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bonpws", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bonpws/does_anyone_do_anything_with_nfc_andor_rfid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bonpws/does_anyone_do_anything_with_nfc_andor_rfid/", "subreddit_subscribers": 172012, "created_utc": 1711498085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As part of my job I have to VPN into customer databases and extract the data that we need. Every customer has a custom table structure and there are often 300-400 tables. I need to find the 5 or so tables that have the specific data I am after.\n\nIs there an easy way to figure out which tables reference other tables and what the keys are?\n\nI\u2019ve tried putting the table, column, and data type into a data frame then searching for words that might be what I am looking for, opening those tables and looking at the data but that\u2019s rather time consuming and they do a lot of weird stuff, for example one of the things I look for is a column called \u2018batch\u2019 or \u2018batch_number\u2019 that has an integer data type but a few dbs I have looked at store batch numbers as strings for some reason", "author_fullname": "t2_cvnuc9q27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Figuring out what tables are what in a database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1boj2rh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711487086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As part of my job I have to VPN into customer databases and extract the data that we need. Every customer has a custom table structure and there are often 300-400 tables. I need to find the 5 or so tables that have the specific data I am after.&lt;/p&gt;\n\n&lt;p&gt;Is there an easy way to figure out which tables reference other tables and what the keys are?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried putting the table, column, and data type into a data frame then searching for words that might be what I am looking for, opening those tables and looking at the data but that\u2019s rather time consuming and they do a lot of weird stuff, for example one of the things I look for is a column called \u2018batch\u2019 or \u2018batch_number\u2019 that has an integer data type but a few dbs I have looked at store batch numbers as strings for some reason&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1boj2rh", "is_robot_indexable": true, "report_reasons": null, "author": "big_data_mike", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boj2rh/figuring_out_what_tables_are_what_in_a_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boj2rh/figuring_out_what_tables_are_what_in_a_database/", "subreddit_subscribers": 172012, "created_utc": 1711487086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit,\n\nI'm part of a team that's been grappling with a challenge I think many of you can relate to: the inefficiencies and security concerns of traditional ETL processes that rely heavily on cloud transit. It led us to ask, \"What if there's a better way?\"\n\nSo, we built Mycelial - an open-source platform designed from the ground up to enable direct, point-to-point ETL. This means your data moves from A to B without being forced to detour through a central cloud, eliminating unnecessary latency, reducing costs, and enhancing data security.\n\n**How Mycelial Works:**\n\nCrafted with love in 100% Rust, Mycelial leverages local daemons to execute data workflows, guaranteeing fast and secure data movement. It features a universal interface between data sources and destinations, meaning once you're connected to the Mycelial network, the possibilities for your data are endless.\n\n**Why we think it's a game changer:**\n\n* **Security**: With data breaches on the rise, keeping your data off public cloud networks reduces exposure.\n* **Efficiency**: Direct transfers mean quicker ETL processes, getting your data where it needs to be faster.\n* **Cost-effective**: Less reliance on cloud infrastructure translates to lower operational costs.\n* **Control**: You maintain complete control over your data's journey, end-to-end.\n\nWe're thrilled (and a tad nervous) to introduce Mycelial to the Reddit community. Your feedback, whether it's praise, criticism, or anything in between, is invaluable to us. We're committed to transparency and community-driven development, so we're all ears for your thoughts, concerns, and questions.\n\n**Be Part of the Beta Test:**\n\nMycelial is in beta, and we're eager for testers to push our platform to its limits. Your insights will directly influence the evolution of Mycelial, helping us refine and perfect the platform. Visit [www.mycelial.com](http://www.mycelial.com/) to sign up and start exploring the possibilities.\n\nFeel free to AMA about the technology, our development journey, or any burning questions you might have. Your input is crucial in our mission to make Mycelial the best it can be for the community.", "author_fullname": "t2_fu5zl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\ude80 Launching Mycelial: A New Way to Handle ETL Without the Cloud - Seeking Your Feedback!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bogg92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711480871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m part of a team that&amp;#39;s been grappling with a challenge I think many of you can relate to: the inefficiencies and security concerns of traditional ETL processes that rely heavily on cloud transit. It led us to ask, &amp;quot;What if there&amp;#39;s a better way?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;So, we built Mycelial - an open-source platform designed from the ground up to enable direct, point-to-point ETL. This means your data moves from A to B without being forced to detour through a central cloud, eliminating unnecessary latency, reducing costs, and enhancing data security.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How Mycelial Works:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Crafted with love in 100% Rust, Mycelial leverages local daemons to execute data workflows, guaranteeing fast and secure data movement. It features a universal interface between data sources and destinations, meaning once you&amp;#39;re connected to the Mycelial network, the possibilities for your data are endless.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why we think it&amp;#39;s a game changer:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: With data breaches on the rise, keeping your data off public cloud networks reduces exposure.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Direct transfers mean quicker ETL processes, getting your data where it needs to be faster.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cost-effective&lt;/strong&gt;: Less reliance on cloud infrastructure translates to lower operational costs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: You maintain complete control over your data&amp;#39;s journey, end-to-end.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;re thrilled (and a tad nervous) to introduce Mycelial to the Reddit community. Your feedback, whether it&amp;#39;s praise, criticism, or anything in between, is invaluable to us. We&amp;#39;re committed to transparency and community-driven development, so we&amp;#39;re all ears for your thoughts, concerns, and questions.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Be Part of the Beta Test:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Mycelial is in beta, and we&amp;#39;re eager for testers to push our platform to its limits. Your insights will directly influence the evolution of Mycelial, helping us refine and perfect the platform. Visit &lt;a href=\"http://www.mycelial.com/\"&gt;www.mycelial.com&lt;/a&gt; to sign up and start exploring the possibilities.&lt;/p&gt;\n\n&lt;p&gt;Feel free to AMA about the technology, our development journey, or any burning questions you might have. Your input is crucial in our mission to make Mycelial the best it can be for the community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bogg92", "is_robot_indexable": true, "report_reasons": null, "author": "jofus101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bogg92/launching_mycelial_a_new_way_to_handle_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bogg92/launching_mycelial_a_new_way_to_handle_etl/", "subreddit_subscribers": 172012, "created_utc": 1711480871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wozut7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage ELT pipelines with code using Airbyte\u2019s Terraform provider", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1bojruu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Df8VcbWW_BM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Manage ELT pipelines with code using Airbyte\u2019s Terraform provider\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Manage ELT pipelines with code using Airbyte\u2019s Terraform provider", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Df8VcbWW_BM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Manage ELT pipelines with code using Airbyte\u2019s Terraform provider\"&gt;&lt;/iframe&gt;", "author_name": "Teradata", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Df8VcbWW_BM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@teradata"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Df8VcbWW_BM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Manage ELT pipelines with code using Airbyte\u2019s Terraform provider\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1bojruu", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BKfAdRE--N6kwz9ml2s0mEGBS5UeDcsqhwmCbg5t_ws.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711488699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=Df8VcbWW_BM&amp;t", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GmhVgAPei2l09KAByjepvSPY-sf2ZTZ9QO5nRLk1I1M.jpg?auto=webp&amp;s=e80deddbab565910d1b2ead814abc55081aeb434", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GmhVgAPei2l09KAByjepvSPY-sf2ZTZ9QO5nRLk1I1M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c5665fa2a63f09086d451a8eaa7bf382cc7bfb27", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GmhVgAPei2l09KAByjepvSPY-sf2ZTZ9QO5nRLk1I1M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f8609b27e7b1793f6e7111cb1a2ecb3f2f3145d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GmhVgAPei2l09KAByjepvSPY-sf2ZTZ9QO5nRLk1I1M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=863eaf9894fc8ebea413529f0d6e5c39bd140307", "width": 320, "height": 240}], "variants": {}, "id": "0c0qDs6NmT_Db-fklM9DKXUzX9NEsTwro3f7VNE9YPU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bojruu", "is_robot_indexable": true, "report_reasons": null, "author": "JanethL", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bojruu/manage_elt_pipelines_with_code_using_airbytes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=Df8VcbWW_BM&amp;t", "subreddit_subscribers": 172012, "created_utc": 1711488699.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Manage ELT pipelines with code using Airbyte\u2019s Terraform provider", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Df8VcbWW_BM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Manage ELT pipelines with code using Airbyte\u2019s Terraform provider\"&gt;&lt;/iframe&gt;", "author_name": "Teradata", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Df8VcbWW_BM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@teradata"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Storying Parquet files comes with extra metdata and also we incur compute costs for conversion. So, at what point do we convert csv to Parquet? Do you have any specific threshold like 100 MB/1GB etc?\n\nOr do you store all files as Parquet in your data lake (say Silver layer) regardless how big, so that file formats are unified?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is minimum file size to justfy storying it as Parquet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bol3zg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711491865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Storying Parquet files comes with extra metdata and also we incur compute costs for conversion. So, at what point do we convert csv to Parquet? Do you have any specific threshold like 100 MB/1GB etc?&lt;/p&gt;\n\n&lt;p&gt;Or do you store all files as Parquet in your data lake (say Silver layer) regardless how big, so that file formats are unified?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bol3zg", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bol3zg/what_is_minimum_file_size_to_justfy_storying_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bol3zg/what_is_minimum_file_size_to_justfy_storying_it/", "subreddit_subscribers": 172012, "created_utc": 1711491865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Something I have been wondering for a while. As long as I have been at my current job we replicate all the data in every table for a set of schemas from our production cluster to a development cluster to facilitate development work. This seems unnecessary to me though. \n\nI feel like we could implement a tiered structure where we keep all data from the current year, half the data from last year, a quarter of the data from the year before, etc. in order to shorten our dev runs while retaining enough information to ensure that we are consistent with historical data as much as current data. \n\nCurious if anyone has implemented something similar.", "author_fullname": "t2_6nf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle replication of data into development environments? Do you replicate everything or do you implement some sort or reduction strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bogde0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711480694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Something I have been wondering for a while. As long as I have been at my current job we replicate all the data in every table for a set of schemas from our production cluster to a development cluster to facilitate development work. This seems unnecessary to me though. &lt;/p&gt;\n\n&lt;p&gt;I feel like we could implement a tiered structure where we keep all data from the current year, half the data from last year, a quarter of the data from the year before, etc. in order to shorten our dev runs while retaining enough information to ensure that we are consistent with historical data as much as current data. &lt;/p&gt;\n\n&lt;p&gt;Curious if anyone has implemented something similar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bogde0", "is_robot_indexable": true, "report_reasons": null, "author": "radil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bogde0/how_do_you_handle_replication_of_data_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bogde0/how_do_you_handle_replication_of_data_into/", "subreddit_subscribers": 172012, "created_utc": 1711480694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is in cost-cutting mode, but we have some little-used servers on-prem. I'm hoping to create a more modern ELT stack than what we have, which is basically separate extract scripts run through a custom scheduler into a relational database. Don't get me started.\n\nI'm currently thinking something like the below, but would be very happy for some advice. Nobody on our team has any experience with any of them, so we're (a) open to new, but (b) wary of steep learning curves:\n\n\\[Sources\\] (many, sql/nosql/flat) -&gt; \\[Flink\\] -&gt; \\[doris\\] -&gt; \\[dbt\\] -&gt; \\[doris\\]\n\nCurrently approx 5TB of data, will probably double this year as more is added.", "author_fullname": "t2_8ha5wvt6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to use for an open source ETL/ELT stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1boc91z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711486876.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711470827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is in cost-cutting mode, but we have some little-used servers on-prem. I&amp;#39;m hoping to create a more modern ELT stack than what we have, which is basically separate extract scripts run through a custom scheduler into a relational database. Don&amp;#39;t get me started.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently thinking something like the below, but would be very happy for some advice. Nobody on our team has any experience with any of them, so we&amp;#39;re (a) open to new, but (b) wary of steep learning curves:&lt;/p&gt;\n\n&lt;p&gt;[Sources] (many, sql/nosql/flat) -&amp;gt; [Flink] -&amp;gt; [doris] -&amp;gt; [dbt] -&amp;gt; [doris]&lt;/p&gt;\n\n&lt;p&gt;Currently approx 5TB of data, will probably double this year as more is added.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1boc91z", "is_robot_indexable": true, "report_reasons": null, "author": "Melodic_One4333", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boc91z/what_to_use_for_an_open_source_etlelt_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boc91z/what_to_use_for_an_open_source_etlelt_stack/", "subreddit_subscribers": 172012, "created_utc": 1711470827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I\u2019m doing a project for work and needs to be resolved asap. \n\nUsing talent 7.1 open studio for reference \n\nSteps done so far: \n\n*Extraction*\n\n1. Python script to get some data from API into a .json file \n\n2. Using talend: TSystem -&gt;tFileInputDelimited. I created a schema and mapped out the fields accordingly and it generated a csv file \n\n\n* load*\n\n3 . Connected tFileInputDelimited-&gt;tDBInput_1 (snowflake) . Mapped out data base and connected the schema\n\n1st QUESTION: how do I do these steps using airflow? 1stly having an issue installing airflow in my local system. So should I use EC2 instance or lambda? If not please help provide some resources. IF not, what orchestration  tool is recommended to capture metrics?!? \n\n\n\n2nd question ( dependent on 1st): if you\u2019re able to solve the 1st, then I want to do some transformations in Snowflake but right code for that so I can capture metrics.\n", "author_fullname": "t2_hkw0w6ao", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Urgent deadline : resources requested-Using Airflow (local or AWS connect Talend &amp; Snowflake to build ELT pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo056j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711431054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I\u2019m doing a project for work and needs to be resolved asap. &lt;/p&gt;\n\n&lt;p&gt;Using talent 7.1 open studio for reference &lt;/p&gt;\n\n&lt;p&gt;Steps done so far: &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Extraction&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Python script to get some data from API into a .json file &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using talend: TSystem -&amp;gt;tFileInputDelimited. I created a schema and mapped out the fields accordingly and it generated a csv file &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;load*&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;3 . Connected tFileInputDelimited-&amp;gt;tDBInput_1 (snowflake) . Mapped out data base and connected the schema&lt;/p&gt;\n\n&lt;p&gt;1st QUESTION: how do I do these steps using airflow? 1stly having an issue installing airflow in my local system. So should I use EC2 instance or lambda? If not please help provide some resources. IF not, what orchestration  tool is recommended to capture metrics?!? &lt;/p&gt;\n\n&lt;p&gt;2nd question ( dependent on 1st): if you\u2019re able to solve the 1st, then I want to do some transformations in Snowflake but right code for that so I can capture metrics.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo056j", "is_robot_indexable": true, "report_reasons": null, "author": "DotMurky2910", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo056j/urgent_deadline_resources_requestedusing_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo056j/urgent_deadline_resources_requestedusing_airflow/", "subreddit_subscribers": 172012, "created_utc": 1711431054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is my first time attempting to tie in an API and some cloud work to an ETL. I am trying to broaden my horizon. I think my main thing I learned is making my python script more functional, instead of one LONG script.\n\nMy goal here is to show a basic Progression and degression of questions asked on programming languages on stack overflow. This shows how much programmers, developers and your day to day John Q relied on this site for information in the 2000's, 2010's and early 2020's. There is a drastic drop off in inquiries in the past 2-3 years with the creation and public availability to AI like ChatGPT, Microsoft Copilot and others.\n\nI have written a python script to connect to kaggles API, place the flat file into an AWS S3 bucket. This then loads into my Snowflake DB, from there I'm loading this into PowerBI to create a basic visualization. I chose Python and SQL cluster column charts at the top, as this is what I used and probably the two most common languages used among DE's and Analysts.", "author_fullname": "t2_8txv38ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "History of questions asked on stack over flow from 2008-2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"oo9encl6lsqc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 213, "x": 108, "u": "https://preview.redd.it/oo9encl6lsqc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=859f2b0e326cbac73b5e60038a9973f383d47a70"}, {"y": 427, "x": 216, "u": "https://preview.redd.it/oo9encl6lsqc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c69f0167518149c4235e12452efc370afda7bae5"}, {"y": 634, "x": 320, "u": "https://preview.redd.it/oo9encl6lsqc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed666ac0a1c1aa6410f4d482bdd08c560cbdb29"}, {"y": 1268, "x": 640, "u": "https://preview.redd.it/oo9encl6lsqc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5904cd82ae0c3039eba31fa460521080eeb4208"}], "s": {"y": 1797, "x": 907, "u": "https://preview.redd.it/oo9encl6lsqc1.jpg?width=907&amp;format=pjpg&amp;auto=webp&amp;s=07dfcf241f817ee33d075d04baf17fcf09bd13d2"}, "id": "oo9encl6lsqc1"}, "e6yzzcl6lsqc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/e6yzzcl6lsqc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e9e1368fe06bf2937f274817da3a910e8745d06"}, {"y": 106, "x": 216, "u": "https://preview.redd.it/e6yzzcl6lsqc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9bcb086380913d2adca8b8d11c22096b6993fd2"}, {"y": 157, "x": 320, "u": "https://preview.redd.it/e6yzzcl6lsqc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0e13466473950be3c349ff0e8a25a94fe9cca68"}, {"y": 314, "x": 640, "u": "https://preview.redd.it/e6yzzcl6lsqc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a7a23b4e891f60c964ae4f855e601083218c6d9"}, {"y": 471, "x": 960, "u": "https://preview.redd.it/e6yzzcl6lsqc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fe58a051ee390554a377f0c5a020e7e7d075c7de"}, {"y": 530, "x": 1080, "u": "https://preview.redd.it/e6yzzcl6lsqc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a44fe2ff31dcff7c0f53d614f979fdd68369f5b"}], "s": {"y": 888, "x": 1808, "u": "https://preview.redd.it/e6yzzcl6lsqc1.jpg?width=1808&amp;format=pjpg&amp;auto=webp&amp;s=740dd56d9410855e53ce7dd95ad63be65fe2d59c"}, "id": "e6yzzcl6lsqc1"}, "5teoacl6lsqc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/5teoacl6lsqc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f7aa89a5895b6e68dcaebe36e07b04fe888d538b"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/5teoacl6lsqc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a893acd76de779ea72341bb7cae45033fcdb593b"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/5teoacl6lsqc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0f5d9ba583b9b89d062093c0f40e7baccd4d2aa"}, {"y": 361, "x": 640, "u": "https://preview.redd.it/5teoacl6lsqc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=101176ab0d7251894088c4090981fc96d0f0c0a3"}, {"y": 541, "x": 960, "u": "https://preview.redd.it/5teoacl6lsqc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dca688d5d35c808a4c729869c8d035c04fe47e2e"}, {"y": 609, "x": 1080, "u": "https://preview.redd.it/5teoacl6lsqc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4893982521588b9fb3fac8e74911c123ce9d585"}], "s": {"y": 813, "x": 1441, "u": "https://preview.redd.it/5teoacl6lsqc1.jpg?width=1441&amp;format=pjpg&amp;auto=webp&amp;s=c81b73a72d63e5d276dd6edb10b4ba9e9bd6c477"}, "id": "5teoacl6lsqc1"}}, "name": "t3_1borix1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "oo9encl6lsqc1", "id": 427153101}, {"media_id": "e6yzzcl6lsqc1", "id": 427153102}, {"media_id": "5teoacl6lsqc1", "id": 427153103}]}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-hEt2aId-4-6XNSYGYzD7m5oAoMwpgnR9UdnYH9SFEk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711508397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my first time attempting to tie in an API and some cloud work to an ETL. I am trying to broaden my horizon. I think my main thing I learned is making my python script more functional, instead of one LONG script.&lt;/p&gt;\n\n&lt;p&gt;My goal here is to show a basic Progression and degression of questions asked on programming languages on stack overflow. This shows how much programmers, developers and your day to day John Q relied on this site for information in the 2000&amp;#39;s, 2010&amp;#39;s and early 2020&amp;#39;s. There is a drastic drop off in inquiries in the past 2-3 years with the creation and public availability to AI like ChatGPT, Microsoft Copilot and others.&lt;/p&gt;\n\n&lt;p&gt;I have written a python script to connect to kaggles API, place the flat file into an AWS S3 bucket. This then loads into my Snowflake DB, from there I&amp;#39;m loading this into PowerBI to create a basic visualization. I chose Python and SQL cluster column charts at the top, as this is what I used and probably the two most common languages used among DE&amp;#39;s and Analysts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/1borix1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1borix1", "is_robot_indexable": true, "report_reasons": null, "author": "Fraiz24", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1borix1/history_of_questions_asked_on_stack_over_flow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/1borix1", "subreddit_subscribers": 172012, "created_utc": 1711508397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm doing a project on football data to detect the most related players in the match based on the passes played. And how those players contribute to the formation.\n\nThe data I have include the playername, x and y coordinates of the pass, end x and end y coordinates of the pass, receiver, I also have data based on the angle of the pass, chipped or not, and I can calculate the Euclidean distance of the pass bases on its coordinates. \n\nMy professor told me to do this SNA making the players as nodes and the passes as edges, he also insisted on the passes to be weighted\n\nThis is my first time doing a SNA, my problem is mainly how can I conduct the dataframe.\nShould the columns be exactly the data I stated above?\n\nI was considering also using multiple passes from multiple players in each row, for example player A passed the ball to B which again passed to A which finally passed to C where the ball was intercepted.\n\nBut I have no idea how can I organize this in my dataframe, what should the shape of my data be? \n\nCan anybody provide some tips, I would be really thankful \ud83d\ude4f\n", "author_fullname": "t2_l8jdxq43u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I reorganize my football data for Social Network Analysis? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bonmf4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711497846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m doing a project on football data to detect the most related players in the match based on the passes played. And how those players contribute to the formation.&lt;/p&gt;\n\n&lt;p&gt;The data I have include the playername, x and y coordinates of the pass, end x and end y coordinates of the pass, receiver, I also have data based on the angle of the pass, chipped or not, and I can calculate the Euclidean distance of the pass bases on its coordinates. &lt;/p&gt;\n\n&lt;p&gt;My professor told me to do this SNA making the players as nodes and the passes as edges, he also insisted on the passes to be weighted&lt;/p&gt;\n\n&lt;p&gt;This is my first time doing a SNA, my problem is mainly how can I conduct the dataframe.\nShould the columns be exactly the data I stated above?&lt;/p&gt;\n\n&lt;p&gt;I was considering also using multiple passes from multiple players in each row, for example player A passed the ball to B which again passed to A which finally passed to C where the ball was intercepted.&lt;/p&gt;\n\n&lt;p&gt;But I have no idea how can I organize this in my dataframe, what should the shape of my data be? &lt;/p&gt;\n\n&lt;p&gt;Can anybody provide some tips, I would be really thankful \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bonmf4", "is_robot_indexable": true, "report_reasons": null, "author": "Darktrader21", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bonmf4/how_can_i_reorganize_my_football_data_for_social/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bonmf4/how_can_i_reorganize_my_football_data_for_social/", "subreddit_subscribers": 172012, "created_utc": 1711497846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Assuming that you have to write some backend or data pipeline to process, filter and store some data from A to B and also merging some data from a database C inbetween. \n\nThe data from A is mapped to some python objects. These objects are filtered/transformed based on data from C. Objects are then converted to B objects and stored to DB with sqlalchemy. Is it good to read data from C in pandas dataframes and work with them? \n\nIt sounds like a stupid question, but I come from java where everything was an object and there was no dataframe.. I find it pretty practical in this case. However I'm a bit skeptical since I know pandas from analytics and not backend systems/data pipelines.. is it a bad practice? \n\nAre there pros and cons? \n\n\n\n", "author_fullname": "t2_aelhnnee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sqlalchemy ORM vs Pandas Dataframes for backend systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bokcqq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711490076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Assuming that you have to write some backend or data pipeline to process, filter and store some data from A to B and also merging some data from a database C inbetween. &lt;/p&gt;\n\n&lt;p&gt;The data from A is mapped to some python objects. These objects are filtered/transformed based on data from C. Objects are then converted to B objects and stored to DB with sqlalchemy. Is it good to read data from C in pandas dataframes and work with them? &lt;/p&gt;\n\n&lt;p&gt;It sounds like a stupid question, but I come from java where everything was an object and there was no dataframe.. I find it pretty practical in this case. However I&amp;#39;m a bit skeptical since I know pandas from analytics and not backend systems/data pipelines.. is it a bad practice? &lt;/p&gt;\n\n&lt;p&gt;Are there pros and cons? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bokcqq", "is_robot_indexable": true, "report_reasons": null, "author": "Rogitus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bokcqq/sqlalchemy_orm_vs_pandas_dataframes_for_backend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bokcqq/sqlalchemy_orm_vs_pandas_dataframes_for_backend/", "subreddit_subscribers": 172012, "created_utc": 1711490076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! I am in the middle of ingesting some excel files and i need an opinion on the architecture.\n\nSo i have the excel files in one drive. My ideia would be to either ingest the files pass them to a database, then from that use airbyte to retrieve the data put it into snowflake using dbt, and make some dimensional modelling and then visualize.\n\n&amp;#x200B;\n\nNow this seems kinda stupid to have a database in the middle right, as well as having the files only in one drive?\n\n&amp;#x200B;\n\nBut i dont see the use of making the transition from one drive to azure storage for example.\n\n&amp;#x200B;\n\nI need help xd", "author_fullname": "t2_ud8fo6hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingestion of Excel files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bok0e9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711489270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I am in the middle of ingesting some excel files and i need an opinion on the architecture.&lt;/p&gt;\n\n&lt;p&gt;So i have the excel files in one drive. My ideia would be to either ingest the files pass them to a database, then from that use airbyte to retrieve the data put it into snowflake using dbt, and make some dimensional modelling and then visualize.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now this seems kinda stupid to have a database in the middle right, as well as having the files only in one drive?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But i dont see the use of making the transition from one drive to azure storage for example.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need help xd&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bok0e9", "is_robot_indexable": true, "report_reasons": null, "author": "PoundPotential3062", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bok0e9/ingestion_of_excel_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bok0e9/ingestion_of_excel_files/", "subreddit_subscribers": 172012, "created_utc": 1711489270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In AWS Redshift data warehouse it costs 0.0256 $ per gb to store data and in S3 (data lake) it costs 0.0018$. So, 0.0018/0.0256 = 18 times cheaper? Does this estimate allign with your experiences?\n\n  \n ", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "By how much is storage in data lakes cheaper compared to data warehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1boia8q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711485229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In AWS Redshift data warehouse it costs 0.0256 $ per gb to store data and in S3 (data lake) it costs 0.0018$. So, 0.0018/0.0256 = 18 times cheaper? Does this estimate allign with your experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1boia8q", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boia8q/by_how_much_is_storage_in_data_lakes_cheaper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boia8q/by_how_much_is_storage_in_data_lakes_cheaper/", "subreddit_subscribers": 172012, "created_utc": 1711485229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nI\u2019m looking for some input for an upcoming project we are about to undertake at my company. We are looking to replace our Azure SQL Server data warehouse with BigQuery. We are traditionally a Microsoft shop and much of our transactional data comes from other SQL Server databases via Azure Data Factory. We are currently looking at how to best ingest this data from SQL Server to BigQuery without breaking the bank. \n\nSome of our considerations are:\n1. Write and deploy our own Cloud Functions which we orchestrate via Airflow. Sounds easy enough, the challenges we see with this approach is private network connectivity and backfill. \n\n2. Try out Datastream which is now in preview for SQL Server. The CDC approach sounds tempting, but we don\u2019t have a clear requirement for streaming data at the moment. \n\n3. Go with Airbyte or Fivetran to ingest the data. If Airbyte, is self hosting a valid option?\n\nIt\u2019s small amounts of data. Roughly 100 gb of changed data each month and 1-2 tb of historical. \n\nWe are a small shop with me and a new junior DE. We are both quite proficient in Python. The rest of the stack is basically BigQuery, Airflow, Dataform and Power Bi. Any input would be greatly appreciated! \ud83d\ude0a\n", "author_fullname": "t2_2irxu023", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Server to BigQuery ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bocu8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711472282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking for some input for an upcoming project we are about to undertake at my company. We are looking to replace our Azure SQL Server data warehouse with BigQuery. We are traditionally a Microsoft shop and much of our transactional data comes from other SQL Server databases via Azure Data Factory. We are currently looking at how to best ingest this data from SQL Server to BigQuery without breaking the bank. &lt;/p&gt;\n\n&lt;p&gt;Some of our considerations are:\n1. Write and deploy our own Cloud Functions which we orchestrate via Airflow. Sounds easy enough, the challenges we see with this approach is private network connectivity and backfill. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Try out Datastream which is now in preview for SQL Server. The CDC approach sounds tempting, but we don\u2019t have a clear requirement for streaming data at the moment. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Go with Airbyte or Fivetran to ingest the data. If Airbyte, is self hosting a valid option?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It\u2019s small amounts of data. Roughly 100 gb of changed data each month and 1-2 tb of historical. &lt;/p&gt;\n\n&lt;p&gt;We are a small shop with me and a new junior DE. We are both quite proficient in Python. The rest of the stack is basically BigQuery, Airflow, Dataform and Power Bi. Any input would be greatly appreciated! \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bocu8c", "is_robot_indexable": true, "report_reasons": null, "author": "AlexanderOlsson", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bocu8c/sql_server_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bocu8c/sql_server_to_bigquery/", "subreddit_subscribers": 172012, "created_utc": 1711472282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm trying to benchmark the TPCx-BB on 30TB scale factor. Any recommended type of nodes and amount of nodes on Google Cloud? I have no experience in this cloud provider.", "author_fullname": "t2_v22qxq2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks TPCx-BB 30TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo85gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711460382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to benchmark the TPCx-BB on 30TB scale factor. Any recommended type of nodes and amount of nodes on Google Cloud? I have no experience in this cloud provider.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo85gj", "is_robot_indexable": true, "report_reasons": null, "author": "All-is-data3891", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo85gj/databricks_tpcxbb_30tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo85gj/databricks_tpcxbb_30tb/", "subreddit_subscribers": 172012, "created_utc": 1711460382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Currently on the market there are integration tools with different capabilities. If we deep dive more into it is that they are tools suitable for application integration ( APIs, event-driven, real time) and data integration (batch, big amount of data). Now, for example, for application integration the suggested tools are Mulesoft or boomi, while for ETL is in general informatica or talend. \n\nCan someone educate me why the design architecture of tool like boomi is not suitable for ETL but informatica is? I guess there must be something, just was not able to find anything concrete.\n", "author_fullname": "t2_3otium74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Application vs Data integration tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo5yzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711453920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently on the market there are integration tools with different capabilities. If we deep dive more into it is that they are tools suitable for application integration ( APIs, event-driven, real time) and data integration (batch, big amount of data). Now, for example, for application integration the suggested tools are Mulesoft or boomi, while for ETL is in general informatica or talend. &lt;/p&gt;\n\n&lt;p&gt;Can someone educate me why the design architecture of tool like boomi is not suitable for ETL but informatica is? I guess there must be something, just was not able to find anything concrete.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bo5yzw", "is_robot_indexable": true, "report_reasons": null, "author": "Legal_Explanation_59", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo5yzw/application_vs_data_integration_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo5yzw/application_vs_data_integration_tools/", "subreddit_subscribers": 172012, "created_utc": 1711453920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working with a transactional database that authors records that change retrospectively.\nEG a supplier may have an attribute that changes each day (as an example).\nThis means a record with the same supplier ID is changing each day, but I dont want this duplicated in my final suppliers table.\nIs there any way to approach this without doing a window function and hence reading/shuffling my entire dataset each time? I'm finding compute limits on my dataset (approx 30gb) because i am deduping and then partitioning by date (resulting in a massivle shuffle) and disk spillage.", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing Duplicates with PySpark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bo5a7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711451570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working with a transactional database that authors records that change retrospectively.\nEG a supplier may have an attribute that changes each day (as an example).\nThis means a record with the same supplier ID is changing each day, but I dont want this duplicated in my final suppliers table.\nIs there any way to approach this without doing a window function and hence reading/shuffling my entire dataset each time? I&amp;#39;m finding compute limits on my dataset (approx 30gb) because i am deduping and then partitioning by date (resulting in a massivle shuffle) and disk spillage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bo5a7j", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bo5a7j/processing_duplicates_with_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bo5a7j/processing_duplicates_with_pyspark/", "subreddit_subscribers": 172012, "created_utc": 1711451570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It might be a silly question, but what volumes of data did you see in data lakes?\n\nedit: and what monthly bills did you see for storage whether in S3 or other provider?\n\nedit 2: how many sources?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How big does big data get? in data lakes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1boh9qk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711482804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It might be a silly question, but what volumes of data did you see in data lakes?&lt;/p&gt;\n\n&lt;p&gt;edit: and what monthly bills did you see for storage whether in S3 or other provider?&lt;/p&gt;\n\n&lt;p&gt;edit 2: how many sources?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1boh9qk", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1boh9qk/how_big_does_big_data_get_in_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1boh9qk/how_big_does_big_data_get_in_data_lakes/", "subreddit_subscribers": 172012, "created_utc": 1711482804.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}