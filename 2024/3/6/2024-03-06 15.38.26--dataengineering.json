{"kind": "Listing", "data": {"after": "t3_1b7l586", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Mentally preparing myself for the eventual request to untangle this mess ", "author_fullname": "t2_ajstu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An actual post in my company Slack today ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7ojk4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 210, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 210, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/45hzb4JKb6XUlfc87joYwCKNrz1bIFjatQAXDtaSKKk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709693068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mentally preparing myself for the eventual request to untangle this mess &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nepsf40anmmc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nepsf40anmmc1.png?auto=webp&amp;s=740ca8fffc4c773df10b30bd58760f98ddb5cf5f", "width": 1008, "height": 1021}, "resolutions": [{"url": "https://preview.redd.it/nepsf40anmmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=306cde57d791a7f7ac2d3fdaf21ee1ccfb7cb1a0", "width": 108, "height": 109}, {"url": "https://preview.redd.it/nepsf40anmmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=11ce1790efe05b5753efed3324f0e089fac759bf", "width": 216, "height": 218}, {"url": "https://preview.redd.it/nepsf40anmmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b02882706943fd5dbf19f353937e3330a0c68260", "width": 320, "height": 324}, {"url": "https://preview.redd.it/nepsf40anmmc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=763a74e15788e9fc58f4b055684cf198bb988f8a", "width": 640, "height": 648}, {"url": "https://preview.redd.it/nepsf40anmmc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d8133f337b034435917344ca0ac8027caf8f395e", "width": 960, "height": 972}], "variants": {}, "id": "mSITuxfL21sELbVbAM8NXZhwUgSwLzyIhIe3SLKgD9E"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1b7ojk4", "is_robot_indexable": true, "report_reasons": null, "author": "OneSixteenthRobot", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7ojk4/an_actual_post_in_my_company_slack_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nepsf40anmmc1.png", "subreddit_subscribers": 166326, "created_utc": 1709693068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently going through a massive reorg and conversion. The existing ETL is built with Ab Initio and the DB is Teradata. We are now moving everything to Databricks / azure. Any advice on learning databricks? I have never utilized python, and now apparently pyspark is what will be used to build the ETL in Databricks. How different is pyspark to python and any advices on learning this as well? I was new to DE when I got here and inherited the legacy systems, so this is going to be the first tools / coding I do from the ground up. Thanks for any advice! ", "author_fullname": "t2_73cw9sv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Company is converting to Databricks!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7fz0q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709671347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently going through a massive reorg and conversion. The existing ETL is built with Ab Initio and the DB is Teradata. We are now moving everything to Databricks / azure. Any advice on learning databricks? I have never utilized python, and now apparently pyspark is what will be used to build the ETL in Databricks. How different is pyspark to python and any advices on learning this as well? I was new to DE when I got here and inherited the legacy systems, so this is going to be the first tools / coding I do from the ground up. Thanks for any advice! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b7fz0q", "is_robot_indexable": true, "report_reasons": null, "author": "ApatheticRart", "discussion_type": null, "num_comments": 63, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7fz0q/company_is_converting_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7fz0q/company_is_converting_to_databricks/", "subreddit_subscribers": 166326, "created_utc": 1709671347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "   \nI'm currently working in the data aspect of engineering. The path I've embarked upon involves SQL, on-premises ETL tools, and reporting. However, I'm eager to transition into Cloud Data Engineering. I've begun analyzing the requirements companies post for such roles and the number of applicants for each position. It's overwhelming to observe that almost everyone is now identifying as a data engineer, regardless of their experience. I know individuals who transitioned from roles such as Database Administration or C/C++ programming to data engineering. Each job application I've seen attracts anywhere from 500 to 1200 applicants. Additionally, companies are requesting a minimum of 10 skills for data engineering roles, spanning from database management to developing streaming applications. With over 10 years of experience, I wonder if I can secure a job within a year, considering the multitude of skills I need to acquire and the intense competition. Is the effort truly worth it, especially given that I need to start from learning Python to mastering various cloud platforms?\n\n   \nTo be honest, I'm more inclined to master a select few skills rather than trying to be a jack of all trades. I'm aiming to specialize in those areas and work towards achieving a decent pay, perhaps around $100k to $120k, instead of chasing after the salaries of data engineers who are earning approximately $250- 500k with over 10 years of experience. \n\n I'd appreciate your thoughts on this matter. ", "author_fullname": "t2_q2p51ehcb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is everyone becoming a data engineer? And is it still worth embarking on this career journey?\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7fcgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709669860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working in the data aspect of engineering. The path I&amp;#39;ve embarked upon involves SQL, on-premises ETL tools, and reporting. However, I&amp;#39;m eager to transition into Cloud Data Engineering. I&amp;#39;ve begun analyzing the requirements companies post for such roles and the number of applicants for each position. It&amp;#39;s overwhelming to observe that almost everyone is now identifying as a data engineer, regardless of their experience. I know individuals who transitioned from roles such as Database Administration or C/C++ programming to data engineering. Each job application I&amp;#39;ve seen attracts anywhere from 500 to 1200 applicants. Additionally, companies are requesting a minimum of 10 skills for data engineering roles, spanning from database management to developing streaming applications. With over 10 years of experience, I wonder if I can secure a job within a year, considering the multitude of skills I need to acquire and the intense competition. Is the effort truly worth it, especially given that I need to start from learning Python to mastering various cloud platforms?&lt;/p&gt;\n\n&lt;p&gt;To be honest, I&amp;#39;m more inclined to master a select few skills rather than trying to be a jack of all trades. I&amp;#39;m aiming to specialize in those areas and work towards achieving a decent pay, perhaps around $100k to $120k, instead of chasing after the salaries of data engineers who are earning approximately $250- 500k with over 10 years of experience. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate your thoughts on this matter. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b7fcgo", "is_robot_indexable": true, "report_reasons": null, "author": "AccomplishedHat9906", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7fcgo/is_everyone_becoming_a_data_engineer_and_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7fcgo/is_everyone_becoming_a_data_engineer_and_is_it/", "subreddit_subscribers": 166326, "created_utc": 1709669860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you have an un-versioned source which unexpectedly changes its schema, how do you deal with that situation? \n\nDo you maintain a manually entered schema and trigger alert on reads if schema validation fails?\n\nIs it assumed the pipeline will eventually break, so you write your pipeline in a more 'defensive' way?\n\nIs there a Sentry-like tool for ETL pipelines, meaning it can alert when things break?\n\nPlease excuse my lack of knowledge on this topic, I'm very new to DE.", "author_fullname": "t2_tnf3rfrjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you detect source schema changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7f9f6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "287cf772-ac9d-11eb-aa84-0ead36cb44af", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709669655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have an un-versioned source which unexpectedly changes its schema, how do you deal with that situation? &lt;/p&gt;\n\n&lt;p&gt;Do you maintain a manually entered schema and trigger alert on reads if schema validation fails?&lt;/p&gt;\n\n&lt;p&gt;Is it assumed the pipeline will eventually break, so you write your pipeline in a more &amp;#39;defensive&amp;#39; way?&lt;/p&gt;\n\n&lt;p&gt;Is there a Sentry-like tool for ETL pipelines, meaning it can alert when things break?&lt;/p&gt;\n\n&lt;p&gt;Please excuse my lack of knowledge on this topic, I&amp;#39;m very new to DE.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Software Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b7f9f6", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Coat5856", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b7f9f6/how_do_you_detect_source_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7f9f6/how_do_you_detect_source_schema_changes/", "subreddit_subscribers": 166326, "created_utc": 1709669655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious to hear about how much the industry sector of your company has affected your career and your job satisfaction. I'm working for a property management company in my first DE job and I'm learning a ton, but I'm not sure if real estate is really my jam so to speak. What are good industries to work for in DE right now?", "author_fullname": "t2_653as", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Senior DE and above: what happened when you changed industries? Did it have a big affect on your career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7mo16", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709687856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious to hear about how much the industry sector of your company has affected your career and your job satisfaction. I&amp;#39;m working for a property management company in my first DE job and I&amp;#39;m learning a ton, but I&amp;#39;m not sure if real estate is really my jam so to speak. What are good industries to work for in DE right now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b7mo16", "is_robot_indexable": true, "report_reasons": null, "author": "tedward27", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7mo16/senior_de_and_above_what_happened_when_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7mo16/senior_de_and_above_what_happened_when_you/", "subreddit_subscribers": 166326, "created_utc": 1709687856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nComing from someone outside the data engineer field, I am just trying to understand what Hadoop is for?\n\nI know, I've googled and read posts, articles and all and it doesn't really stick on me.\n\nIs it really considered a file system or is it just an algorithm to distribute data across nodes in a cluster?\n\nDocs says it's good to process datasets. What are datasets? Are these files, databases? CSVs?\\\\\n\nHow does Hadoop relates to these concepts, datasets, HDFS and ORC (Optimized Row Columnar)\n\nI really think a simple example that I couldnt really find in internet could be the key for me to understand that once and for all.\n\nAnd finally, how Spark and BigData relates to it?", "author_fullname": "t2_7al5p0w0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Hadoop and it's relation with Spark and BigData?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7mhka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709687753.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709687367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Coming from someone outside the data engineer field, I am just trying to understand what Hadoop is for?&lt;/p&gt;\n\n&lt;p&gt;I know, I&amp;#39;ve googled and read posts, articles and all and it doesn&amp;#39;t really stick on me.&lt;/p&gt;\n\n&lt;p&gt;Is it really considered a file system or is it just an algorithm to distribute data across nodes in a cluster?&lt;/p&gt;\n\n&lt;p&gt;Docs says it&amp;#39;s good to process datasets. What are datasets? Are these files, databases? CSVs?\\&lt;/p&gt;\n\n&lt;p&gt;How does Hadoop relates to these concepts, datasets, HDFS and ORC (Optimized Row Columnar)&lt;/p&gt;\n\n&lt;p&gt;I really think a simple example that I couldnt really find in internet could be the key for me to understand that once and for all.&lt;/p&gt;\n\n&lt;p&gt;And finally, how Spark and BigData relates to it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b7mhka", "is_robot_indexable": true, "report_reasons": null, "author": "erudes91", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7mhka/what_is_hadoop_and_its_relation_with_spark_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7mhka/what_is_hadoop_and_its_relation_with_spark_and/", "subreddit_subscribers": 166326, "created_utc": 1709687367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings,\n\nI am seeking assistance to comprehend how TDD (Test Driven Development), a concept from software engineering, is adapted for data engineering. As a data scientist, I want to stay informed about trends in the data domain, though my expertise is not primarily in data engineering.\n\nI grasp the core principles of TDD in software development, but its application in data engineering eludes me, despite reading numerous articles and examining some training examples that appeared impractical to me.\n\nMy experience mainly involves external data acquisition, especially in the finance sector. In data engineering, if one constructs a pipeline to ingest data from an external source for ML models, setting up this pipeline constitutes the crux of the work, besides establishing the necessary infrastructure, correct? With TDD, the focus would be on the methodology for coding transformations along the pipeline. However, how can one formulate sample, edge, and corner cases for transformation unit tests without first experimenting with data from the source? Incoming datasets and sources seem far more stochastic, dynamic, and complex than what a typical software module faces, making it challenging to establish test cases and expected results without preliminary data experimentation. And it seems to me, one needs to experiment with the transformation steps to reach to meaningful test cases. But then the whole idea behind TDD is lost if you have the code before the test case, is this not true?\n\nFor example, I have seen some training example with Python-based pytests for simple Spark row-wise transformations on streaming CSV data, like breaking down a date column into its components. This are just atomic transformations, based on built in functions which should be rather tested by the developer building that specific module (e.g. pyspark), am I not correct?\n\nIn my experience, data pipeline failures often result from changes in incoming raw data rather than code errors in basic transformations. These anomalies in data, such as definition changes, are hard to anticipate with testable code. Wouldn't it be more practical to focus on detecting quality issues at certain points and testing the data itself?\n\nFurthermore, most problems I've encountered were not something to capture by row-level windows. Many transformations involve complex aggregations and windows, where checking longitudinal distributions and detecting drifting is critical. A basic example is a time series panel, which should contain all business days within a range, and you need to make sure if there are no gaps in the data. However, preparing for these issues seems more aligned with data contracts and data testing (like dbt data tests), rather than TDD. While I recognize dbt's new unit testing capabilities and understand their [example](https://docs.getdbt.com/docs/build/unit-tests) of unit testing an embedded complex function in a SQL query, such scenarios seem unrealistic in my previous work environment. Failures typically rather stemmed from incoming data changes and definition shifts, making tests around these aspects more critical.\n\nI would appreciate any insights on applying TDD to data engineering work, especially in scenarios involving the ingestion of dynamically changing, low-quality external data from various vendors.", "author_fullname": "t2_ldzpx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me understand Test Driven Development within Data Engineering work.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b79p9w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709656649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings,&lt;/p&gt;\n\n&lt;p&gt;I am seeking assistance to comprehend how TDD (Test Driven Development), a concept from software engineering, is adapted for data engineering. As a data scientist, I want to stay informed about trends in the data domain, though my expertise is not primarily in data engineering.&lt;/p&gt;\n\n&lt;p&gt;I grasp the core principles of TDD in software development, but its application in data engineering eludes me, despite reading numerous articles and examining some training examples that appeared impractical to me.&lt;/p&gt;\n\n&lt;p&gt;My experience mainly involves external data acquisition, especially in the finance sector. In data engineering, if one constructs a pipeline to ingest data from an external source for ML models, setting up this pipeline constitutes the crux of the work, besides establishing the necessary infrastructure, correct? With TDD, the focus would be on the methodology for coding transformations along the pipeline. However, how can one formulate sample, edge, and corner cases for transformation unit tests without first experimenting with data from the source? Incoming datasets and sources seem far more stochastic, dynamic, and complex than what a typical software module faces, making it challenging to establish test cases and expected results without preliminary data experimentation. And it seems to me, one needs to experiment with the transformation steps to reach to meaningful test cases. But then the whole idea behind TDD is lost if you have the code before the test case, is this not true?&lt;/p&gt;\n\n&lt;p&gt;For example, I have seen some training example with Python-based pytests for simple Spark row-wise transformations on streaming CSV data, like breaking down a date column into its components. This are just atomic transformations, based on built in functions which should be rather tested by the developer building that specific module (e.g. pyspark), am I not correct?&lt;/p&gt;\n\n&lt;p&gt;In my experience, data pipeline failures often result from changes in incoming raw data rather than code errors in basic transformations. These anomalies in data, such as definition changes, are hard to anticipate with testable code. Wouldn&amp;#39;t it be more practical to focus on detecting quality issues at certain points and testing the data itself?&lt;/p&gt;\n\n&lt;p&gt;Furthermore, most problems I&amp;#39;ve encountered were not something to capture by row-level windows. Many transformations involve complex aggregations and windows, where checking longitudinal distributions and detecting drifting is critical. A basic example is a time series panel, which should contain all business days within a range, and you need to make sure if there are no gaps in the data. However, preparing for these issues seems more aligned with data contracts and data testing (like dbt data tests), rather than TDD. While I recognize dbt&amp;#39;s new unit testing capabilities and understand their &lt;a href=\"https://docs.getdbt.com/docs/build/unit-tests\"&gt;example&lt;/a&gt; of unit testing an embedded complex function in a SQL query, such scenarios seem unrealistic in my previous work environment. Failures typically rather stemmed from incoming data changes and definition shifts, making tests around these aspects more critical.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate any insights on applying TDD to data engineering work, especially in scenarios involving the ingestion of dynamically changing, low-quality external data from various vendors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?auto=webp&amp;s=2a89f01968bbb7160773570a5739ba364e017ebf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e39c972215449e24ba187a3b3e6d0289aad02d1b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e48b5b0440098be5b7b54dcdd6d78e80f77e948", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c583ec988ffb5d6d8292b88b38a2a7ac9fc2b799", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a97be3626c69aab79c2204db47f040a6a8bb9820", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ba90b674ccf1906f5a13abd09b27db16d203bd0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197f95d4689989cecbdb537c3aa18035536b0c50", "width": 1080, "height": 567}], "variants": {}, "id": "s9XQCWayWEjVSYNiK4ez8RIl3EBcstjT4Cv_3rHuPvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b79p9w", "is_robot_indexable": true, "report_reasons": null, "author": "petkow", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b79p9w/help_me_understand_test_driven_development_within/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b79p9w/help_me_understand_test_driven_development_within/", "subreddit_subscribers": 166326, "created_utc": 1709656649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, recently I completed another personal project. Any suggestions are welcome.\n\n[Github Repo](https://github.com/Zzdragon66/stock-streaming-project)\n\n## Project Description\n\n* This project leverages Python, Kafka, and Spark to process real-time streaming data from both stock markets and Reddit. It employs a Long Short-Term Memory (LSTM) deep learning model to conduct real-time predictions on SPY (S&amp;P 500 ETF) stock data. Additionally, the project utilizes Grafana for the real-time visualization of stock data, predictive analytics, and reddit data, providing a comprehensive and dynamic overview of market trends and sentiments.\n\n## Demo\n\n&amp;#x200B;\n\nhttps://i.redd.it/t85j4210dpmc1.gif\n\n## Project Structure\n\n&amp;#x200B;\n\nhttps://preview.redd.it/n292wc61dpmc1.png?width=4164&amp;format=png&amp;auto=webp&amp;s=76dcc8279e38327babe8c954c05b17906ba8453c\n\n## Tools\n\n1. Apache Airflow: Data pipeline orchestration\n2. Apache Kafka: Stream data handling\n3. Apache Spark: batch data processing\n4. Apache Cassandra: NoSQL database to store time series data\n5. Docker + Kubernets: Containerization and Docker Orchestration\n6. Pytorch: Deep learning model\n7. Grafna: Stream Data visualization\n8. Python: produce streaming data with multithreading\n\n## Project Design Choice\n\n## Kafka\n\n* Why Kafka?\n   * Kafak serves a stream data handler to feed data into spark and deep learning model\n* Design of kafka\n   * I utilize Python's multi-threading capabilities to simultaneously produce stock data, enhancing the throughput by exploiting parallelism. Consequently, I partition the topic according to the number of stocks, allowing each thread to direct its data into a distinct partition, thereby optimizing the data flow and maximizing efficiency\n\n## Cassandra Database Design\n\n* Stock data contains the data of `stock` symbol and `utc_timestamp`, which can be used to uniquely identify the single data point. Therefore I use those two features as the primary key\n* Use `utc_timestamp` as the clustering key to store the time series data in ascending order for efficient read(sequantial read for a time series data) and high throughput write(real-time data only appends to the end of parition)\n\n## Deep learning model Discussion\n\n* Data\n   * Train Data Dimension (N, T, D)\n      * N is number of data in a batch\n      * T=200 look back two hundred seconds data\n      * D=5 the features in the data (price, number of transactions, high price, low price, volumes)\n   * Prediction Data Dimension (1, 200, 5)\n* Data Preprocessing:\n   * Use MinMaxScaler to make sure each feature has similar scale\n* Model Structure:\n   * X-&gt;\\[LSTM \\* 5\\]-&gt;Linear-&gt;Price-Prediction\n* How the Model works:\n   * At current timestamp t, get latest 200 time sereis data before $t$ in ascending `utc_timestamp` order. Feed the data into deep learning model which will predict the current SPY stock prie at time t.\n* Due to the limited computational resources on my local machine, the \"real-time\" prediction lags behind actual time because of the long computation duration required.\n\n## Future Directions\n\n1. Deploy the local kubernets to AWS EKS and Use GPU accelerator on cloud\n2. Train a better deep learning model to make prediction more accurate and faster", "author_fullname": "t2_5igde9z6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End-End Stock Streaming Project(K8S, Airflow, Kafka, Spark, Pytorch, Docker, Cassandra, Grafna)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"t85j4210dpmc1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=31a407bb797466af045aaacbc93e581ff4f9c604"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=3f984dbc901edba448538f4312ee5c65378d0457"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=b9d568e98b52007048fd496e3e88d056677ca222"}, {"y": 336, "x": 640, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=1f055006987c65e7d8a44ab5e684b7968448b785"}, {"y": 505, "x": 960, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=973819231667912f2bd709258a716265b1aae6b9"}, {"y": 568, "x": 1080, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=8729eaaf8bd74765e4bc387a1e38d29f3135397f"}], "s": {"y": 720, "gif": "https://i.redd.it/t85j4210dpmc1.gif", "mp4": "https://preview.redd.it/t85j4210dpmc1.gif?format=mp4&amp;s=86060fce964241e914ea0f962b15505fa701cb45", "x": 1368}, "id": "t85j4210dpmc1"}, "n292wc61dpmc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/n292wc61dpmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=06952bd781e59b01b59e1ea9f70e28a414a59b10"}, {"y": 107, "x": 216, "u": "https://preview.redd.it/n292wc61dpmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0054f645ce39a1e55d9387184a0bdfe10d5a4c45"}, {"y": 158, "x": 320, "u": "https://preview.redd.it/n292wc61dpmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8690d6dd977fa313da788fea8dedc619474cb31"}, {"y": 317, "x": 640, "u": "https://preview.redd.it/n292wc61dpmc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=af2fb8e45950c24c2eb0119ccd66a20b1707a97d"}, {"y": 475, "x": 960, "u": "https://preview.redd.it/n292wc61dpmc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64755b34f27cae4ccca0d0cbb161851aa388f5a2"}, {"y": 535, "x": 1080, "u": "https://preview.redd.it/n292wc61dpmc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2acf223149cbe17a5e306798d5248d776ee3897"}], "s": {"y": 2064, "x": 4164, "u": "https://preview.redd.it/n292wc61dpmc1.png?width=4164&amp;format=png&amp;auto=webp&amp;s=76dcc8279e38327babe8c954c05b17906ba8453c"}, "id": "n292wc61dpmc1"}}, "name": "t3_1b7xuw3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NnTWeD7Jq6sjwYJ_3JRMAz7fH0IDgK0LKZoP69eodVA.jpg", "edited": 1709726718.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1709726056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, recently I completed another personal project. Any suggestions are welcome.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Zzdragon66/stock-streaming-project\"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Project Description&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;This project leverages Python, Kafka, and Spark to process real-time streaming data from both stock markets and Reddit. It employs a Long Short-Term Memory (LSTM) deep learning model to conduct real-time predictions on SPY (S&amp;amp;P 500 ETF) stock data. Additionally, the project utilizes Grafana for the real-time visualization of stock data, predictive analytics, and reddit data, providing a comprehensive and dynamic overview of market trends and sentiments.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Demo&lt;/h2&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/t85j4210dpmc1.gif\"&gt;https://i.redd.it/t85j4210dpmc1.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Project Structure&lt;/h2&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/n292wc61dpmc1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76dcc8279e38327babe8c954c05b17906ba8453c\"&gt;https://preview.redd.it/n292wc61dpmc1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76dcc8279e38327babe8c954c05b17906ba8453c&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Tools&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Apache Airflow: Data pipeline orchestration&lt;/li&gt;\n&lt;li&gt;Apache Kafka: Stream data handling&lt;/li&gt;\n&lt;li&gt;Apache Spark: batch data processing&lt;/li&gt;\n&lt;li&gt;Apache Cassandra: NoSQL database to store time series data&lt;/li&gt;\n&lt;li&gt;Docker + Kubernets: Containerization and Docker Orchestration&lt;/li&gt;\n&lt;li&gt;Pytorch: Deep learning model&lt;/li&gt;\n&lt;li&gt;Grafna: Stream Data visualization&lt;/li&gt;\n&lt;li&gt;Python: produce streaming data with multithreading&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h2&gt;Project Design Choice&lt;/h2&gt;\n\n&lt;h2&gt;Kafka&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Why Kafka?\n\n&lt;ul&gt;\n&lt;li&gt;Kafak serves a stream data handler to feed data into spark and deep learning model&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Design of kafka\n\n&lt;ul&gt;\n&lt;li&gt;I utilize Python&amp;#39;s multi-threading capabilities to simultaneously produce stock data, enhancing the throughput by exploiting parallelism. Consequently, I partition the topic according to the number of stocks, allowing each thread to direct its data into a distinct partition, thereby optimizing the data flow and maximizing efficiency&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Cassandra Database Design&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Stock data contains the data of &lt;code&gt;stock&lt;/code&gt; symbol and &lt;code&gt;utc_timestamp&lt;/code&gt;, which can be used to uniquely identify the single data point. Therefore I use those two features as the primary key&lt;/li&gt;\n&lt;li&gt;Use &lt;code&gt;utc_timestamp&lt;/code&gt; as the clustering key to store the time series data in ascending order for efficient read(sequantial read for a time series data) and high throughput write(real-time data only appends to the end of parition)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Deep learning model Discussion&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data\n\n&lt;ul&gt;\n&lt;li&gt;Train Data Dimension (N, T, D)\n\n&lt;ul&gt;\n&lt;li&gt;N is number of data in a batch&lt;/li&gt;\n&lt;li&gt;T=200 look back two hundred seconds data&lt;/li&gt;\n&lt;li&gt;D=5 the features in the data (price, number of transactions, high price, low price, volumes)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Prediction Data Dimension (1, 200, 5)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Data Preprocessing:\n\n&lt;ul&gt;\n&lt;li&gt;Use MinMaxScaler to make sure each feature has similar scale&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Model Structure:\n\n&lt;ul&gt;\n&lt;li&gt;X-&amp;gt;[LSTM * 5]-&amp;gt;Linear-&amp;gt;Price-Prediction&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;How the Model works:\n\n&lt;ul&gt;\n&lt;li&gt;At current timestamp t, get latest 200 time sereis data before $t$ in ascending &lt;code&gt;utc_timestamp&lt;/code&gt; order. Feed the data into deep learning model which will predict the current SPY stock prie at time t.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Due to the limited computational resources on my local machine, the &amp;quot;real-time&amp;quot; prediction lags behind actual time because of the long computation duration required.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Future Directions&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Deploy the local kubernets to AWS EKS and Use GPU accelerator on cloud&lt;/li&gt;\n&lt;li&gt;Train a better deep learning model to make prediction more accurate and faster&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/R7FEafIHAyD-tonTPsdPH3AAjgxXtjnudb3jzwo2vms.jpg?auto=webp&amp;s=0dffb8e257a9c56177690da3b4d7eb28b035313f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/R7FEafIHAyD-tonTPsdPH3AAjgxXtjnudb3jzwo2vms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=986b81c3c895b8bdc7d8fbc7b157598533d0708a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/R7FEafIHAyD-tonTPsdPH3AAjgxXtjnudb3jzwo2vms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76e711b7b52c25ae632cdb1d13e2c0afc9088b95", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/R7FEafIHAyD-tonTPsdPH3AAjgxXtjnudb3jzwo2vms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1fc993ea8ea657a4f0b6218d1ab80f8f785ffec3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/R7FEafIHAyD-tonTPsdPH3AAjgxXtjnudb3jzwo2vms.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=179bcba689e824ef31d56f95e4d24c55a352bbd2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/R7FEafIHAyD-tonTPsdPH3AAjgxXtjnudb3jzwo2vms.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=04d04a27c7a25944add1502cbacc5123cc428847", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/R7FEafIHAyD-tonTPsdPH3AAjgxXtjnudb3jzwo2vms.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7327f06c591c2a76917f274a05850fbc8f4e182", "width": 1080, "height": 540}], "variants": {}, "id": "Rj7Hkx8aj--pA4V1ZFv7Ec4bNEzMf6LrYPbsGILQTk8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1b7xuw3", "is_robot_indexable": true, "report_reasons": null, "author": "AffectionateEmu8146", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7xuw3/endend_stock_streaming_projectk8s_airflow_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7xuw3/endend_stock_streaming_projectk8s_airflow_kafka/", "subreddit_subscribers": 166326, "created_utc": 1709726056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data analytics professional of 6 years who started becoming interested in DE after finding software engineering more interesting than my statistics/data analysis work. I first started out by taking python OOP, computer science fundamental and full stack courses. I enjoyed that stuff and had some full stack knowledge beforehand, but the issue was from a career perspective that I had so much experience in data that full stack dev seemed like a disconnect from my past work and I started to look at something that would blend my data skills with software engineering interests. \n\nI started looking into DE, but I didn't quite understand where to even start. I have automated data workflows in the past, but I was using SQL, R and Cron-literally leveraging whatever tools I had because the work previous analysts were doing was very manual and tedious. I don't have formal experience with cloud technologies, airflow, kafka, databricks, etc. I used Docker and AWS on personal projects and kind of understand it, but I'm not an expert by any means. \n\nI just got my first DE job after hundreds of applications for data engineer, software engineer and data science roles. I'm excited but I also don't know what I should prioritize in my learning journey. I bought the joe reis data engineering book months ago and am doing the data engineering zoomcamp, but am so behind on that due to other life things. The DE zoomcamp seems like a great resource so far and it's amazing it's free-but I'm sort of feeling like I'm going through the motions without really understanding what exactly I'm doing. I also saw that AWS has certifications and the job I'm going to be doing uses Redshift so I thought I'd look into that. \n\nAny suggestions on what to start with? ", "author_fullname": "t2_2prckadt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I prioritize learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7nrzd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709690933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data analytics professional of 6 years who started becoming interested in DE after finding software engineering more interesting than my statistics/data analysis work. I first started out by taking python OOP, computer science fundamental and full stack courses. I enjoyed that stuff and had some full stack knowledge beforehand, but the issue was from a career perspective that I had so much experience in data that full stack dev seemed like a disconnect from my past work and I started to look at something that would blend my data skills with software engineering interests. &lt;/p&gt;\n\n&lt;p&gt;I started looking into DE, but I didn&amp;#39;t quite understand where to even start. I have automated data workflows in the past, but I was using SQL, R and Cron-literally leveraging whatever tools I had because the work previous analysts were doing was very manual and tedious. I don&amp;#39;t have formal experience with cloud technologies, airflow, kafka, databricks, etc. I used Docker and AWS on personal projects and kind of understand it, but I&amp;#39;m not an expert by any means. &lt;/p&gt;\n\n&lt;p&gt;I just got my first DE job after hundreds of applications for data engineer, software engineer and data science roles. I&amp;#39;m excited but I also don&amp;#39;t know what I should prioritize in my learning journey. I bought the joe reis data engineering book months ago and am doing the data engineering zoomcamp, but am so behind on that due to other life things. The DE zoomcamp seems like a great resource so far and it&amp;#39;s amazing it&amp;#39;s free-but I&amp;#39;m sort of feeling like I&amp;#39;m going through the motions without really understanding what exactly I&amp;#39;m doing. I also saw that AWS has certifications and the job I&amp;#39;m going to be doing uses Redshift so I thought I&amp;#39;d look into that. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions on what to start with? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b7nrzd", "is_robot_indexable": true, "report_reasons": null, "author": "thro0away12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7nrzd/what_should_i_prioritize_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7nrzd/what_should_i_prioritize_learning/", "subreddit_subscribers": 166326, "created_utc": 1709690933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m just shocked\u2026 flabbergasted, actually, that my entire degree had no mention of this word.\n\nConnascence is a concept similar to dependencies and coupling. If a change in one part of something requires changes in other parts of something, that\u2019s connascence. Apparently there\u2019s a decent amount of academic work done on this subject, fresh for the study. \n\nMy first impressions is that you can probably explain very well why Docker is so successful, with only a solid understanding of connascence and how servers / applications work. It minimizes connascence between the machine and the app.\n\nThere are categories: static connascence and dynamic connascence. Both have subcategories, such as value connascence, identity connascence, name connascence, \u2026\n\nI\u2019m finding this neat because it makes talking about an abstract and difficult topic, easier. I\u2019m suddenly making realizations about many of my systems and their connascence, how I could change their connascence measures\u2026\n\nThis is cool stuff, guys.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Let\u2019s have a talk about connascence, shall we?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7meq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709687142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m just shocked\u2026 flabbergasted, actually, that my entire degree had no mention of this word.&lt;/p&gt;\n\n&lt;p&gt;Connascence is a concept similar to dependencies and coupling. If a change in one part of something requires changes in other parts of something, that\u2019s connascence. Apparently there\u2019s a decent amount of academic work done on this subject, fresh for the study. &lt;/p&gt;\n\n&lt;p&gt;My first impressions is that you can probably explain very well why Docker is so successful, with only a solid understanding of connascence and how servers / applications work. It minimizes connascence between the machine and the app.&lt;/p&gt;\n\n&lt;p&gt;There are categories: static connascence and dynamic connascence. Both have subcategories, such as value connascence, identity connascence, name connascence, \u2026&lt;/p&gt;\n\n&lt;p&gt;I\u2019m finding this neat because it makes talking about an abstract and difficult topic, easier. I\u2019m suddenly making realizations about many of my systems and their connascence, how I could change their connascence measures\u2026&lt;/p&gt;\n\n&lt;p&gt;This is cool stuff, guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b7meq0", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7meq0/lets_have_a_talk_about_connascence_shall_we/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7meq0/lets_have_a_talk_about_connascence_shall_we/", "subreddit_subscribers": 166326, "created_utc": 1709687142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_s1sqpvie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A tool to quickly extract data from websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7zezu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/94pkdfw2rpmc1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/94pkdfw2rpmc1/DASH_96.mp4", "dash_url": "https://v.redd.it/94pkdfw2rpmc1/DASHPlaylist.mpd?a=1712331505%2CNjdhNzVjZDBkNjkyNzk3ZDM2MTc4OWRkYTY5ZTAzZWQ3OWM0NjJkOGI3NTBjYTM2NjA1ZWQ1OWY2NzM1ZWQzYQ%3D%3D&amp;v=1&amp;f=sd", "duration": 53, "hls_url": "https://v.redd.it/94pkdfw2rpmc1/HLSPlaylist.m3u8?a=1712331505%2CZGFkNGM3NzRjMmMzMWJmZDk5MzJkMTNlZTZiYzViYTY1MGI4NTBiMjg4MDViMWQ1NGJiZmM0Y2MzN2E2MzNjZg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=fd3f9a9e3fa773dbf7d1f607766b00cdb0eb9644", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709730925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/94pkdfw2rpmc1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?format=pjpg&amp;auto=webp&amp;s=0430c356c59c7aa0f15f767d9fead38ac1e60ae9", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ce8ea7ec58f56de5a5aaac23f675c954344c7f1", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b76caff515a156b91183bda6484e43c6bba0d59b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1cfbc805b2bfd0b7078fe23ee907fc05dff6adb2", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0c11fd20ec6c937c2f5bb29322c58e282eeef471", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=65e00eb3dd620fd51fb9237e8c128ac8de834ea4", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1382a8ecc535e238086087b11a6f25c2dc0d01f7", "width": 1080, "height": 607}], "variants": {}, "id": "MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b7zezu", "is_robot_indexable": true, "report_reasons": null, "author": "GeekLifer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7zezu/a_tool_to_quickly_extract_data_from_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/94pkdfw2rpmc1", "subreddit_subscribers": 166326, "created_utc": 1709730925.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/94pkdfw2rpmc1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/94pkdfw2rpmc1/DASH_96.mp4", "dash_url": "https://v.redd.it/94pkdfw2rpmc1/DASHPlaylist.mpd?a=1712331505%2CNjdhNzVjZDBkNjkyNzk3ZDM2MTc4OWRkYTY5ZTAzZWQ3OWM0NjJkOGI3NTBjYTM2NjA1ZWQ1OWY2NzM1ZWQzYQ%3D%3D&amp;v=1&amp;f=sd", "duration": 53, "hls_url": "https://v.redd.it/94pkdfw2rpmc1/HLSPlaylist.m3u8?a=1712331505%2CZGFkNGM3NzRjMmMzMWJmZDk5MzJkMTNlZTZiYzViYTY1MGI4NTBiMjg4MDViMWQ1NGJiZmM0Y2MzN2E2MzNjZg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working for a small market research company and I'm trying to do some machine learning on survey data. Most of the data we get from clients is in bad shape and I need to be able to format it into a particular shape for this. I'm looking for a general metadata, machine readable format for survey questionnaires. There doesn't seem to be an industrial standard or even any attempts at one?\n\nThe closest thing I've found is what [google forms exports](https://github.com/stevenschmatz/export-google-form), but this is very Google. I thought there would be an RDF schema but there is none (that I have found). Can't see anything from the W3C?\n\nSurely this is a \"good idea\" as it will allow the same survey to be taken on different platforms or reproduced at different times/places/ languages or even act as a description of the resulting data?\n\nHoping that there is something I've missed...\n\nUpdate: Given the lackluster response I take it there is no such standard. Sooooo, do you want to join me in making one?", "author_fullname": "t2_372kgi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Survey questionnaire metadata industry format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7k1yc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709696014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709680951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working for a small market research company and I&amp;#39;m trying to do some machine learning on survey data. Most of the data we get from clients is in bad shape and I need to be able to format it into a particular shape for this. I&amp;#39;m looking for a general metadata, machine readable format for survey questionnaires. There doesn&amp;#39;t seem to be an industrial standard or even any attempts at one?&lt;/p&gt;\n\n&lt;p&gt;The closest thing I&amp;#39;ve found is what &lt;a href=\"https://github.com/stevenschmatz/export-google-form\"&gt;google forms exports&lt;/a&gt;, but this is very Google. I thought there would be an RDF schema but there is none (that I have found). Can&amp;#39;t see anything from the W3C?&lt;/p&gt;\n\n&lt;p&gt;Surely this is a &amp;quot;good idea&amp;quot; as it will allow the same survey to be taken on different platforms or reproduced at different times/places/ languages or even act as a description of the resulting data?&lt;/p&gt;\n\n&lt;p&gt;Hoping that there is something I&amp;#39;ve missed...&lt;/p&gt;\n\n&lt;p&gt;Update: Given the lackluster response I take it there is no such standard. Sooooo, do you want to join me in making one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0rx3D6F4Pw_uzq81Wg-QDGd21-hGKqshQjkKENmXHBw.jpg?auto=webp&amp;s=521d2c08c06aa839648c62860b9875f49707731e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0rx3D6F4Pw_uzq81Wg-QDGd21-hGKqshQjkKENmXHBw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e742ac0b522646c28559017e503893a84d30af6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0rx3D6F4Pw_uzq81Wg-QDGd21-hGKqshQjkKENmXHBw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0cdf3873d55e8e2597e69502f25492d8156c8e4e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0rx3D6F4Pw_uzq81Wg-QDGd21-hGKqshQjkKENmXHBw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=57f81186553242346008567ad4065231a0672fe4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0rx3D6F4Pw_uzq81Wg-QDGd21-hGKqshQjkKENmXHBw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f154d7bfc7d76a6824cfcd4dccb379c6b3625564", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0rx3D6F4Pw_uzq81Wg-QDGd21-hGKqshQjkKENmXHBw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e4e15023816850591c3bb16359fc5f8a4da93d4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0rx3D6F4Pw_uzq81Wg-QDGd21-hGKqshQjkKENmXHBw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fc2ee66c82baa786c59ab46a78d1732235a869ed", "width": 1080, "height": 540}], "variants": {}, "id": "HivzooITJkQS0duDn7tteq8fvzzueQT2V9g0N74DIA4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7k1yc", "is_robot_indexable": true, "report_reasons": null, "author": "FMWizard", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7k1yc/survey_questionnaire_metadata_industry_format/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7k1yc/survey_questionnaire_metadata_industry_format/", "subreddit_subscribers": 166326, "created_utc": 1709680951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m from the Mechatronics and Indistrial Technology (Supply Chain and Data and Analytics) education background with a couple years of Data Engineering Experience. I\u2019m currently applying for new opportunities and getting rejected, i believe because i\u2019m not from the standard CSE/Statistics/Mathematics background.\n\nI\u2019m looking to take the CS50 course from Harvard, will that help me justify my education for employers to consider me?.\n\nOr are there any other suggestions?", "author_fullname": "t2_cdfmceonl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Education Justification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b78fwh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709653746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m from the Mechatronics and Indistrial Technology (Supply Chain and Data and Analytics) education background with a couple years of Data Engineering Experience. I\u2019m currently applying for new opportunities and getting rejected, i believe because i\u2019m not from the standard CSE/Statistics/Mathematics background.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking to take the CS50 course from Harvard, will that help me justify my education for employers to consider me?.&lt;/p&gt;\n\n&lt;p&gt;Or are there any other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b78fwh", "is_robot_indexable": true, "report_reasons": null, "author": "GoldLatter1084", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b78fwh/data_engineering_education_justification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b78fwh/data_engineering_education_justification/", "subreddit_subscribers": 166326, "created_utc": 1709653746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On your data team, if you were to implement a weekly meeting to demo new tools or techniques, what are some of the things you would want to see?", "author_fullname": "t2_h2e0l0gf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Team meeting demo topics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7oap5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709692378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On your data team, if you were to implement a weekly meeting to demo new tools or techniques, what are some of the things you would want to see?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b7oap5", "is_robot_indexable": true, "report_reasons": null, "author": "curiosickly", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7oap5/team_meeting_demo_topics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7oap5/team_meeting_demo_topics/", "subreddit_subscribers": 166326, "created_utc": 1709692378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m curious what people use for translating various data formats. My team has been using gdal (python bindings) converting spatial (shapefile, geodabase) and non-spatial data (csv, json, excel). we are currently exploring other tools that could perform a similar function and that:\n\n1) has more readable code\n2) work with wide variety of data formats\n3) performant \n\n", "author_fullname": "t2_72w13hsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data format translation tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7juwd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709680473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m curious what people use for translating various data formats. My team has been using gdal (python bindings) converting spatial (shapefile, geodabase) and non-spatial data (csv, json, excel). we are currently exploring other tools that could perform a similar function and that:&lt;/p&gt;\n\n&lt;p&gt;1) has more readable code\n2) work with wide variety of data formats\n3) performant &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7juwd", "is_robot_indexable": true, "report_reasons": null, "author": "sashathecrimean", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7juwd/data_format_translation_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7juwd/data_format_translation_tools/", "subreddit_subscribers": 166326, "created_utc": 1709680473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a new BA (have always been more IT-adjacent and hobbyist programmer) in a company with a very immature IT department and no data warehousing. \n\nWhat are some ways in which I can build applications for the team/department level that can utilize the stability of databases and move us away from running operations via Excel. Are there best practices or standard toolkits that I should look towards when building solutions?\n\nFrom a cursory search I saw that Docker + Python + Postgres may be an approach to consider, but as this is my first foray I wasn't sure if that is overengineering or even a standard approach, and didn't know whether there is a simpler tech stack that might be more business friendly / stable / easy to maintain.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_vkdfjfmo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on architecting data solutions for newbie", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7i2u5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709676263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a new BA (have always been more IT-adjacent and hobbyist programmer) in a company with a very immature IT department and no data warehousing. &lt;/p&gt;\n\n&lt;p&gt;What are some ways in which I can build applications for the team/department level that can utilize the stability of databases and move us away from running operations via Excel. Are there best practices or standard toolkits that I should look towards when building solutions?&lt;/p&gt;\n\n&lt;p&gt;From a cursory search I saw that Docker + Python + Postgres may be an approach to consider, but as this is my first foray I wasn&amp;#39;t sure if that is overengineering or even a standard approach, and didn&amp;#39;t know whether there is a simpler tech stack that might be more business friendly / stable / easy to maintain.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7i2u5", "is_robot_indexable": true, "report_reasons": null, "author": "YoghurtDangerous893", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7i2u5/question_on_architecting_data_solutions_for_newbie/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7i2u5/question_on_architecting_data_solutions_for_newbie/", "subreddit_subscribers": 166326, "created_utc": 1709676263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a pyspark dataframe that has a column with values in this format (read.json on json files):\n\n{50:{\"A\":3, \"B\":2}, 60:{\"A\":6, \"B\":5}} (This field is a StructField with StructTypes)\n\nI have been trying to figure out how to get the data into this format:\n\nColumns: |value|A|B|\n\n|\\[50,60\\]|\\[3,2\\]|\\[2,5\\]|\n\nThis is my immediate issue, but to those who are interested in even more of a challenge I actually have two columns with nested dictionaries:\n\ncolumn1| column2\n\n{50: {\"A\":3, \"B\":2}, 60:{\"A\":6, \"B\":5}} | {\"value\": 16:{certain\\_info1: 16}, \"value\": 60 : {certain\\_info1: 42}}\n\nmy ultimate goal is to have the data in this format\n\nColumns: |value|A|B|certain\\_info1|\n\n|60|6|5|42|\n\nTo be clear, the \"value\" info is not in the same order in the two columns, and the \"value\" info is not a key but the value TO a key in the second column.\n\nI have been banging my head on this all day. Would love some advice or help. Thanks!", "author_fullname": "t2_615382bo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Grab Keys of a Nested Dictionary in a Pyspark Column? Put Them as Values in New Column?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7d4qw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709669891.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709664656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a pyspark dataframe that has a column with values in this format (read.json on json files):&lt;/p&gt;\n\n&lt;p&gt;{50:{&amp;quot;A&amp;quot;:3, &amp;quot;B&amp;quot;:2}, 60:{&amp;quot;A&amp;quot;:6, &amp;quot;B&amp;quot;:5}} (This field is a StructField with StructTypes)&lt;/p&gt;\n\n&lt;p&gt;I have been trying to figure out how to get the data into this format:&lt;/p&gt;\n\n&lt;p&gt;Columns: |value|A|B|&lt;/p&gt;\n\n&lt;p&gt;|[50,60]|[3,2]|[2,5]|&lt;/p&gt;\n\n&lt;p&gt;This is my immediate issue, but to those who are interested in even more of a challenge I actually have two columns with nested dictionaries:&lt;/p&gt;\n\n&lt;p&gt;column1| column2&lt;/p&gt;\n\n&lt;p&gt;{50: {&amp;quot;A&amp;quot;:3, &amp;quot;B&amp;quot;:2}, 60:{&amp;quot;A&amp;quot;:6, &amp;quot;B&amp;quot;:5}} | {&amp;quot;value&amp;quot;: 16:{certain_info1: 16}, &amp;quot;value&amp;quot;: 60 : {certain_info1: 42}}&lt;/p&gt;\n\n&lt;p&gt;my ultimate goal is to have the data in this format&lt;/p&gt;\n\n&lt;p&gt;Columns: |value|A|B|certain_info1|&lt;/p&gt;\n\n&lt;p&gt;|60|6|5|42|&lt;/p&gt;\n\n&lt;p&gt;To be clear, the &amp;quot;value&amp;quot; info is not in the same order in the two columns, and the &amp;quot;value&amp;quot; info is not a key but the value TO a key in the second column.&lt;/p&gt;\n\n&lt;p&gt;I have been banging my head on this all day. Would love some advice or help. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7d4qw", "is_robot_indexable": true, "report_reasons": null, "author": "datatastic08200", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7d4qw/how_to_grab_keys_of_a_nested_dictionary_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7d4qw/how_to_grab_keys_of_a_nested_dictionary_in_a/", "subreddit_subscribers": 166326, "created_utc": 1709664656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi y'all,  \n\n\nIs there a best practice how to create custom roles in azure? Or better how to choose from thousands of grants and making sure the custom role contains exactly what it needs - not to much and not to less?  \nOr is everyone just reusing/combining built-in roles?\n\nDetails and background:\n\nWe, a smol team of data engineers, are doing our first steps in setting up our azure platform, currently.  \nWe created a pipeline with Azure DevOps and can create resources via Terraform.  \n\n\nSo far so good, but now we realized we want to create multiple resource groups in the future. And setting up devops service principle on a higher level -&gt; on the management group level is not working because Terraform has n open enhancement that stops us, see here:   \n[https://github.com/microsoft/azure-pipelines-terraform/issues/81](https://github.com/microsoft/azure-pipelines-terraform/issues/81)\n\nNow we are thinking of implementing the mentioned workaround in Git Hub, for which we need to create a custom role. And that leads me back to my opening question.  \n\n\nFurthermore, we are concerned that creating these custom roles might cause n endless debugging loop in the future as MS loves to change and rename things, or add new stuff. And from one day to another your application/pipeline is offline and you start researching which role needs adaption or addition.", "author_fullname": "t2_7lgct3ea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice how to create and maintain custom roles in Azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b81qqc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709737060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi y&amp;#39;all,  &lt;/p&gt;\n\n&lt;p&gt;Is there a best practice how to create custom roles in azure? Or better how to choose from thousands of grants and making sure the custom role contains exactly what it needs - not to much and not to less?&lt;br/&gt;\nOr is everyone just reusing/combining built-in roles?&lt;/p&gt;\n\n&lt;p&gt;Details and background:&lt;/p&gt;\n\n&lt;p&gt;We, a smol team of data engineers, are doing our first steps in setting up our azure platform, currently.&lt;br/&gt;\nWe created a pipeline with Azure DevOps and can create resources via Terraform.  &lt;/p&gt;\n\n&lt;p&gt;So far so good, but now we realized we want to create multiple resource groups in the future. And setting up devops service principle on a higher level -&amp;gt; on the management group level is not working because Terraform has n open enhancement that stops us, see here:&lt;br/&gt;\n&lt;a href=\"https://github.com/microsoft/azure-pipelines-terraform/issues/81\"&gt;https://github.com/microsoft/azure-pipelines-terraform/issues/81&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Now we are thinking of implementing the mentioned workaround in Git Hub, for which we need to create a custom role. And that leads me back to my opening question.  &lt;/p&gt;\n\n&lt;p&gt;Furthermore, we are concerned that creating these custom roles might cause n endless debugging loop in the future as MS loves to change and rename things, or add new stuff. And from one day to another your application/pipeline is offline and you start researching which role needs adaption or addition.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b81qqc", "is_robot_indexable": true, "report_reasons": null, "author": "hansguckdieluft", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b81qqc/best_practice_how_to_create_and_maintain_custom/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b81qqc/best_practice_how_to_create_and_maintain_custom/", "subreddit_subscribers": 166326, "created_utc": 1709737060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nwe have data stored in an S3 bucket in the form of raw CSV files. Adobe is currently accessing this data directly from the S3 bucket. However, these files contain sensitive information. I\u2019m exploring options to ensure that we maintain ownership of the data while allowing Adobe to access it securely.\n\nAny suggestions or use cases or right way to get this done propely . We are having mulitple third parties using the data not only adobe. Just asking what are my options here please help.", "author_fullname": "t2_3sqs3uub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expose data from S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b81nx5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709736863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;we have data stored in an S3 bucket in the form of raw CSV files. Adobe is currently accessing this data directly from the S3 bucket. However, these files contain sensitive information. I\u2019m exploring options to ensure that we maintain ownership of the data while allowing Adobe to access it securely.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or use cases or right way to get this done propely . We are having mulitple third parties using the data not only adobe. Just asking what are my options here please help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b81nx5", "is_robot_indexable": true, "report_reasons": null, "author": "priyasweety1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b81nx5/expose_data_from_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b81nx5/expose_data_from_s3/", "subreddit_subscribers": 166326, "created_utc": 1709736863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Disclaimer:** I'm NOT a StreamNative employee, but recently became an Apache Pulsar committer.\n\nOxia is a new metadata store and coordination system similar to Zookeeper or Etcd. But in comparison with the others, it can store 100s of GBs of data and can handle millions of reads and writes per second.\n\nOxia GitHub repository: [https://github.com/streamnative/oxia](https://github.com/streamnative/oxia)\n\nIt's licensed under Apache License 2.0.\n\nThe Java client has been just open-sourced. Here is a blog post: [https://streamnative.io/blog/the-oxia-java-client-library-is-now-open-source](https://streamnative.io/blog/the-oxia-java-client-library-is-now-open-source)\n\nOxia is already integrated with Pulsar but isn't a default option yet. Folks from StreamNative claim that they used it in the cloud for a few months without any problems.\n\nI didn't see any independent benchmarks yet, therefore I can't validate the performance and stability claims. Probably this post may change it and attract engineers who are interested in trying Oxia for their projects and publicly share the results.\n\nI understand that it may be not a very interesting topic for many data engineers, but it may be interesting for engineers who build distributed systems FOR data engineers.  \nI would highly value hearing your thoughts on the project.", "author_fullname": "t2_15hg9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think about Oxia: a new high-performant metadata store?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b81nob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709736847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; I&amp;#39;m NOT a StreamNative employee, but recently became an Apache Pulsar committer.&lt;/p&gt;\n\n&lt;p&gt;Oxia is a new metadata store and coordination system similar to Zookeeper or Etcd. But in comparison with the others, it can store 100s of GBs of data and can handle millions of reads and writes per second.&lt;/p&gt;\n\n&lt;p&gt;Oxia GitHub repository: &lt;a href=\"https://github.com/streamnative/oxia\"&gt;https://github.com/streamnative/oxia&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s licensed under Apache License 2.0.&lt;/p&gt;\n\n&lt;p&gt;The Java client has been just open-sourced. Here is a blog post: &lt;a href=\"https://streamnative.io/blog/the-oxia-java-client-library-is-now-open-source\"&gt;https://streamnative.io/blog/the-oxia-java-client-library-is-now-open-source&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Oxia is already integrated with Pulsar but isn&amp;#39;t a default option yet. Folks from StreamNative claim that they used it in the cloud for a few months without any problems.&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t see any independent benchmarks yet, therefore I can&amp;#39;t validate the performance and stability claims. Probably this post may change it and attract engineers who are interested in trying Oxia for their projects and publicly share the results.&lt;/p&gt;\n\n&lt;p&gt;I understand that it may be not a very interesting topic for many data engineers, but it may be interesting for engineers who build distributed systems FOR data engineers.&lt;br/&gt;\nI would highly value hearing your thoughts on the project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mzjpLNWi54W1gfKLejILwwxOWZvINXC4ViqeZjuRcds.jpg?auto=webp&amp;s=75dde46b1ba4cc2af23609aa9e76196bd173bac5", "width": 512, "height": 513}, "resolutions": [{"url": "https://external-preview.redd.it/mzjpLNWi54W1gfKLejILwwxOWZvINXC4ViqeZjuRcds.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=684a3475bb119dded60790249428235b273d55bc", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/mzjpLNWi54W1gfKLejILwwxOWZvINXC4ViqeZjuRcds.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecfd2125da458bf74f933f9323c15c227aebec34", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/mzjpLNWi54W1gfKLejILwwxOWZvINXC4ViqeZjuRcds.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=43c7c5815dc486d61d74b2b089a4e098a9076311", "width": 320, "height": 320}], "variants": {}, "id": "gtJeIP7C-oK_pJrnq_xIjJoSKvc043IABr5lsGVE56E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b81nob", "is_robot_indexable": true, "report_reasons": null, "author": "visortelle", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b81nob/what_do_you_think_about_oxia_a_new_highperformant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b81nob/what_do_you_think_about_oxia_a_new_highperformant/", "subreddit_subscribers": 166326, "created_utc": 1709736847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Testing around some ideas for a project, happy to get your feedback\n\n[View Poll](https://www.reddit.com/poll/1b7ze88)", "author_fullname": "t2_nmytf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would make your life easier?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7ze88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709730867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Testing around some ideas for a project, happy to get your feedback&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1b7ze88\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1b7ze88", "is_robot_indexable": true, "report_reasons": null, "author": "razkaplan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1709990067174, "options": [{"text": "Writing everything, including SQL, in Python", "id": "27323646"}, {"text": "Writing everything, including Python scripts, in SQL", "id": "27323647"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 30, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7ze88/what_would_make_your_life_easier/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1b7ze88/what_would_make_your_life_easier/", "subreddit_subscribers": 166326, "created_utc": 1709730867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I'm working on my thesis, and the problem is integrating diverse Big data (mainly JSON) from multiple providers effectively. My goal is to develop and process a versatile ETL Framework that integrates and handles diverse data formats. I have a Postgres database with the ideal output format that the input file has to be transformed in. I have a table with global rules, mainly regex expressions, to apply to every file. I have a providers table with the names of the providers that I receive, and the last table is the providers\\_rules that have a provider\\_id associated, the id of the attribute of the input file, and the id of the attribute of the output that corresponds to a match.   \nExample: I want to tell my framework that the [`event.4314985.id`](https://event.4314985.id) from input\\_file corresponds to [`data.id`](https://data.id) in the output file.   \nThe problem is, when the attributes aren't in the same depth, how can I do this matching?   \nPS: I'm using Python and Streamlit (for now)", "author_fullname": "t2_7q6abge7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7x543", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709723487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;m working on my thesis, and the problem is integrating diverse Big data (mainly JSON) from multiple providers effectively. My goal is to develop and process a versatile ETL Framework that integrates and handles diverse data formats. I have a Postgres database with the ideal output format that the input file has to be transformed in. I have a table with global rules, mainly regex expressions, to apply to every file. I have a providers table with the names of the providers that I receive, and the last table is the providers_rules that have a provider_id associated, the id of the attribute of the input file, and the id of the attribute of the output that corresponds to a match.&lt;br/&gt;\nExample: I want to tell my framework that the &lt;a href=\"https://event.4314985.id\"&gt;&lt;code&gt;event.4314985.id&lt;/code&gt;&lt;/a&gt; from input_file corresponds to &lt;a href=\"https://data.id\"&gt;&lt;code&gt;data.id&lt;/code&gt;&lt;/a&gt; in the output file.&lt;br/&gt;\nThe problem is, when the attributes aren&amp;#39;t in the same depth, how can I do this matching?&lt;br/&gt;\nPS: I&amp;#39;m using Python and Streamlit (for now)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7x543", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPeripherals1683", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7x543/data_engineering_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7x543/data_engineering_solution/", "subreddit_subscribers": 166326, "created_utc": 1709723487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I'm starting as a data engineer/SQL developer and I was given a task. My goal is to create a MERGE on tables w/ different schemas. These tables are stored in a single SQL srv DB. I was also told to create mapping and control tables. Can anyone explain to me based on what should I create the latter (i.e. control table), please? What is the purpose of having both mapping and control tables?", "author_fullname": "t2_6rsr8kjls", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MERGE on tables w/ different schemas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7vtfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709718292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I&amp;#39;m starting as a data engineer/SQL developer and I was given a task. My goal is to create a MERGE on tables w/ different schemas. These tables are stored in a single SQL srv DB. I was also told to create mapping and control tables. Can anyone explain to me based on what should I create the latter (i.e. control table), please? What is the purpose of having both mapping and control tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7vtfq", "is_robot_indexable": true, "report_reasons": null, "author": "allk1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7vtfq/merge_on_tables_w_different_schemas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7vtfq/merge_on_tables_w_different_schemas/", "subreddit_subscribers": 166326, "created_utc": 1709718292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI have parquet files arriving at S3 regularly. So I created folder structure as root/yyyy/mm/dd/ and in the day I have multiple small parquet files.\nThe parquet file contain time series data.\nI want to generate a histogram in grafana for one of the columns.  How can efficiently achieve this. I cannot use grafana histogram plot because it requires all the rows. \nFor couple of days it's fine. But when I want to calculate over multiple months I get timeout error. And even in Athena it takes more than a minute to calculate the buckets and counts. What I was thinking is to use ctas command to create a table with  statistics for each column and insert or update the fields every day with stats from new data. Is this the correct approach?", "author_fullname": "t2_40nv3bcr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions on using Athena ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7vmpd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709717500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI have parquet files arriving at S3 regularly. So I created folder structure as root/yyyy/mm/dd/ and in the day I have multiple small parquet files.\nThe parquet file contain time series data.\nI want to generate a histogram in grafana for one of the columns.  How can efficiently achieve this. I cannot use grafana histogram plot because it requires all the rows. \nFor couple of days it&amp;#39;s fine. But when I want to calculate over multiple months I get timeout error. And even in Athena it takes more than a minute to calculate the buckets and counts. What I was thinking is to use ctas command to create a table with  statistics for each column and insert or update the fields every day with stats from new data. Is this the correct approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7vmpd", "is_robot_indexable": true, "report_reasons": null, "author": "nanosuituser", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7vmpd/need_suggestions_on_using_athena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7vmpd/need_suggestions_on_using_athena/", "subreddit_subscribers": 166326, "created_utc": 1709717500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've worked at a small startup for a couple of years as a data analyst, though I have a background in data science and software engineering. I want to completely transform the data stack at my company as the current system is haphazard and causes a lot of overhead and operational issues. However, I often feel overwhelmed and don't feel like I have enough knowledge or experience to go about this properly (I am the only data analyst, and have no similar experience in such a digital transformation). \n\nLet me break it down into a few details.\n\nMy company is an urban research consultancy. They collect data in the form of surveys which contain respondent demographic information, and scores given by respondents to aspects of a neighbourhood. When data collection for a project is complete, a CSV file is extracted from the survey platform, the data is cleaned (more on that later), and some R scripts are run on the clean data to produce a JSON file with all the insights needed to make a report. \n\nFor some of these JSON files (i.e for type 1 reports):\n\nThis JSON file is then paired with an existing InDesign template, and some JavaScript (Adobe ExtendScript) runs to produce a report with the data integrated into it. \n\nFor some other of these JSON files (i.e for type 2 reports):\n\nThe JSON file is loaded into a web platform built using VueJS. This then creates an online report that is interactive, supports drilling down by several dimensions and looks quite spiffy if i might say so. The catch? All values are precalculated into the JSON - the web based report is completely static and could run offline. No calls to any API are made in the report. This results in minified JSON files being upto 21Mb's each, not to mention the whole thing being rather inflexible and tightly coupled. \n\nAt the end of each month, we take data collected during the month and append it to an Excel file which contains all our data (Let's call that AllData). Sometimes we want to run bespoke reports on data from AllData on some XYZ dimensions with ABC filters. I then have to manually go, snip out that data from the Excel file and paste it as a CSV before writing my R code on it. I know there has to be a better way.\n\nI promised more on the data cleaning. It consists of some or all of the following:\n\n1. Removing and/or renaming columns until the schema matches that of the AllData - this is because our scripts recognise the AllData schema, and because the ultimate destination of this data will be the AllData file. This part is easy.\n2. Flagging and/or removing dirty records. This part is somewhat tricky. We eyeball responses that we feel might not be honest or well thought out. These could be where all/most scores are similar, some columns contradict each other etc. \n3. Flagging and/or removing spam. This is the trickiest. Spam records may look like legit responses but often we recognise them as coming in batches, so I can recognise and eliminate a batch of spam records with some shared characteristic, rather than 1 spam record by itself, if that makes sense. Sometimes individual spam records do give themselves away with characteristics like email address (optional) and name (also optional) being completely and laughably different, or open ended answers (also optional) being extremely vague and unrelated to the question.\n\nWhat I want to ultimately pull off is some kind of RDBMS in to which our responses are collected. I also want to build an ETL/ELT process that runs monthly - or on trigger, performs all data cleaning and builds some kind of a data warehouse from which all our reports can be based - whether from InDesign, or from our SaaS - which would be fetching data in real time via a RESTful interface.\n\nI think i've just blurbed out a whole bunch here - will add more details later. Does anyone have any thoughts that come to mind?", "author_fullname": "t2_aa2vvwd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me transform my company data stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7l586", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709683766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve worked at a small startup for a couple of years as a data analyst, though I have a background in data science and software engineering. I want to completely transform the data stack at my company as the current system is haphazard and causes a lot of overhead and operational issues. However, I often feel overwhelmed and don&amp;#39;t feel like I have enough knowledge or experience to go about this properly (I am the only data analyst, and have no similar experience in such a digital transformation). &lt;/p&gt;\n\n&lt;p&gt;Let me break it down into a few details.&lt;/p&gt;\n\n&lt;p&gt;My company is an urban research consultancy. They collect data in the form of surveys which contain respondent demographic information, and scores given by respondents to aspects of a neighbourhood. When data collection for a project is complete, a CSV file is extracted from the survey platform, the data is cleaned (more on that later), and some R scripts are run on the clean data to produce a JSON file with all the insights needed to make a report. &lt;/p&gt;\n\n&lt;p&gt;For some of these JSON files (i.e for type 1 reports):&lt;/p&gt;\n\n&lt;p&gt;This JSON file is then paired with an existing InDesign template, and some JavaScript (Adobe ExtendScript) runs to produce a report with the data integrated into it. &lt;/p&gt;\n\n&lt;p&gt;For some other of these JSON files (i.e for type 2 reports):&lt;/p&gt;\n\n&lt;p&gt;The JSON file is loaded into a web platform built using VueJS. This then creates an online report that is interactive, supports drilling down by several dimensions and looks quite spiffy if i might say so. The catch? All values are precalculated into the JSON - the web based report is completely static and could run offline. No calls to any API are made in the report. This results in minified JSON files being upto 21Mb&amp;#39;s each, not to mention the whole thing being rather inflexible and tightly coupled. &lt;/p&gt;\n\n&lt;p&gt;At the end of each month, we take data collected during the month and append it to an Excel file which contains all our data (Let&amp;#39;s call that AllData). Sometimes we want to run bespoke reports on data from AllData on some XYZ dimensions with ABC filters. I then have to manually go, snip out that data from the Excel file and paste it as a CSV before writing my R code on it. I know there has to be a better way.&lt;/p&gt;\n\n&lt;p&gt;I promised more on the data cleaning. It consists of some or all of the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Removing and/or renaming columns until the schema matches that of the AllData - this is because our scripts recognise the AllData schema, and because the ultimate destination of this data will be the AllData file. This part is easy.&lt;/li&gt;\n&lt;li&gt;Flagging and/or removing dirty records. This part is somewhat tricky. We eyeball responses that we feel might not be honest or well thought out. These could be where all/most scores are similar, some columns contradict each other etc. &lt;/li&gt;\n&lt;li&gt;Flagging and/or removing spam. This is the trickiest. Spam records may look like legit responses but often we recognise them as coming in batches, so I can recognise and eliminate a batch of spam records with some shared characteristic, rather than 1 spam record by itself, if that makes sense. Sometimes individual spam records do give themselves away with characteristics like email address (optional) and name (also optional) being completely and laughably different, or open ended answers (also optional) being extremely vague and unrelated to the question.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What I want to ultimately pull off is some kind of RDBMS in to which our responses are collected. I also want to build an ETL/ELT process that runs monthly - or on trigger, performs all data cleaning and builds some kind of a data warehouse from which all our reports can be based - whether from InDesign, or from our SaaS - which would be fetching data in real time via a RESTful interface.&lt;/p&gt;\n\n&lt;p&gt;I think i&amp;#39;ve just blurbed out a whole bunch here - will add more details later. Does anyone have any thoughts that come to mind?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7l586", "is_robot_indexable": true, "report_reasons": null, "author": "ColdMango7786", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7l586/help_me_transform_my_company_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7l586/help_me_transform_my_company_data_stack/", "subreddit_subscribers": 166326, "created_utc": 1709683766.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}