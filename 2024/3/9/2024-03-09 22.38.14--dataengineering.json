{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let me give a brief context. So, we are implementing a data lake project for a client and they are into manufacturing. Their IT team and manufacturing teams are two entities by itself and we work closely with their IT team or Digital team. The issue we are facing is that the data that is coming to the system is changing its structure now and then. To give an example, one of the data sources is SAP now when we extract the data based on the T-code for a year say 2021 we see the set of columns, but for the next year i.e.2022, we see some of the columns are missing or additional columns are present!! Needless to say, this is making the life of our team into a nightmare as we need to keep on loading the data again and again, pipelines are breaking, rewriting the logic... the list goes on. People will be thinking why the data at the source is changed, the answer is that some of the data is entered manually by the people in the manufacturing unit and they might not be following proper standards. At the organization level their IT team has very little say as revenue is generated by the manufacturing team hence it's very difficult for the IT/Digital team to force a standard for these manual data, even though they are trying from their end many of the times manufacturing unit people seldom follows it.\nNow what I want to know is has anyone faced a similar kind of challenge in implementing a datalake project. If so how did your team overcome this issue or how did your team handle this situation? Because I feel our team is highly exhausted due to the same repetitive work. Please put your thoughts it will be really helpful.\n\n\nNB: Please don't ask why we committed to this project, as it's not under my control or my decision!!!", "author_fullname": "t2_spo003v40", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with challenges in implementing data lake projects in an organization where the structure of the data at the source itself is changing frequently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bah0v3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709990373.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709988581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let me give a brief context. So, we are implementing a data lake project for a client and they are into manufacturing. Their IT team and manufacturing teams are two entities by itself and we work closely with their IT team or Digital team. The issue we are facing is that the data that is coming to the system is changing its structure now and then. To give an example, one of the data sources is SAP now when we extract the data based on the T-code for a year say 2021 we see the set of columns, but for the next year i.e.2022, we see some of the columns are missing or additional columns are present!! Needless to say, this is making the life of our team into a nightmare as we need to keep on loading the data again and again, pipelines are breaking, rewriting the logic... the list goes on. People will be thinking why the data at the source is changed, the answer is that some of the data is entered manually by the people in the manufacturing unit and they might not be following proper standards. At the organization level their IT team has very little say as revenue is generated by the manufacturing team hence it&amp;#39;s very difficult for the IT/Digital team to force a standard for these manual data, even though they are trying from their end many of the times manufacturing unit people seldom follows it.\nNow what I want to know is has anyone faced a similar kind of challenge in implementing a datalake project. If so how did your team overcome this issue or how did your team handle this situation? Because I feel our team is highly exhausted due to the same repetitive work. Please put your thoughts it will be really helpful.&lt;/p&gt;\n\n&lt;p&gt;NB: Please don&amp;#39;t ask why we committed to this project, as it&amp;#39;s not under my control or my decision!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bah0v3", "is_robot_indexable": true, "report_reasons": null, "author": "MoonWalker212", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bah0v3/how_to_deal_with_challenges_in_implementing_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bah0v3/how_to_deal_with_challenges_in_implementing_data/", "subreddit_subscribers": 167186, "created_utc": 1709988581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I am currently documenting the data flow of our project and I wanted the documentation to be easily understood by analysts in a visual manner. What is the appropriate way to diagram data flow between views, tables, left joins? ", "author_fullname": "t2_9o6vpzoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to diagram sql queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ba4g7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709944933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I am currently documenting the data flow of our project and I wanted the documentation to be easily understood by analysts in a visual manner. What is the appropriate way to diagram data flow between views, tables, left joins? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ba4g7v", "is_robot_indexable": true, "report_reasons": null, "author": "SlingBag", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ba4g7v/how_to_diagram_sql_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ba4g7v/how_to_diagram_sql_queries/", "subreddit_subscribers": 167186, "created_utc": 1709944933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit fam,\n\n5-year Data Analyst here feeling like I've hit a wall. I'm a CS grad with proficiency in SQL, Tableau, Metabase, dbt, dimensional modeling, and a sprinkle of Python. My experience spans across Fintech, Health, and Telecom.\n\nWhile I appreciate data analysis, I crave a more challenging and technical role. With my programming background, I feel underutilized. Is it too late to switch gears?\n\nI'm torn between Data Engineering and Data Science for upskilling. My goal? Advance to a more technical position within data.\n\nHere's the kicker: I don't just want courses. I crave practical learning that integrates with my current skillset and is job-market relevant.\n\nAny advice on the best path forward? Open to all suggestions! Happy to share more details about my experience\n\nThanks in advance!", "author_fullname": "t2_mpil1bv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analyst (SQL &amp; Viz ONLY!) - Feeling Stuck. What's the Next Step?!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1baknx0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709999226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit fam,&lt;/p&gt;\n\n&lt;p&gt;5-year Data Analyst here feeling like I&amp;#39;ve hit a wall. I&amp;#39;m a CS grad with proficiency in SQL, Tableau, Metabase, dbt, dimensional modeling, and a sprinkle of Python. My experience spans across Fintech, Health, and Telecom.&lt;/p&gt;\n\n&lt;p&gt;While I appreciate data analysis, I crave a more challenging and technical role. With my programming background, I feel underutilized. Is it too late to switch gears?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m torn between Data Engineering and Data Science for upskilling. My goal? Advance to a more technical position within data.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the kicker: I don&amp;#39;t just want courses. I crave practical learning that integrates with my current skillset and is job-market relevant.&lt;/p&gt;\n\n&lt;p&gt;Any advice on the best path forward? Open to all suggestions! Happy to share more details about my experience&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1baknx0", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Avocado-226", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1baknx0/data_analyst_sql_viz_only_feeling_stuck_whats_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1baknx0/data_analyst_sql_viz_only_feeling_stuck_whats_the/", "subreddit_subscribers": 167186, "created_utc": 1709999226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a one man data team who is responsible for too much. In an effort to make our reporting tool easier to use the data model needs to be simplified. Our analytics system is a bloated mess in PowerBI and I am planning on rebuilding it, and planning to hire a consultant to help me build a best practice retail data model for reporting using DBT core + metric flow (open source semantic layer). The source data is already being served in BigQuery with an active DBT project so it\u2019s purely a modeling design exercise.\n\nFor serious data professionals where might be a good place to search for consultants in this domain?", "author_fullname": "t2_ronx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to find good data modeling consultants?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bahyli", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "8cf4f390-e787-11ed-81a4-ca7b65282907", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709991590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a one man data team who is responsible for too much. In an effort to make our reporting tool easier to use the data model needs to be simplified. Our analytics system is a bloated mess in PowerBI and I am planning on rebuilding it, and planning to hire a consultant to help me build a best practice retail data model for reporting using DBT core + metric flow (open source semantic layer). The source data is already being served in BigQuery with an active DBT project so it\u2019s purely a modeling design exercise.&lt;/p&gt;\n\n&lt;p&gt;For serious data professionals where might be a good place to search for consultants in this domain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Lead Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bahyli", "is_robot_indexable": true, "report_reasons": null, "author": "seanpool3", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bahyli/where_to_find_good_data_modeling_consultants/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bahyli/where_to_find_good_data_modeling_consultants/", "subreddit_subscribers": 167186, "created_utc": 1709991590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building systems that business users will eventually rely upon to do their job. Data will be collected via Webapp, asynchronously from hundreds of remote super intendants at any time of day. This will get cleaned and funneled into an analytical database so that project managers can review and make changes over time. In a way, it transitions the state of their project from implicit to explicit, as the most current state of any project is now tracked and represented via the database. Eventually, project managers should rely completely on my system for the service it provides.\n\nThis means that if super intendants can't access my web app for ANY reason, they can't do their job. That's unacceptable to me.\n\nAs a failsafe, I'm thinking of creating a physical version of the data collection form via Scantron. I've never done this though ..\n\nWould anyone have some insight into the kinds of problems I'm likely to face and what sort of tradeoffs are made when choosing a solution to those problems?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used Scantron sheets yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bapiy7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710011535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building systems that business users will eventually rely upon to do their job. Data will be collected via Webapp, asynchronously from hundreds of remote super intendants at any time of day. This will get cleaned and funneled into an analytical database so that project managers can review and make changes over time. In a way, it transitions the state of their project from implicit to explicit, as the most current state of any project is now tracked and represented via the database. Eventually, project managers should rely completely on my system for the service it provides.&lt;/p&gt;\n\n&lt;p&gt;This means that if super intendants can&amp;#39;t access my web app for ANY reason, they can&amp;#39;t do their job. That&amp;#39;s unacceptable to me.&lt;/p&gt;\n\n&lt;p&gt;As a failsafe, I&amp;#39;m thinking of creating a physical version of the data collection form via Scantron. I&amp;#39;ve never done this though ..&lt;/p&gt;\n\n&lt;p&gt;Would anyone have some insight into the kinds of problems I&amp;#39;m likely to face and what sort of tradeoffs are made when choosing a solution to those problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bapiy7", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bapiy7/has_anyone_used_scantron_sheets_yet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bapiy7/has_anyone_used_scantron_sheets_yet/", "subreddit_subscribers": 167186, "created_utc": 1710011535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI read that lsm trees is used by dynamo db, cassandra and scylla db. But nowhere saw link between relational db(sql db) and lsm trees. Is lsm trees only used in nosql databases if that is the case why can't we used them in sql ?  \n", "author_fullname": "t2_aqmxwdoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Database using LSM Trees", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1badyyn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709976826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I read that lsm trees is used by dynamo db, cassandra and scylla db. But nowhere saw link between relational db(sql db) and lsm trees. Is lsm trees only used in nosql databases if that is the case why can&amp;#39;t we used them in sql ?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1badyyn", "is_robot_indexable": true, "report_reasons": null, "author": "AggravatingParsnip89", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1badyyn/sql_database_using_lsm_trees/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1badyyn/sql_database_using_lsm_trees/", "subreddit_subscribers": 167186, "created_utc": 1709976826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working on a patent analysis project and I'm seeking insights on how to efficiently scrape data from the United States Patent and Trademark Office (USPTO) website. We aim to stream this data into a data lake for further analysis. I'm particularly interested in methods or tools that can help streamline the process and ensure accuracy in collecting patent information, while also facilitating the integration of the scraped data into our data lake infrastructure. Any advice or recommendations on best practices, relevant libraries, or specific techniques would be greatly appreciated. Thank you in advance for your insights!", "author_fullname": "t2_7eqe0vfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering student in need of help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1banv68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710007357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a patent analysis project and I&amp;#39;m seeking insights on how to efficiently scrape data from the United States Patent and Trademark Office (USPTO) website. We aim to stream this data into a data lake for further analysis. I&amp;#39;m particularly interested in methods or tools that can help streamline the process and ensure accuracy in collecting patent information, while also facilitating the integration of the scraped data into our data lake infrastructure. Any advice or recommendations on best practices, relevant libraries, or specific techniques would be greatly appreciated. Thank you in advance for your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1banv68", "is_robot_indexable": true, "report_reasons": null, "author": "Positive_Temporary77", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1banv68/data_engineering_student_in_need_of_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1banv68/data_engineering_student_in_need_of_help/", "subreddit_subscribers": 167186, "created_utc": 1710007357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently we don\u2019t have vaccum of data happening on regular basis and so size of object storage is growing in huge scale. We need actual data\u2019s retention to be 1 year at-least -  in that case do I need to vaccum delta table (object storage bucket) - by setting  vaccum retention period as 1 year? This is so when in-case 1 year old data is needed we can time travel using timestamp.\n\ndeltaTable.vacuum(8766) \n\n# 8766 is 1 year in hours\n\nAs it is delta table though it\u2019s object storage bucket - rather than life cycle policy, vaccum is what should be done? Any thoughts or suggestions? ", "author_fullname": "t2_asn24r2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on Vaccum data to save storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1badj0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709974977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we don\u2019t have vaccum of data happening on regular basis and so size of object storage is growing in huge scale. We need actual data\u2019s retention to be 1 year at-least -  in that case do I need to vaccum delta table (object storage bucket) - by setting  vaccum retention period as 1 year? This is so when in-case 1 year old data is needed we can time travel using timestamp.&lt;/p&gt;\n\n&lt;p&gt;deltaTable.vacuum(8766) &lt;/p&gt;\n\n&lt;h1&gt;8766 is 1 year in hours&lt;/h1&gt;\n\n&lt;p&gt;As it is delta table though it\u2019s object storage bucket - rather than life cycle policy, vaccum is what should be done? Any thoughts or suggestions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1badj0t", "is_robot_indexable": true, "report_reasons": null, "author": "Slow-Woodpecker-3629", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1badj0t/question_on_vaccum_data_to_save_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1badj0t/question_on_vaccum_data_to_save_storage/", "subreddit_subscribers": 167186, "created_utc": 1709974977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learn the simple yet powerful optimization techniques that helped me reduce BigQuery spend by $70,000 a month.\n\nI think lot of folks can take help from this one:\nhttps://www.junaideffendi.com/p/how-i-saved-70k-a-month-in-bigquery\n\nThese techniques can be applied to most of the data warehouses in the market today.\n\nLet me know what else have you done to save $$$.\n\nThanks for reading :)\n", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving $70k a month in DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bapzzr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710013765.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710012772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learn the simple yet powerful optimization techniques that helped me reduce BigQuery spend by $70,000 a month.&lt;/p&gt;\n\n&lt;p&gt;I think lot of folks can take help from this one:\n&lt;a href=\"https://www.junaideffendi.com/p/how-i-saved-70k-a-month-in-bigquery\"&gt;https://www.junaideffendi.com/p/how-i-saved-70k-a-month-in-bigquery&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;These techniques can be applied to most of the data warehouses in the market today.&lt;/p&gt;\n\n&lt;p&gt;Let me know what else have you done to save $$$.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?auto=webp&amp;s=d28ff71c6b5bd71061c223703a1a03ec24994d59", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23aeef673fefebb88d5345c3c1a8852dd7c5e097", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=976978b5380b045c766822cf9fd75448d0834cf6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b76d2509f73cbfa97eefd22d554d78f20d1648c1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=05329556dea12385f46545b45e7bb139c6efdd99", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3be9e1a3ffd4133c17b3046cf032eb77f448839", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8281fed66fd14c2a45dc709b0fc6f9fa7eebb99a", "width": 1080, "height": 540}], "variants": {}, "id": "57zTR0op2xToniSMTCu-C9Fq6-ZMfwz5tXxPBNdNYTE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bapzzr", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bapzzr/saving_70k_a_month_in_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bapzzr/saving_70k_a_month_in_dwh/", "subreddit_subscribers": 167186, "created_utc": 1710012772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIs there a way to edit Azure Synapse Analytics notebooks in VsCode? I can clone the repository, but the code is stored as JSON, eg as follows:\n\n\n\n\t{\n\t\t\"cell_type\": \"code\",\n\t\t\"source\": [\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"import http\\r\\n\",\n\t\t\t\"import logging\\r\\n\",\n\t\t\t\"import azure.functions as func\\r\\n\",\n\t\t\t\"import requests\\r\\n\",\n\t\t\t\"from azure.storage.blob import ContainerClient\\r\\n\",\n\t\t\t\"import sys\\r\\n\",\n\t\t\t\"import pandas as pd\\r\\n\",\n\t\t\t\"import json\\r\\n\",\n\t\t\t\"from datetime import datetime\\r\\n\",\n\t\t\t\"from tqdm import tqdm\\r\\n\",\n\t\t\t\"from notebookutils import mssparkutils\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"# Grab API key\\r\\n\",\n\t\t\t\"# (ETC - All code is foramtted like this as a list of strings)\\r\\n\",\n\n\t\t],\n\t\t\"execution_count\": 23\n\t}\n\n\nSo, none of my code formatting / linting tools will work out of the box.\n\nI thought about importing a .py file, but this is difficult as the nodebook code is run on an Apache spark cluster.\n\nAny ideas on this would be appreciated.\n\nPS: Azure Synapse Analytics is a trash product.", "author_fullname": "t2_5i2it6n2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics - Editing notebooks with VSCode?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ba7vl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709954872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Is there a way to edit Azure Synapse Analytics notebooks in VsCode? I can clone the repository, but the code is stored as JSON, eg as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;cell_type&amp;quot;: &amp;quot;code&amp;quot;,\n    &amp;quot;source&amp;quot;: [\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;import http\\r\\n&amp;quot;,\n        &amp;quot;import logging\\r\\n&amp;quot;,\n        &amp;quot;import azure.functions as func\\r\\n&amp;quot;,\n        &amp;quot;import requests\\r\\n&amp;quot;,\n        &amp;quot;from azure.storage.blob import ContainerClient\\r\\n&amp;quot;,\n        &amp;quot;import sys\\r\\n&amp;quot;,\n        &amp;quot;import pandas as pd\\r\\n&amp;quot;,\n        &amp;quot;import json\\r\\n&amp;quot;,\n        &amp;quot;from datetime import datetime\\r\\n&amp;quot;,\n        &amp;quot;from tqdm import tqdm\\r\\n&amp;quot;,\n        &amp;quot;from notebookutils import mssparkutils\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;# Grab API key\\r\\n&amp;quot;,\n        &amp;quot;# (ETC - All code is foramtted like this as a list of strings)\\r\\n&amp;quot;,\n\n    ],\n    &amp;quot;execution_count&amp;quot;: 23\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So, none of my code formatting / linting tools will work out of the box.&lt;/p&gt;\n\n&lt;p&gt;I thought about importing a .py file, but this is difficult as the nodebook code is run on an Apache spark cluster.&lt;/p&gt;\n\n&lt;p&gt;Any ideas on this would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;PS: Azure Synapse Analytics is a trash product.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ba7vl3", "is_robot_indexable": true, "report_reasons": null, "author": "mr-dre", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ba7vl3/azure_synapse_analytics_editing_notebooks_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ba7vl3/azure_synapse_analytics_editing_notebooks_with/", "subreddit_subscribers": 167186, "created_utc": 1709954872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working as a DE for about a year and a half the following is my opinion on basis for my experience and looking at mu friends\u2019 experiences. 90% of the roles in data are usually analytics, BI, data science. Even if it is a DE role it usually falls into one of the above. These roles typically exist in orgs which are not mature with and in data and execs work on excel. If this is the case, then the \u2018data\u2019 team\u2019s priority is making a case for yourself/selling yourself with your initiatives add value to the execs. In my opinion this is very close to consulting. This causes a de-prioritization of DE work which can be lack of data modeling, no focus on data infra, data quality sucks etc. This makes DE a support role and a visibility lacking role. On the other hand, orgs which are mature with data, say Netflix, few mid sized startups and maybe few companies actually have real DE roles where focus is equal on infra, data quality, analytics, DS. If I want to get into these roles, it makes it tougher as there are so few of these. Would like to know thoughts of DEs/Senior DEs here who have been thru this/navigated/transitioned into something else from DE", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u2018Data\u2019 is essentially a consulting field in most companies ? Thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1basru9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710019796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a DE for about a year and a half the following is my opinion on basis for my experience and looking at mu friends\u2019 experiences. 90% of the roles in data are usually analytics, BI, data science. Even if it is a DE role it usually falls into one of the above. These roles typically exist in orgs which are not mature with and in data and execs work on excel. If this is the case, then the \u2018data\u2019 team\u2019s priority is making a case for yourself/selling yourself with your initiatives add value to the execs. In my opinion this is very close to consulting. This causes a de-prioritization of DE work which can be lack of data modeling, no focus on data infra, data quality sucks etc. This makes DE a support role and a visibility lacking role. On the other hand, orgs which are mature with data, say Netflix, few mid sized startups and maybe few companies actually have real DE roles where focus is equal on infra, data quality, analytics, DS. If I want to get into these roles, it makes it tougher as there are so few of these. Would like to know thoughts of DEs/Senior DEs here who have been thru this/navigated/transitioned into something else from DE&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1basru9", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1basru9/data_is_essentially_a_consulting_field_in_most/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1basru9/data_is_essentially_a_consulting_field_in_most/", "subreddit_subscribers": 167186, "created_utc": 1710019796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking to enroll in the DE bootcamp by WeCloudData- the self-paced one and wanted to know if anyone has gone through their bootcamp. Would like to know your experience- how was there course material, TA support, and career support? I'm currently working as SWE and looking to pivot into a de role. I know there are other ways of developing those skills either by watching YouTube videos or through Udemy but I like to have a structured course.\n\nPlease mention If you tried other boot camps and found it helpful.", "author_fullname": "t2_86hjd67r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinion on Data Engineering Bootcamp- WeCloudData", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ba8g6b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709956689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to enroll in the DE bootcamp by WeCloudData- the self-paced one and wanted to know if anyone has gone through their bootcamp. Would like to know your experience- how was there course material, TA support, and career support? I&amp;#39;m currently working as SWE and looking to pivot into a de role. I know there are other ways of developing those skills either by watching YouTube videos or through Udemy but I like to have a structured course.&lt;/p&gt;\n\n&lt;p&gt;Please mention If you tried other boot camps and found it helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ba8g6b", "is_robot_indexable": true, "report_reasons": null, "author": "Reporter-Soggy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ba8g6b/opinion_on_data_engineering_bootcamp_weclouddata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ba8g6b/opinion_on_data_engineering_bootcamp_weclouddata/", "subreddit_subscribers": 167186, "created_utc": 1709956689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "had a few questions pertaining to what problems have you encountered persistently ?  \n\n\nDo you struggle with ensuring data accuracy across different projects?\n\n&amp;#x200B;\n\nHow often do you encounter issues with outdated or inconsistent data?\n\n&amp;#x200B;\n\nHave data quality problems ever impacted your ability to deliver projects successfully?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4p4v9ov31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "questions regarding problems faced by data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bafeap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.14, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709982657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;had a few questions pertaining to what problems have you encountered persistently ?  &lt;/p&gt;\n\n&lt;p&gt;Do you struggle with ensuring data accuracy across different projects?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How often do you encounter issues with outdated or inconsistent data?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Have data quality problems ever impacted your ability to deliver projects successfully?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bafeap", "is_robot_indexable": true, "report_reasons": null, "author": "Decent_Ice1528", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bafeap/questions_regarding_problems_faced_by_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bafeap/questions_regarding_problems_faced_by_data/", "subreddit_subscribers": 167186, "created_utc": 1709982657.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}