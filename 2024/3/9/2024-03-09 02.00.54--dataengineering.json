{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I've found gifs with this style on LinkedIn a couple of times and I wonder which tool was it made on. In some posts they say it's Excalidraw, but I can't find the moving arrows style.\n\nBut let's not make it so specific. Which visual tools do you use to diagram your data architectures/solutions? I almost always use **draw.io**\n\nhttps://i.redd.it/wwy0b4byi0nc1.gif", "author_fullname": "t2_9d3dxxer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data architecture diagram tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"wwy0b4byi0nc1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 129, "x": 108, "u": "https://preview.redd.it/wwy0b4byi0nc1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=f6fccae06d86170a03777bd3bcb198fcdaf49de0"}, {"y": 258, "x": 216, "u": "https://preview.redd.it/wwy0b4byi0nc1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=b1767f3a5e350e5a9c6495c370dad93f0f0fb1a3"}, {"y": 383, "x": 320, "u": "https://preview.redd.it/wwy0b4byi0nc1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=d8779f649af117c04b557551dac3d92476086e1f"}, {"y": 767, "x": 640, "u": "https://preview.redd.it/wwy0b4byi0nc1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=bda04f64c43d62dc2b1e442ab699d092a5599527"}, {"y": 1150, "x": 960, "u": "https://preview.redd.it/wwy0b4byi0nc1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=467ddf6a798a5083023710960b9a99eb719a8f41"}, {"y": 1294, "x": 1080, "u": "https://preview.redd.it/wwy0b4byi0nc1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=a803dd832179b89545c952d1b515ab76dca8c946"}], "s": {"y": 1496, "gif": "https://i.redd.it/wwy0b4byi0nc1.gif", "mp4": "https://preview.redd.it/wwy0b4byi0nc1.gif?format=mp4&amp;s=0d9dbbe1552c35488d79d1f4fa73ad49e9306506", "x": 1248}, "id": "wwy0b4byi0nc1"}}, "name": "t3_1b9ba5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/A7N4PCE0t_kDJIsOZAwZ9FDPvb9k9U79N-xJ_Q1db1E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709861244.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;ve found gifs with this style on LinkedIn a couple of times and I wonder which tool was it made on. In some posts they say it&amp;#39;s Excalidraw, but I can&amp;#39;t find the moving arrows style.&lt;/p&gt;\n\n&lt;p&gt;But let&amp;#39;s not make it so specific. Which visual tools do you use to diagram your data architectures/solutions? I almost always use &lt;strong&gt;draw.io&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/wwy0b4byi0nc1.gif\"&gt;https://i.redd.it/wwy0b4byi0nc1.gif&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9ba5c", "is_robot_indexable": true, "report_reasons": null, "author": "thehelptea", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9ba5c/data_architecture_diagram_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9ba5c/data_architecture_diagram_tools/", "subreddit_subscribers": 166997, "created_utc": 1709861244.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa5dw92do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stop Paying Snowflake for Failing Workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9oqr9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wOM8ib6rTYoiI0FU8X1XFvJdcf2IDdqmc2UArIGxy-w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709906752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "baselit.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://baselit.ai/blog/stop-paying-snowflake-for-failing-workloads", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?auto=webp&amp;s=8ee32ef232f1d9b4ad2813e8f617ec61acdebbf3", "width": 1024, "height": 773}, "resolutions": [{"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=543ee50b5969a2641d08e32aaebcbf150d40c49b", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b7208f26ffcd6ba77f097a8ba46da5e5c680ff7", "width": 216, "height": 163}, {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=827fe7c2852c9add08b2640bd191572a846a4354", "width": 320, "height": 241}, {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=36dfead6e99a4bee4a2ddf38c8dcc663472ed621", "width": 640, "height": 483}, {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=846d7d76ba880b788d64436cef4ffe9e1ca0edac", "width": 960, "height": 724}], "variants": {}, "id": "r3CgWc1G_RZzYfh9ko9igTDnHUgYIslIfknt6RfU6cw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b9oqr9", "is_robot_indexable": true, "report_reasons": null, "author": "sahil_singla", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9oqr9/stop_paying_snowflake_for_failing_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://baselit.ai/blog/stop-paying-snowflake-for-failing-workloads", "subreddit_subscribers": 166997, "created_utc": 1709906752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Leveraging Schipol Dev API, I've built an interactive dashboard for flight data, while also fetching datasets from various sources stored in GCS Bucket. Using Google Cloud, Big Query, and MageAI for orchestration, the pipeline runs via Docker containers on a VM, scheduled as a cron job for market hours automation. Check out the dashboard [here](https://aeroatlas.streamlit.app/). I'd love your feedback, suggestions, and opinions to enhance this data-driven journey!", "author_fullname": "t2_rw01dudv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just launched my first data engineering project!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9hnn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709880414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Leveraging Schipol Dev API, I&amp;#39;ve built an interactive dashboard for flight data, while also fetching datasets from various sources stored in GCS Bucket. Using Google Cloud, Big Query, and MageAI for orchestration, the pipeline runs via Docker containers on a VM, scheduled as a cron job for market hours automation. Check out the dashboard &lt;a href=\"https://aeroatlas.streamlit.app/\"&gt;here&lt;/a&gt;. I&amp;#39;d love your feedback, suggestions, and opinions to enhance this data-driven journey!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ec4cXXHvPH9sI7OkgKV88CdSKaGINkXxd_IXgo-pT6o.jpg?auto=webp&amp;s=9865438c46b36f6ceceae2a4f691e77823f49706", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Ec4cXXHvPH9sI7OkgKV88CdSKaGINkXxd_IXgo-pT6o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f2a7fdcf1696abd365a02221e38e0fec9fbccf0", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Ec4cXXHvPH9sI7OkgKV88CdSKaGINkXxd_IXgo-pT6o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c78b890a0a78713482641dfc23ab4bb06accfbb0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Ec4cXXHvPH9sI7OkgKV88CdSKaGINkXxd_IXgo-pT6o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ead203f9bd786ae0ce939c56d4de097a9b7bbb9a", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Ec4cXXHvPH9sI7OkgKV88CdSKaGINkXxd_IXgo-pT6o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c17981a8c5361bba27539e884ea4406a31af7e9", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Ec4cXXHvPH9sI7OkgKV88CdSKaGINkXxd_IXgo-pT6o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=987bef2e4dbcf6165f0aafad164259f6f3b31560", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Ec4cXXHvPH9sI7OkgKV88CdSKaGINkXxd_IXgo-pT6o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b9eb15378a2902c32993c4d17ff71148add906e4", "width": 1080, "height": 567}], "variants": {}, "id": "5noDdCvNFXIpfSURWR11QJ056eivlSF0Fxt4b3n6Wyk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1b9hnn0", "is_robot_indexable": true, "report_reasons": null, "author": "botuleman", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9hnn0/just_launched_my_first_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9hnn0/just_launched_my_first_data_engineering_project/", "subreddit_subscribers": 166997, "created_utc": 1709880414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I can't get concurrent users to increase no matter the server's CPU power.\n\nHello, I'm working on a production web application that has a giant MySQL database at the backend. The database is constantly updated with new information from various sources at different timestamps every single day. The web application is report-generation-based, where the user 'generates reports' of data from a certain time range they specify, which is done by querying against the database. This querying of MySQL takes a lot of time and is CPU intensive (observed from htop). MySQL contains various types of data, especially large-string data. Now, to generate a complex report for a single user, it uses 1 CPU (thread or vCPU), not the whole number of CPUs available. Similarly, for 4 users, 4 CPUs, and the rest of the CPUs are idle. I simulate multiple concurrent users' report generation tests using the PostMan application. **Now, no matter how powerful the CPU I use, it is not being efficient and caps at around 30-40 concurrent users (powerful CPU results in higher caps) and also takes a lot of time.**\n\nWhen multiple users are simultaneously querying the database, all logical cores of the server become preoccupied with handling MySQL queries, which in turn reduces the application's ability to manage concurrent users effectively. For example, a single user might generate a report for one month's worth of data in 5 minutes. However, if 20 to 30 users attempt to generate the same report simultaneously, the completion time can extend to as much as 30 minutes. Also, **when the volume of concurrent requests grows further, some users may experience failures in receiving their report outputs successfully.**\n\nI am thinking of parallel computing and using all available CPUs for each report generation instead of using only 1 CPU, but it has its disadvantages. If a rogue user constantly keeps generating very complex reports, other users will not be able to get fruitful results. So I'm currently not considering this option.\n\nIs there any other way I can improve this from a query perspective or any other perspective? Please can anyone help me find a solution to this problem? What type of architecture should be used to keep the same performance for all concurrent users and also increase the concurrent users cap (our requirement is about 100+ concurrent users)?\n\n# Additional Information:\n\nBackend: Dotnet Core 6 Web API (MVC)\n\n# Database:\n\nMySql Community Server\u00a0(free\u00a0version)  \n**table 48, data length 3,368,960,000, indexes\u00a081,920**  \nBut in my calculation, I mostly only need to query from 2 big tables:\n\n# 1st table information:\n\nEvery 24 hours, 7,153 rows are inserted into our database, each identified by a timestamp range from start (timestamp) to finish (timestamp, which may be Null). When retrieving data from this table over a long date range\u2014using both start and finish times\u2014alongside an integer field representing a list of user IDs.  \nFor example, a user might request data spanning from January 1, 2024, to February 29, 2024. This duration could vary significantly, ranging from 6 months to 1 year. Additionally, the query includes a large list of user IDs (e.g., 112, 23, 45, 78, 45, 56, etc.), with each userID associated with multiple rows in the database.\n\n|Type|\n|:-|\n|bigint(20) unassigned Auto Increment|\n|int(11)|\n|int(11)|\n|timestamp \\[current\\_timestamp()\\]|\n|timestamp NULL|\n|double(10,2) NULL|\n|int(11) \\[1\\]|\n|int(11) \\[1\\]|\n|int(11) NULL|\n\n# 2nd table information:\n\nThe second table in our database experiences an insertion of 2,000 rows every 24 hours. Similar to the first, this table records data within specific time ranges, set by a start and finish timestamp. Additionally, it stores variable character data (VARCHAR) as well.  \nQueries on this table are executed over time ranges, similar to those for table one, with durations typically spanning 3 to 6 months. Along with time-based criteria like Table 1, these queries also filter for five extensive lists of string values, each list containing approximately 100 to 200 string values.\n\n|Type|\n|:-|\n|int(11) Auto Increment|\n|date|\n|int(10)|\n|varchar(200)|\n|varchar(100)|\n|varchar(100)|\n|time|\n|int(10)|\n|timestamp \\[current\\_timestamp()\\]|\n|timestamp \\[current\\_timestamp()\\]|\n|varchar(200)|\n|varchar(100)|\n|varchar(100)|\n|varchar(100)|\n|varchar(100)|\n|varchar(100)|\n|varchar(200)|\n|varchar(100)|\n|int(10)|\n|int(10)|\n|varchar(200) NULL|\n|int(100)|\n|varchar(100) NULL|\n\n# Test Results (Dedicated Bare Metal Servers):\n\nSystemInfo: Intel Xeon E5-2696 v4 | 2 sockets x 22 cores/CPU x 2 thread/core = 88\u00a0threads | 448GB DDR4 RAM  \nSingle User Report Generation time: 3mins (for 1 week's data)  \n20 Concurrent Users Report Generation time: 25 min (for 1 week's data) and 2 users report generation were unsuccessful.  \n**Maximum concurrent users it\u00a0can\u00a0handle:\u00a040**", "author_fullname": "t2_58tp8ktc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help: Optimizing MySQL for 100 Concurrent Users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9mh05", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709900020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t get concurrent users to increase no matter the server&amp;#39;s CPU power.&lt;/p&gt;\n\n&lt;p&gt;Hello, I&amp;#39;m working on a production web application that has a giant MySQL database at the backend. The database is constantly updated with new information from various sources at different timestamps every single day. The web application is report-generation-based, where the user &amp;#39;generates reports&amp;#39; of data from a certain time range they specify, which is done by querying against the database. This querying of MySQL takes a lot of time and is CPU intensive (observed from htop). MySQL contains various types of data, especially large-string data. Now, to generate a complex report for a single user, it uses 1 CPU (thread or vCPU), not the whole number of CPUs available. Similarly, for 4 users, 4 CPUs, and the rest of the CPUs are idle. I simulate multiple concurrent users&amp;#39; report generation tests using the PostMan application. &lt;strong&gt;Now, no matter how powerful the CPU I use, it is not being efficient and caps at around 30-40 concurrent users (powerful CPU results in higher caps) and also takes a lot of time.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;When multiple users are simultaneously querying the database, all logical cores of the server become preoccupied with handling MySQL queries, which in turn reduces the application&amp;#39;s ability to manage concurrent users effectively. For example, a single user might generate a report for one month&amp;#39;s worth of data in 5 minutes. However, if 20 to 30 users attempt to generate the same report simultaneously, the completion time can extend to as much as 30 minutes. Also, &lt;strong&gt;when the volume of concurrent requests grows further, some users may experience failures in receiving their report outputs successfully.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am thinking of parallel computing and using all available CPUs for each report generation instead of using only 1 CPU, but it has its disadvantages. If a rogue user constantly keeps generating very complex reports, other users will not be able to get fruitful results. So I&amp;#39;m currently not considering this option.&lt;/p&gt;\n\n&lt;p&gt;Is there any other way I can improve this from a query perspective or any other perspective? Please can anyone help me find a solution to this problem? What type of architecture should be used to keep the same performance for all concurrent users and also increase the concurrent users cap (our requirement is about 100+ concurrent users)?&lt;/p&gt;\n\n&lt;h1&gt;Additional Information:&lt;/h1&gt;\n\n&lt;p&gt;Backend: Dotnet Core 6 Web API (MVC)&lt;/p&gt;\n\n&lt;h1&gt;Database:&lt;/h1&gt;\n\n&lt;p&gt;MySql Community Server\u00a0(free\u00a0version)&lt;br/&gt;\n&lt;strong&gt;table 48, data length 3,368,960,000, indexes\u00a081,920&lt;/strong&gt;&lt;br/&gt;\nBut in my calculation, I mostly only need to query from 2 big tables:&lt;/p&gt;\n\n&lt;h1&gt;1st table information:&lt;/h1&gt;\n\n&lt;p&gt;Every 24 hours, 7,153 rows are inserted into our database, each identified by a timestamp range from start (timestamp) to finish (timestamp, which may be Null). When retrieving data from this table over a long date range\u2014using both start and finish times\u2014alongside an integer field representing a list of user IDs.&lt;br/&gt;\nFor example, a user might request data spanning from January 1, 2024, to February 29, 2024. This duration could vary significantly, ranging from 6 months to 1 year. Additionally, the query includes a large list of user IDs (e.g., 112, 23, 45, 78, 45, 56, etc.), with each userID associated with multiple rows in the database.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Type&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;bigint(20) unassigned Auto Increment&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;timestamp [current_timestamp()]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;timestamp NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;double(10,2) NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11) [1]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11) [1]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11) NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;2nd table information:&lt;/h1&gt;\n\n&lt;p&gt;The second table in our database experiences an insertion of 2,000 rows every 24 hours. Similar to the first, this table records data within specific time ranges, set by a start and finish timestamp. Additionally, it stores variable character data (VARCHAR) as well.&lt;br/&gt;\nQueries on this table are executed over time ranges, similar to those for table one, with durations typically spanning 3 to 6 months. Along with time-based criteria like Table 1, these queries also filter for five extensive lists of string values, each list containing approximately 100 to 200 string values.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Type&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11) Auto Increment&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;date&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(10)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(200)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;time&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(10)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;timestamp [current_timestamp()]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;timestamp [current_timestamp()]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(200)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(200)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(10)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(10)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(200) NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100) NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Test Results (Dedicated Bare Metal Servers):&lt;/h1&gt;\n\n&lt;p&gt;SystemInfo: Intel Xeon E5-2696 v4 | 2 sockets x 22 cores/CPU x 2 thread/core = 88\u00a0threads | 448GB DDR4 RAM&lt;br/&gt;\nSingle User Report Generation time: 3mins (for 1 week&amp;#39;s data)&lt;br/&gt;\n20 Concurrent Users Report Generation time: 25 min (for 1 week&amp;#39;s data) and 2 users report generation were unsuccessful.&lt;br/&gt;\n&lt;strong&gt;Maximum concurrent users it\u00a0can\u00a0handle:\u00a040&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9mh05", "is_robot_indexable": true, "report_reasons": null, "author": "Dr-Double-A", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9mh05/need_help_optimizing_mysql_for_100_concurrent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9mh05/need_help_optimizing_mysql_for_100_concurrent/", "subreddit_subscribers": 166997, "created_utc": 1709900020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I just recently started working as a consultant, I'm being offered to work in a project with databricks. My main question is how valuable can be the experience I could get from that project in the look for developing my career? I've seen there's a a lot of mix reviews on databricks, so I would like to hear your opinions and experiences.\nThanks in advance :)", "author_fullname": "t2_q9k8q4cy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks value ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9b9oa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709861205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I just recently started working as a consultant, I&amp;#39;m being offered to work in a project with databricks. My main question is how valuable can be the experience I could get from that project in the look for developing my career? I&amp;#39;ve seen there&amp;#39;s a a lot of mix reviews on databricks, so I would like to hear your opinions and experiences.\nThanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b9b9oa", "is_robot_indexable": true, "report_reasons": null, "author": "Cid-Cthulhu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9b9oa/databricks_value/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9b9oa/databricks_value/", "subreddit_subscribers": 166997, "created_utc": 1709861205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The docs don\u2019t explain how to actually use Apache iceberg just how to connect to existing icebergs tables. How do you actually implement the iceberg format of your data lake?", "author_fullname": "t2_160eq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to implement Apache iceberg on my data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9rwqe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709914502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The docs don\u2019t explain how to actually use Apache iceberg just how to connect to existing icebergs tables. How do you actually implement the iceberg format of your data lake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9rwqe", "is_robot_indexable": true, "report_reasons": null, "author": "SirAutismx7", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9rwqe/how_to_implement_apache_iceberg_on_my_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9rwqe/how_to_implement_apache_iceberg_on_my_data_lake/", "subreddit_subscribers": 166997, "created_utc": 1709914502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Productizing data services: Removing fears with the LEAP framework", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9jo2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4_KghhkXNwF2MIVMXdjqEUFrvMGkB0MN-MCA3SHlJQ4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709888125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arch.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arch.dev/blog/productizing-data-services-removing-fears-with-the-leap-framework/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QjupfQk_2KaMyanQW-ao3y1RNHsw-3F7zdzcr_s7QG4.jpg?auto=webp&amp;s=22d019227459e8dd7ec6d7a1301a5e1950f97720", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/QjupfQk_2KaMyanQW-ao3y1RNHsw-3F7zdzcr_s7QG4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=39d71177e71f497f070771f54b052c7461e2750b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/QjupfQk_2KaMyanQW-ao3y1RNHsw-3F7zdzcr_s7QG4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=321ddb2bf8d4648c8c5c48707ec103d0a4f55ca5", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/QjupfQk_2KaMyanQW-ao3y1RNHsw-3F7zdzcr_s7QG4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=19a4e2baad28afcd3a08c53ea65e3ffc6a543855", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/QjupfQk_2KaMyanQW-ao3y1RNHsw-3F7zdzcr_s7QG4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f05bd0cb86c47d0b4102f59fdf74ce84228b3cb8", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/QjupfQk_2KaMyanQW-ao3y1RNHsw-3F7zdzcr_s7QG4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f984af9a0053478168a803638ccff44f27b7bf9c", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/QjupfQk_2KaMyanQW-ao3y1RNHsw-3F7zdzcr_s7QG4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d76b6c44c877e3ff283b61bf924d27d6d6c4fdfc", "width": 1080, "height": 607}], "variants": {}, "id": "FvmfvAiv20GoQlJeug9X3S4maUuzOWIK6kXMvqVo8GY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b9jo2d", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9jo2d/productizing_data_services_removing_fears_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arch.dev/blog/productizing-data-services-removing-fears-with-the-leap-framework/", "subreddit_subscribers": 166997, "created_utc": 1709888125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm a business Intelligence student and Data warehousing isn't my strong suit. I'm working on a project in which I need to evaluate the performance of members in a test. I created a constellation star schema where I have fact tables that mesure the overall performance of that type of test, with mesures such as number of participants, number succeded, number in progresss , etc ...\n\nI also have one fact table that mesures the individual performance of a participant rather than the entire test, with mesures such as score, startdate, finishdate , durationForCompletion, etc ...\n\nEach attempt made by a participant has a timestamp. I want to mesure the performance of the test by using the test fact table, and the time axis of analysis would analyse by quarter, year, month, etc ...\n\nI also want to mesure advancement by individual so the time axis would analyse progress by hour, day , month , etc ...\n\nDue to the granularity being different between both fact tables, do I create two time dimensions or is there a better approach ?", "author_fullname": "t2_kibbksmcq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Constellation schema problem : One or two Time Dimensions ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9pcs2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709908360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m a business Intelligence student and Data warehousing isn&amp;#39;t my strong suit. I&amp;#39;m working on a project in which I need to evaluate the performance of members in a test. I created a constellation star schema where I have fact tables that mesure the overall performance of that type of test, with mesures such as number of participants, number succeded, number in progresss , etc ...&lt;/p&gt;\n\n&lt;p&gt;I also have one fact table that mesures the individual performance of a participant rather than the entire test, with mesures such as score, startdate, finishdate , durationForCompletion, etc ...&lt;/p&gt;\n\n&lt;p&gt;Each attempt made by a participant has a timestamp. I want to mesure the performance of the test by using the test fact table, and the time axis of analysis would analyse by quarter, year, month, etc ...&lt;/p&gt;\n\n&lt;p&gt;I also want to mesure advancement by individual so the time axis would analyse progress by hour, day , month , etc ...&lt;/p&gt;\n\n&lt;p&gt;Due to the granularity being different between both fact tables, do I create two time dimensions or is there a better approach ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9pcs2", "is_robot_indexable": true, "report_reasons": null, "author": "Responsible-Word-137", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9pcs2/constellation_schema_problem_one_or_two_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9pcs2/constellation_schema_problem_one_or_two_time/", "subreddit_subscribers": 166997, "created_utc": 1709908360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nI have a basic question today, so please bear with me. I've been working with Azure Databricks for just a few months.\n\nMy question is about storage options. Initially, I was under the impression that using ADLS and storing files extracted from SAP S/4HANA in the raw layer as Parquet was the optimal approach. I believed that Parquet was the most suitable format for this purpose. However, I've recently learned that aside from ADLS, it's also possible to store data in Cassandra. This discovery has led me to wonder: if Cassandra is capable of storing highly structured data from SAP, might it be a better choice?\n\nAm I oversimplifying the issue by not fully considering factors like the cost differences between saving data in Parquet versus storing it in Cassandra, or potential use cases like analytic reporting versus write preference?\n\nI would greatly appreciate your perspective on how to make this decision.\n\nThanks.", "author_fullname": "t2_5b8k9nl1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cassandra or Praquet for structured data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9bnak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709862251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I have a basic question today, so please bear with me. I&amp;#39;ve been working with Azure Databricks for just a few months.&lt;/p&gt;\n\n&lt;p&gt;My question is about storage options. Initially, I was under the impression that using ADLS and storing files extracted from SAP S/4HANA in the raw layer as Parquet was the optimal approach. I believed that Parquet was the most suitable format for this purpose. However, I&amp;#39;ve recently learned that aside from ADLS, it&amp;#39;s also possible to store data in Cassandra. This discovery has led me to wonder: if Cassandra is capable of storing highly structured data from SAP, might it be a better choice?&lt;/p&gt;\n\n&lt;p&gt;Am I oversimplifying the issue by not fully considering factors like the cost differences between saving data in Parquet versus storing it in Cassandra, or potential use cases like analytic reporting versus write preference?&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate your perspective on how to make this decision.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b9bnak", "is_robot_indexable": true, "report_reasons": null, "author": "scht1980", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9bnak/cassandra_or_praquet_for_structured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9bnak/cassandra_or_praquet_for_structured_data/", "subreddit_subscribers": 166997, "created_utc": 1709862251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " A very common problem in querying ERP or manufacturing MRP systems is that they use \u201cself-join\u201d tables to store their part information. These tables store \u201cbill of materials\u201d or \u201cBOM\u201d information, where complete assemblies of items can be found.\n\nI have this table with approximately 200k rows, that's similar to a bill of materials. For example: product A1 is made of 2 A2s, A2 is made of 2 A3s and A3 is made of 2 A4s, so on and so forth. We have thousands of products in this table.\n\nBasically I need to transform this in a table that says A1 is made of 8 A4s, and does this for every product. We need to know which and how many items the products have in their final layer. I've tried doing a bunch of joins in SQL but the problem is a don't know how many layers each product can have.\n\nDo any of you have an efficient way to approach this?\n\n&amp;#x200B;\n\n[Simple example](https://preview.redd.it/2h3ygsyul5nc1.png?width=679&amp;format=png&amp;auto=webp&amp;s=1c3bcb0b000f8ad5e05c4342b7bbec474b721e7b)", "author_fullname": "t2_1fcpawj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bill of materials table - SQL help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 45, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2h3ygsyul5nc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=19bac9127cd540bb036aa0bd8939278207626963"}, {"y": 69, "x": 216, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a671abae8d14edabe1294437a19ae40aa550ddf"}, {"y": 103, "x": 320, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=287e5cc9b94d20e5c9506b9614c7c75bec6c63ad"}, {"y": 206, "x": 640, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d204c9264670d4b35f9d55769f8d51bc814d1308"}], "s": {"y": 219, "x": 679, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=679&amp;format=png&amp;auto=webp&amp;s=1c3bcb0b000f8ad5e05c4342b7bbec474b721e7b"}, "id": "2h3ygsyul5nc1"}}, "name": "t3_1b9vh4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/aP2NDvmYmw5Xbsh4TrCD8XYdwSuN1qQKGAVeC276YU8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709922874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A very common problem in querying ERP or manufacturing MRP systems is that they use \u201cself-join\u201d tables to store their part information. These tables store \u201cbill of materials\u201d or \u201cBOM\u201d information, where complete assemblies of items can be found.&lt;/p&gt;\n\n&lt;p&gt;I have this table with approximately 200k rows, that&amp;#39;s similar to a bill of materials. For example: product A1 is made of 2 A2s, A2 is made of 2 A3s and A3 is made of 2 A4s, so on and so forth. We have thousands of products in this table.&lt;/p&gt;\n\n&lt;p&gt;Basically I need to transform this in a table that says A1 is made of 8 A4s, and does this for every product. We need to know which and how many items the products have in their final layer. I&amp;#39;ve tried doing a bunch of joins in SQL but the problem is a don&amp;#39;t know how many layers each product can have.&lt;/p&gt;\n\n&lt;p&gt;Do any of you have an efficient way to approach this?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2h3ygsyul5nc1.png?width=679&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c3bcb0b000f8ad5e05c4342b7bbec474b721e7b\"&gt;Simple example&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9vh4s", "is_robot_indexable": true, "report_reasons": null, "author": "TulioCM", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9vh4s/bill_of_materials_table_sql_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9vh4s/bill_of_materials_table_sql_help/", "subreddit_subscribers": 166997, "created_utc": 1709922874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi team,\n\nI'm relatively new to the Azure data engineering world and have taken a leap of faith to step out of my comfort zone, which was primarily in SAP data warehousing. I am thankful for how this role has supported my family and me, but I am eager to learn about what everyone else is up to in the field. For instance, I'd love to hear about the exciting projects you are working on, like using big data to tackle poverty or advanced machine learning projects in search of extraterrestrial life.\n\nI'm curious about the challenges you face daily and the successes you quietly celebrate when things go smoothly. With the rapid introduction of new tools, how do you cope with the uncertainty of the future while maintaining a balance? It must be quite a juggling act.\n\nLet me share my experience to start the conversation. Currently, I am working in the agriculture sector, extracting SAP data into ADLS and processing it with Databricks. It's not overly complicated or fancy \u2013 the output is a straightforward PowerBI dashboard. The most engaging aspect so far has been writing simple PySpark scripts according to specifications. The main challenge lies in coordinating among multiple teams, ensuring that SAP views are available for data orchestration. As for the future, considering my background in SAP BW, I aim to delve into the open-source realm. It's a new frontier for me, and I remind myself to stay focused and aim for my Databricks certification before venturing further.\n\nWhile I'm not yet tackling global issues like world hunger, I'm really interested to know about the complex problems you all are solving. Thanks for sharing your insights!", "author_fullname": "t2_5b8k9nl1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Share Your Big Data and ML Adventures!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9tv07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709919112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m relatively new to the Azure data engineering world and have taken a leap of faith to step out of my comfort zone, which was primarily in SAP data warehousing. I am thankful for how this role has supported my family and me, but I am eager to learn about what everyone else is up to in the field. For instance, I&amp;#39;d love to hear about the exciting projects you are working on, like using big data to tackle poverty or advanced machine learning projects in search of extraterrestrial life.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious about the challenges you face daily and the successes you quietly celebrate when things go smoothly. With the rapid introduction of new tools, how do you cope with the uncertainty of the future while maintaining a balance? It must be quite a juggling act.&lt;/p&gt;\n\n&lt;p&gt;Let me share my experience to start the conversation. Currently, I am working in the agriculture sector, extracting SAP data into ADLS and processing it with Databricks. It&amp;#39;s not overly complicated or fancy \u2013 the output is a straightforward PowerBI dashboard. The most engaging aspect so far has been writing simple PySpark scripts according to specifications. The main challenge lies in coordinating among multiple teams, ensuring that SAP views are available for data orchestration. As for the future, considering my background in SAP BW, I aim to delve into the open-source realm. It&amp;#39;s a new frontier for me, and I remind myself to stay focused and aim for my Databricks certification before venturing further.&lt;/p&gt;\n\n&lt;p&gt;While I&amp;#39;m not yet tackling global issues like world hunger, I&amp;#39;m really interested to know about the complex problems you all are solving. Thanks for sharing your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b9tv07", "is_robot_indexable": true, "report_reasons": null, "author": "scht1980", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9tv07/share_your_big_data_and_ml_adventures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9tv07/share_your_big_data_and_ml_adventures/", "subreddit_subscribers": 166997, "created_utc": 1709919112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Firstly I am new to the Data Engineering role, but have been working with SQL and data for many years. My knowledge on all the tools out there is limited.\n\nThe company I work for are regularly supplied with a .bak file of a third party system the company uses and I have been tasked with extracting data from it and loading it to our Azure SQL DB. I have a local docker container with an instance of SQL which I use to restore the .bak file to. I have tried to export a bacpac file from the DB but there are many errors due to unresolved references to objects.\n\nI can extract the data and load to Azure SQL manually using the restored DB in docker, but want to avoid this and automate the process. Even if the bacpac file worked I would like to automate the process rather than relying on a manual process.\n\nHas anyone done something similar to this, or have any ideas on how I could go about accomplishing this? ", "author_fullname": "t2_hk1fdp7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline using .bak file and move data to an Azure SQL DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9rmyv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709913877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Firstly I am new to the Data Engineering role, but have been working with SQL and data for many years. My knowledge on all the tools out there is limited.&lt;/p&gt;\n\n&lt;p&gt;The company I work for are regularly supplied with a .bak file of a third party system the company uses and I have been tasked with extracting data from it and loading it to our Azure SQL DB. I have a local docker container with an instance of SQL which I use to restore the .bak file to. I have tried to export a bacpac file from the DB but there are many errors due to unresolved references to objects.&lt;/p&gt;\n\n&lt;p&gt;I can extract the data and load to Azure SQL manually using the restored DB in docker, but want to avoid this and automate the process. Even if the bacpac file worked I would like to automate the process rather than relying on a manual process.&lt;/p&gt;\n\n&lt;p&gt;Has anyone done something similar to this, or have any ideas on how I could go about accomplishing this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9rmyv", "is_robot_indexable": true, "report_reasons": null, "author": "l0lez", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9rmyv/pipeline_using_bak_file_and_move_data_to_an_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9rmyv/pipeline_using_bak_file_and_move_data_to_an_azure/", "subreddit_subscribers": 166997, "created_utc": 1709913877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sup lads, \nFor context: I am currently in my second semester doing a Master's in Big Data and data science technologies in London. \nI m new to the data science field and whilst i v done my undergraduate in computer science and have decent madtery of certain base skills, m still realtively new to the data science field. I am very interested in data engineering in contrats to typical data analysis and vizualisation. \nFor my Master's final dissertation, I noticed that most students just pick a research topic in healthcare for instance, look for a dataset, do analysis, show findings and thats it. \nTo me I find this aspect of data science uninteresting and trivial and i have no idea which research area to aim for anyways. \nSince i still lack experience I would very much like for my research project to be slightly more technical and can sort of nudge me to the DE field. \nIf anyone has any ideas or past experiences i would very much like to hear anything, this choosing topic phase is my least favourite and I need some sort of brainstorming to get me started so I can snowball!", "author_fullname": "t2_77nma56i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master's dissertation research project ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9qfw8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709911019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sup lads, \nFor context: I am currently in my second semester doing a Master&amp;#39;s in Big Data and data science technologies in London. \nI m new to the data science field and whilst i v done my undergraduate in computer science and have decent madtery of certain base skills, m still realtively new to the data science field. I am very interested in data engineering in contrats to typical data analysis and vizualisation. \nFor my Master&amp;#39;s final dissertation, I noticed that most students just pick a research topic in healthcare for instance, look for a dataset, do analysis, show findings and thats it. \nTo me I find this aspect of data science uninteresting and trivial and i have no idea which research area to aim for anyways. \nSince i still lack experience I would very much like for my research project to be slightly more technical and can sort of nudge me to the DE field. \nIf anyone has any ideas or past experiences i would very much like to hear anything, this choosing topic phase is my least favourite and I need some sort of brainstorming to get me started so I can snowball!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9qfw8", "is_robot_indexable": true, "report_reasons": null, "author": "Laxaizen", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9qfw8/masters_dissertation_research_project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9qfw8/masters_dissertation_research_project_ideas/", "subreddit_subscribers": 166997, "created_utc": 1709911019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/b4zv93asb2nc1.png?width=1610&amp;format=png&amp;auto=webp&amp;s=20b4e39ec6d2c9fa3ac4d43b64660ef6fc443d79\n\nI created a multi-armed bandit simulator as a personal project: https://github.com/FlynnOwen/multi-armed-bandits/tree/main  \nI work as a data engineer/scientist but don't often get to play around with new software, and sometimes work on projects outside of work hours to stay fresh and learn more about the space. I thought members of this sub may appreciate this piece of software I worked on.  \nStay cool data devs 8)", "author_fullname": "t2_fwzjyaet", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-Armed Bandit Simulator [https://github.com/FlynnOwen/multi-armed-bandits/tree/main]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 108, "top_awarded_type": null, "hide_score": false, "media_metadata": {"b4zv93asb2nc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/b4zv93asb2nc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f49804f658fdf6b17cfe29d64cb3551d73598b71"}, {"y": 167, "x": 216, "u": "https://preview.redd.it/b4zv93asb2nc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cfbd2dfda59f3f315759a8f9e1749c00bcf4b92"}, {"y": 248, "x": 320, "u": "https://preview.redd.it/b4zv93asb2nc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3aa4d7f0eafe502fb03f3b1b23b2c2a43c660cea"}, {"y": 496, "x": 640, "u": "https://preview.redd.it/b4zv93asb2nc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d968cf106f25faf646db4ac262c8577bcc9547d0"}, {"y": 744, "x": 960, "u": "https://preview.redd.it/b4zv93asb2nc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b80f460d70a8cc360d1e75b4ccee89964f8d6264"}, {"y": 837, "x": 1080, "u": "https://preview.redd.it/b4zv93asb2nc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d49b1ed4d1a370bac5d6647ec1c3a47905d6b2ea"}], "s": {"y": 1248, "x": 1610, "u": "https://preview.redd.it/b4zv93asb2nc1.png?width=1610&amp;format=png&amp;auto=webp&amp;s=20b4e39ec6d2c9fa3ac4d43b64660ef6fc443d79"}, "id": "b4zv93asb2nc1"}}, "name": "t3_1b9ic2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4rngoA4erfW4rlEKT2OUcHVvsWshK3SYMHOhq5Hmlqs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709882935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b4zv93asb2nc1.png?width=1610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20b4e39ec6d2c9fa3ac4d43b64660ef6fc443d79\"&gt;https://preview.redd.it/b4zv93asb2nc1.png?width=1610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20b4e39ec6d2c9fa3ac4d43b64660ef6fc443d79&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I created a multi-armed bandit simulator as a personal project: &lt;a href=\"https://github.com/FlynnOwen/multi-armed-bandits/tree/main\"&gt;https://github.com/FlynnOwen/multi-armed-bandits/tree/main&lt;/a&gt;&lt;br/&gt;\nI work as a data engineer/scientist but don&amp;#39;t often get to play around with new software, and sometimes work on projects outside of work hours to stay fresh and learn more about the space. I thought members of this sub may appreciate this piece of software I worked on.&lt;br/&gt;\nStay cool data devs 8)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1b9ic2j", "is_robot_indexable": true, "report_reasons": null, "author": "engineering-scienct", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9ic2j/multiarmed_bandit_simulator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9ic2j/multiarmed_bandit_simulator/", "subreddit_subscribers": 166997, "created_utc": 1709882935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR:\n\nFeels left out of the handover process after team lead resigned. Concerned about career growth &amp; potential reassignment of tasks to less technically skilled coworker. Time to look for a new role or tough it out while trying to get buy-in from management?    \n.  \n.  \n.  \n\n---\nHi all, relatively pretty green &amp; junior here (esp. with regards to navigating the human side of things in a corporate setting). Looking for advice/perspective from the more experienced DE folks here. \n\nMy team lead has decided to resign recently &amp; we're about to start the handover process. I've been fortunate to be working with him as he's been a great manager who can advocate for me to management &amp; thus far I'd like to say that I've sufficiently contributed to the data team. AFAIK I've been doing just fine since nobody has raised any issues regarding my performance.\n\nBut I just caught wind that I was not meant to be involved in the handover meeting. My team lead, bless his heart, does want to involve me so he will invite me in but it's strange to originally leave me out as the rest of data team are required to be in attendance. \n\nSome of the newer, currently in development reporting/dashboard stack that's definitely right up my alley is even to be assigned to a coworker who's not as technical as me. I don't mean that as a demeaning remark as said coworker does an amazing data admin-related job (which is his main role in the data team), but in a more DA/DE capacity, has to frequently ask me for ad-hoc, quite simple queries. Just doesn't make sense to me.\n\nIt seems like for now there's a still a chance to convince management that I have something to offer from a more technical standpoint to the data team through the handover meeting but the whole thing just leaves a sour note on my end. Is it time to start looking for a new role just in case management remains unconvinced?", "author_fullname": "t2_lp3x6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dealing with Management's Lack of Awareness (and Potential Underappreciation of Technical Contributions)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9b188", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709863615.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709860561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR:&lt;/p&gt;\n\n&lt;p&gt;Feels left out of the handover process after team lead resigned. Concerned about career growth &amp;amp; potential reassignment of tasks to less technically skilled coworker. Time to look for a new role or tough it out while trying to get buy-in from management?&lt;br/&gt;\n.&lt;br/&gt;\n.&lt;br/&gt;\n.  &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Hi all, relatively pretty green &amp;amp; junior here (esp. with regards to navigating the human side of things in a corporate setting). Looking for advice/perspective from the more experienced DE folks here. &lt;/p&gt;\n\n&lt;p&gt;My team lead has decided to resign recently &amp;amp; we&amp;#39;re about to start the handover process. I&amp;#39;ve been fortunate to be working with him as he&amp;#39;s been a great manager who can advocate for me to management &amp;amp; thus far I&amp;#39;d like to say that I&amp;#39;ve sufficiently contributed to the data team. AFAIK I&amp;#39;ve been doing just fine since nobody has raised any issues regarding my performance.&lt;/p&gt;\n\n&lt;p&gt;But I just caught wind that I was not meant to be involved in the handover meeting. My team lead, bless his heart, does want to involve me so he will invite me in but it&amp;#39;s strange to originally leave me out as the rest of data team are required to be in attendance. &lt;/p&gt;\n\n&lt;p&gt;Some of the newer, currently in development reporting/dashboard stack that&amp;#39;s definitely right up my alley is even to be assigned to a coworker who&amp;#39;s not as technical as me. I don&amp;#39;t mean that as a demeaning remark as said coworker does an amazing data admin-related job (which is his main role in the data team), but in a more DA/DE capacity, has to frequently ask me for ad-hoc, quite simple queries. Just doesn&amp;#39;t make sense to me.&lt;/p&gt;\n\n&lt;p&gt;It seems like for now there&amp;#39;s a still a chance to convince management that I have something to offer from a more technical standpoint to the data team through the handover meeting but the whole thing just leaves a sour note on my end. Is it time to start looking for a new role just in case management remains unconvinced?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b9b188", "is_robot_indexable": true, "report_reasons": null, "author": "YsrYsl", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9b188/dealing_with_managements_lack_of_awareness_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9b188/dealing_with_managements_lack_of_awareness_and/", "subreddit_subscribers": 166997, "created_utc": 1709860561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I am currently documenting the data flow of our project and I wanted the documentation to be easily understood by analysts in a visual manner. What is the appropriate way to diagram data flow between views, tables, left joins? ", "author_fullname": "t2_9o6vpzoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to diagram sql queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ba4g7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709944933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I am currently documenting the data flow of our project and I wanted the documentation to be easily understood by analysts in a visual manner. What is the appropriate way to diagram data flow between views, tables, left joins? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ba4g7v", "is_robot_indexable": true, "report_reasons": null, "author": "SlingBag", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ba4g7v/how_to_diagram_sql_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ba4g7v/how_to_diagram_sql_queries/", "subreddit_subscribers": 166997, "created_utc": 1709944933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://medium.com/gitconnected/turnkey-data-analysis-using-duckdb-and-metabase-for-postgresql-cloudwatch-1255b925b7b4", "author_fullname": "t2_14lsag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Turnkey Data Analysis Using DuckDB and Metabase for PostgreSQL &amp; CloudWatch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9p7eb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709907986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/gitconnected/turnkey-data-analysis-using-duckdb-and-metabase-for-postgresql-cloudwatch-1255b925b7b4\"&gt;https://medium.com/gitconnected/turnkey-data-analysis-using-duckdb-and-metabase-for-postgresql-cloudwatch-1255b925b7b4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?auto=webp&amp;s=abaa6360bb09d94c3227fe8f18ad59de5848a42b", "width": 686, "height": 686}, "resolutions": [{"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2468b6c6a2be58cf42aa7e20614daa68ea0f79c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8ddf297cb6021e7f0d92ef24cb4ec81cdf6b736", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c1c5e93640b894f5dca0ae91121f73a3cf353a6", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fad19306cfa8405de1555516d3364ec20081a882", "width": 640, "height": 640}], "variants": {}, "id": "VGarDclfmkWIQ7EJ1_v3fMXiX3AflWHP_k0JGGKcL4E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b9p7eb", "is_robot_indexable": true, "report_reasons": null, "author": "redgeoff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9p7eb/turnkey_data_analysis_using_duckdb_and_metabase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9p7eb/turnkey_data_analysis_using_duckdb_and_metabase/", "subreddit_subscribers": 166997, "created_utc": 1709907986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_m38rr95q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Hunt for the Missing Data Type", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9kfaw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1709891306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hillelwayne.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.hillelwayne.com/post/graph-types//", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b9kfaw", "is_robot_indexable": true, "report_reasons": null, "author": "lotwoefax", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9kfaw/the_hunt_for_the_missing_data_type/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.hillelwayne.com/post/graph-types//", "subreddit_subscribers": 166997, "created_utc": 1709891306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have a pipeline loading data from SAP CRM via a dataflow with SAP CDC as a source. I was able to parameterise most of the details but I am struggling with selecting only a given list of columns from the source instead of the entire object. \n\nThe setup:\nIn my pipeline, I have a lookup activity that returns the ODP name, context, connection, etc. as well as a list of the columns I need from that particular ODP. Then the dataflow to which I assign these parameters and which sinks the data into .parquet. All ODPs passed to the dataflow (around 500) are fully loaded on each run. \n\nAny insight on this would be greatly appreciated!!", "author_fullname": "t2_i6l8y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Select columns in ADF\u2019s CDC connector", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9qsqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709911884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have a pipeline loading data from SAP CRM via a dataflow with SAP CDC as a source. I was able to parameterise most of the details but I am struggling with selecting only a given list of columns from the source instead of the entire object. &lt;/p&gt;\n\n&lt;p&gt;The setup:\nIn my pipeline, I have a lookup activity that returns the ODP name, context, connection, etc. as well as a list of the columns I need from that particular ODP. Then the dataflow to which I assign these parameters and which sinks the data into .parquet. All ODPs passed to the dataflow (around 500) are fully loaded on each run. &lt;/p&gt;\n\n&lt;p&gt;Any insight on this would be greatly appreciated!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9qsqx", "is_robot_indexable": true, "report_reasons": null, "author": "givnv", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9qsqx/select_columns_in_adfs_cdc_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9qsqx/select_columns_in_adfs_cdc_connector/", "subreddit_subscribers": 166997, "created_utc": 1709911884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,  \n(Im personally quite new to data engineering)  \none of our customers has hundreds of distributed databases, each with one important table of sensor records. We have an existing data pipeline to process this kind of data (with Dagster), but never ingested data at this scale.  \nNormally we do ingestion with a simple Python script, but at this scale, we would need to build a set of features that any database-to-database ingestion framework would need (keep a list of databases, connect, check table exists, check what's new, fetch it, allow the user to add a new database table to be synced).\n\nI know about Airbyte and Meltano, both are quite powerful and heavy frameworks with many connectors. I was hoping for something simpler. I just need keep the local table in sync with the remote one via SQL at regular intervals (not realtime). Im tempted to build it myself, please stop me.\n\nWould be happy if you would share your ideas.", "author_fullname": "t2_otqll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion tools to keep databases in sync", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9niyy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709905565.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709903318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\n(Im personally quite new to data engineering)&lt;br/&gt;\none of our customers has hundreds of distributed databases, each with one important table of sensor records. We have an existing data pipeline to process this kind of data (with Dagster), but never ingested data at this scale.&lt;br/&gt;\nNormally we do ingestion with a simple Python script, but at this scale, we would need to build a set of features that any database-to-database ingestion framework would need (keep a list of databases, connect, check table exists, check what&amp;#39;s new, fetch it, allow the user to add a new database table to be synced).&lt;/p&gt;\n\n&lt;p&gt;I know about Airbyte and Meltano, both are quite powerful and heavy frameworks with many connectors. I was hoping for something simpler. I just need keep the local table in sync with the remote one via SQL at regular intervals (not realtime). Im tempted to build it myself, please stop me.&lt;/p&gt;\n\n&lt;p&gt;Would be happy if you would share your ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9niyy", "is_robot_indexable": true, "report_reasons": null, "author": "Cominous", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9niyy/data_ingestion_tools_to_keep_databases_in_sync/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9niyy/data_ingestion_tools_to_keep_databases_in_sync/", "subreddit_subscribers": 166997, "created_utc": 1709903318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is data project really difficult compared to normal software development, i recently moved to data domain , so this is enhanment project which was already build we need to fix the issues and bugs like why data is not populating , why this values are missing, we neeed to back track a lot through millions of data this is so much stress and difficult, I like analysing but here there are many different filter conditions used so if we remove one we may get answer , since our project does not have any product owners or why this filter condition is been used no one can answer \n\nI want to know is this how data projects are or is it because this project is enhancement kind of that why I am facing this much issue, i definitely know if this is from scratch i would be knowing in and out of every code and fetaures, kindly help", "author_fullname": "t2_bjea0hes", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data project really difficult compared to normal software development project, please help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9ha96", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709879091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is data project really difficult compared to normal software development, i recently moved to data domain , so this is enhanment project which was already build we need to fix the issues and bugs like why data is not populating , why this values are missing, we neeed to back track a lot through millions of data this is so much stress and difficult, I like analysing but here there are many different filter conditions used so if we remove one we may get answer , since our project does not have any product owners or why this filter condition is been used no one can answer &lt;/p&gt;\n\n&lt;p&gt;I want to know is this how data projects are or is it because this project is enhancement kind of that why I am facing this much issue, i definitely know if this is from scratch i would be knowing in and out of every code and fetaures, kindly help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b9ha96", "is_robot_indexable": true, "report_reasons": null, "author": "Zestyclose_Ad6748", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9ha96/is_data_project_really_difficult_compared_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9ha96/is_data_project_really_difficult_compared_to/", "subreddit_subscribers": 166997, "created_utc": 1709879091.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}