{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa5dw92do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stop Paying Snowflake for Failing Workloads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9oqr9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wOM8ib6rTYoiI0FU8X1XFvJdcf2IDdqmc2UArIGxy-w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709906752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "baselit.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://baselit.ai/blog/stop-paying-snowflake-for-failing-workloads", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?auto=webp&amp;s=8ee32ef232f1d9b4ad2813e8f617ec61acdebbf3", "width": 1024, "height": 773}, "resolutions": [{"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=543ee50b5969a2641d08e32aaebcbf150d40c49b", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b7208f26ffcd6ba77f097a8ba46da5e5c680ff7", "width": 216, "height": 163}, {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=827fe7c2852c9add08b2640bd191572a846a4354", "width": 320, "height": 241}, {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=36dfead6e99a4bee4a2ddf38c8dcc663472ed621", "width": 640, "height": 483}, {"url": "https://external-preview.redd.it/mFxH8sLMQjxtNCSxcLFOp9GRmZS2bBRwesfpb1vev3I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=846d7d76ba880b788d64436cef4ffe9e1ca0edac", "width": 960, "height": 724}], "variants": {}, "id": "r3CgWc1G_RZzYfh9ko9igTDnHUgYIslIfknt6RfU6cw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b9oqr9", "is_robot_indexable": true, "report_reasons": null, "author": "sahil_singla", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9oqr9/stop_paying_snowflake_for_failing_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://baselit.ai/blog/stop-paying-snowflake-for-failing-workloads", "subreddit_subscribers": 167086, "created_utc": 1709906752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I am currently documenting the data flow of our project and I wanted the documentation to be easily understood by analysts in a visual manner. What is the appropriate way to diagram data flow between views, tables, left joins? ", "author_fullname": "t2_9o6vpzoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to diagram sql queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ba4g7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709944933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I am currently documenting the data flow of our project and I wanted the documentation to be easily understood by analysts in a visual manner. What is the appropriate way to diagram data flow between views, tables, left joins? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ba4g7v", "is_robot_indexable": true, "report_reasons": null, "author": "SlingBag", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ba4g7v/how_to_diagram_sql_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ba4g7v/how_to_diagram_sql_queries/", "subreddit_subscribers": 167086, "created_utc": 1709944933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I can't get concurrent users to increase no matter the server's CPU power.\n\nHello, I'm working on a production web application that has a giant MySQL database at the backend. The database is constantly updated with new information from various sources at different timestamps every single day. The web application is report-generation-based, where the user 'generates reports' of data from a certain time range they specify, which is done by querying against the database. This querying of MySQL takes a lot of time and is CPU intensive (observed from htop). MySQL contains various types of data, especially large-string data. Now, to generate a complex report for a single user, it uses 1 CPU (thread or vCPU), not the whole number of CPUs available. Similarly, for 4 users, 4 CPUs, and the rest of the CPUs are idle. I simulate multiple concurrent users' report generation tests using the PostMan application. **Now, no matter how powerful the CPU I use, it is not being efficient and caps at around 30-40 concurrent users (powerful CPU results in higher caps) and also takes a lot of time.**\n\nWhen multiple users are simultaneously querying the database, all logical cores of the server become preoccupied with handling MySQL queries, which in turn reduces the application's ability to manage concurrent users effectively. For example, a single user might generate a report for one month's worth of data in 5 minutes. However, if 20 to 30 users attempt to generate the same report simultaneously, the completion time can extend to as much as 30 minutes. Also, **when the volume of concurrent requests grows further, some users may experience failures in receiving their report outputs successfully.**\n\nI am thinking of parallel computing and using all available CPUs for each report generation instead of using only 1 CPU, but it has its disadvantages. If a rogue user constantly keeps generating very complex reports, other users will not be able to get fruitful results. So I'm currently not considering this option.\n\nIs there any other way I can improve this from a query perspective or any other perspective? Please can anyone help me find a solution to this problem? What type of architecture should be used to keep the same performance for all concurrent users and also increase the concurrent users cap (our requirement is about 100+ concurrent users)?\n\n# Additional Information:\n\nBackend: Dotnet Core 6 Web API (MVC)\n\n# Database:\n\nMySql Community Server\u00a0(free\u00a0version)  \n**table 48, data length 3,368,960,000, indexes\u00a081,920**  \nBut in my calculation, I mostly only need to query from 2 big tables:\n\n# 1st table information:\n\nEvery 24 hours, 7,153 rows are inserted into our database, each identified by a timestamp range from start (timestamp) to finish (timestamp, which may be Null). When retrieving data from this table over a long date range\u2014using both start and finish times\u2014alongside an integer field representing a list of user IDs.  \nFor example, a user might request data spanning from January 1, 2024, to February 29, 2024. This duration could vary significantly, ranging from 6 months to 1 year. Additionally, the query includes a large list of user IDs (e.g., 112, 23, 45, 78, 45, 56, etc.), with each userID associated with multiple rows in the database.\n\n|Type|\n|:-|\n|bigint(20) unassigned Auto Increment|\n|int(11)|\n|int(11)|\n|timestamp \\[current\\_timestamp()\\]|\n|timestamp NULL|\n|double(10,2) NULL|\n|int(11) \\[1\\]|\n|int(11) \\[1\\]|\n|int(11) NULL|\n\n# 2nd table information:\n\nThe second table in our database experiences an insertion of 2,000 rows every 24 hours. Similar to the first, this table records data within specific time ranges, set by a start and finish timestamp. Additionally, it stores variable character data (VARCHAR) as well.  \nQueries on this table are executed over time ranges, similar to those for table one, with durations typically spanning 3 to 6 months. Along with time-based criteria like Table 1, these queries also filter for five extensive lists of string values, each list containing approximately 100 to 200 string values.\n\n|Type|\n|:-|\n|int(11) Auto Increment|\n|date|\n|int(10)|\n|varchar(200)|\n|varchar(100)|\n|varchar(100)|\n|time|\n|int(10)|\n|timestamp \\[current\\_timestamp()\\]|\n|timestamp \\[current\\_timestamp()\\]|\n|varchar(200)|\n|varchar(100)|\n|varchar(100)|\n|varchar(100)|\n|varchar(100)|\n|varchar(100)|\n|varchar(200)|\n|varchar(100)|\n|int(10)|\n|int(10)|\n|varchar(200) NULL|\n|int(100)|\n|varchar(100) NULL|\n\n# Test Results (Dedicated Bare Metal Servers):\n\nSystemInfo: Intel Xeon E5-2696 v4 | 2 sockets x 22 cores/CPU x 2 thread/core = 88\u00a0threads | 448GB DDR4 RAM  \nSingle User Report Generation time: 3mins (for 1 week's data)  \n20 Concurrent Users Report Generation time: 25 min (for 1 week's data) and 2 users report generation were unsuccessful.  \n**Maximum concurrent users it\u00a0can\u00a0handle:\u00a040**", "author_fullname": "t2_58tp8ktc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help: Optimizing MySQL for 100 Concurrent Users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9mh05", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709900020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t get concurrent users to increase no matter the server&amp;#39;s CPU power.&lt;/p&gt;\n\n&lt;p&gt;Hello, I&amp;#39;m working on a production web application that has a giant MySQL database at the backend. The database is constantly updated with new information from various sources at different timestamps every single day. The web application is report-generation-based, where the user &amp;#39;generates reports&amp;#39; of data from a certain time range they specify, which is done by querying against the database. This querying of MySQL takes a lot of time and is CPU intensive (observed from htop). MySQL contains various types of data, especially large-string data. Now, to generate a complex report for a single user, it uses 1 CPU (thread or vCPU), not the whole number of CPUs available. Similarly, for 4 users, 4 CPUs, and the rest of the CPUs are idle. I simulate multiple concurrent users&amp;#39; report generation tests using the PostMan application. &lt;strong&gt;Now, no matter how powerful the CPU I use, it is not being efficient and caps at around 30-40 concurrent users (powerful CPU results in higher caps) and also takes a lot of time.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;When multiple users are simultaneously querying the database, all logical cores of the server become preoccupied with handling MySQL queries, which in turn reduces the application&amp;#39;s ability to manage concurrent users effectively. For example, a single user might generate a report for one month&amp;#39;s worth of data in 5 minutes. However, if 20 to 30 users attempt to generate the same report simultaneously, the completion time can extend to as much as 30 minutes. Also, &lt;strong&gt;when the volume of concurrent requests grows further, some users may experience failures in receiving their report outputs successfully.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am thinking of parallel computing and using all available CPUs for each report generation instead of using only 1 CPU, but it has its disadvantages. If a rogue user constantly keeps generating very complex reports, other users will not be able to get fruitful results. So I&amp;#39;m currently not considering this option.&lt;/p&gt;\n\n&lt;p&gt;Is there any other way I can improve this from a query perspective or any other perspective? Please can anyone help me find a solution to this problem? What type of architecture should be used to keep the same performance for all concurrent users and also increase the concurrent users cap (our requirement is about 100+ concurrent users)?&lt;/p&gt;\n\n&lt;h1&gt;Additional Information:&lt;/h1&gt;\n\n&lt;p&gt;Backend: Dotnet Core 6 Web API (MVC)&lt;/p&gt;\n\n&lt;h1&gt;Database:&lt;/h1&gt;\n\n&lt;p&gt;MySql Community Server\u00a0(free\u00a0version)&lt;br/&gt;\n&lt;strong&gt;table 48, data length 3,368,960,000, indexes\u00a081,920&lt;/strong&gt;&lt;br/&gt;\nBut in my calculation, I mostly only need to query from 2 big tables:&lt;/p&gt;\n\n&lt;h1&gt;1st table information:&lt;/h1&gt;\n\n&lt;p&gt;Every 24 hours, 7,153 rows are inserted into our database, each identified by a timestamp range from start (timestamp) to finish (timestamp, which may be Null). When retrieving data from this table over a long date range\u2014using both start and finish times\u2014alongside an integer field representing a list of user IDs.&lt;br/&gt;\nFor example, a user might request data spanning from January 1, 2024, to February 29, 2024. This duration could vary significantly, ranging from 6 months to 1 year. Additionally, the query includes a large list of user IDs (e.g., 112, 23, 45, 78, 45, 56, etc.), with each userID associated with multiple rows in the database.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Type&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;bigint(20) unassigned Auto Increment&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;timestamp [current_timestamp()]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;timestamp NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;double(10,2) NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11) [1]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11) [1]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11) NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;2nd table information:&lt;/h1&gt;\n\n&lt;p&gt;The second table in our database experiences an insertion of 2,000 rows every 24 hours. Similar to the first, this table records data within specific time ranges, set by a start and finish timestamp. Additionally, it stores variable character data (VARCHAR) as well.&lt;br/&gt;\nQueries on this table are executed over time ranges, similar to those for table one, with durations typically spanning 3 to 6 months. Along with time-based criteria like Table 1, these queries also filter for five extensive lists of string values, each list containing approximately 100 to 200 string values.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Type&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(11) Auto Increment&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;date&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(10)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(200)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;time&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(10)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;timestamp [current_timestamp()]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;timestamp [current_timestamp()]&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(200)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(200)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(10)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(10)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(200) NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;int(100)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;varchar(100) NULL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Test Results (Dedicated Bare Metal Servers):&lt;/h1&gt;\n\n&lt;p&gt;SystemInfo: Intel Xeon E5-2696 v4 | 2 sockets x 22 cores/CPU x 2 thread/core = 88\u00a0threads | 448GB DDR4 RAM&lt;br/&gt;\nSingle User Report Generation time: 3mins (for 1 week&amp;#39;s data)&lt;br/&gt;\n20 Concurrent Users Report Generation time: 25 min (for 1 week&amp;#39;s data) and 2 users report generation were unsuccessful.&lt;br/&gt;\n&lt;strong&gt;Maximum concurrent users it\u00a0can\u00a0handle:\u00a040&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9mh05", "is_robot_indexable": true, "report_reasons": null, "author": "Dr-Double-A", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9mh05/need_help_optimizing_mysql_for_100_concurrent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9mh05/need_help_optimizing_mysql_for_100_concurrent/", "subreddit_subscribers": 167086, "created_utc": 1709900020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The docs don\u2019t explain how to actually use Apache iceberg just how to connect to existing icebergs tables. How do you actually implement the iceberg format of your data lake?", "author_fullname": "t2_160eq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to implement Apache iceberg on my data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9rwqe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709914502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The docs don\u2019t explain how to actually use Apache iceberg just how to connect to existing icebergs tables. How do you actually implement the iceberg format of your data lake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9rwqe", "is_robot_indexable": true, "report_reasons": null, "author": "SirAutismx7", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9rwqe/how_to_implement_apache_iceberg_on_my_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9rwqe/how_to_implement_apache_iceberg_on_my_data_lake/", "subreddit_subscribers": 167086, "created_utc": 1709914502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " A very common problem in querying ERP or manufacturing MRP systems is that they use \u201cself-join\u201d tables to store their part information. These tables store \u201cbill of materials\u201d or \u201cBOM\u201d information, where complete assemblies of items can be found.\n\nI have this table with approximately 200k rows, that's similar to a bill of materials. For example: product A1 is made of 2 A2s, A2 is made of 2 A3s and A3 is made of 2 A4s, so on and so forth. We have thousands of products in this table.\n\nBasically I need to transform this in a table that says A1 is made of 8 A4s, and does this for every product. We need to know which and how many items the products have in their final layer. I've tried doing a bunch of joins in SQL but the problem is a don't know how many layers each product can have.\n\nDo any of you have an efficient way to approach this?\n\n&amp;#x200B;\n\n[Simple example](https://preview.redd.it/2h3ygsyul5nc1.png?width=679&amp;format=png&amp;auto=webp&amp;s=1c3bcb0b000f8ad5e05c4342b7bbec474b721e7b)", "author_fullname": "t2_1fcpawj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bill of materials table - SQL help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 45, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2h3ygsyul5nc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=19bac9127cd540bb036aa0bd8939278207626963"}, {"y": 69, "x": 216, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a671abae8d14edabe1294437a19ae40aa550ddf"}, {"y": 103, "x": 320, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=287e5cc9b94d20e5c9506b9614c7c75bec6c63ad"}, {"y": 206, "x": 640, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d204c9264670d4b35f9d55769f8d51bc814d1308"}], "s": {"y": 219, "x": 679, "u": "https://preview.redd.it/2h3ygsyul5nc1.png?width=679&amp;format=png&amp;auto=webp&amp;s=1c3bcb0b000f8ad5e05c4342b7bbec474b721e7b"}, "id": "2h3ygsyul5nc1"}}, "name": "t3_1b9vh4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/aP2NDvmYmw5Xbsh4TrCD8XYdwSuN1qQKGAVeC276YU8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709922874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A very common problem in querying ERP or manufacturing MRP systems is that they use \u201cself-join\u201d tables to store their part information. These tables store \u201cbill of materials\u201d or \u201cBOM\u201d information, where complete assemblies of items can be found.&lt;/p&gt;\n\n&lt;p&gt;I have this table with approximately 200k rows, that&amp;#39;s similar to a bill of materials. For example: product A1 is made of 2 A2s, A2 is made of 2 A3s and A3 is made of 2 A4s, so on and so forth. We have thousands of products in this table.&lt;/p&gt;\n\n&lt;p&gt;Basically I need to transform this in a table that says A1 is made of 8 A4s, and does this for every product. We need to know which and how many items the products have in their final layer. I&amp;#39;ve tried doing a bunch of joins in SQL but the problem is a don&amp;#39;t know how many layers each product can have.&lt;/p&gt;\n\n&lt;p&gt;Do any of you have an efficient way to approach this?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2h3ygsyul5nc1.png?width=679&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c3bcb0b000f8ad5e05c4342b7bbec474b721e7b\"&gt;Simple example&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9vh4s", "is_robot_indexable": true, "report_reasons": null, "author": "TulioCM", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9vh4s/bill_of_materials_table_sql_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9vh4s/bill_of_materials_table_sql_help/", "subreddit_subscribers": 167086, "created_utc": 1709922874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm a business Intelligence student and Data warehousing isn't my strong suit. I'm working on a project in which I need to evaluate the performance of members in a test. I created a constellation star schema where I have fact tables that mesure the overall performance of that type of test, with mesures such as number of participants, number succeded, number in progresss , etc ...\n\nI also have one fact table that mesures the individual performance of a participant rather than the entire test, with mesures such as score, startdate, finishdate , durationForCompletion, etc ...\n\nEach attempt made by a participant has a timestamp. I want to mesure the performance of the test by using the test fact table, and the time axis of analysis would analyse by quarter, year, month, etc ...\n\nI also want to mesure advancement by individual so the time axis would analyse progress by hour, day , month , etc ...\n\nDue to the granularity being different between both fact tables, do I create two time dimensions or is there a better approach ?", "author_fullname": "t2_kibbksmcq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Constellation schema problem : One or two Time Dimensions ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9pcs2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709908360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m a business Intelligence student and Data warehousing isn&amp;#39;t my strong suit. I&amp;#39;m working on a project in which I need to evaluate the performance of members in a test. I created a constellation star schema where I have fact tables that mesure the overall performance of that type of test, with mesures such as number of participants, number succeded, number in progresss , etc ...&lt;/p&gt;\n\n&lt;p&gt;I also have one fact table that mesures the individual performance of a participant rather than the entire test, with mesures such as score, startdate, finishdate , durationForCompletion, etc ...&lt;/p&gt;\n\n&lt;p&gt;Each attempt made by a participant has a timestamp. I want to mesure the performance of the test by using the test fact table, and the time axis of analysis would analyse by quarter, year, month, etc ...&lt;/p&gt;\n\n&lt;p&gt;I also want to mesure advancement by individual so the time axis would analyse progress by hour, day , month , etc ...&lt;/p&gt;\n\n&lt;p&gt;Due to the granularity being different between both fact tables, do I create two time dimensions or is there a better approach ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9pcs2", "is_robot_indexable": true, "report_reasons": null, "author": "Responsible-Word-137", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9pcs2/constellation_schema_problem_one_or_two_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9pcs2/constellation_schema_problem_one_or_two_time/", "subreddit_subscribers": 167086, "created_utc": 1709908360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI read that lsm trees is used by dynamo db, cassandra and scylla db. But nowhere saw link between relational db(sql db) and lsm trees. Is lsm trees only used in nosql databases if that is the case why can't we used them in sql ?  \n", "author_fullname": "t2_aqmxwdoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Database using LSM Trees", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1badyyn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709976826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I read that lsm trees is used by dynamo db, cassandra and scylla db. But nowhere saw link between relational db(sql db) and lsm trees. Is lsm trees only used in nosql databases if that is the case why can&amp;#39;t we used them in sql ?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1badyyn", "is_robot_indexable": true, "report_reasons": null, "author": "AggravatingParsnip89", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1badyyn/sql_database_using_lsm_trees/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1badyyn/sql_database_using_lsm_trees/", "subreddit_subscribers": 167086, "created_utc": 1709976826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi team,\n\nI'm relatively new to the Azure data engineering world and have taken a leap of faith to step out of my comfort zone, which was primarily in SAP data warehousing. I am thankful for how this role has supported my family and me, but I am eager to learn about what everyone else is up to in the field. For instance, I'd love to hear about the exciting projects you are working on, like using big data to tackle poverty or advanced machine learning projects in search of extraterrestrial life.\n\nI'm curious about the challenges you face daily and the successes you quietly celebrate when things go smoothly. With the rapid introduction of new tools, how do you cope with the uncertainty of the future while maintaining a balance? It must be quite a juggling act.\n\nLet me share my experience to start the conversation. Currently, I am working in the agriculture sector, extracting SAP data into ADLS and processing it with Databricks. It's not overly complicated or fancy \u2013 the output is a straightforward PowerBI dashboard. The most engaging aspect so far has been writing simple PySpark scripts according to specifications. The main challenge lies in coordinating among multiple teams, ensuring that SAP views are available for data orchestration. As for the future, considering my background in SAP BW, I aim to delve into the open-source realm. It's a new frontier for me, and I remind myself to stay focused and aim for my Databricks certification before venturing further.\n\nWhile I'm not yet tackling global issues like world hunger, I'm really interested to know about the complex problems you all are solving. Thanks for sharing your insights!", "author_fullname": "t2_5b8k9nl1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Share Your Big Data and ML Adventures!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9tv07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709919112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m relatively new to the Azure data engineering world and have taken a leap of faith to step out of my comfort zone, which was primarily in SAP data warehousing. I am thankful for how this role has supported my family and me, but I am eager to learn about what everyone else is up to in the field. For instance, I&amp;#39;d love to hear about the exciting projects you are working on, like using big data to tackle poverty or advanced machine learning projects in search of extraterrestrial life.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious about the challenges you face daily and the successes you quietly celebrate when things go smoothly. With the rapid introduction of new tools, how do you cope with the uncertainty of the future while maintaining a balance? It must be quite a juggling act.&lt;/p&gt;\n\n&lt;p&gt;Let me share my experience to start the conversation. Currently, I am working in the agriculture sector, extracting SAP data into ADLS and processing it with Databricks. It&amp;#39;s not overly complicated or fancy \u2013 the output is a straightforward PowerBI dashboard. The most engaging aspect so far has been writing simple PySpark scripts according to specifications. The main challenge lies in coordinating among multiple teams, ensuring that SAP views are available for data orchestration. As for the future, considering my background in SAP BW, I aim to delve into the open-source realm. It&amp;#39;s a new frontier for me, and I remind myself to stay focused and aim for my Databricks certification before venturing further.&lt;/p&gt;\n\n&lt;p&gt;While I&amp;#39;m not yet tackling global issues like world hunger, I&amp;#39;m really interested to know about the complex problems you all are solving. Thanks for sharing your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b9tv07", "is_robot_indexable": true, "report_reasons": null, "author": "scht1980", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9tv07/share_your_big_data_and_ml_adventures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9tv07/share_your_big_data_and_ml_adventures/", "subreddit_subscribers": 167086, "created_utc": 1709919112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Firstly I am new to the Data Engineering role, but have been working with SQL and data for many years. My knowledge on all the tools out there is limited.\n\nThe company I work for are regularly supplied with a .bak file of a third party system the company uses and I have been tasked with extracting data from it and loading it to our Azure SQL DB. I have a local docker container with an instance of SQL which I use to restore the .bak file to. I have tried to export a bacpac file from the DB but there are many errors due to unresolved references to objects.\n\nI can extract the data and load to Azure SQL manually using the restored DB in docker, but want to avoid this and automate the process. Even if the bacpac file worked I would like to automate the process rather than relying on a manual process.\n\nHas anyone done something similar to this, or have any ideas on how I could go about accomplishing this? ", "author_fullname": "t2_hk1fdp7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline using .bak file and move data to an Azure SQL DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9rmyv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709913877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Firstly I am new to the Data Engineering role, but have been working with SQL and data for many years. My knowledge on all the tools out there is limited.&lt;/p&gt;\n\n&lt;p&gt;The company I work for are regularly supplied with a .bak file of a third party system the company uses and I have been tasked with extracting data from it and loading it to our Azure SQL DB. I have a local docker container with an instance of SQL which I use to restore the .bak file to. I have tried to export a bacpac file from the DB but there are many errors due to unresolved references to objects.&lt;/p&gt;\n\n&lt;p&gt;I can extract the data and load to Azure SQL manually using the restored DB in docker, but want to avoid this and automate the process. Even if the bacpac file worked I would like to automate the process rather than relying on a manual process.&lt;/p&gt;\n\n&lt;p&gt;Has anyone done something similar to this, or have any ideas on how I could go about accomplishing this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9rmyv", "is_robot_indexable": true, "report_reasons": null, "author": "l0lez", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9rmyv/pipeline_using_bak_file_and_move_data_to_an_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9rmyv/pipeline_using_bak_file_and_move_data_to_an_azure/", "subreddit_subscribers": 167086, "created_utc": 1709913877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sup lads, \nFor context: I am currently in my second semester doing a Master's in Big Data and data science technologies in London. \nI m new to the data science field and whilst i v done my undergraduate in computer science and have decent madtery of certain base skills, m still realtively new to the data science field. I am very interested in data engineering in contrats to typical data analysis and vizualisation. \nFor my Master's final dissertation, I noticed that most students just pick a research topic in healthcare for instance, look for a dataset, do analysis, show findings and thats it. \nTo me I find this aspect of data science uninteresting and trivial and i have no idea which research area to aim for anyways. \nSince i still lack experience I would very much like for my research project to be slightly more technical and can sort of nudge me to the DE field. \nIf anyone has any ideas or past experiences i would very much like to hear anything, this choosing topic phase is my least favourite and I need some sort of brainstorming to get me started so I can snowball!", "author_fullname": "t2_77nma56i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master's dissertation research project ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9qfw8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709911019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sup lads, \nFor context: I am currently in my second semester doing a Master&amp;#39;s in Big Data and data science technologies in London. \nI m new to the data science field and whilst i v done my undergraduate in computer science and have decent madtery of certain base skills, m still realtively new to the data science field. I am very interested in data engineering in contrats to typical data analysis and vizualisation. \nFor my Master&amp;#39;s final dissertation, I noticed that most students just pick a research topic in healthcare for instance, look for a dataset, do analysis, show findings and thats it. \nTo me I find this aspect of data science uninteresting and trivial and i have no idea which research area to aim for anyways. \nSince i still lack experience I would very much like for my research project to be slightly more technical and can sort of nudge me to the DE field. \nIf anyone has any ideas or past experiences i would very much like to hear anything, this choosing topic phase is my least favourite and I need some sort of brainstorming to get me started so I can snowball!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9qfw8", "is_robot_indexable": true, "report_reasons": null, "author": "Laxaizen", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9qfw8/masters_dissertation_research_project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9qfw8/masters_dissertation_research_project_ideas/", "subreddit_subscribers": 167086, "created_utc": 1709911019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://medium.com/gitconnected/turnkey-data-analysis-using-duckdb-and-metabase-for-postgresql-cloudwatch-1255b925b7b4", "author_fullname": "t2_14lsag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Turnkey Data Analysis Using DuckDB and Metabase for PostgreSQL &amp; CloudWatch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9p7eb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709907986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/gitconnected/turnkey-data-analysis-using-duckdb-and-metabase-for-postgresql-cloudwatch-1255b925b7b4\"&gt;https://medium.com/gitconnected/turnkey-data-analysis-using-duckdb-and-metabase-for-postgresql-cloudwatch-1255b925b7b4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?auto=webp&amp;s=abaa6360bb09d94c3227fe8f18ad59de5848a42b", "width": 686, "height": 686}, "resolutions": [{"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2468b6c6a2be58cf42aa7e20614daa68ea0f79c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8ddf297cb6021e7f0d92ef24cb4ec81cdf6b736", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c1c5e93640b894f5dca0ae91121f73a3cf353a6", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/3OVLPtWNIqfKWMkM0ZpyDCj9I-6nBOJRoaAF4MnNwDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fad19306cfa8405de1555516d3364ec20081a882", "width": 640, "height": 640}], "variants": {}, "id": "VGarDclfmkWIQ7EJ1_v3fMXiX3AflWHP_k0JGGKcL4E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b9p7eb", "is_robot_indexable": true, "report_reasons": null, "author": "redgeoff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9p7eb/turnkey_data_analysis_using_duckdb_and_metabase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9p7eb/turnkey_data_analysis_using_duckdb_and_metabase/", "subreddit_subscribers": 167086, "created_utc": 1709907986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,  \n(Im personally quite new to data engineering)  \none of our customers has hundreds of distributed databases, each with one important table of sensor records. We have an existing data pipeline to process this kind of data (with Dagster), but never ingested data at this scale.  \nNormally we do ingestion with a simple Python script, but at this scale, we would need to build a set of features that any database-to-database ingestion framework would need (keep a list of databases, connect, check table exists, check what's new, fetch it, allow the user to add a new database table to be synced).\n\nI know about Airbyte and Meltano, both are quite powerful and heavy frameworks with many connectors. I was hoping for something simpler. I just need keep the local table in sync with the remote one via SQL at regular intervals (not realtime). Im tempted to build it myself, please stop me.\n\nWould be happy if you would share your ideas.", "author_fullname": "t2_otqll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion tools to keep databases in sync", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9niyy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709905565.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709903318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\n(Im personally quite new to data engineering)&lt;br/&gt;\none of our customers has hundreds of distributed databases, each with one important table of sensor records. We have an existing data pipeline to process this kind of data (with Dagster), but never ingested data at this scale.&lt;br/&gt;\nNormally we do ingestion with a simple Python script, but at this scale, we would need to build a set of features that any database-to-database ingestion framework would need (keep a list of databases, connect, check table exists, check what&amp;#39;s new, fetch it, allow the user to add a new database table to be synced).&lt;/p&gt;\n\n&lt;p&gt;I know about Airbyte and Meltano, both are quite powerful and heavy frameworks with many connectors. I was hoping for something simpler. I just need keep the local table in sync with the remote one via SQL at regular intervals (not realtime). Im tempted to build it myself, please stop me.&lt;/p&gt;\n\n&lt;p&gt;Would be happy if you would share your ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9niyy", "is_robot_indexable": true, "report_reasons": null, "author": "Cominous", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9niyy/data_ingestion_tools_to_keep_databases_in_sync/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9niyy/data_ingestion_tools_to_keep_databases_in_sync/", "subreddit_subscribers": 167086, "created_utc": 1709903318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently we don\u2019t have vaccum of data happening on regular basis and so size of object storage is growing in huge scale. We need actual data\u2019s retention to be 1 year at-least -  in that case do I need to vaccum delta table (object storage bucket) - by setting  vaccum retention period as 1 year? This is so when in-case 1 year old data is needed we can time travel using timestamp.\n\ndeltaTable.vacuum(8766) \n\n# 8766 is 1 year in hours\n\nAs it is delta table though it\u2019s object storage bucket - rather than life cycle policy, vaccum is what should be done? Any thoughts or suggestions? ", "author_fullname": "t2_asn24r2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on Vaccum data to save storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1badj0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709974977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we don\u2019t have vaccum of data happening on regular basis and so size of object storage is growing in huge scale. We need actual data\u2019s retention to be 1 year at-least -  in that case do I need to vaccum delta table (object storage bucket) - by setting  vaccum retention period as 1 year? This is so when in-case 1 year old data is needed we can time travel using timestamp.&lt;/p&gt;\n\n&lt;p&gt;deltaTable.vacuum(8766) &lt;/p&gt;\n\n&lt;h1&gt;8766 is 1 year in hours&lt;/h1&gt;\n\n&lt;p&gt;As it is delta table though it\u2019s object storage bucket - rather than life cycle policy, vaccum is what should be done? Any thoughts or suggestions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1badj0t", "is_robot_indexable": true, "report_reasons": null, "author": "Slow-Woodpecker-3629", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1badj0t/question_on_vaccum_data_to_save_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1badj0t/question_on_vaccum_data_to_save_storage/", "subreddit_subscribers": 167086, "created_utc": 1709974977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking to enroll in the DE bootcamp by WeCloudData- the self-paced one and wanted to know if anyone has gone through their bootcamp. Would like to know your experience- how was there course material, TA support, and career support? I'm currently working as SWE and looking to pivot into a de role. I know there are other ways of developing those skills either by watching YouTube videos or through Udemy but I like to have a structured course.\n\nPlease mention If you tried other boot camps and found it helpful.", "author_fullname": "t2_86hjd67r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinion on Data Engineering Bootcamp- WeCloudData", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ba8g6b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709956689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to enroll in the DE bootcamp by WeCloudData- the self-paced one and wanted to know if anyone has gone through their bootcamp. Would like to know your experience- how was there course material, TA support, and career support? I&amp;#39;m currently working as SWE and looking to pivot into a de role. I know there are other ways of developing those skills either by watching YouTube videos or through Udemy but I like to have a structured course.&lt;/p&gt;\n\n&lt;p&gt;Please mention If you tried other boot camps and found it helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1ba8g6b", "is_robot_indexable": true, "report_reasons": null, "author": "Reporter-Soggy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ba8g6b/opinion_on_data_engineering_bootcamp_weclouddata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ba8g6b/opinion_on_data_engineering_bootcamp_weclouddata/", "subreddit_subscribers": 167086, "created_utc": 1709956689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIs there a way to edit Azure Synapse Analytics notebooks in VsCode? I can clone the repository, but the code is stored as JSON, eg as follows:\n\n\n\n\t{\n\t\t\"cell_type\": \"code\",\n\t\t\"source\": [\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"import http\\r\\n\",\n\t\t\t\"import logging\\r\\n\",\n\t\t\t\"import azure.functions as func\\r\\n\",\n\t\t\t\"import requests\\r\\n\",\n\t\t\t\"from azure.storage.blob import ContainerClient\\r\\n\",\n\t\t\t\"import sys\\r\\n\",\n\t\t\t\"import pandas as pd\\r\\n\",\n\t\t\t\"import json\\r\\n\",\n\t\t\t\"from datetime import datetime\\r\\n\",\n\t\t\t\"from tqdm import tqdm\\r\\n\",\n\t\t\t\"from notebookutils import mssparkutils\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"\\r\\n\",\n\t\t\t\"# Grab API key\\r\\n\",\n\t\t\t\"# (ETC - All code is foramtted like this as a list of strings)\\r\\n\",\n\n\t\t],\n\t\t\"execution_count\": 23\n\t}\n\n\nSo, none of my code formatting / linting tools will work out of the box.\n\nI thought about importing a .py file, but this is difficult as the nodebook code is run on an Apache spark cluster.\n\nAny ideas on this would be appreciated.\n\nPS: Azure Synapse Analytics is a trash product.", "author_fullname": "t2_5i2it6n2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics - Editing notebooks with VSCode?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ba7vl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709954872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Is there a way to edit Azure Synapse Analytics notebooks in VsCode? I can clone the repository, but the code is stored as JSON, eg as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;cell_type&amp;quot;: &amp;quot;code&amp;quot;,\n    &amp;quot;source&amp;quot;: [\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;import http\\r\\n&amp;quot;,\n        &amp;quot;import logging\\r\\n&amp;quot;,\n        &amp;quot;import azure.functions as func\\r\\n&amp;quot;,\n        &amp;quot;import requests\\r\\n&amp;quot;,\n        &amp;quot;from azure.storage.blob import ContainerClient\\r\\n&amp;quot;,\n        &amp;quot;import sys\\r\\n&amp;quot;,\n        &amp;quot;import pandas as pd\\r\\n&amp;quot;,\n        &amp;quot;import json\\r\\n&amp;quot;,\n        &amp;quot;from datetime import datetime\\r\\n&amp;quot;,\n        &amp;quot;from tqdm import tqdm\\r\\n&amp;quot;,\n        &amp;quot;from notebookutils import mssparkutils\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;\\r\\n&amp;quot;,\n        &amp;quot;# Grab API key\\r\\n&amp;quot;,\n        &amp;quot;# (ETC - All code is foramtted like this as a list of strings)\\r\\n&amp;quot;,\n\n    ],\n    &amp;quot;execution_count&amp;quot;: 23\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So, none of my code formatting / linting tools will work out of the box.&lt;/p&gt;\n\n&lt;p&gt;I thought about importing a .py file, but this is difficult as the nodebook code is run on an Apache spark cluster.&lt;/p&gt;\n\n&lt;p&gt;Any ideas on this would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;PS: Azure Synapse Analytics is a trash product.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1ba7vl3", "is_robot_indexable": true, "report_reasons": null, "author": "mr-dre", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ba7vl3/azure_synapse_analytics_editing_notebooks_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ba7vl3/azure_synapse_analytics_editing_notebooks_with/", "subreddit_subscribers": 167086, "created_utc": 1709954872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have a pipeline loading data from SAP CRM via a dataflow with SAP CDC as a source. I was able to parameterise most of the details but I am struggling with selecting only a given list of columns from the source instead of the entire object. \n\nThe setup:\nIn my pipeline, I have a lookup activity that returns the ODP name, context, connection, etc. as well as a list of the columns I need from that particular ODP. Then the dataflow to which I assign these parameters and which sinks the data into .parquet. All ODPs passed to the dataflow (around 500) are fully loaded on each run. \n\nAny insight on this would be greatly appreciated!!", "author_fullname": "t2_i6l8y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Select columns in ADF\u2019s CDC connector", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b9qsqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709911884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have a pipeline loading data from SAP CRM via a dataflow with SAP CDC as a source. I was able to parameterise most of the details but I am struggling with selecting only a given list of columns from the source instead of the entire object. &lt;/p&gt;\n\n&lt;p&gt;The setup:\nIn my pipeline, I have a lookup activity that returns the ODP name, context, connection, etc. as well as a list of the columns I need from that particular ODP. Then the dataflow to which I assign these parameters and which sinks the data into .parquet. All ODPs passed to the dataflow (around 500) are fully loaded on each run. &lt;/p&gt;\n\n&lt;p&gt;Any insight on this would be greatly appreciated!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b9qsqx", "is_robot_indexable": true, "report_reasons": null, "author": "givnv", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b9qsqx/select_columns_in_adfs_cdc_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b9qsqx/select_columns_in_adfs_cdc_connector/", "subreddit_subscribers": 167086, "created_utc": 1709911884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "had a few questions pertaining to what problems have you encountered persistently ?  \n\n\nDo you struggle with ensuring data accuracy across different projects?\n\n&amp;#x200B;\n\nHow often do you encounter issues with outdated or inconsistent data?\n\n&amp;#x200B;\n\nHave data quality problems ever impacted your ability to deliver projects successfully?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4p4v9ov31", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "questions regarding problems faced by data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bafeap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709982657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;had a few questions pertaining to what problems have you encountered persistently ?  &lt;/p&gt;\n\n&lt;p&gt;Do you struggle with ensuring data accuracy across different projects?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How often do you encounter issues with outdated or inconsistent data?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Have data quality problems ever impacted your ability to deliver projects successfully?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bafeap", "is_robot_indexable": true, "report_reasons": null, "author": "Decent_Ice1528", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bafeap/questions_regarding_problems_faced_by_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bafeap/questions_regarding_problems_faced_by_data/", "subreddit_subscribers": 167086, "created_utc": 1709982657.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}