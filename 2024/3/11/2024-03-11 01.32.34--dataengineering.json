{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My background ~11 years of expert level ETL/Datawarehousing exp with Informatica Powercenter, Informatica Cloud, Matillion (1Year)\n\nSome basic snowflake, basic AWS, training level exposure to python (I remember defining functions - could not recollect syntax of objects, methods in objects etc. no realtime project exp with python) I have been trying to learn Pandas and Pyspark over the last few days. ", "author_fullname": "t2_6cuqg1k5d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I learn to call myself a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbe7ab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710087716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My background ~11 years of expert level ETL/Datawarehousing exp with Informatica Powercenter, Informatica Cloud, Matillion (1Year)&lt;/p&gt;\n\n&lt;p&gt;Some basic snowflake, basic AWS, training level exposure to python (I remember defining functions - could not recollect syntax of objects, methods in objects etc. no realtime project exp with python) I have been trying to learn Pandas and Pyspark over the last few days. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bbe7ab", "is_robot_indexable": true, "report_reasons": null, "author": "sneekeeei", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bbe7ab/what_should_i_learn_to_call_myself_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbe7ab/what_should_i_learn_to_call_myself_a_data_engineer/", "subreddit_subscribers": 167438, "created_utc": 1710087716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For exemple, if I want to read and process multiples csv files or writes multiples csv files from coalesce(1) dataframes. Since for the latest, spark will rely on only 1 node ", "author_fullname": "t2_uhw0wx8az", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a good practice to mix spark with python parellel thread?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbd3oi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710084902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For exemple, if I want to read and process multiples csv files or writes multiples csv files from coalesce(1) dataframes. Since for the latest, spark will rely on only 1 node &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bbd3oi", "is_robot_indexable": true, "report_reasons": null, "author": "randomWasabiEnjoyer", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bbd3oi/is_it_a_good_practice_to_mix_spark_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbd3oi/is_it_a_good_practice_to_mix_spark_with_python/", "subreddit_subscribers": 167438, "created_utc": 1710084902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nJust a quick one, I know this isn\u2019t the best way to recruit contributors but I\u2019m really still laying down the mental groundwork.\n\nI want to build a system that helps people manage their own money. Think like Privacy Card, as in you get multiple isolated ways to charge a single bank account. With Privacy, the cards are managed with spend limits or merchant restrictions. It\u2019s a debit account proxy with a debit card interface and customizable access control, basically.\n\nWhat I want to do is build a bucket system. You put money in a bucket and you can charge that bucket until it\u2019s depleted. When the bucket is depleted, charges will fail due to insufficient funds EVEN IF the underlying bank has funds. \n\nEach bucket gets its own virtual card to make digital transactions directly to it.\n\nThen, I want to offer a paid service (because this costs money\u2026) to print an actual card for users and send it to them. They can, via a cellular app, change which bucket the card charges to at any time.\n\nI don\u2019t know about you guys, but I\u2019ve dreamed of software like this for awhile. I want to build it now. Does the idea sound cool enough to anyone that you\u2019d want to help?\n\nSome key features:\n\n* Automated event based and/or scheduled bucket reloading of funds.\n\n* Merchant detection, to auto charge the correct bucket. Put the card in \u201cauto detect\u201d mode via the app, and it\u2019ll reference a set of user defined rules to allocate gas, groceries, etc. to the correct buckets based on merchant name. With a sensical default bucket or something in case the system cannot figure it out.\n\n\nEdit: Thanks for the feedback guys. Least to say, I\u2019m excited now!\n\nI\u2019ve already begun planning. I\u2019ll reach out to everyone who posted here when the time is right.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone interested in helping build an open source banking app?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbfh3f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710107782.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710090934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Just a quick one, I know this isn\u2019t the best way to recruit contributors but I\u2019m really still laying down the mental groundwork.&lt;/p&gt;\n\n&lt;p&gt;I want to build a system that helps people manage their own money. Think like Privacy Card, as in you get multiple isolated ways to charge a single bank account. With Privacy, the cards are managed with spend limits or merchant restrictions. It\u2019s a debit account proxy with a debit card interface and customizable access control, basically.&lt;/p&gt;\n\n&lt;p&gt;What I want to do is build a bucket system. You put money in a bucket and you can charge that bucket until it\u2019s depleted. When the bucket is depleted, charges will fail due to insufficient funds EVEN IF the underlying bank has funds. &lt;/p&gt;\n\n&lt;p&gt;Each bucket gets its own virtual card to make digital transactions directly to it.&lt;/p&gt;\n\n&lt;p&gt;Then, I want to offer a paid service (because this costs money\u2026) to print an actual card for users and send it to them. They can, via a cellular app, change which bucket the card charges to at any time.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t know about you guys, but I\u2019ve dreamed of software like this for awhile. I want to build it now. Does the idea sound cool enough to anyone that you\u2019d want to help?&lt;/p&gt;\n\n&lt;p&gt;Some key features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Automated event based and/or scheduled bucket reloading of funds.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Merchant detection, to auto charge the correct bucket. Put the card in \u201cauto detect\u201d mode via the app, and it\u2019ll reference a set of user defined rules to allocate gas, groceries, etc. to the correct buckets based on merchant name. With a sensical default bucket or something in case the system cannot figure it out.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Edit: Thanks for the feedback guys. Least to say, I\u2019m excited now!&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve already begun planning. I\u2019ll reach out to everyone who posted here when the time is right.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bbfh3f", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bbfh3f/anyone_interested_in_helping_build_an_open_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbfh3f/anyone_interested_in_helping_build_an_open_source/", "subreddit_subscribers": 167438, "created_utc": 1710090934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys\n\nI am joining a new company as a first data guy. This company doesn't have basically anything set up right now. I'll have to set up a data lake and plan all the data architecture, etc. I've been working with data in data analyst/ scientist / BI roles for 3 years, so I know how to code and how to work with data, mostly from the analyst perspective.\n\nDo you know any courses, documentations and resources that I could study in order to learn how to do stuff related to setting basic infrastructure, designing data architecture, and building the basic stuff so we could start a data team?", "author_fullname": "t2_ua22sufp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joining a new company as a first data engineer with a data analyst/scientist background", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbdm13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710086237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys&lt;/p&gt;\n\n&lt;p&gt;I am joining a new company as a first data guy. This company doesn&amp;#39;t have basically anything set up right now. I&amp;#39;ll have to set up a data lake and plan all the data architecture, etc. I&amp;#39;ve been working with data in data analyst/ scientist / BI roles for 3 years, so I know how to code and how to work with data, mostly from the analyst perspective.&lt;/p&gt;\n\n&lt;p&gt;Do you know any courses, documentations and resources that I could study in order to learn how to do stuff related to setting basic infrastructure, designing data architecture, and building the basic stuff so we could start a data team?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bbdm13", "is_robot_indexable": true, "report_reasons": null, "author": "iengmind", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bbdm13/joining_a_new_company_as_a_first_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbdm13/joining_a_new_company_as_a_first_data_engineer/", "subreddit_subscribers": 167438, "created_utc": 1710086237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you orchestrating AI calls in your pipelines. I often come across the case that I have to run classifications on subsets of data, which is then processed further based on these classifications.   \n\n\nThe classification might run on raw documents (e.g. decide whether it is an invoice, an email, or something else) or on text. Mostly the data is either reinserted into the database or another stage of the lakehouse, but sometimes it is also passed to the next processing step.   \n\n\nI would love to know what tools you are using and how you are orchestrating these AI calls. I personally often have to rely on serverless AI deployments, which I autoscale, so I try wherever possible trigger it with a simple Lambda, which calls the different AI endpoints and alters the data and then reinserts it wherever necessary. I would love to know whether you deem that a bad approach.  \n\n\nIf you know or have seen any other interesting libraries or tools, drop them below.  ", "author_fullname": "t2_as93aiie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the tools you use to integrate AI in your dags?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bb3alv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710051060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you orchestrating AI calls in your pipelines. I often come across the case that I have to run classifications on subsets of data, which is then processed further based on these classifications.   &lt;/p&gt;\n\n&lt;p&gt;The classification might run on raw documents (e.g. decide whether it is an invoice, an email, or something else) or on text. Mostly the data is either reinserted into the database or another stage of the lakehouse, but sometimes it is also passed to the next processing step.   &lt;/p&gt;\n\n&lt;p&gt;I would love to know what tools you are using and how you are orchestrating these AI calls. I personally often have to rely on serverless AI deployments, which I autoscale, so I try wherever possible trigger it with a simple Lambda, which calls the different AI endpoints and alters the data and then reinserts it wherever necessary. I would love to know whether you deem that a bad approach.  &lt;/p&gt;\n\n&lt;p&gt;If you know or have seen any other interesting libraries or tools, drop them below.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bb3alv", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedAd895", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bb3alv/what_are_the_tools_you_use_to_integrate_ai_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bb3alv/what_are_the_tools_you_use_to_integrate_ai_in/", "subreddit_subscribers": 167438, "created_utc": 1710051060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! \nGiven any hypothetical data (you may assume we have to scrape for the data ourselves, or have access to an API, or anything - up to you), what would you do with it to ensure it\u2019s a project that teaches you the most crucial data engineering concepts, and one you can display on your portfolio?\n\nPlease help me ideate! \n\nP.S. The cheaper the better!", "author_fullname": "t2_7gvb151", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should an end to end personal project encompass for you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bb3wa2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710053259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! \nGiven any hypothetical data (you may assume we have to scrape for the data ourselves, or have access to an API, or anything - up to you), what would you do with it to ensure it\u2019s a project that teaches you the most crucial data engineering concepts, and one you can display on your portfolio?&lt;/p&gt;\n\n&lt;p&gt;Please help me ideate! &lt;/p&gt;\n\n&lt;p&gt;P.S. The cheaper the better!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bb3wa2", "is_robot_indexable": true, "report_reasons": null, "author": "SueTupp", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bb3wa2/what_should_an_end_to_end_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bb3wa2/what_should_an_end_to_end_personal_project/", "subreddit_subscribers": 167438, "created_utc": 1710053259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roast my data project &amp; video editing skills. I made a python script that acts like a Google Tasks plugin on Obsidian, the note-taking app. Works with the watchdog library to watch for file changes in your Obsidian vault's daily note. Code in comments!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1bazt75", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/jywlhev28fnc1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jywlhev28fnc1/DASH_96.mp4", "dash_url": "https://v.redd.it/jywlhev28fnc1/DASHPlaylist.mpd?a=1712712754%2CODE1NjQxZTAzN2EzNzRhYzU5NzhhNjBmNTY4ZDcyMDc5OTEwMDZjMjkzNTQ0ODQzNGRlNzBhMGFhNmQwYWRhZg%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/jywlhev28fnc1/HLSPlaylist.m3u8?a=1712712754%2CMGQzOTYxMmIwMWMyYzVmYzdhZmNlZWE4MGYzMDA0NmEyNTJhOTczMTY2NDJjZmE0MmVlYWRhNWI2NTBiY2Y2Nw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=e7b1dfb3eaff091841eac51109a8323a51acbce8", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710039316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/jywlhev28fnc1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?format=pjpg&amp;auto=webp&amp;s=e52ff659189eed228a1c74e3bd75e07ef0eb491d", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e810fda3fdfe6d0276a7e26fd40812cce197c5f1", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a43cb69274f0205c680b3eadb43bca5be24e655a", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e2ef78bac704831c6e3fe4b3a45d593a71047219", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=844f3360c52fbfce67cf4b092b4b7177419b0a34", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b47e35f6d386c07cadc4bf61c46813ce0943d5ee", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bd0a9daf229e11028a53cc98a1ce2e436dbe0571", "width": 1080, "height": 607}], "variants": {}, "id": "bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bazt75", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bazt75/roast_my_data_project_video_editing_skills_i_made/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/jywlhev28fnc1", "subreddit_subscribers": 167438, "created_utc": 1710039316.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/jywlhev28fnc1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jywlhev28fnc1/DASH_96.mp4", "dash_url": "https://v.redd.it/jywlhev28fnc1/DASHPlaylist.mpd?a=1712712754%2CODE1NjQxZTAzN2EzNzRhYzU5NzhhNjBmNTY4ZDcyMDc5OTEwMDZjMjkzNTQ0ODQzNGRlNzBhMGFhNmQwYWRhZg%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/jywlhev28fnc1/HLSPlaylist.m3u8?a=1712712754%2CMGQzOTYxMmIwMWMyYzVmYzdhZmNlZWE4MGYzMDA0NmEyNTJhOTczMTY2NDJjZmE0MmVlYWRhNWI2NTBiY2Y2Nw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_655tq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this a good bundle for a Junior Data Engineer? I'm looking for good ways to spend the last $100 CAD of PD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbcrrl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hdgrgoo_qTCqduXQSEhSlssOyR4fuyhGZMHVBcUb_AM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710084049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "humblebundle.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.humblebundle.com/software/packt-data-engineering-software?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_2_layout_type_threes_tile_index_2_c_packtdataengineering_softwarebundle", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/svNcOxrRvOLCOxk8M9_cS9bzd057ToAoluZ3N5XLkBU.jpg?auto=webp&amp;s=9173e01a77d46a0a5e412d942fd0cf6883304071", "width": 1120, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/svNcOxrRvOLCOxk8M9_cS9bzd057ToAoluZ3N5XLkBU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4635fbc4d6f777bc04c6abcc63ce22624200df0e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/svNcOxrRvOLCOxk8M9_cS9bzd057ToAoluZ3N5XLkBU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d28636d4db2991b3336f96b8960dd780c2f9c50", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/svNcOxrRvOLCOxk8M9_cS9bzd057ToAoluZ3N5XLkBU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d0d891d13010fa0edbd3776a5aa76f061849673", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/svNcOxrRvOLCOxk8M9_cS9bzd057ToAoluZ3N5XLkBU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e6ab8ccb587c2bfacda3088e137d55b0d3d80e16", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/svNcOxrRvOLCOxk8M9_cS9bzd057ToAoluZ3N5XLkBU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f364728ac62f521797282459358149e201cadb0", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/svNcOxrRvOLCOxk8M9_cS9bzd057ToAoluZ3N5XLkBU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49aef8848ba79674a328c31970838d5251ccee48", "width": 1080, "height": 607}], "variants": {}, "id": "9FP6H7cMVe4FRgTW1gIOCw1HUZHlflM-rJYSbyt157k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bbcrrl", "is_robot_indexable": true, "report_reasons": null, "author": "Fuhrmaaj", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bbcrrl/is_this_a_good_bundle_for_a_junior_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.humblebundle.com/software/packt-data-engineering-software?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_2_layout_type_threes_tile_index_2_c_packtdataengineering_softwarebundle", "subreddit_subscribers": 167438, "created_utc": 1710084049.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for advice on efficiently storing historical market order book data for analysis purposes. Eventually, I'll have to deal with around 400GB of data (10 pairs, 1 year worth of data, with an entry per second, and each entry being roughly 1 kilobyte). I need to collect this data manually since the API I'm using doesn't provide historical data.\n\nAt the moment, I'm storing the data locally in a Parquet file containing a Pandas DataFrame. However, this file is getting too large to fit into memory. I'd like to be able to scale to the cloud in the future.\n\nI've thought about two approaches:\n\n1.Recording into a traditional database and then converting it into a columnar format.\n2.Recording directly into a columnar format.\n\nI'm unsure about which tools to learn for this task and don't want to reinvent the wheel. \n\nCan anyone suggest tools or approaches that would be suitable for this?", "author_fullname": "t2_175zcl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Help] Market order book storage for further analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbhvxs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710096959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for advice on efficiently storing historical market order book data for analysis purposes. Eventually, I&amp;#39;ll have to deal with around 400GB of data (10 pairs, 1 year worth of data, with an entry per second, and each entry being roughly 1 kilobyte). I need to collect this data manually since the API I&amp;#39;m using doesn&amp;#39;t provide historical data.&lt;/p&gt;\n\n&lt;p&gt;At the moment, I&amp;#39;m storing the data locally in a Parquet file containing a Pandas DataFrame. However, this file is getting too large to fit into memory. I&amp;#39;d like to be able to scale to the cloud in the future.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve thought about two approaches:&lt;/p&gt;\n\n&lt;p&gt;1.Recording into a traditional database and then converting it into a columnar format.\n2.Recording directly into a columnar format.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m unsure about which tools to learn for this task and don&amp;#39;t want to reinvent the wheel. &lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest tools or approaches that would be suitable for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bbhvxs", "is_robot_indexable": true, "report_reasons": null, "author": "f0kes", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bbhvxs/help_market_order_book_storage_for_further/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbhvxs/help_market_order_book_storage_for_further/", "subreddit_subscribers": 167438, "created_utc": 1710096959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You had to convince any business they were underutilizing their data in order to keep your job \n\nHow would you do it?\nWhat data is probably being overlooked?\nWhat\u2019s your solution?\nWhat\u2019s the impact?\n\nGo!\n\nP.S a big tech company asks this questions to staff data engineer ", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Making Businesses Realize Data potential ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbdii9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710085973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You had to convince any business they were underutilizing their data in order to keep your job &lt;/p&gt;\n\n&lt;p&gt;How would you do it?\nWhat data is probably being overlooked?\nWhat\u2019s your solution?\nWhat\u2019s the impact?&lt;/p&gt;\n\n&lt;p&gt;Go!&lt;/p&gt;\n\n&lt;p&gt;P.S a big tech company asks this questions to staff data engineer &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of Data Engineer Academy", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bbdii9", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bbdii9/making_businesses_realize_data_potential/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbdii9/making_businesses_realize_data_potential/", "subreddit_subscribers": 167438, "created_utc": 1710085973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI want to exctract infos from CV such as first name, second name, gender, ... and fill them into an excel table automatically, the scenario is having many CVs inside a folder, and then have an excel table where each row represent a candidate data. the CVs can be either in English or French, pdf or .docx formats\n\nDoes anyone have an idea how to achieve that ? or have previously done that before ? I saw there is an AWS service that can help ? what are your thoughts ?\n\nThanks", "author_fullname": "t2_tz5gveo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extract CV infos automatically with NLP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbiqwk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710099101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I want to exctract infos from CV such as first name, second name, gender, ... and fill them into an excel table automatically, the scenario is having many CVs inside a folder, and then have an excel table where each row represent a candidate data. the CVs can be either in English or French, pdf or .docx formats&lt;/p&gt;\n\n&lt;p&gt;Does anyone have an idea how to achieve that ? or have previously done that before ? I saw there is an AWS service that can help ? what are your thoughts ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bbiqwk", "is_robot_indexable": true, "report_reasons": null, "author": "abdelhakim54", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bbiqwk/extract_cv_infos_automatically_with_nlp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbiqwk/extract_cv_infos_automatically_with_nlp/", "subreddit_subscribers": 167438, "created_utc": 1710099101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nI want to create a parallel copy activity for 5 components to the datalake. Three of them are from legacy database to datalake, two of them from sharepoint to datalake. Out of those three from legacy, two are from same db and schema, one is from different. From sharepoint everything is in the same directory just different folders\n\nSo far we dont use something like metadata pipeline but it will be soon either config from db/databricks or so however for the time being its not there. And I wonder what is the best approach for that?\n\nShould I create one executor pipeline and in the sub pipeline 5 copy activity with directly set db/schema and query for each copy activity and with bearer tokens to get sharepoint access and ingest files from there?  I know most sense makes split by source, atleast by db and sharepoint but then, how it is parallel? I believe what they want is to click debug or trigger will get all 5 copy activities running at the same time, so \"for each\" is not the case? Do I really need this master pipeline?\n\nEDIT:okay I figured out that if I have \"IsSequential\" set to false and I run via Trigger it will be parallel, not sequential, but still, how to create one properly and set the metadata including what query to run\n\nAlso if I were to create a json file with config metadata for each, where should I store it? in the raw directory in datalake, where all the files/tables should go after ingestion?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qpx7gcnj3inc1.png?width=1345&amp;format=png&amp;auto=webp&amp;s=77fa499c16575f1da01c393fc4646c8f9db979a3", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my \"architecture\" ok?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 43, "top_awarded_type": null, "hide_score": false, "media_metadata": {"qpx7gcnj3inc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7918510c51e106c80f2c121cc08f1c6c0a5eccbc"}, {"y": 66, "x": 216, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7327008d21956a77d0ff5a5c6bfe0ba79109205"}, {"y": 99, "x": 320, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=93c27d7223b4385ab4a876e3d747cc50ca746a4c"}, {"y": 198, "x": 640, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84775eaaada206f8ee36594bb8ac27bf77b309fc"}, {"y": 297, "x": 960, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cfde75ab0f72a8e2b3cc273021bef945bc8f575"}, {"y": 334, "x": 1080, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=032ca72cf2fcc67403e78e8ffc96ebbf3f717c26"}], "s": {"y": 417, "x": 1345, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=1345&amp;format=png&amp;auto=webp&amp;s=77fa499c16575f1da01c393fc4646c8f9db979a3"}, "id": "qpx7gcnj3inc1"}}, "name": "t3_1bb9am5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/THUhHFK-q07g153t_SPIWLCXvX2Pkc4BCKTFXvlzaqY.jpg", "edited": 1710074905.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710073955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I want to create a parallel copy activity for 5 components to the datalake. Three of them are from legacy database to datalake, two of them from sharepoint to datalake. Out of those three from legacy, two are from same db and schema, one is from different. From sharepoint everything is in the same directory just different folders&lt;/p&gt;\n\n&lt;p&gt;So far we dont use something like metadata pipeline but it will be soon either config from db/databricks or so however for the time being its not there. And I wonder what is the best approach for that?&lt;/p&gt;\n\n&lt;p&gt;Should I create one executor pipeline and in the sub pipeline 5 copy activity with directly set db/schema and query for each copy activity and with bearer tokens to get sharepoint access and ingest files from there?  I know most sense makes split by source, atleast by db and sharepoint but then, how it is parallel? I believe what they want is to click debug or trigger will get all 5 copy activities running at the same time, so &amp;quot;for each&amp;quot; is not the case? Do I really need this master pipeline?&lt;/p&gt;\n\n&lt;p&gt;EDIT:okay I figured out that if I have &amp;quot;IsSequential&amp;quot; set to false and I run via Trigger it will be parallel, not sequential, but still, how to create one properly and set the metadata including what query to run&lt;/p&gt;\n\n&lt;p&gt;Also if I were to create a json file with config metadata for each, where should I store it? in the raw directory in datalake, where all the files/tables should go after ingestion?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qpx7gcnj3inc1.png?width=1345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77fa499c16575f1da01c393fc4646c8f9db979a3\"&gt;https://preview.redd.it/qpx7gcnj3inc1.png?width=1345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77fa499c16575f1da01c393fc4646c8f9db979a3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bb9am5", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bb9am5/is_my_architecture_ok/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bb9am5/is_my_architecture_ok/", "subreddit_subscribers": 167438, "created_utc": 1710073955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in my first data analytics course, and I have been assigned to present on HADOOP. It's been a challenging task, learning something without actually putting my hands on it. After many many hours of reading and watching videos, I still can't seem to figure out some major pieces: 1) Is there a definitive list of what is considered part of the Hadoop ecosystem? and 2) if not, then what exactly would qualify a piece of software as part of the ecosystem? Just if it can integrate with Hadoop?\n\nI feel like these should be simple questions to answer, but every time I think I find an answer, I find a different list with different software packages.", "author_fullname": "t2_h36q6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hadoop Ecosystem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbhg34", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710095871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in my first data analytics course, and I have been assigned to present on HADOOP. It&amp;#39;s been a challenging task, learning something without actually putting my hands on it. After many many hours of reading and watching videos, I still can&amp;#39;t seem to figure out some major pieces: 1) Is there a definitive list of what is considered part of the Hadoop ecosystem? and 2) if not, then what exactly would qualify a piece of software as part of the ecosystem? Just if it can integrate with Hadoop?&lt;/p&gt;\n\n&lt;p&gt;I feel like these should be simple questions to answer, but every time I think I find an answer, I find a different list with different software packages.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bbhg34", "is_robot_indexable": true, "report_reasons": null, "author": "Duckyes", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bbhg34/hadoop_ecosystem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbhg34/hadoop_ecosystem/", "subreddit_subscribers": 167438, "created_utc": 1710095871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if anyone is in that situation? Or do you have to move to the US? ", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any data engineers in Canada working in the US?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bbcqvo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710083988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if anyone is in that situation? Or do you have to move to the US? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of Data Engineer Academy", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bbcqvo", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bbcqvo/any_data_engineers_in_canada_working_in_the_us/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bbcqvo/any_data_engineers_in_canada_working_in_the_us/", "subreddit_subscribers": 167438, "created_utc": 1710083988.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}