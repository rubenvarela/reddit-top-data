{"kind": "Listing", "data": {"after": "t3_1b6ndyb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was reading some older posts in this sub about working with legacy systems for your first role, and one thread was all in on the idea that your first job is the most important job for setting the stage of your career, and to switch if your work if all Excel files and legacy database management.\n\nI thought, of course every role is important, but are you doomed to fail if your first job isn't exactly what you wanted? Especially in the current market where everyone would take whatever they can get?\n\nIt's a little discouraging for me because I turned down an offer to work with an AWS stack for my first job, to instead work for a huge corporate with amazing benefits but Access/Excel/SQL server stack. At the very least, the company is very well known and respected in my area, and one of my primary responsibilities is using Python and SQL to replace our ancient VBA and Access macros, so I think I'm getting relevant experience for marketable skills. \n\nIs your first job really all that important, especially if you plan on job hopping? Is there hope for data analysts/engineers working with legacy systems to make the jump into a modern cloud stack 1-2 yrs later?", "author_fullname": "t2_tt7gml0lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How important is your first role for the trajectory of your career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6vwta", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709611409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was reading some older posts in this sub about working with legacy systems for your first role, and one thread was all in on the idea that your first job is the most important job for setting the stage of your career, and to switch if your work if all Excel files and legacy database management.&lt;/p&gt;\n\n&lt;p&gt;I thought, of course every role is important, but are you doomed to fail if your first job isn&amp;#39;t exactly what you wanted? Especially in the current market where everyone would take whatever they can get?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a little discouraging for me because I turned down an offer to work with an AWS stack for my first job, to instead work for a huge corporate with amazing benefits but Access/Excel/SQL server stack. At the very least, the company is very well known and respected in my area, and one of my primary responsibilities is using Python and SQL to replace our ancient VBA and Access macros, so I think I&amp;#39;m getting relevant experience for marketable skills. &lt;/p&gt;\n\n&lt;p&gt;Is your first job really all that important, especially if you plan on job hopping? Is there hope for data analysts/engineers working with legacy systems to make the jump into a modern cloud stack 1-2 yrs later?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b6vwta", "is_robot_indexable": true, "report_reasons": null, "author": "date_uh", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6vwta/how_important_is_your_first_role_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6vwta/how_important_is_your_first_role_for_the/", "subreddit_subscribers": 166022, "created_utc": 1709611409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context, I have implemented a large scale project for a hospital and the entire infrastructure was built on Azure. I set up the ELT pipelines using ADF and pyspark(for data manipulation and enrichment) and the company created API endpoints around their data sources. And so I used to extract data from the API and load it into a data lake. I then use this weekly generated data to create a dashboard which then auto refreshes weekly. I've never had to use SQL and even if I did have to use it, OpenAI's GPT-4-Turbo-preview model via the API has been absolutely great. \n\nNot to mention that I do know basics of SQL, doing transformations, Window functions, etc. But since OpenAI was able to write the queries exactly how I needed it, I wanted to know if it is worth investing significant amount of time to master SQL.  Yes, OpenAI may get expensive and I need to kno0w when to step in to get the correct O/P, but whenever I put in the schema, what I want, and an example of input and output, it gets the query right 95% of the time in the first go. So is it worth going into advanced SQL or to learn more about the different technologies involved in DE? Any advice would be great, thank you!", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Important is SQL for Data Engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6iw84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709578443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I have implemented a large scale project for a hospital and the entire infrastructure was built on Azure. I set up the ELT pipelines using ADF and pyspark(for data manipulation and enrichment) and the company created API endpoints around their data sources. And so I used to extract data from the API and load it into a data lake. I then use this weekly generated data to create a dashboard which then auto refreshes weekly. I&amp;#39;ve never had to use SQL and even if I did have to use it, OpenAI&amp;#39;s GPT-4-Turbo-preview model via the API has been absolutely great. &lt;/p&gt;\n\n&lt;p&gt;Not to mention that I do know basics of SQL, doing transformations, Window functions, etc. But since OpenAI was able to write the queries exactly how I needed it, I wanted to know if it is worth investing significant amount of time to master SQL.  Yes, OpenAI may get expensive and I need to kno0w when to step in to get the correct O/P, but whenever I put in the schema, what I want, and an example of input and output, it gets the query right 95% of the time in the first go. So is it worth going into advanced SQL or to learn more about the different technologies involved in DE? Any advice would be great, thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b6iw84", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6iw84/how_important_is_sql_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6iw84/how_important_is_sql_for_data_engineers/", "subreddit_subscribers": 166022, "created_utc": 1709578443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a data scientist for years and hate the analytical part of creating reports/dashboards/KPIs/analysis giving analytical insights and presentations to end users (mostly sales people) that will never use it, give me urgent adhoc requests that will again never use, since they dont know what they want. \n\nI love the technical backend part of creating the dashboards using Power BI, using Python to automate reports, create models, forecast, graphs, use SQL to acquire data etc. \n\nWhat jobs can I look for that are less about having to analyze/provide insights to end users and more on the backend, like creating all the technical processes/automation/etc for maybe other analysts?  dont want to be a software developer. Could it be data engineering? ", "author_fullname": "t2_j0ebc61y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Jobs that are less analytical more technical ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b72khy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709655101.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709636380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a data scientist for years and hate the analytical part of creating reports/dashboards/KPIs/analysis giving analytical insights and presentations to end users (mostly sales people) that will never use it, give me urgent adhoc requests that will again never use, since they dont know what they want. &lt;/p&gt;\n\n&lt;p&gt;I love the technical backend part of creating the dashboards using Power BI, using Python to automate reports, create models, forecast, graphs, use SQL to acquire data etc. &lt;/p&gt;\n\n&lt;p&gt;What jobs can I look for that are less about having to analyze/provide insights to end users and more on the backend, like creating all the technical processes/automation/etc for maybe other analysts?  dont want to be a software developer. Could it be data engineering? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b72khy", "is_robot_indexable": true, "report_reasons": null, "author": "chicric", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b72khy/data_jobs_that_are_less_analytical_more_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b72khy/data_jobs_that_are_less_analytical_more_technical/", "subreddit_subscribers": 166022, "created_utc": 1709636380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is common but wanted to hear any success stories from anyone who applied to role that were under qualified for but got the job and killed it.\n\nBeen looking at [Sr.Data](https://Sr.Data) Eng roles and I just get so intimated with the list of technologies they expect you to manage. All I really do is write up stored procs in BQ to create a dimensional model then throw it in a DAG for orchestration.", "author_fullname": "t2_5ukitegd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Intimidated and discouraged by Job Description", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6ydx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709619391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is common but wanted to hear any success stories from anyone who applied to role that were under qualified for but got the job and killed it.&lt;/p&gt;\n\n&lt;p&gt;Been looking at &lt;a href=\"https://Sr.Data\"&gt;Sr.Data&lt;/a&gt; Eng roles and I just get so intimated with the list of technologies they expect you to manage. All I really do is write up stored procs in BQ to create a dimensional model then throw it in a DAG for orchestration.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b6ydx2", "is_robot_indexable": true, "report_reasons": null, "author": "burningburnerbern", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6ydx2/intimidated_and_discouraged_by_job_description/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6ydx2/intimidated_and_discouraged_by_job_description/", "subreddit_subscribers": 166022, "created_utc": 1709619391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'd like to run some basic checks on our data in a fairly generic way. Basically, passing a table (or view) and a list of fields for dims and list of fields for measures\n\nAnd have it slice by each dimension for each measure to see if there is anything that is greater than X number of standard deviations compared to same day last week or compared to rolling week average, for example.\n\nWe randomly encounter data quality issues for metrics within a certain dimension which isn't caught by more high level checks.\n\nExample, for our Transactions fact table, we stopped received transactions for a *specific* product type. At the high level, it was almost impossible to see any deviation when charted. but when split by different dimensions, the anomaly immediately pops up.\n\nexample trendline for transaction count for Product XYZ\n\n3/1  : 200\n\n3/2 : 250\n\n3/3: 190\n\n3/4: 0   -- or tiny number like 7\n\nWe're on SQL Server/AWS Redshift. I might do something in dynamic sql to handle this but is there a \\*free\\* solution out there? Don't actually need anything really sophisticated, we're a small company.\n\nWe're not on dbt, but can dbt generic tests help with this?\n\nThere's a few python related solutions that I've looked at, as well but would prefer to do the analysis on the DB engine for larger datasets.\n\n* [https://github.com/ydataai/ydata-profiling](https://github.com/ydataai/ydata-profiling)", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick and easy anomaly detection in SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6rkhb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709646680.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709599448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to run some basic checks on our data in a fairly generic way. Basically, passing a table (or view) and a list of fields for dims and list of fields for measures&lt;/p&gt;\n\n&lt;p&gt;And have it slice by each dimension for each measure to see if there is anything that is greater than X number of standard deviations compared to same day last week or compared to rolling week average, for example.&lt;/p&gt;\n\n&lt;p&gt;We randomly encounter data quality issues for metrics within a certain dimension which isn&amp;#39;t caught by more high level checks.&lt;/p&gt;\n\n&lt;p&gt;Example, for our Transactions fact table, we stopped received transactions for a &lt;em&gt;specific&lt;/em&gt; product type. At the high level, it was almost impossible to see any deviation when charted. but when split by different dimensions, the anomaly immediately pops up.&lt;/p&gt;\n\n&lt;p&gt;example trendline for transaction count for Product XYZ&lt;/p&gt;\n\n&lt;p&gt;3/1  : 200&lt;/p&gt;\n\n&lt;p&gt;3/2 : 250&lt;/p&gt;\n\n&lt;p&gt;3/3: 190&lt;/p&gt;\n\n&lt;p&gt;3/4: 0   -- or tiny number like 7&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re on SQL Server/AWS Redshift. I might do something in dynamic sql to handle this but is there a *free* solution out there? Don&amp;#39;t actually need anything really sophisticated, we&amp;#39;re a small company.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re not on dbt, but can dbt generic tests help with this?&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a few python related solutions that I&amp;#39;ve looked at, as well but would prefer to do the analysis on the DB engine for larger datasets.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ydataai/ydata-profiling\"&gt;https://github.com/ydataai/ydata-profiling&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Tm6jpBA63jAb04C6xa_3wDaO-JmQI753gcAlT9J3wro.jpg?auto=webp&amp;s=d9b5b8ff273ef55984d2b84516414d5c742b7d54", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Tm6jpBA63jAb04C6xa_3wDaO-JmQI753gcAlT9J3wro.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ddc17a5b476dc4bc9bc7850d45241b871daafed4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Tm6jpBA63jAb04C6xa_3wDaO-JmQI753gcAlT9J3wro.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ab3f9f5002a95c0088b661d27161fad11eb6b24", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Tm6jpBA63jAb04C6xa_3wDaO-JmQI753gcAlT9J3wro.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a13390688cd5bfa2f8fdf3a5d5b79712c981468", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Tm6jpBA63jAb04C6xa_3wDaO-JmQI753gcAlT9J3wro.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9a9b4164cc9a9f7e673f4a12dee7455c6ea6581", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Tm6jpBA63jAb04C6xa_3wDaO-JmQI753gcAlT9J3wro.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=008a7d36e1ad1d7b587e8485d7a644e1915132ab", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Tm6jpBA63jAb04C6xa_3wDaO-JmQI753gcAlT9J3wro.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6868bfb85e1dfc49222eeb75205ccc128034336", "width": 1080, "height": 540}], "variants": {}, "id": "twBVkMc_k1BYnGPSPsroeGloYvtLSGEcNAiOHgXdwEQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b6rkhb", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6rkhb/quick_and_easy_anomaly_detection_in_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6rkhb/quick_and_easy_anomaly_detection_in_sql/", "subreddit_subscribers": 166022, "created_utc": 1709599448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, we've just created a new medium publication on content for running &amp; scaling data consultancies. The link is this one: [Data Consulting Club](https://medium.com/data-consulting-club).\n\n&amp;#x200B;\n\nThe first article is on selling data services to non-data-savvy businesses; I hope you enjoy it.\n\n&amp;#x200B;\n\nI know it sounds like an arcane topic, but really I looked almost everywhere, there is no single source of content out there that helps with scaling data consultancies, and yet there are thousands of them just inside the US. \n\n*Enjoy, feel free to comment, share ideas, questions for content!*\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Consulting Club \u2013 Medium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b74adw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709642546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, we&amp;#39;ve just created a new medium publication on content for running &amp;amp; scaling data consultancies. The link is this one: &lt;a href=\"https://medium.com/data-consulting-club\"&gt;Data Consulting Club&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The first article is on selling data services to non-data-savvy businesses; I hope you enjoy it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I know it sounds like an arcane topic, but really I looked almost everywhere, there is no single source of content out there that helps with scaling data consultancies, and yet there are thousands of them just inside the US. &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Enjoy, feel free to comment, share ideas, questions for content!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/u_2TYordzOTKZdqTNXcKsx2crSd6T_BxxdOCasWoHd0.jpg?auto=webp&amp;s=6f54c774481eaa68275e471f7cd7cf01b6a5f0a5", "width": 800, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/u_2TYordzOTKZdqTNXcKsx2crSd6T_BxxdOCasWoHd0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=08af7a2adaf1e7ac452ee0dd3019130887110d5b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/u_2TYordzOTKZdqTNXcKsx2crSd6T_BxxdOCasWoHd0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4ec2baf40da327ca96fcfb71fb24497b22caed1", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/u_2TYordzOTKZdqTNXcKsx2crSd6T_BxxdOCasWoHd0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4028c546e01c5eab66bd1c793e79906477faf957", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/u_2TYordzOTKZdqTNXcKsx2crSd6T_BxxdOCasWoHd0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=210d02e6c5137b4b4f19669ef5355e9dab9f95f0", "width": 640, "height": 640}], "variants": {}, "id": "L6jeuPDAfgBs4qNFtmcSHiCFWUPIzQfJRaVYELKf47c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b74adw", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b74adw/data_consulting_club_medium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b74adw/data_consulting_club_medium/", "subreddit_subscribers": 166022, "created_utc": 1709642546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\nI am a recent graduate and I got a job offer for a data engineer position where I would have to work on a project with palantir. At the moment I am not familiar with this framework at all (the company would enroll me into a training program) so I wonder how useful is this going to be for my long term career of becoming a data scientist. Could someone shed some light on how niche exactly is the Palantir system and is it worth it to except this job offer if I don't yet know if I want to be working with it in the future. ", "author_fullname": "t2_917g34y9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is palantir framework experience transferable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b71k0x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709632301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a recent graduate and I got a job offer for a data engineer position where I would have to work on a project with palantir. At the moment I am not familiar with this framework at all (the company would enroll me into a training program) so I wonder how useful is this going to be for my long term career of becoming a data scientist. Could someone shed some light on how niche exactly is the Palantir system and is it worth it to except this job offer if I don&amp;#39;t yet know if I want to be working with it in the future. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b71k0x", "is_robot_indexable": true, "report_reasons": null, "author": "Sand-Frosty", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b71k0x/is_palantir_framework_experience_transferable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b71k0x/is_palantir_framework_experience_transferable/", "subreddit_subscribers": 166022, "created_utc": 1709632301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nTrying to give guidance to a company in mid market 600 FTE. They have heard about Mage AI and are interested in it. Any experiences with it? Does it solve certain problems that Airflow/Prefect/Astronomer doesn\u2019t? ", "author_fullname": "t2_putygiix7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mage AI experiences?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6k06l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709581113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Trying to give guidance to a company in mid market 600 FTE. They have heard about Mage AI and are interested in it. Any experiences with it? Does it solve certain problems that Airflow/Prefect/Astronomer doesn\u2019t? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b6k06l", "is_robot_indexable": true, "report_reasons": null, "author": "ElephantParty6489", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6k06l/mage_ai_experiences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6k06l/mage_ai_experiences/", "subreddit_subscribers": 166022, "created_utc": 1709581113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to build a DB that will house years worth of 15 or 30 minute data for a couple hundred facilities. So about \\~15M rows/year with about a dozen columns and some dimension tables. End goal is using the data for visualization and reporting\n\nRather than just ask what tool I should use for this specific project I wanted to know if there are general rules for what tools to use based on expected table size. I feel like for smaller data like my setup something like SQL Server Express would be fine but I figured I'd ask. I also want to use tools that will help me self market for my next role.", "author_fullname": "t2_1ad62ux7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a guide for how to choose tools based on DB size?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6rxl1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709600410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to build a DB that will house years worth of 15 or 30 minute data for a couple hundred facilities. So about ~15M rows/year with about a dozen columns and some dimension tables. End goal is using the data for visualization and reporting&lt;/p&gt;\n\n&lt;p&gt;Rather than just ask what tool I should use for this specific project I wanted to know if there are general rules for what tools to use based on expected table size. I feel like for smaller data like my setup something like SQL Server Express would be fine but I figured I&amp;#39;d ask. I also want to use tools that will help me self market for my next role.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b6rxl1", "is_robot_indexable": true, "report_reasons": null, "author": "VegaGT-VZ", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6rxl1/is_there_a_guide_for_how_to_choose_tools_based_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6rxl1/is_there_a_guide_for_how_to_choose_tools_based_on/", "subreddit_subscribers": 166022, "created_utc": 1709600410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an upcoming job meeting (we can't say inter-view in post body?) where data governance and observability were specifically called out as topics for discussion. \n\nI haven't gotten much exposure to the concepts in my career so far. I was hoping this sub could point me toward articles, videos, courses, etc. that you've found helpful on the topics. Thanks!", "author_fullname": "t2_r6skptvze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Primer materials for Data Governance and Observability?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b79vy6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709657083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an upcoming job meeting (we can&amp;#39;t say inter-view in post body?) where data governance and observability were specifically called out as topics for discussion. &lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t gotten much exposure to the concepts in my career so far. I was hoping this sub could point me toward articles, videos, courses, etc. that you&amp;#39;ve found helpful on the topics. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b79vy6", "is_robot_indexable": true, "report_reasons": null, "author": "UnaccompaniedNeffew", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b79vy6/primer_materials_for_data_governance_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b79vy6/primer_materials_for_data_governance_and/", "subreddit_subscribers": 166022, "created_utc": 1709657083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m from the Mechatronics and Indistrial Technology (Supply Chain and Data and Analytics) education background with a couple years of Data Engineering Experience. I\u2019m currently applying for new opportunities and getting rejected, i believe because i\u2019m not from the standard CSE/Statistics/Mathematics background.\n\nI\u2019m looking to take the CS50 course from Harvard, will that help me justify my education for employers to consider me?.\n\nOr are there any other suggestions?", "author_fullname": "t2_cdfmceonl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Education Justification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b78fwh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709653746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m from the Mechatronics and Indistrial Technology (Supply Chain and Data and Analytics) education background with a couple years of Data Engineering Experience. I\u2019m currently applying for new opportunities and getting rejected, i believe because i\u2019m not from the standard CSE/Statistics/Mathematics background.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking to take the CS50 course from Harvard, will that help me justify my education for employers to consider me?.&lt;/p&gt;\n\n&lt;p&gt;Or are there any other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b78fwh", "is_robot_indexable": true, "report_reasons": null, "author": "GoldLatter1084", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b78fwh/data_engineering_education_justification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b78fwh/data_engineering_education_justification/", "subreddit_subscribers": 166022, "created_utc": 1709653746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys\nI'm just learning data engineering by joining DE Zoomcamp. I want to create a end-to-end project from processing files until creating a dashboard. I will be using mage.ai as workflow orchestrator and DBT as data modeling tool.\n\nHere is my dataset from kaggle. https://www.kaggle.com/datasets/antonukolga/cyclistic-bike-share-data-12-months\nIn my mind, I need to download all files then upload to google cloud storage after that to bigQuery. Then trigger DBT to build it.\n\nI'm confused what should I do in the process of extracting and loading? Just as simple as download and upload?", "author_fullname": "t2_2lvigk1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask the advice for (EL) process in ELT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b719yv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709631104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys\nI&amp;#39;m just learning data engineering by joining DE Zoomcamp. I want to create a end-to-end project from processing files until creating a dashboard. I will be using mage.ai as workflow orchestrator and DBT as data modeling tool.&lt;/p&gt;\n\n&lt;p&gt;Here is my dataset from kaggle. &lt;a href=\"https://www.kaggle.com/datasets/antonukolga/cyclistic-bike-share-data-12-months\"&gt;https://www.kaggle.com/datasets/antonukolga/cyclistic-bike-share-data-12-months&lt;/a&gt;\nIn my mind, I need to download all files then upload to google cloud storage after that to bigQuery. Then trigger DBT to build it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m confused what should I do in the process of extracting and loading? Just as simple as download and upload?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/56l_IxqVtAo5Te9aVPGb0XosO6Wd979gKxtBeBi3rrU.jpg?auto=webp&amp;s=86ac445a852630efa82b17be2bd74300d44d1364", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/56l_IxqVtAo5Te9aVPGb0XosO6Wd979gKxtBeBi3rrU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bee3324f41d097e9bb2806164a72c2ba7315d05f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/56l_IxqVtAo5Te9aVPGb0XosO6Wd979gKxtBeBi3rrU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd557d65200c3e91ddffa2786fbcf771e07a79b7", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/56l_IxqVtAo5Te9aVPGb0XosO6Wd979gKxtBeBi3rrU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e25be11f3d7768597c95744a9586356b852d689", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/56l_IxqVtAo5Te9aVPGb0XosO6Wd979gKxtBeBi3rrU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6314e93cdfdf1ee9bfa2a34dee89ab7b1e52ddf1", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/56l_IxqVtAo5Te9aVPGb0XosO6Wd979gKxtBeBi3rrU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=06a0f4527cc69125f698a9b6d77e377c662d639c", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/56l_IxqVtAo5Te9aVPGb0XosO6Wd979gKxtBeBi3rrU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d1e1767c195f3d756313b37a9a8c5ea39ea28a1c", "width": 1080, "height": 1080}], "variants": {}, "id": "-ruYmDMVtAVEZ2yLCTdRTVKICXf94FCru05mqoUQoPI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b719yv", "is_robot_indexable": true, "report_reasons": null, "author": "muh_ilhamfajar", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b719yv/ask_the_advice_for_el_process_in_elt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b719yv/ask_the_advice_for_el_process_in_elt/", "subreddit_subscribers": 166022, "created_utc": 1709631104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have configured a local environment that consists of the following containers:\n\n* Trino\n* Hive\n* Minio\n\nI have successfully set up these three containers to create and query external tables using Trino, which are stored in the Hive Metastore and on Minio as Parquet.   This all works great for me.\n\nHowever, when I try to drop the table in Trino that I created, I get this error:\n\n&gt;Access Denied: Cannot drop table schema.tablename io.trino.spi.security.AccessDeniedException: Access Denied: Cannot drop table schema.tablename\n\nThis doesn't make any sense because there is no user setup in Hive Metastore, nor security setup.  If I connect to the Hive metastore using Beeline, I can drop the table with no problem.\n\nDo you have any clue why Trino is unable to drop the table?", "author_fullname": "t2_l04by", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cannot drop table in Trino using Hive connector for no good reason", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6woqe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709613842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have configured a local environment that consists of the following containers:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Trino&lt;/li&gt;\n&lt;li&gt;Hive&lt;/li&gt;\n&lt;li&gt;Minio&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have successfully set up these three containers to create and query external tables using Trino, which are stored in the Hive Metastore and on Minio as Parquet.   This all works great for me.&lt;/p&gt;\n\n&lt;p&gt;However, when I try to drop the table in Trino that I created, I get this error:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Access Denied: Cannot drop table schema.tablename io.trino.spi.security.AccessDeniedException: Access Denied: Cannot drop table schema.tablename&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This doesn&amp;#39;t make any sense because there is no user setup in Hive Metastore, nor security setup.  If I connect to the Hive metastore using Beeline, I can drop the table with no problem.&lt;/p&gt;\n\n&lt;p&gt;Do you have any clue why Trino is unable to drop the table?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b6woqe", "is_robot_indexable": true, "report_reasons": null, "author": "kentmaxwell", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6woqe/cannot_drop_table_in_trino_using_hive_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6woqe/cannot_drop_table_in_trino_using_hive_connector/", "subreddit_subscribers": 166022, "created_utc": 1709613842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\n&amp;#x200B;\n\nI'm looking for some advice and I'm hoping this is the right spot. \n\n&amp;#x200B;\n\nI've recently started a new role as a Machine Learning Engineer for a R&amp;D team focusing on computer vision applications. It has become clear that the biggest weakness in our operation stems from our approach to data. We have \\~terabytes of proprietary image and video data in various formats and no system to efficiently process and label it.  I have been assigned the task of developing a data pipeline to curate custom datasets for our CV models but I don't have any real expertise in data engineering other than some experience with SQL/NoSQL databases. \n\n&amp;#x200B;\n\nI'm aware that a team of dedicated Data Engineers is really what's required for such a task but unfortunately I am the only one available to work on this problem. With that in mind, I'd very much appreciate any guidance on the following: \n\n1. What resources are available for learning Data Engineering for CV? I have spent a lot of time researching this area now but most resources that I've come across really only focus on structured tabular data. I haven't come across any resource that discusses best practices when dealing with video or image data. \n2. How do I decide on what tools to use? I am overwhelmed by the choice of tools relating to the 'modern data stack'. I have come across Activeloop's DeepLake which seems promising but then Delta Lake seems to be more popular. Do I even need a Lakehouse architecture? \n3. I have set up CVAT for image annotation with semi-automatic labelling to improve efficiency. Are there better tools out there for labelling? I have seen LabelBox mentioned as a viable alternative. \n\n&amp;#x200B;\n\nOf course I have many other questions but I'll keep this short as the main thing I'm looking for is advice on how to improve and learn. Any information would be much appreciated! ", "author_fullname": "t2_a1k94kfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering for Computer Vision - Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6kgk6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709582238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for some advice and I&amp;#39;m hoping this is the right spot. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently started a new role as a Machine Learning Engineer for a R&amp;amp;D team focusing on computer vision applications. It has become clear that the biggest weakness in our operation stems from our approach to data. We have ~terabytes of proprietary image and video data in various formats and no system to efficiently process and label it.  I have been assigned the task of developing a data pipeline to curate custom datasets for our CV models but I don&amp;#39;t have any real expertise in data engineering other than some experience with SQL/NoSQL databases. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that a team of dedicated Data Engineers is really what&amp;#39;s required for such a task but unfortunately I am the only one available to work on this problem. With that in mind, I&amp;#39;d very much appreciate any guidance on the following: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What resources are available for learning Data Engineering for CV? I have spent a lot of time researching this area now but most resources that I&amp;#39;ve come across really only focus on structured tabular data. I haven&amp;#39;t come across any resource that discusses best practices when dealing with video or image data. &lt;/li&gt;\n&lt;li&gt;How do I decide on what tools to use? I am overwhelmed by the choice of tools relating to the &amp;#39;modern data stack&amp;#39;. I have come across Activeloop&amp;#39;s DeepLake which seems promising but then Delta Lake seems to be more popular. Do I even need a Lakehouse architecture? &lt;/li&gt;\n&lt;li&gt;I have set up CVAT for image annotation with semi-automatic labelling to improve efficiency. Are there better tools out there for labelling? I have seen LabelBox mentioned as a viable alternative. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Of course I have many other questions but I&amp;#39;ll keep this short as the main thing I&amp;#39;m looking for is advice on how to improve and learn. Any information would be much appreciated! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b6kgk6", "is_robot_indexable": true, "report_reasons": null, "author": "distracted-ferret", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6kgk6/data_engineering_for_computer_vision_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6kgk6/data_engineering_for_computer_vision_advice/", "subreddit_subscribers": 166022, "created_utc": 1709582238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to prep for some upcoming interviews.\n\nAll I know is confluence, Jira, Jenkins, airflow and data bricks are what are being used.\n\nAny advice is appreciated. ", "author_fullname": "t2_p7krf9ck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to prep for upcoming connect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6jflr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709579735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to prep for some upcoming interviews.&lt;/p&gt;\n\n&lt;p&gt;All I know is confluence, Jira, Jenkins, airflow and data bricks are what are being used.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b6jflr", "is_robot_indexable": true, "report_reasons": null, "author": "Poyal_Rines", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6jflr/trying_to_prep_for_upcoming_connect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6jflr/trying_to_prep_for_upcoming_connect/", "subreddit_subscribers": 166022, "created_utc": 1709579735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all!\n\nWe made a tool for BigQuery users. You can generate a view from a JSON object if you are storing your data as us in a JSON field, it's very useful!\n\nAny feedback is welcome! :)\n\nThe tool is here: [https://vg.persio.io](https://vg.persio.io)", "author_fullname": "t2_1mwhn72z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free RAW Json to View tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6i9st", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709580170.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709576950.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;We made a tool for BigQuery users. You can generate a view from a JSON object if you are storing your data as us in a JSON field, it&amp;#39;s very useful!&lt;/p&gt;\n\n&lt;p&gt;Any feedback is welcome! :)&lt;/p&gt;\n\n&lt;p&gt;The tool is here: &lt;a href=\"https://vg.persio.io\"&gt;https://vg.persio.io&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5ZO939ydYbQ0_CsYPeCqzD8n-BW7wuzz38r2Ic7FLH8.jpg?auto=webp&amp;s=5e69c52008196472a67548003a28f8f20d609d3b", "width": 878, "height": 460}, "resolutions": [{"url": "https://external-preview.redd.it/5ZO939ydYbQ0_CsYPeCqzD8n-BW7wuzz38r2Ic7FLH8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f75eb1a72319934c5328dd4c88160047a46ebfc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/5ZO939ydYbQ0_CsYPeCqzD8n-BW7wuzz38r2Ic7FLH8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=61e2acfe92daa0f3277fe698099d56f6738b5c23", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/5ZO939ydYbQ0_CsYPeCqzD8n-BW7wuzz38r2Ic7FLH8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47742f25f6f66e00357e66fb8567f3b30e58c0fa", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/5ZO939ydYbQ0_CsYPeCqzD8n-BW7wuzz38r2Ic7FLH8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19fa8ef1b854a514192bdd51e9b6981d77bc8bc6", "width": 640, "height": 335}], "variants": {}, "id": "5z7wVcO2sxef0OZxRjwC5Wp8rwhxWzWvcKNb8uUTPeI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b6i9st", "is_robot_indexable": true, "report_reasons": null, "author": "pigri", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6i9st/free_raw_json_to_view_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6i9st/free_raw_json_to_view_tool/", "subreddit_subscribers": 166022, "created_utc": 1709576950.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings,\n\nI am seeking assistance to comprehend how TDD (Test Driven Development), a concept from software engineering, is adapted for data engineering. As a data scientist, I want to stay informed about trends in the data domain, though my expertise is not primarily in data engineering.\n\nI grasp the core principles of TDD in software development, but its application in data engineering eludes me, despite reading numerous articles and examining some training examples that appeared impractical to me.\n\nMy experience mainly involves external data acquisition, especially in the finance sector. In data engineering, if one constructs a pipeline to ingest data from an external source for ML models, setting up this pipeline constitutes the crux of the work, besides establishing the necessary infrastructure, correct? With TDD, the focus would be on the methodology for coding transformations along the pipeline. However, how can one formulate sample, edge, and corner cases for transformation unit tests without first experimenting with data from the source? Incoming datasets and sources seem far more stochastic, dynamic, and complex than what a typical software module faces, making it challenging to establish test cases and expected results without preliminary data experimentation. And it seems to me, one needs to experiment with the transformation steps to reach to meaningful test cases. But then the whole idea behind TDD is lost if you have the code before the test case, is this not true?\n\nFor example, I have seen some training example with Python-based pytests for simple Spark row-wise transformations on streaming CSV data, like breaking down a date column into its components. This are just atomic transformations, based on built in functions which should be rather tested by the developer building that specific module (e.g. pyspark), am I not correct?\n\nIn my experience, data pipeline failures often result from changes in incoming raw data rather than code errors in basic transformations. These anomalies in data, such as definition changes, are hard to anticipate with testable code. Wouldn't it be more practical to focus on detecting quality issues at certain points and testing the data itself?\n\nFurthermore, most problems I've encountered were not something to capture by row-level windows. Many transformations involve complex aggregations and windows, where checking longitudinal distributions and detecting drifting is critical. A basic example is a time series panel, which should contain all business days within a range, and you need to make sure if there are no gaps in the data. However, preparing for these issues seems more aligned with data contracts and data testing (like dbt data tests), rather than TDD. While I recognize dbt's new unit testing capabilities and understand their [example](https://docs.getdbt.com/docs/build/unit-tests) of unit testing an embedded complex function in a SQL query, such scenarios seem unrealistic in my previous work environment. Failures typically rather stemmed from incoming data changes and definition shifts, making tests around these aspects more critical.\n\nI would appreciate any insights on applying TDD to data engineering work, especially in scenarios involving the ingestion of dynamically changing, low-quality external data from various vendors.", "author_fullname": "t2_ldzpx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me understand Test Driven Development within Data Engineering work.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b79p9w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709656649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings,&lt;/p&gt;\n\n&lt;p&gt;I am seeking assistance to comprehend how TDD (Test Driven Development), a concept from software engineering, is adapted for data engineering. As a data scientist, I want to stay informed about trends in the data domain, though my expertise is not primarily in data engineering.&lt;/p&gt;\n\n&lt;p&gt;I grasp the core principles of TDD in software development, but its application in data engineering eludes me, despite reading numerous articles and examining some training examples that appeared impractical to me.&lt;/p&gt;\n\n&lt;p&gt;My experience mainly involves external data acquisition, especially in the finance sector. In data engineering, if one constructs a pipeline to ingest data from an external source for ML models, setting up this pipeline constitutes the crux of the work, besides establishing the necessary infrastructure, correct? With TDD, the focus would be on the methodology for coding transformations along the pipeline. However, how can one formulate sample, edge, and corner cases for transformation unit tests without first experimenting with data from the source? Incoming datasets and sources seem far more stochastic, dynamic, and complex than what a typical software module faces, making it challenging to establish test cases and expected results without preliminary data experimentation. And it seems to me, one needs to experiment with the transformation steps to reach to meaningful test cases. But then the whole idea behind TDD is lost if you have the code before the test case, is this not true?&lt;/p&gt;\n\n&lt;p&gt;For example, I have seen some training example with Python-based pytests for simple Spark row-wise transformations on streaming CSV data, like breaking down a date column into its components. This are just atomic transformations, based on built in functions which should be rather tested by the developer building that specific module (e.g. pyspark), am I not correct?&lt;/p&gt;\n\n&lt;p&gt;In my experience, data pipeline failures often result from changes in incoming raw data rather than code errors in basic transformations. These anomalies in data, such as definition changes, are hard to anticipate with testable code. Wouldn&amp;#39;t it be more practical to focus on detecting quality issues at certain points and testing the data itself?&lt;/p&gt;\n\n&lt;p&gt;Furthermore, most problems I&amp;#39;ve encountered were not something to capture by row-level windows. Many transformations involve complex aggregations and windows, where checking longitudinal distributions and detecting drifting is critical. A basic example is a time series panel, which should contain all business days within a range, and you need to make sure if there are no gaps in the data. However, preparing for these issues seems more aligned with data contracts and data testing (like dbt data tests), rather than TDD. While I recognize dbt&amp;#39;s new unit testing capabilities and understand their &lt;a href=\"https://docs.getdbt.com/docs/build/unit-tests\"&gt;example&lt;/a&gt; of unit testing an embedded complex function in a SQL query, such scenarios seem unrealistic in my previous work environment. Failures typically rather stemmed from incoming data changes and definition shifts, making tests around these aspects more critical.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate any insights on applying TDD to data engineering work, especially in scenarios involving the ingestion of dynamically changing, low-quality external data from various vendors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?auto=webp&amp;s=2a89f01968bbb7160773570a5739ba364e017ebf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e39c972215449e24ba187a3b3e6d0289aad02d1b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e48b5b0440098be5b7b54dcdd6d78e80f77e948", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c583ec988ffb5d6d8292b88b38a2a7ac9fc2b799", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a97be3626c69aab79c2204db47f040a6a8bb9820", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ba90b674ccf1906f5a13abd09b27db16d203bd0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197f95d4689989cecbdb537c3aa18035536b0c50", "width": 1080, "height": 567}], "variants": {}, "id": "s9XQCWayWEjVSYNiK4ez8RIl3EBcstjT4Cv_3rHuPvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b79p9w", "is_robot_indexable": true, "report_reasons": null, "author": "petkow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b79p9w/help_me_understand_test_driven_development_within/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b79p9w/help_me_understand_test_driven_development_within/", "subreddit_subscribers": 166022, "created_utc": 1709656649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm interested in setting Primary Key and Foreign Key constraints in Redshift as I've read it can help the Query Optimizer create more efficient queries. \n\nI've noticed that when setting these constraints in the table creation they show up in the INFORMATION\\_SCHEMA.TABLE\\_CONSTRAINTS table, but when setting them with an ALTER command instead, they do not show up there. However, they both show up in PG\\_CONSTRAINT table. \n\nMy intention is to eventually set these in DBT, which also only shows up in the PG\\_CONSTRAINT table.\n\nBeing somewhat new to optimizing queries on Redshift, I'm curious if anyone knows:  \n1. Does this difference matter to the query optimizer (ie. can it only optimize queries if it has the constraint info in the INFORMATION\\_SCHEMA.TABLE\\_CONSTRAINTS table)  \n2. Has anyone seen noticeable performance gains using these constraints vs just focusing on DIST and SORT keys?  \n3. Do you have any other general advise on optimizing Redshift Queries with DBT in mind? ", "author_fullname": "t2_15uzwf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Differences in setting Primary Key and Foreign Key constraints in Redshift using CREATE TABLE vs ALTER TABLE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b77l4q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709651683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in setting Primary Key and Foreign Key constraints in Redshift as I&amp;#39;ve read it can help the Query Optimizer create more efficient queries. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed that when setting these constraints in the table creation they show up in the INFORMATION_SCHEMA.TABLE_CONSTRAINTS table, but when setting them with an ALTER command instead, they do not show up there. However, they both show up in PG_CONSTRAINT table. &lt;/p&gt;\n\n&lt;p&gt;My intention is to eventually set these in DBT, which also only shows up in the PG_CONSTRAINT table.&lt;/p&gt;\n\n&lt;p&gt;Being somewhat new to optimizing queries on Redshift, I&amp;#39;m curious if anyone knows:&lt;br/&gt;\n1. Does this difference matter to the query optimizer (ie. can it only optimize queries if it has the constraint info in the INFORMATION_SCHEMA.TABLE_CONSTRAINTS table)&lt;br/&gt;\n2. Has anyone seen noticeable performance gains using these constraints vs just focusing on DIST and SORT keys?&lt;br/&gt;\n3. Do you have any other general advise on optimizing Redshift Queries with DBT in mind? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b77l4q", "is_robot_indexable": true, "report_reasons": null, "author": "EngiNerd9000", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b77l4q/differences_in_setting_primary_key_and_foreign/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b77l4q/differences_in_setting_primary_key_and_foreign/", "subreddit_subscribers": 166022, "created_utc": 1709651683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nI'm currently building a data model for a reporting system. While looking for references or best practices, I stumbled upon the Star Schema. So I'm wondering, does my picture, which is a small cutout of our model, still count as one since it has a dimension in the middle?\n\n[https://imgur.com/a/e4CVooX](https://imgur.com/a/e4CVooX)", "author_fullname": "t2_ffhcii5f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to the world of Data, does this count as a Star Schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b72js6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709636298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently building a data model for a reporting system. While looking for references or best practices, I stumbled upon the Star Schema. So I&amp;#39;m wondering, does my picture, which is a small cutout of our model, still count as one since it has a dimension in the middle?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/e4CVooX\"&gt;https://imgur.com/a/e4CVooX&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/10MnfSVzgNZQuCRdrXFqUIFU9y3P0Syii9NmhZrF6lw.jpg?auto=webp&amp;s=b77fc0f034a6fd5ba465d264c1212946891c6402", "width": 848, "height": 467}, "resolutions": [{"url": "https://external-preview.redd.it/10MnfSVzgNZQuCRdrXFqUIFU9y3P0Syii9NmhZrF6lw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3fa64b8a4184ee2363e8849be2ab85342898ba7", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/10MnfSVzgNZQuCRdrXFqUIFU9y3P0Syii9NmhZrF6lw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e52b7b843c391eef43fcf9c988a5835e9523600d", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/10MnfSVzgNZQuCRdrXFqUIFU9y3P0Syii9NmhZrF6lw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8db965dac775e8b2b8af73e5a5e9519f64cb5b4", "width": 320, "height": 176}, {"url": "https://external-preview.redd.it/10MnfSVzgNZQuCRdrXFqUIFU9y3P0Syii9NmhZrF6lw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc196d76f90f96cf31e0c50fb0ce882d808a5f95", "width": 640, "height": 352}], "variants": {}, "id": "l0v-_hfICkaa6htcwY_CsokbzOActHoCk9044epftsY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b72js6", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic-Let251", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b72js6/new_to_the_world_of_data_does_this_count_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b72js6/new_to_the_world_of_data_does_this_count_as_a/", "subreddit_subscribers": 166022, "created_utc": 1709636298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Your Clients Are Resistant To Data Literacy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b71mkb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/moEmme4k-DEAQYqrJrABWmduhEanQQX15vtD2cEosKk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709632602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arch.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arch.dev/blog/why-your-clients-are-resistant-to-data-literacy/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BlLYN4xR5fDfidKXMRyhneFLggZfR4F2s4rc7L6ztNo.jpg?auto=webp&amp;s=3c3862c2a079bafdd08f21614afdab07ec564287", "width": 1024, "height": 768}, "resolutions": [{"url": "https://external-preview.redd.it/BlLYN4xR5fDfidKXMRyhneFLggZfR4F2s4rc7L6ztNo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2fa3623212d281857f2f7324fe31e4dd7447bab6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/BlLYN4xR5fDfidKXMRyhneFLggZfR4F2s4rc7L6ztNo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5dd76a27478cb9892f77895b2fb4ee2924243044", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/BlLYN4xR5fDfidKXMRyhneFLggZfR4F2s4rc7L6ztNo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=178ea1cd19eadf57b26ab8bc9963a7fc99af2a25", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/BlLYN4xR5fDfidKXMRyhneFLggZfR4F2s4rc7L6ztNo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9224a210744fa375913c2b9fc15ff68b2245ab3a", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/BlLYN4xR5fDfidKXMRyhneFLggZfR4F2s4rc7L6ztNo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b7d7d3b63ef3496224ae18ecc6a9b8d715b5789", "width": 960, "height": 720}], "variants": {}, "id": "WzsUtQy5NIoU5e2m8Qif1MSJV-kIvtdRIHIvcn-ju_Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b71mkb", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b71mkb/why_your_clients_are_resistant_to_data_literacy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arch.dev/blog/why-your-clients-are-resistant-to-data-literacy/", "subreddit_subscribers": 166022, "created_utc": 1709632602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The business I work for needs to replicate a database server (MySQL) (in our AWS infrastructure) to another DB server (MySQL) in a customer's AWS infrastructure over the internet. Originally they wanted log shipping however that's not feasible given that the machines are not locally connected.\n\n&amp;#x200B;\n\nThe business has decided to look into purchasing a solution as Microsoft makes this extremely difficult given the gap between infrastructures. \n\n&amp;#x200B;\n\nWas looking into fivetran/airbyte however I am looking for other solutions/recommendations as I am new to this myself.", "author_fullname": "t2_qdd0yns5d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Needing advice for replication software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6smez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709602279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The business I work for needs to replicate a database server (MySQL) (in our AWS infrastructure) to another DB server (MySQL) in a customer&amp;#39;s AWS infrastructure over the internet. Originally they wanted log shipping however that&amp;#39;s not feasible given that the machines are not locally connected.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The business has decided to look into purchasing a solution as Microsoft makes this extremely difficult given the gap between infrastructures. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Was looking into fivetran/airbyte however I am looking for other solutions/recommendations as I am new to this myself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b6smez", "is_robot_indexable": true, "report_reasons": null, "author": "Capital-Bedroom-2420", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6smez/needing_advice_for_replication_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6smez/needing_advice_for_replication_software/", "subreddit_subscribers": 166022, "created_utc": 1709602279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Greetings all. Experienced data engineer, but AWS Novice and looking for best practices (if any)\n\nI have a MySQL RDS db, which I want to create a real- or near realtime CDC into a new Aurora PostgreSQL db. This is a database replatform, and both databases will be in use for some time. (The old MySQL db will still be having data written to it, but over time, we'll be migrating code over to the Postgres db.) In addition, the Postgres db is not even close to the same schema as the MySQL db, and significant ETL will be needed.\n\nIt doesn't look like anything in AWS really accomplishes both a realtime data migration *and* complex ETL. What are tried and tested third-party tools that can accomplish this?\n\nMany thanks in advance.", "author_fullname": "t2_9h42b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complex CDC ETL from RDS MySQL to Aurora Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6niyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709589619.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings all. Experienced data engineer, but AWS Novice and looking for best practices (if any)&lt;/p&gt;\n\n&lt;p&gt;I have a MySQL RDS db, which I want to create a real- or near realtime CDC into a new Aurora PostgreSQL db. This is a database replatform, and both databases will be in use for some time. (The old MySQL db will still be having data written to it, but over time, we&amp;#39;ll be migrating code over to the Postgres db.) In addition, the Postgres db is not even close to the same schema as the MySQL db, and significant ETL will be needed.&lt;/p&gt;\n\n&lt;p&gt;It doesn&amp;#39;t look like anything in AWS really accomplishes both a realtime data migration &lt;em&gt;and&lt;/em&gt; complex ETL. What are tried and tested third-party tools that can accomplish this?&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b6niyw", "is_robot_indexable": true, "report_reasons": null, "author": "Lightsider", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6niyw/complex_cdc_etl_from_rds_mysql_to_aurora_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6niyw/complex_cdc_etl_from_rds_mysql_to_aurora_postgres/", "subreddit_subscribers": 166022, "created_utc": 1709589619.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_t1gzbxlm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Hunt for the Missing Data Type", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6zu8j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1709624998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hillelwayne.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.hillelwayne.com/post/graph-types/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b6zu8j", "is_robot_indexable": true, "report_reasons": null, "author": "dyeagokin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6zu8j/the_hunt_for_the_missing_data_type/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.hillelwayne.com/post/graph-types/", "subreddit_subscribers": 166022, "created_utc": 1709624998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am integrating DBT into a RDS instance, a reports need to run daily for aggregated metrics from a big table, and spit it into other tables within the DB. Which service do you recommend running these DBT scripts and how to orchestrate it at noon and update the tables without the instance. Hopefully the solution is AWS, scalable as more models are added or time of triggering. I am familiar with lambda functions and ECS but not sure how not done this before. TIA!!!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Runs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6ose8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709592591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am integrating DBT into a RDS instance, a reports need to run daily for aggregated metrics from a big table, and spit it into other tables within the DB. Which service do you recommend running these DBT scripts and how to orchestrate it at noon and update the tables without the instance. Hopefully the solution is AWS, scalable as more models are added or time of triggering. I am familiar with lambda functions and ECS but not sure how not done this before. TIA!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b6ose8", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6ose8/dbt_runs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6ose8/dbt_runs/", "subreddit_subscribers": 166022, "created_utc": 1709592591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Erwin can reverse engineer a view but it doesn't seem to handle CASE statements and errors out. This is for Hive. Does anyone know if reverse engineering of views can handle CASE statements?", "author_fullname": "t2_kkmygn8e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reverse engineering using Erwin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6ndyb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709589281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Erwin can reverse engineer a view but it doesn&amp;#39;t seem to handle CASE statements and errors out. This is for Hive. Does anyone know if reverse engineering of views can handle CASE statements?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b6ndyb", "is_robot_indexable": true, "report_reasons": null, "author": "KarmicDharmic", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b6ndyb/reverse_engineering_using_erwin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b6ndyb/reverse_engineering_using_erwin/", "subreddit_subscribers": 166022, "created_utc": 1709589281.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}