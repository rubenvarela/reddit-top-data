{"kind": "Listing", "data": {"after": "t3_1b6z5zu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "When comparing the costs of having a copy of your data for long term storage in the cloud (plus one portable hard drive local copy) versus 1-2 additional portable hard drives, it seems the benefits skew in favour of hard drives within 2-3 years. \n\nFor example, 200GB (assume this is enough for me) on ProtonDrive costs USD 83.76 for 2 years. That\u2019s about the cost of a 500GB portable hard drive. I could buy a new portable hard drive every 2 years and keep 3+ copies of my data spread out geographically:\n- One hard drive with myself at home for immediate use\n- 1-2 copies kept at work or with family/friends in the same city (in case of fire)\n- One copy with family in a different country that you update once a year just cos you want to be geographically diversified\n- If you really need it on the go, then one copy on a compact external SSD that you can take with you\n\nWhen you want to access the photos, I would be surprised if the local copies don\u2019t work much faster than streaming from the cloud too. So what is it that I may be missing that is the benefit of having the cloud if this is all your storage needs are? ", "author_fullname": "t2_96fh3xs8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there really a need for cloud storage if you have 3+ copies of your data on portable hard drives and don\u2019t need it immediately?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6q9bx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709596106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When comparing the costs of having a copy of your data for long term storage in the cloud (plus one portable hard drive local copy) versus 1-2 additional portable hard drives, it seems the benefits skew in favour of hard drives within 2-3 years. &lt;/p&gt;\n\n&lt;p&gt;For example, 200GB (assume this is enough for me) on ProtonDrive costs USD 83.76 for 2 years. That\u2019s about the cost of a 500GB portable hard drive. I could buy a new portable hard drive every 2 years and keep 3+ copies of my data spread out geographically:\n- One hard drive with myself at home for immediate use\n- 1-2 copies kept at work or with family/friends in the same city (in case of fire)\n- One copy with family in a different country that you update once a year just cos you want to be geographically diversified\n- If you really need it on the go, then one copy on a compact external SSD that you can take with you&lt;/p&gt;\n\n&lt;p&gt;When you want to access the photos, I would be surprised if the local copies don\u2019t work much faster than streaming from the cloud too. So what is it that I may be missing that is the benefit of having the cloud if this is all your storage needs are? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b6q9bx", "is_robot_indexable": true, "report_reasons": null, "author": "HippityHoppityBoop", "discussion_type": null, "num_comments": 67, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b6q9bx/is_there_really_a_need_for_cloud_storage_if_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b6q9bx/is_there_really_a_need_for_cloud_storage_if_you/", "subreddit_subscribers": 736399, "created_utc": 1709596106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nLong time lurker, first time posted in this subreddit. I currently have a bunch of DVDs and Blurays. I was watching a video ([https://youtu.be/xA9Xq7hb6Q0](https://youtu.be/xA9Xq7hb6Q0)) where he talks about life expectancy for storage. This made me worry a bit about the DVDs and Blurays I have. I would like to preserve the content as long as I can, as streaming services keep getting pricier. Is converting these into MKV with MakeMKV and keep moving them from drive to drive the way to go? My goal for the future is to essentially have a NAS that can hold all of it.\n\nTangential question: If I wanted it in a disk form for longer term, is exporting the full disk items, converting it to an ISO, and then \"flashing it\" to a M-DISC the best way for this?", "author_fullname": "t2_qfy2y98e1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to preserve DVD/Bly-Ray media as long as possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7ecai", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709667500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Long time lurker, first time posted in this subreddit. I currently have a bunch of DVDs and Blurays. I was watching a video (&lt;a href=\"https://youtu.be/xA9Xq7hb6Q0\"&gt;https://youtu.be/xA9Xq7hb6Q0&lt;/a&gt;) where he talks about life expectancy for storage. This made me worry a bit about the DVDs and Blurays I have. I would like to preserve the content as long as I can, as streaming services keep getting pricier. Is converting these into MKV with MakeMKV and keep moving them from drive to drive the way to go? My goal for the future is to essentially have a NAS that can hold all of it.&lt;/p&gt;\n\n&lt;p&gt;Tangential question: If I wanted it in a disk form for longer term, is exporting the full disk items, converting it to an ISO, and then &amp;quot;flashing it&amp;quot; to a M-DISC the best way for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/388RYXejWZq3ec4w7sEojJ1jec-383rmFwT553Esu5o.jpg?auto=webp&amp;s=2d51b297d3948118df0d37297ccde5e4538bf20c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/388RYXejWZq3ec4w7sEojJ1jec-383rmFwT553Esu5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3aea2c56a5853ebb03a9c8b593ca53eba163295e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/388RYXejWZq3ec4w7sEojJ1jec-383rmFwT553Esu5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9075a334005a91cf6cf247ff7d9c14080d76ce3a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/388RYXejWZq3ec4w7sEojJ1jec-383rmFwT553Esu5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=75c2e1d31bc3adaccd9ab1f3bfd8c8d6f904f904", "width": 320, "height": 240}], "variants": {}, "id": "__mzULFT7h1UTg5sKKQExNpAqGNYf0AJNsLFpk_Jzhw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7ecai", "is_robot_indexable": true, "report_reasons": null, "author": "Delegator0765", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7ecai/how_to_preserve_dvdblyray_media_as_long_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7ecai/how_to_preserve_dvdblyray_media_as_long_as/", "subreddit_subscribers": 736399, "created_utc": 1709667500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a photo/video shooter with 25 years of images and videos building up.  Despite all the incredible advances in storage, it's still a costly and complicated proposition just to keep everything safe, and there's a lot of hidden cost (like replacing drives every few years) that can really bite you.  And once you get to 30TB, it seems like you're in between consumer and enterprise solutions (especially for cloud storage) - sort of a storage no-mans-land.   I currently use a Pegasus2 5 drive raid as my primary storage, with various external HDDs for backup, and a backblaze account in the cloud.  The problem is that I'm outgrowing that storage (5x8TB at raid 6=24TB storage).  One possibility is to get 2 Glyph 40TB raid 0 (one primary, one backup) - another would be to go to raid 5 on my Pegasus and do a single glyph as a backup.  I know there is a  higher failure rate with raid 0, so I'm not sure if the glyph makes sense as a backup drive?  Would buying two 16TB drives and backing 1/2 of my raid to each make more sense?  I'm also going to run into higher rates with backblaze, but I don't see a way around that.  One other thing - I have some data that's on 2 drives that aren't spun up - how often do I transfer that data to new drives?   Thanks for any and all advice!", "author_fullname": "t2_w7is6oa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting past 30TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7by3y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709661905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a photo/video shooter with 25 years of images and videos building up.  Despite all the incredible advances in storage, it&amp;#39;s still a costly and complicated proposition just to keep everything safe, and there&amp;#39;s a lot of hidden cost (like replacing drives every few years) that can really bite you.  And once you get to 30TB, it seems like you&amp;#39;re in between consumer and enterprise solutions (especially for cloud storage) - sort of a storage no-mans-land.   I currently use a Pegasus2 5 drive raid as my primary storage, with various external HDDs for backup, and a backblaze account in the cloud.  The problem is that I&amp;#39;m outgrowing that storage (5x8TB at raid 6=24TB storage).  One possibility is to get 2 Glyph 40TB raid 0 (one primary, one backup) - another would be to go to raid 5 on my Pegasus and do a single glyph as a backup.  I know there is a  higher failure rate with raid 0, so I&amp;#39;m not sure if the glyph makes sense as a backup drive?  Would buying two 16TB drives and backing 1/2 of my raid to each make more sense?  I&amp;#39;m also going to run into higher rates with backblaze, but I don&amp;#39;t see a way around that.  One other thing - I have some data that&amp;#39;s on 2 drives that aren&amp;#39;t spun up - how often do I transfer that data to new drives?   Thanks for any and all advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7by3y", "is_robot_indexable": true, "report_reasons": null, "author": "lyrebird2", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7by3y/getting_past_30tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7by3y/getting_past_30tb/", "subreddit_subscribers": 736399, "created_utc": 1709661905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am converting my DVD collection to several hard drives. I had three DVD burners hooked up to my computer then they all fried. Still not sure why. So I set out looking for a better solution. I bought a used DiscMakers Reflex for $150. It has 7 drives but I am not going to use the control board. My plan was to just use the case as housing and connect all the DVD drives to a port multiplier and connect that to my PC. Problem is I failed to check what kind of interface the drives were. I stupidly assumed from the pictures that this machine was relatively newer. I opened up the case and lo and behold they are all Pata IDE interfaces. I know I can buy Pata to Sata adapters but I don't want to miss a step and nothing works with my PC because of some incompatibility. \n\nIn your professional opinions what, if any, is the best way to connect 7 PATA/IDE drives to my PC? Or did I just buy a $150 brick?\n\nPlease let me know what information you need. Thanks", "author_fullname": "t2_rx4i3ka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bought a duplicator to turn into a burner tower.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7gez1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709672388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am converting my DVD collection to several hard drives. I had three DVD burners hooked up to my computer then they all fried. Still not sure why. So I set out looking for a better solution. I bought a used DiscMakers Reflex for $150. It has 7 drives but I am not going to use the control board. My plan was to just use the case as housing and connect all the DVD drives to a port multiplier and connect that to my PC. Problem is I failed to check what kind of interface the drives were. I stupidly assumed from the pictures that this machine was relatively newer. I opened up the case and lo and behold they are all Pata IDE interfaces. I know I can buy Pata to Sata adapters but I don&amp;#39;t want to miss a step and nothing works with my PC because of some incompatibility. &lt;/p&gt;\n\n&lt;p&gt;In your professional opinions what, if any, is the best way to connect 7 PATA/IDE drives to my PC? Or did I just buy a $150 brick?&lt;/p&gt;\n\n&lt;p&gt;Please let me know what information you need. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7gez1", "is_robot_indexable": true, "report_reasons": null, "author": "Phedis", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7gez1/bought_a_duplicator_to_turn_into_a_burner_tower/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7gez1/bought_a_duplicator_to_turn_into_a_burner_tower/", "subreddit_subscribers": 736399, "created_utc": 1709672388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How do I do a cold storage effectively? Trying the 321(?) I think that\u2019s what it\u2019s called for backup purposes \n\n", "author_fullname": "t2_25gmsdyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do cold storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b73dmi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709639416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do I do a cold storage effectively? Trying the 321(?) I think that\u2019s what it\u2019s called for backup purposes &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b73dmi", "is_robot_indexable": true, "report_reasons": null, "author": "Feeya_b", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b73dmi/how_to_do_cold_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b73dmi/how_to_do_cold_storage/", "subreddit_subscribers": 736399, "created_utc": 1709639416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Right now I face the problem, that my current PC (2x1TB M2 SSD, Ubuntu 22.04 LTS) is running out of space. I do not have a real backup solution right now besides encrypting and uploading important files to the cloud. \n\nAnyway, I am planing to build an Unraid system later this year, but I need more space now and to invest in components and HDDs is just a little too much for me right now. To solve the storage problem my plan is to buy an HDD now and transfer the drive into the Unraid system later on.\n\nI was thinking about buying the Seagate IronWolf NAS 8TB ST8000VN004.\n\nShould I buy 2 of these drives, install them into my PC and mirror them,  or is there a good software solution for a decent backup with just one 8TB Drive?\n\nAlso can I just transfer the HDD into the Unraid later on or can this be problematic?", "author_fullname": "t2_bp3qppz4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Buying HDD now, building NAS later", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b78ir4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709653926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now I face the problem, that my current PC (2x1TB M2 SSD, Ubuntu 22.04 LTS) is running out of space. I do not have a real backup solution right now besides encrypting and uploading important files to the cloud. &lt;/p&gt;\n\n&lt;p&gt;Anyway, I am planing to build an Unraid system later this year, but I need more space now and to invest in components and HDDs is just a little too much for me right now. To solve the storage problem my plan is to buy an HDD now and transfer the drive into the Unraid system later on.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about buying the Seagate IronWolf NAS 8TB ST8000VN004.&lt;/p&gt;\n\n&lt;p&gt;Should I buy 2 of these drives, install them into my PC and mirror them,  or is there a good software solution for a decent backup with just one 8TB Drive?&lt;/p&gt;\n\n&lt;p&gt;Also can I just transfer the HDD into the Unraid later on or can this be problematic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b78ir4", "is_robot_indexable": true, "report_reasons": null, "author": "Zswole", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b78ir4/buying_hdd_now_building_nas_later/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b78ir4/buying_hdd_now_building_nas_later/", "subreddit_subscribers": 736399, "created_utc": 1709653926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't mean how it finds and moves the tape to the drive, but how does it know where the files are on which tape?\n\nDoes it store an index on its own internal storage? Does it keep track of where the tape stopped?\n\nOr doesn't it do any of that and when you have one you just send a command to \"read tape 4 until file G is found\"?\n\nTape has been an interest to me and a while back I read heavily into it, but I don't ever recall reading about this. Perhaps I'm not searching the correct words either.", "author_fullname": "t2_16u0wi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do tape autoloaders work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b71hke", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709632012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t mean how it finds and moves the tape to the drive, but how does it know where the files are on which tape?&lt;/p&gt;\n\n&lt;p&gt;Does it store an index on its own internal storage? Does it keep track of where the tape stopped?&lt;/p&gt;\n\n&lt;p&gt;Or doesn&amp;#39;t it do any of that and when you have one you just send a command to &amp;quot;read tape 4 until file G is found&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;Tape has been an interest to me and a while back I read heavily into it, but I don&amp;#39;t ever recall reading about this. Perhaps I&amp;#39;m not searching the correct words either.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "140 TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b71hke", "is_robot_indexable": true, "report_reasons": null, "author": "TheGleanerBaldwin", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b71hke/how_do_tape_autoloaders_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b71hke/how_do_tape_autoloaders_work/", "subreddit_subscribers": 736399, "created_utc": 1709632012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to correctly archive Yuzu and Citra repositories", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6zezr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_tm28o939", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Kh5Kr_1kcKpym7Cvqt_CVhCKfiMJaN3kgcNVoasl9p8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "yuzu", "selftext": "The #citra and #yuzu repositories have been deleted by #Nintendo. Those who have a repository of them follow these steps to be able to share it:\n\nTo archive a repository correctly, it can be done in several ways.\n\n\n\n1. Zip the whole repo and upload it to [archive.org](https://archive.org)\n\n\n2. Add a new remote\n\ncd yuzu-folder \ngit remote add (RemoteName [can be anything]) git@{NewRepoURL} \ngit push --all {RemoteName}\n\n\n3. Making a .bundle and upload it to [archive.org](https://archive.org)\n\n\n\nWith any way all the branches and commits of the repository that you are archiving will be saved.\n\nLong life to emulators and open source!", "author_fullname": "t2_tm28o939", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to correctly archive Yuzu and Citra repositories", "link_flair_richtext": [], "subreddit_name_prefixed": "r/yuzu", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6q3d6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Kh5Kr_1kcKpym7Cvqt_CVhCKfiMJaN3kgcNVoasl9p8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "created": 1709595700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The #citra and #yuzu repositories have been deleted by #Nintendo. Those who have a repository of them follow these steps to be able to share it:&lt;/p&gt;\n\n&lt;p&gt;To archive a repository correctly, it can be done in several ways.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Zip the whole repo and upload it to &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Add a new remote&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;cd yuzu-folder \ngit remote add (RemoteName [can be anything]) git@{NewRepoURL} \ngit push --all {RemoteName}&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Making a .bundle and upload it to &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;With any way all the branches and commits of the repository that you are archiving will be saved.&lt;/p&gt;\n\n&lt;p&gt;Long life to emulators and open source!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wn5axz0rlemc1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?auto=webp&amp;s=5cd9e21349cf2b0651620498def83fc02e70cc44", "width": 753, "height": 471}, "resolutions": [{"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71e8dbd0d0d1393ce73d3746e89ec83f556a3fbd", "width": 108, "height": 67}, {"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=870aab91e3e3f23b7126fe05232da2e1997e35fc", "width": 216, "height": 135}, {"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89bba54b934bcde5786fbe9e1c7f6bbb3abfcc96", "width": 320, "height": 200}, {"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b1c51ce4e6b9294372fb4d6e1b5f05bdcdfbdd4", "width": 640, "height": 400}], "variants": {}, "id": "Q8NF3mFw3RljBEoGiQGte_xlsPSKmal9A1vhIFNM1Qs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_39i9t", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1b6q3d6", "is_robot_indexable": true, "report_reasons": null, "author": "PGSCOM", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/yuzu/comments/1b6q3d6/how_to_correctly_archive_yuzu_and_citra/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wn5axz0rlemc1.jpeg", "subreddit_subscribers": 87628, "created_utc": 1709595700.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1709623265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wn5axz0rlemc1.jpeg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?auto=webp&amp;s=5cd9e21349cf2b0651620498def83fc02e70cc44", "width": 753, "height": 471}, "resolutions": [{"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71e8dbd0d0d1393ce73d3746e89ec83f556a3fbd", "width": 108, "height": 67}, {"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=870aab91e3e3f23b7126fe05232da2e1997e35fc", "width": 216, "height": 135}, {"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89bba54b934bcde5786fbe9e1c7f6bbb3abfcc96", "width": 320, "height": 200}, {"url": "https://preview.redd.it/wn5axz0rlemc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b1c51ce4e6b9294372fb4d6e1b5f05bdcdfbdd4", "width": 640, "height": 400}], "variants": {}, "id": "Q8NF3mFw3RljBEoGiQGte_xlsPSKmal9A1vhIFNM1Qs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b6zezr", "is_robot_indexable": true, "report_reasons": null, "author": "PGSCOM", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1b6q3d6", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b6zezr/how_to_correctly_archive_yuzu_and_citra/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wn5axz0rlemc1.jpeg", "subreddit_subscribers": 736399, "created_utc": 1709623265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a drive off of Amazon and i bought it as used but S.M.A.R.T data is saying its new. But my question is when it's writing data it makes a sound kinda like its water hitting a empty barrel or like a tub facet dripping. Is that normal?", "author_fullname": "t2_lhaxyst29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Used\" drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b7iwmw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709678218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a drive off of Amazon and i bought it as used but S.M.A.R.T data is saying its new. But my question is when it&amp;#39;s writing data it makes a sound kinda like its water hitting a empty barrel or like a tub facet dripping. Is that normal?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7iwmw", "is_robot_indexable": true, "report_reasons": null, "author": "WhiteKnight4369", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7iwmw/used_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7iwmw/used_drive/", "subreddit_subscribers": 736399, "created_utc": 1709678218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking through a deleted Twitter, but there are thousands of posts.  I'm looking for a specific image they posted a while back.  Is there a way to filter for only tweets that have images?", "author_fullname": "t2_13x2zu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Search Wayback for Tweets with images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b7i0na", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709676122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking through a deleted Twitter, but there are thousands of posts.  I&amp;#39;m looking for a specific image they posted a while back.  Is there a way to filter for only tweets that have images?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7i0na", "is_robot_indexable": true, "report_reasons": null, "author": "getridofme12345", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7i0na/search_wayback_for_tweets_with_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7i0na/search_wayback_for_tweets_with_images/", "subreddit_subscribers": 736399, "created_utc": 1709676122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought two Samsung 2TB T7 Sheilds brand new sold and fulfilled by Newegg. I do photography and just need a bit more storage for now (I know I should have a RAID or NAS system in the future) but for now this is more than enough. I don't know what to do with them before I start using them. One is a backup and one is a main drive so I have 2TBs worth of storage. Should I be doing any tests or just trust in them and that the likelihood of getting two faulty T7 Shields is low.", "author_fullname": "t2_11kdbe89", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2 New T7 Shields from Newegg. Do I Need to Test Them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7egrc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709667784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought two Samsung 2TB T7 Sheilds brand new sold and fulfilled by Newegg. I do photography and just need a bit more storage for now (I know I should have a RAID or NAS system in the future) but for now this is more than enough. I don&amp;#39;t know what to do with them before I start using them. One is a backup and one is a main drive so I have 2TBs worth of storage. Should I be doing any tests or just trust in them and that the likelihood of getting two faulty T7 Shields is low.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7egrc", "is_robot_indexable": true, "report_reasons": null, "author": "Pinkhead12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7egrc/2_new_t7_shields_from_newegg_do_i_need_to_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7egrc/2_new_t7_shields_from_newegg_do_i_need_to_test/", "subreddit_subscribers": 736399, "created_utc": 1709667784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have list of let's say 100 links that I want to download form google drive, size up to 4GB per one link.\n\nI tried using JDownloader and even hacking at it with python, but I can't download them en-masse.\n\nAny way of going around this?", "author_fullname": "t2_44sdgiue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mass downloading from Google Drive links?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7dum3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709666337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have list of let&amp;#39;s say 100 links that I want to download form google drive, size up to 4GB per one link.&lt;/p&gt;\n\n&lt;p&gt;I tried using JDownloader and even hacking at it with python, but I can&amp;#39;t download them en-masse.&lt;/p&gt;\n\n&lt;p&gt;Any way of going around this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b7dum3", "is_robot_indexable": true, "report_reasons": null, "author": "ASatyros", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b7dum3/mass_downloading_from_google_drive_links/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7dum3/mass_downloading_from_google_drive_links/", "subreddit_subscribers": 736399, "created_utc": 1709666337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI saw a post on here around 12 months regarding [Roberts Electronics](https://robertelectronics.co.uk/), and the cheap prices for drives, the cheapest I've found for the UK market.\n\nI was wondering if anyone had any experience with them, as the price almost seems too cheap.\n\nAnd one other thing does anyone have any other recommendations for cheap HDD's for a NAS in the UK, as annoyingly serverpartsdeals has really expensive UK shipping.\n\nThanks", "author_fullname": "t2_7n3fap45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap Drives - UK - Roberts Electronics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7da3f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709665009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I saw a post on here around 12 months regarding &lt;a href=\"https://robertelectronics.co.uk/\"&gt;Roberts Electronics&lt;/a&gt;, and the cheap prices for drives, the cheapest I&amp;#39;ve found for the UK market.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone had any experience with them, as the price almost seems too cheap.&lt;/p&gt;\n\n&lt;p&gt;And one other thing does anyone have any other recommendations for cheap HDD&amp;#39;s for a NAS in the UK, as annoyingly serverpartsdeals has really expensive UK shipping.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5u8etjVPUwkmCC4dzuBv_pZciVmt5N52UfzHhWqRh4I.jpg?auto=webp&amp;s=37854a66352bd889bdeb494671c725fc490f8469", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/5u8etjVPUwkmCC4dzuBv_pZciVmt5N52UfzHhWqRh4I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0b37794d7b21621b3c76b858ecaee46706cd5fe", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/5u8etjVPUwkmCC4dzuBv_pZciVmt5N52UfzHhWqRh4I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3dee3ab5378684525a043c71d67d66194f11b7a", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/5u8etjVPUwkmCC4dzuBv_pZciVmt5N52UfzHhWqRh4I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1eaddf9429977018601d10b64b15150656d5631", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/5u8etjVPUwkmCC4dzuBv_pZciVmt5N52UfzHhWqRh4I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cdd36a705aca5b12d4eb2a1cb373509b988e7bbd", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/5u8etjVPUwkmCC4dzuBv_pZciVmt5N52UfzHhWqRh4I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a8ef62b20d9bcd03f62046197b8c627bfef1df8", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/5u8etjVPUwkmCC4dzuBv_pZciVmt5N52UfzHhWqRh4I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=19ef8e19eccd91febd10a64564c8c374b943eff7", "width": 1080, "height": 565}], "variants": {}, "id": "fRXUiS_f3WlETeAXLSWxly78LtaQpTxyxHFqHr02QhQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7da3f", "is_robot_indexable": true, "report_reasons": null, "author": "Camspoon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7da3f/cheap_drives_uk_roberts_electronics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7da3f/cheap_drives_uk_roberts_electronics/", "subreddit_subscribers": 736399, "created_utc": 1709665009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have this one old Samsung 1TB hard drive and I want to use it in RAID 1 with another new hard drive,  I'm concerned about some smart data tho but I don't really understand it that well so I'm asking for some help on what some errors mean, the only thing i understand is that it has 1 pending sector and I know that is not good but I was thinking that I could maybe force the drive to reallocate it and than use the drive, I know that it means that it could fail soon but there is still a chance that it can last some time if I carefully monitor the drive over time and since its RAID 1 I still won't lose any data if it fails, of course I will do backups too. I'm worried about other errors tho because I don't understand them really well so could someone please explain if there is anything else wrong with this drive?\n\n&amp;#x200B;\n\nHere is the smart data:\n\n    smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.0-17-amd64] (local build)\n    Copyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n    \n    === START OF INFORMATION SECTION ===\n    Model Family:     SAMSUNG SpinPoint F3\n    Device Model:     SAMSUNG HD103SJ\n    Serial Number:    S246J9GB801670\n    LU WWN Device Id: 5 0024e9 205ea6511\n    Firmware Version: 1AJ10001\n    User Capacity:    1,000,204,886,016 bytes [1.00 TB]\n    Sector Size:      512 bytes logical/physical\n    Rotation Rate:    7200 rpm\n    Form Factor:      3.5 inches\n    Device is:        In smartctl database 7.3/5319\n    ATA Version is:   ATA8-ACS T13/1699-D revision 6\n    SATA Version is:  SATA 2.6, 3.0 Gb/s\n    Local Time is:    Sun Mar  3 18:39:54 2024 CET\n    SMART support is: Available - device has SMART capability.\n    SMART support is: Enabled\n    AAM feature is:   Disabled\n    APM feature is:   Disabled\n    Rd look-ahead is: Enabled\n    Write cache is:   Enabled\n    DSN feature is:   Unavailable\n    ATA Security is:  Disabled, frozen [SEC2]\n    \n    === START OF READ SMART DATA SECTION ===\n    SMART overall-health self-assessment test result: PASSED\n    \n    General SMART Values:\n    Offline data collection status:  (0x80)\tOffline data collection activity\n    \t\t\t\t\twas never started.\n    \t\t\t\t\tAuto Offline Data Collection: Enabled.\n    Self-test execution status:      ( 114)\tThe previous self-test completed having\n    \t\t\t\t\tthe read element of the test failed.\n    Total time to complete Offline \n    data collection: \t\t( 9180) seconds.\n    Offline data collection\n    capabilities: \t\t\t (0x5b) SMART execute Offline immediate.\n    \t\t\t\t\tAuto Offline data collection on/off support.\n    \t\t\t\t\tSuspend Offline collection upon new\n    \t\t\t\t\tcommand.\n    \t\t\t\t\tOffline surface scan supported.\n    \t\t\t\t\tSelf-test supported.\n    \t\t\t\t\tNo Conveyance Self-test supported.\n    \t\t\t\t\tSelective Self-test supported.\n    SMART capabilities:            (0x0003)\tSaves SMART data before entering\n    \t\t\t\t\tpower-saving mode.\n    \t\t\t\t\tSupports SMART auto save timer.\n    Error logging capability:        (0x01)\tError logging supported.\n    \t\t\t\t\tGeneral Purpose Logging supported.\n    Short self-test routine \n    recommended polling time: \t (   2) minutes.\n    Extended self-test routine\n    recommended polling time: \t ( 153) minutes.\n    SCT capabilities: \t       (0x003f)\tSCT Status supported.\n    \t\t\t\t\tSCT Error Recovery Control supported.\n    \t\t\t\t\tSCT Feature Control supported.\n    \t\t\t\t\tSCT Data Table supported.\n    \n    SMART Attributes Data Structure revision number: 16\n    Vendor Specific SMART Attributes with Thresholds:\n    ID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n      1 Raw_Read_Error_Rate     POSR-K   100   100   051    -    2\n      2 Throughput_Performance  -OS--K   252   252   000    -    0\n      3 Spin_Up_Time            PO---K   069   068   025    -    9426\n      4 Start_Stop_Count        -O--CK   089   089   000    -    11964\n      5 Reallocated_Sector_Ct   PO--CK   252   252   010    -    0\n      7 Seek_Error_Rate         -OSR-K   252   252   051    -    0\n      8 Seek_Time_Performance   --S--K   252   252   015    -    0\n      9 Power_On_Hours          -O--CK   100   100   000    -    21330\n     10 Spin_Retry_Count        -O--CK   252   252   051    -    0\n     11 Calibration_Retry_Count -O--CK   252   252   000    -    0\n     12 Power_Cycle_Count       -O--CK   094   094   000    -    6486\n    191 G-Sense_Error_Rate      -O---K   100   100   000    -    100\n    192 Power-Off_Retract_Count -O---K   252   252   000    -    0\n    194 Temperature_Celsius     -O----   064   055   000    -    36 (Min/Max 11/46)\n    195 Hardware_ECC_Recovered  -O-RCK   100   100   000    -    0\n    196 Reallocated_Event_Count -O--CK   252   252   000    -    0\n    197 Current_Pending_Sector  -O--CK   100   100   000    -    1\n    198 Offline_Uncorrectable   ----CK   252   252   000    -    0\n    199 UDMA_CRC_Error_Count    -OS-CK   100   100   000    -    15\n    200 Multi_Zone_Error_Rate   -O-R-K   100   100   000    -    584\n    223 Load_Retry_Count        -O--CK   252   252   000    -    0\n    225 Load_Cycle_Count        -O--CK   099   099   000    -    13285\n                                ||||||_ K auto-keep\n                                |||||__ C event count\n                                ||||___ R error rate\n                                |||____ S speed/performance\n                                ||_____ O updated online\n                                |______ P prefailure warning\n    \n    General Purpose Log Directory Version 1\n    SMART           Log Directory Version 1 [multi-sector log support]\n    Address    Access  R/W   Size  Description\n    0x00       GPL,SL  R/O      1  Log Directory\n    0x01           SL  R/O      1  Summary SMART error log\n    0x02           SL  R/O      2  Comprehensive SMART error log\n    0x03       GPL     R/O      2  Ext. Comprehensive SMART error log\n    0x06           SL  R/O      1  SMART self-test log\n    0x07       GPL     R/O      2  Extended self-test log\n    0x08       GPL     R/O      2  Power Conditions log\n    0x09           SL  R/W      1  Selective self-test log\n    0x10       GPL     R/O      1  NCQ Command Error log\n    0x11       GPL     R/O      1  SATA Phy Event Counters log\n    0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n    0xe0       GPL,SL  R/W      1  SCT Command/Status\n    0xe1       GPL,SL  R/W      1  SCT Data Transfer\n    \n    SMART Extended Comprehensive Error Log Version: 1 (2 sectors)\n    Device Error Count: 16 (device log contains only the most recent 8 errors)\n    \tCR     = Command Register\n    \tFEATR  = Features Register\n    \tCOUNT  = Count (was: Sector Count) Register\n    \tLBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    \tLH     = LBA High (was: Cylinder High) Register    ]   LBA\n    \tLM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    \tLL     = LBA Low (was: Sector Number) Register     ]\n    \tDV     = Device (was: Device/Head) Register\n    \tDC     = Device Control Register\n    \tER     = Error register\n    \tST     = Status register\n    Powered_Up_Time is measured from power on, and printed as\n    DDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n    SS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n    \n    Error 16 [7] occurred at disk power-on lifetime: 9927 hours (413 days + 15 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      84 -- 51 16 50 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      61 00 00 00 08 00 00 00 5f 16 48 40 00     00:00:06.036  WRITE FPDMA QUEUED\n      61 00 00 00 08 00 00 00 5f 16 48 40 00     00:00:06.130  WRITE FPDMA QUEUED\n      61 00 00 00 01 00 00 06 44 67 a0 40 00     00:00:06.130  WRITE FPDMA QUEUED\n      ea 00 00 00 00 00 00 00 00 00 00 e0 00     00:00:06.130  FLUSH CACHE EXT\n      61 00 00 00 08 00 00 00 5f 07 48 40 00     00:00:06.130  WRITE FPDMA QUEUED\n    \n    Error 15 [6] occurred at disk power-on lifetime: 9610 hours (400 days + 10 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      84 -- 51 79 da 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      60 00 00 00 10 00 00 00 8a 79 da 40 00     00:00:00.107  READ FPDMA QUEUED\n      60 00 00 00 20 00 00 00 8a 79 ca 40 00     00:00:00.108  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 00 69 3f 50 40 00     00:00:00.108  READ FPDMA QUEUED\n      ea 00 00 00 00 00 00 00 00 00 00 e0 00     00:00:00.108  FLUSH CACHE EXT\n      60 00 00 00 40 00 00 05 1f 73 00 40 00     00:00:00.108  READ FPDMA QUEUED\n    \n    Error 14 [5] occurred at disk power-on lifetime: 8774 hours (365 days + 14 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      84 -- 51 bd 48 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      60 00 00 00 00 00 00 10 bf bd 50 40 00     00:00:00.059  READ FPDMA QUEUED\n      60 00 00 00 0f 00 00 01 c2 b5 0c 40 00     00:00:00.059  READ FPDMA QUEUED\n      60 00 00 00 20 00 00 00 c8 99 ba 40 00     00:00:00.059  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 10 bf bd 48 40 00     00:00:00.059  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 10 bf d3 b8 40 00     00:00:00.059  READ FPDMA QUEUED\n    \n    Error 13 [4] occurred at disk power-on lifetime: 8674 hours (361 days + 10 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      84 -- 51 52 80 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      60 00 00 00 30 00 00 03 92 52 60 40 00     00:00:00.084  READ FPDMA QUEUED\n      60 00 00 00 40 00 00 03 92 52 50 40 00     00:00:00.085  READ FPDMA QUEUED\n      60 00 00 00 c8 00 00 02 6d 90 a2 40 00     00:00:00.085  READ FPDMA QUEUED\n      60 00 00 01 00 00 00 02 6d 8f a2 40 00     00:00:00.085  READ FPDMA QUEUED\n      60 00 00 01 00 00 00 02 6d 8e a2 40 00     00:00:00.085  READ FPDMA QUEUED\n    \n    Error 12 [3] occurred at disk power-on lifetime: 8672 hours (361 days + 8 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      84 -- 51 9b 60 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      60 00 00 00 70 00 00 00 10 9b 00 40 00     00:00:00.049  READ FPDMA QUEUED\n      60 00 00 00 80 00 00 00 10 9a f0 40 00     00:00:00.050  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 00 03 b1 38 40 00     00:00:00.050  READ FPDMA QUEUED\n      61 00 00 01 00 00 00 08 a5 c5 f8 40 00     00:00:00.050  WRITE FPDMA QUEUED\n      61 00 00 01 00 00 00 08 a5 c4 f8 40 00     00:00:00.050  WRITE FPDMA QUEUED\n    \n    Error 11 [2] occurred at disk power-on lifetime: 8419 hours (350 days + 19 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      84 -- 51 75 00 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      60 00 00 00 00 00 00 00 bf 75 10 40 00     00:00:00.035  READ FPDMA QUEUED\n      60 00 00 00 10 00 00 00 bf 75 00 40 00     00:00:00.035  READ FPDMA QUEUED\n      60 00 00 00 80 00 00 00 40 93 58 40 00     00:00:00.035  READ FPDMA QUEUED\n      ea 00 00 00 00 00 00 00 00 00 00 e0 00     00:00:00.035  FLUSH CACHE EXT\n      60 00 00 00 78 00 00 00 c5 b3 10 40 00     00:00:00.035  READ FPDMA QUEUED\n    \n    Error 10 [1] occurred at disk power-on lifetime: 8263 hours (344 days + 7 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      84 -- 51 80 a8 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      60 00 00 00 00 00 00 06 13 80 b0 40 00     00:00:00.114  READ FPDMA QUEUED\n      61 00 00 00 20 00 00 02 22 83 20 40 00     00:00:00.114  WRITE FPDMA QUEUED\n      60 00 00 00 08 00 00 05 54 55 b8 40 00     00:00:00.114  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 06 13 80 a8 40 00     00:00:00.114  READ FPDMA QUEUED\n      60 00 00 00 18 00 00 02 2a 96 0a 40 00     00:00:00.114  READ FPDMA QUEUED\n    \n    Error 9 [0] occurred at disk power-on lifetime: 8158 hours (339 days + 22 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      84 -- 51 3e 70 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      60 00 00 00 00 00 00 45 fc 3e 78 40 00     00:00:00.150  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 45 fc 3e 70 40 00     00:00:00.151  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 13 01 88 68 40 00     00:00:00.151  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 00 68 33 c0 40 00     00:00:00.151  READ FPDMA QUEUED\n      60 00 00 00 08 00 00 10 bb 97 10 40 00     00:00:00.151  READ FPDMA QUEUED\n    \n    SMART Extended Self-test Log Version: 1 (2 sectors)\n    Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n    # 1  Extended offline    Completed: read failure       20%     21330         1886208607\n    \n    SMART Selective self-test log data structure revision number 0\n    Note: revision number not 1 implies that no selective self-test has ever been run\n     SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n        1        0        0  Completed_read_failure [20% left] (0-65535)\n        2        0        0  Not_testing\n        3        0        0  Not_testing\n        4        0        0  Not_testing\n        5        0        0  Not_testing\n    Selective self-test flags (0x0):\n      After scanning selected spans, do NOT read-scan remainder of disk.\n    If Selective self-test is pending on power-up, resume after 0 minute delay.\n    \n    SCT Status Version:                  2\n    SCT Version (vendor specific):       256 (0x0100)\n    Device State:                        Active (0)\n    Current Temperature:                    36 Celsius\n    Power Cycle Min/Max Temperature:     19/38 Celsius\n    Lifetime    Min/Max Temperature:     13/63 Celsius\n    Specified Max Operating Temperature:    80 Celsius\n    Under/Over Temperature Limit Count:   0/0\n    \n    SCT Temperature History Version:     2\n    Temperature Sampling Period:         5 minutes\n    Temperature Logging Interval:        5 minutes\n    Min/Max recommended Temperature:     -5/80 Celsius\n    Min/Max Temperature Limit:           -10/85 Celsius\n    Temperature History Size (Index):    128 (98)\n    \n    Index    Estimated Time   Temperature Celsius\n      99    2024-03-03 08:00    26  *******\n     100    2024-03-03 08:05    27  ********\n     101    2024-03-03 08:10    28  *********\n     102    2024-03-03 08:15    21  **\n     103    2024-03-03 08:20    24  *****\n     104    2024-03-03 08:25    26  *******\n     105    2024-03-03 08:30    27  ********\n     106    2024-03-03 08:35    28  *********\n     107    2024-03-03 08:40    29  **********\n     108    2024-03-03 08:45    26  *******\n     109    2024-03-03 08:50    27  ********\n     110    2024-03-03 08:55    28  *********\n     111    2024-03-03 09:00    28  *********\n     112    2024-03-03 09:05    22  ***\n     113    2024-03-03 09:10    24  *****\n     114    2024-03-03 09:15    26  *******\n     115    2024-03-03 09:20    27  ********\n     116    2024-03-03 09:25    28  *********\n     117    2024-03-03 09:30    24  *****\n     118    2024-03-03 09:35    25  ******\n     119    2024-03-03 09:40    27  ********\n     120    2024-03-03 09:45    21  **\n     121    2024-03-03 09:50    24  *****\n     122    2024-03-03 09:55    25  ******\n     123    2024-03-03 10:00    26  *******\n     124    2024-03-03 10:05    27  ********\n     125    2024-03-03 10:10    28  *********\n     126    2024-03-03 10:15    28  *********\n     127    2024-03-03 10:20    29  **********\n     ...    ..(  2 skipped).    ..  **********\n       2    2024-03-03 10:35    29  **********\n       3    2024-03-03 10:40    30  ***********\n       4    2024-03-03 10:45    30  ***********\n       5    2024-03-03 10:50    31  ************\n     ...    ..(  2 skipped).    ..  ************\n       8    2024-03-03 11:05    31  ************\n       9    2024-03-03 11:10    32  *************\n     ...    ..(  6 skipped).    ..  *************\n      16    2024-03-03 11:45    32  *************\n      17    2024-03-03 11:50    31  ************\n      18    2024-03-03 11:55    30  ***********\n     ...    ..(  2 skipped).    ..  ***********\n      21    2024-03-03 12:10    30  ***********\n      22    2024-03-03 12:15    24  *****\n      23    2024-03-03 12:20    26  *******\n      24    2024-03-03 12:25    27  ********\n      25    2024-03-03 12:30    29  **********\n      26    2024-03-03 12:35    30  ***********\n      27    2024-03-03 12:40    23  ****\n      28    2024-03-03 12:45    25  ******\n      29    2024-03-03 12:50    26  *******\n      30    2024-03-03 12:55    27  ********\n      31    2024-03-03 13:00    25  ******\n      32    2024-03-03 13:05    26  *******\n      33    2024-03-03 13:10    28  *********\n      34    2024-03-03 13:15    21  **\n      35    2024-03-03 13:20    24  *****\n      36    2024-03-03 13:25    26  *******\n      37    2024-03-03 13:30    27  ********\n      38    2024-03-03 13:35    28  *********\n      39    2024-03-03 13:40    29  **********\n      40    2024-03-03 13:45    29  **********\n      41    2024-03-03 13:50    23  ****\n      42    2024-03-03 13:55    25  ******\n      43    2024-03-03 14:00    27  ********\n      44    2024-03-03 14:05    28  *********\n      45    2024-03-03 14:10    26  *******\n      46    2024-03-03 14:15    27  ********\n      47    2024-03-03 14:20    28  *********\n      48    2024-03-03 14:25    28  *********\n      49    2024-03-03 14:30    24  *****\n      50    2024-03-03 14:35    25  ******\n      51    2024-03-03 14:40    26  *******\n      52    2024-03-03 14:45    19  -\n      53    2024-03-03 14:50    22  ***\n      54    2024-03-03 14:55    24  *****\n      55    2024-03-03 15:00    26  *******\n      56    2024-03-03 15:05    27  ********\n      57    2024-03-03 15:10    28  *********\n      58    2024-03-03 15:15    29  **********\n      59    2024-03-03 15:20    30  ***********\n      60    2024-03-03 15:25    30  ***********\n      61    2024-03-03 15:30    31  ************\n      62    2024-03-03 15:35    31  ************\n      63    2024-03-03 15:40    32  *************\n     ...    ..(  3 skipped).    ..  *************\n      67    2024-03-03 16:00    32  *************\n      68    2024-03-03 16:05    33  **************\n     ...    ..( 10 skipped).    ..  **************\n      79    2024-03-03 17:00    33  **************\n      80    2024-03-03 17:05    34  ***************\n      81    2024-03-03 17:10    36  *****************\n      82    2024-03-03 17:15    37  ******************\n      83    2024-03-03 17:20    37  ******************\n      84    2024-03-03 17:25    37  ******************\n      85    2024-03-03 17:30    38  *******************\n     ...    ..(  6 skipped).    ..  *******************\n      92    2024-03-03 18:05    38  *******************\n      93    2024-03-03 18:10    37  ******************\n     ...    ..(  3 skipped).    ..  ******************\n      97    2024-03-03 18:30    37  ******************\n      98    2024-03-03 18:35    36  *****************\n    \n    SCT Error Recovery Control:\n               Read: Disabled\n              Write: Disabled\n    \n    Device Statistics (GP/SMART Log 0x04) not supported\n    \n    SATA Phy Event Counters (GP Log 0x11)\n    ID      Size     Value  Description\n    0x0001  4            0  Command failed due to ICRC error\n    0x0002  4            0  R_ERR response for data FIS\n    0x0003  4            0  R_ERR response for device-to-host data FIS\n    0x0004  4            0  R_ERR response for host-to-device data FIS\n    0x0005  4            0  R_ERR response for non-data FIS\n    0x0006  4            0  R_ERR response for device-to-host non-data FIS\n    0x0007  4            0  R_ERR response for host-to-device non-data FIS\n    0x0008  4            0  Device-to-host non-data FIS retries\n    0x0009  4           11  Transition from drive PhyRdy to drive PhyNRdy\n    0x000a  4           10  Device-to-host register FISes sent due to a COMRESET\n    0x000b  4            0  CRC errors within host-to-device FIS\n    0x000d  4            0  Non-CRC errors within host-to-device FIS\n    0x000f  4            0  R_ERR response for host-to-device data FIS, CRC\n    0x0010  4            0  R_ERR response for host-to-device data FIS, non-CRC\n    0x0012  4            0  R_ERR response for host-to-device non-data FIS, CRC\n    0x0013  4            0  R_ERR response for host-to-device non-data FIS, non-CRC\n    0x8e00  4            0  Vendor specific\n    0x8e01  4            0  Vendor specific\n    0x8e02  4            0  Vendor specific\n    0x8e03  4            0  Vendor specific\n    0x8e04  4            0  Vendor specific\n    0x8e05  4            0  Vendor specific\n    0x8e06  4            0  Vendor specific\n    0x8e07  4            0  Vendor specific\n    0x8e08  4            0  Vendor specific\n    0x8e09  4            0  Vendor specific\n    0x8e0a  4            0  Vendor specific\n    0x8e0b  4            0  Vendor specific\n    0x8e0c  4            0  Vendor specific\n    0x8e0d  4            0  Vendor specific\n    0x8e0e  4            0  Vendor specific\n    0x8e0f  4            0  Vendor specific\n    0x8e10  4            0  Vendor specific\n    0x8e11  4            0  Vendor specific\n\n\n&amp;#x200B;", "author_fullname": "t2_6c7lxhn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can i use this hard drive for RAID 1?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7cyc6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709664245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have this one old Samsung 1TB hard drive and I want to use it in RAID 1 with another new hard drive,  I&amp;#39;m concerned about some smart data tho but I don&amp;#39;t really understand it that well so I&amp;#39;m asking for some help on what some errors mean, the only thing i understand is that it has 1 pending sector and I know that is not good but I was thinking that I could maybe force the drive to reallocate it and than use the drive, I know that it means that it could fail soon but there is still a chance that it can last some time if I carefully monitor the drive over time and since its RAID 1 I still won&amp;#39;t lose any data if it fails, of course I will do backups too. I&amp;#39;m worried about other errors tho because I don&amp;#39;t understand them really well so could someone please explain if there is anything else wrong with this drive?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here is the smart data:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.0-17-amd64] (local build)\nCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Family:     SAMSUNG SpinPoint F3\nDevice Model:     SAMSUNG HD103SJ\nSerial Number:    S246J9GB801670\nLU WWN Device Id: 5 0024e9 205ea6511\nFirmware Version: 1AJ10001\nUser Capacity:    1,000,204,886,016 bytes [1.00 TB]\nSector Size:      512 bytes logical/physical\nRotation Rate:    7200 rpm\nForm Factor:      3.5 inches\nDevice is:        In smartctl database 7.3/5319\nATA Version is:   ATA8-ACS T13/1699-D revision 6\nSATA Version is:  SATA 2.6, 3.0 Gb/s\nLocal Time is:    Sun Mar  3 18:39:54 2024 CET\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\nAAM feature is:   Disabled\nAPM feature is:   Disabled\nRd look-ahead is: Enabled\nWrite cache is:   Enabled\nDSN feature is:   Unavailable\nATA Security is:  Disabled, frozen [SEC2]\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x80) Offline data collection activity\n                    was never started.\n                    Auto Offline Data Collection: Enabled.\nSelf-test execution status:      ( 114) The previous self-test completed having\n                    the read element of the test failed.\nTotal time to complete Offline \ndata collection:        ( 9180) seconds.\nOffline data collection\ncapabilities:            (0x5b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    No Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    ( 153) minutes.\nSCT capabilities:          (0x003f) SCT Status supported.\n                    SCT Error Recovery Control supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n  1 Raw_Read_Error_Rate     POSR-K   100   100   051    -    2\n  2 Throughput_Performance  -OS--K   252   252   000    -    0\n  3 Spin_Up_Time            PO---K   069   068   025    -    9426\n  4 Start_Stop_Count        -O--CK   089   089   000    -    11964\n  5 Reallocated_Sector_Ct   PO--CK   252   252   010    -    0\n  7 Seek_Error_Rate         -OSR-K   252   252   051    -    0\n  8 Seek_Time_Performance   --S--K   252   252   015    -    0\n  9 Power_On_Hours          -O--CK   100   100   000    -    21330\n 10 Spin_Retry_Count        -O--CK   252   252   051    -    0\n 11 Calibration_Retry_Count -O--CK   252   252   000    -    0\n 12 Power_Cycle_Count       -O--CK   094   094   000    -    6486\n191 G-Sense_Error_Rate      -O---K   100   100   000    -    100\n192 Power-Off_Retract_Count -O---K   252   252   000    -    0\n194 Temperature_Celsius     -O----   064   055   000    -    36 (Min/Max 11/46)\n195 Hardware_ECC_Recovered  -O-RCK   100   100   000    -    0\n196 Reallocated_Event_Count -O--CK   252   252   000    -    0\n197 Current_Pending_Sector  -O--CK   100   100   000    -    1\n198 Offline_Uncorrectable   ----CK   252   252   000    -    0\n199 UDMA_CRC_Error_Count    -OS-CK   100   100   000    -    15\n200 Multi_Zone_Error_Rate   -O-R-K   100   100   000    -    584\n223 Load_Retry_Count        -O--CK   252   252   000    -    0\n225 Load_Cycle_Count        -O--CK   099   099   000    -    13285\n                            ||||||_ K auto-keep\n                            |||||__ C event count\n                            ||||___ R error rate\n                            |||____ S speed/performance\n                            ||_____ O updated online\n                            |______ P prefailure warning\n\nGeneral Purpose Log Directory Version 1\nSMART           Log Directory Version 1 [multi-sector log support]\nAddress    Access  R/W   Size  Description\n0x00       GPL,SL  R/O      1  Log Directory\n0x01           SL  R/O      1  Summary SMART error log\n0x02           SL  R/O      2  Comprehensive SMART error log\n0x03       GPL     R/O      2  Ext. Comprehensive SMART error log\n0x06           SL  R/O      1  SMART self-test log\n0x07       GPL     R/O      2  Extended self-test log\n0x08       GPL     R/O      2  Power Conditions log\n0x09           SL  R/W      1  Selective self-test log\n0x10       GPL     R/O      1  NCQ Command Error log\n0x11       GPL     R/O      1  SATA Phy Event Counters log\n0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n0xe0       GPL,SL  R/W      1  SCT Command/Status\n0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\nSMART Extended Comprehensive Error Log Version: 1 (2 sectors)\nDevice Error Count: 16 (device log contains only the most recent 8 errors)\n    CR     = Command Register\n    FEATR  = Features Register\n    COUNT  = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    LH     = LBA High (was: Cylinder High) Register    ]   LBA\n    LM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    LL     = LBA Low (was: Sector Number) Register     ]\n    DV     = Device (was: Device/Head) Register\n    DC     = Device Control Register\n    ER     = Error register\n    ST     = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\n\nError 16 [7] occurred at disk power-on lifetime: 9927 hours (413 days + 15 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  84 -- 51 16 50 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  61 00 00 00 08 00 00 00 5f 16 48 40 00     00:00:06.036  WRITE FPDMA QUEUED\n  61 00 00 00 08 00 00 00 5f 16 48 40 00     00:00:06.130  WRITE FPDMA QUEUED\n  61 00 00 00 01 00 00 06 44 67 a0 40 00     00:00:06.130  WRITE FPDMA QUEUED\n  ea 00 00 00 00 00 00 00 00 00 00 e0 00     00:00:06.130  FLUSH CACHE EXT\n  61 00 00 00 08 00 00 00 5f 07 48 40 00     00:00:06.130  WRITE FPDMA QUEUED\n\nError 15 [6] occurred at disk power-on lifetime: 9610 hours (400 days + 10 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  84 -- 51 79 da 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 00 00 00 10 00 00 00 8a 79 da 40 00     00:00:00.107  READ FPDMA QUEUED\n  60 00 00 00 20 00 00 00 8a 79 ca 40 00     00:00:00.108  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 00 69 3f 50 40 00     00:00:00.108  READ FPDMA QUEUED\n  ea 00 00 00 00 00 00 00 00 00 00 e0 00     00:00:00.108  FLUSH CACHE EXT\n  60 00 00 00 40 00 00 05 1f 73 00 40 00     00:00:00.108  READ FPDMA QUEUED\n\nError 14 [5] occurred at disk power-on lifetime: 8774 hours (365 days + 14 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  84 -- 51 bd 48 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 00 00 00 00 00 00 10 bf bd 50 40 00     00:00:00.059  READ FPDMA QUEUED\n  60 00 00 00 0f 00 00 01 c2 b5 0c 40 00     00:00:00.059  READ FPDMA QUEUED\n  60 00 00 00 20 00 00 00 c8 99 ba 40 00     00:00:00.059  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 10 bf bd 48 40 00     00:00:00.059  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 10 bf d3 b8 40 00     00:00:00.059  READ FPDMA QUEUED\n\nError 13 [4] occurred at disk power-on lifetime: 8674 hours (361 days + 10 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  84 -- 51 52 80 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 00 00 00 30 00 00 03 92 52 60 40 00     00:00:00.084  READ FPDMA QUEUED\n  60 00 00 00 40 00 00 03 92 52 50 40 00     00:00:00.085  READ FPDMA QUEUED\n  60 00 00 00 c8 00 00 02 6d 90 a2 40 00     00:00:00.085  READ FPDMA QUEUED\n  60 00 00 01 00 00 00 02 6d 8f a2 40 00     00:00:00.085  READ FPDMA QUEUED\n  60 00 00 01 00 00 00 02 6d 8e a2 40 00     00:00:00.085  READ FPDMA QUEUED\n\nError 12 [3] occurred at disk power-on lifetime: 8672 hours (361 days + 8 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  84 -- 51 9b 60 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 00 00 00 70 00 00 00 10 9b 00 40 00     00:00:00.049  READ FPDMA QUEUED\n  60 00 00 00 80 00 00 00 10 9a f0 40 00     00:00:00.050  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 00 03 b1 38 40 00     00:00:00.050  READ FPDMA QUEUED\n  61 00 00 01 00 00 00 08 a5 c5 f8 40 00     00:00:00.050  WRITE FPDMA QUEUED\n  61 00 00 01 00 00 00 08 a5 c4 f8 40 00     00:00:00.050  WRITE FPDMA QUEUED\n\nError 11 [2] occurred at disk power-on lifetime: 8419 hours (350 days + 19 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  84 -- 51 75 00 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 00 00 00 00 00 00 00 bf 75 10 40 00     00:00:00.035  READ FPDMA QUEUED\n  60 00 00 00 10 00 00 00 bf 75 00 40 00     00:00:00.035  READ FPDMA QUEUED\n  60 00 00 00 80 00 00 00 40 93 58 40 00     00:00:00.035  READ FPDMA QUEUED\n  ea 00 00 00 00 00 00 00 00 00 00 e0 00     00:00:00.035  FLUSH CACHE EXT\n  60 00 00 00 78 00 00 00 c5 b3 10 40 00     00:00:00.035  READ FPDMA QUEUED\n\nError 10 [1] occurred at disk power-on lifetime: 8263 hours (344 days + 7 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  84 -- 51 80 a8 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 00 00 00 00 00 00 06 13 80 b0 40 00     00:00:00.114  READ FPDMA QUEUED\n  61 00 00 00 20 00 00 02 22 83 20 40 00     00:00:00.114  WRITE FPDMA QUEUED\n  60 00 00 00 08 00 00 05 54 55 b8 40 00     00:00:00.114  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 06 13 80 a8 40 00     00:00:00.114  READ FPDMA QUEUED\n  60 00 00 00 18 00 00 02 2a 96 0a 40 00     00:00:00.114  READ FPDMA QUEUED\n\nError 9 [0] occurred at disk power-on lifetime: 8158 hours (339 days + 22 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  84 -- 51 3e 70 00 00 00 00 00 00 40 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 00 00 00 00 00 00 45 fc 3e 78 40 00     00:00:00.150  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 45 fc 3e 70 40 00     00:00:00.151  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 13 01 88 68 40 00     00:00:00.151  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 00 68 33 c0 40 00     00:00:00.151  READ FPDMA QUEUED\n  60 00 00 00 08 00 00 10 bb 97 10 40 00     00:00:00.151  READ FPDMA QUEUED\n\nSMART Extended Self-test Log Version: 1 (2 sectors)\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Extended offline    Completed: read failure       20%     21330         1886208607\n\nSMART Selective self-test log data structure revision number 0\nNote: revision number not 1 implies that no selective self-test has ever been run\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Completed_read_failure [20% left] (0-65535)\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\nSCT Status Version:                  2\nSCT Version (vendor specific):       256 (0x0100)\nDevice State:                        Active (0)\nCurrent Temperature:                    36 Celsius\nPower Cycle Min/Max Temperature:     19/38 Celsius\nLifetime    Min/Max Temperature:     13/63 Celsius\nSpecified Max Operating Temperature:    80 Celsius\nUnder/Over Temperature Limit Count:   0/0\n\nSCT Temperature History Version:     2\nTemperature Sampling Period:         5 minutes\nTemperature Logging Interval:        5 minutes\nMin/Max recommended Temperature:     -5/80 Celsius\nMin/Max Temperature Limit:           -10/85 Celsius\nTemperature History Size (Index):    128 (98)\n\nIndex    Estimated Time   Temperature Celsius\n  99    2024-03-03 08:00    26  *******\n 100    2024-03-03 08:05    27  ********\n 101    2024-03-03 08:10    28  *********\n 102    2024-03-03 08:15    21  **\n 103    2024-03-03 08:20    24  *****\n 104    2024-03-03 08:25    26  *******\n 105    2024-03-03 08:30    27  ********\n 106    2024-03-03 08:35    28  *********\n 107    2024-03-03 08:40    29  **********\n 108    2024-03-03 08:45    26  *******\n 109    2024-03-03 08:50    27  ********\n 110    2024-03-03 08:55    28  *********\n 111    2024-03-03 09:00    28  *********\n 112    2024-03-03 09:05    22  ***\n 113    2024-03-03 09:10    24  *****\n 114    2024-03-03 09:15    26  *******\n 115    2024-03-03 09:20    27  ********\n 116    2024-03-03 09:25    28  *********\n 117    2024-03-03 09:30    24  *****\n 118    2024-03-03 09:35    25  ******\n 119    2024-03-03 09:40    27  ********\n 120    2024-03-03 09:45    21  **\n 121    2024-03-03 09:50    24  *****\n 122    2024-03-03 09:55    25  ******\n 123    2024-03-03 10:00    26  *******\n 124    2024-03-03 10:05    27  ********\n 125    2024-03-03 10:10    28  *********\n 126    2024-03-03 10:15    28  *********\n 127    2024-03-03 10:20    29  **********\n ...    ..(  2 skipped).    ..  **********\n   2    2024-03-03 10:35    29  **********\n   3    2024-03-03 10:40    30  ***********\n   4    2024-03-03 10:45    30  ***********\n   5    2024-03-03 10:50    31  ************\n ...    ..(  2 skipped).    ..  ************\n   8    2024-03-03 11:05    31  ************\n   9    2024-03-03 11:10    32  *************\n ...    ..(  6 skipped).    ..  *************\n  16    2024-03-03 11:45    32  *************\n  17    2024-03-03 11:50    31  ************\n  18    2024-03-03 11:55    30  ***********\n ...    ..(  2 skipped).    ..  ***********\n  21    2024-03-03 12:10    30  ***********\n  22    2024-03-03 12:15    24  *****\n  23    2024-03-03 12:20    26  *******\n  24    2024-03-03 12:25    27  ********\n  25    2024-03-03 12:30    29  **********\n  26    2024-03-03 12:35    30  ***********\n  27    2024-03-03 12:40    23  ****\n  28    2024-03-03 12:45    25  ******\n  29    2024-03-03 12:50    26  *******\n  30    2024-03-03 12:55    27  ********\n  31    2024-03-03 13:00    25  ******\n  32    2024-03-03 13:05    26  *******\n  33    2024-03-03 13:10    28  *********\n  34    2024-03-03 13:15    21  **\n  35    2024-03-03 13:20    24  *****\n  36    2024-03-03 13:25    26  *******\n  37    2024-03-03 13:30    27  ********\n  38    2024-03-03 13:35    28  *********\n  39    2024-03-03 13:40    29  **********\n  40    2024-03-03 13:45    29  **********\n  41    2024-03-03 13:50    23  ****\n  42    2024-03-03 13:55    25  ******\n  43    2024-03-03 14:00    27  ********\n  44    2024-03-03 14:05    28  *********\n  45    2024-03-03 14:10    26  *******\n  46    2024-03-03 14:15    27  ********\n  47    2024-03-03 14:20    28  *********\n  48    2024-03-03 14:25    28  *********\n  49    2024-03-03 14:30    24  *****\n  50    2024-03-03 14:35    25  ******\n  51    2024-03-03 14:40    26  *******\n  52    2024-03-03 14:45    19  -\n  53    2024-03-03 14:50    22  ***\n  54    2024-03-03 14:55    24  *****\n  55    2024-03-03 15:00    26  *******\n  56    2024-03-03 15:05    27  ********\n  57    2024-03-03 15:10    28  *********\n  58    2024-03-03 15:15    29  **********\n  59    2024-03-03 15:20    30  ***********\n  60    2024-03-03 15:25    30  ***********\n  61    2024-03-03 15:30    31  ************\n  62    2024-03-03 15:35    31  ************\n  63    2024-03-03 15:40    32  *************\n ...    ..(  3 skipped).    ..  *************\n  67    2024-03-03 16:00    32  *************\n  68    2024-03-03 16:05    33  **************\n ...    ..( 10 skipped).    ..  **************\n  79    2024-03-03 17:00    33  **************\n  80    2024-03-03 17:05    34  ***************\n  81    2024-03-03 17:10    36  *****************\n  82    2024-03-03 17:15    37  ******************\n  83    2024-03-03 17:20    37  ******************\n  84    2024-03-03 17:25    37  ******************\n  85    2024-03-03 17:30    38  *******************\n ...    ..(  6 skipped).    ..  *******************\n  92    2024-03-03 18:05    38  *******************\n  93    2024-03-03 18:10    37  ******************\n ...    ..(  3 skipped).    ..  ******************\n  97    2024-03-03 18:30    37  ******************\n  98    2024-03-03 18:35    36  *****************\n\nSCT Error Recovery Control:\n           Read: Disabled\n          Write: Disabled\n\nDevice Statistics (GP/SMART Log 0x04) not supported\n\nSATA Phy Event Counters (GP Log 0x11)\nID      Size     Value  Description\n0x0001  4            0  Command failed due to ICRC error\n0x0002  4            0  R_ERR response for data FIS\n0x0003  4            0  R_ERR response for device-to-host data FIS\n0x0004  4            0  R_ERR response for host-to-device data FIS\n0x0005  4            0  R_ERR response for non-data FIS\n0x0006  4            0  R_ERR response for device-to-host non-data FIS\n0x0007  4            0  R_ERR response for host-to-device non-data FIS\n0x0008  4            0  Device-to-host non-data FIS retries\n0x0009  4           11  Transition from drive PhyRdy to drive PhyNRdy\n0x000a  4           10  Device-to-host register FISes sent due to a COMRESET\n0x000b  4            0  CRC errors within host-to-device FIS\n0x000d  4            0  Non-CRC errors within host-to-device FIS\n0x000f  4            0  R_ERR response for host-to-device data FIS, CRC\n0x0010  4            0  R_ERR response for host-to-device data FIS, non-CRC\n0x0012  4            0  R_ERR response for host-to-device non-data FIS, CRC\n0x0013  4            0  R_ERR response for host-to-device non-data FIS, non-CRC\n0x8e00  4            0  Vendor specific\n0x8e01  4            0  Vendor specific\n0x8e02  4            0  Vendor specific\n0x8e03  4            0  Vendor specific\n0x8e04  4            0  Vendor specific\n0x8e05  4            0  Vendor specific\n0x8e06  4            0  Vendor specific\n0x8e07  4            0  Vendor specific\n0x8e08  4            0  Vendor specific\n0x8e09  4            0  Vendor specific\n0x8e0a  4            0  Vendor specific\n0x8e0b  4            0  Vendor specific\n0x8e0c  4            0  Vendor specific\n0x8e0d  4            0  Vendor specific\n0x8e0e  4            0  Vendor specific\n0x8e0f  4            0  Vendor specific\n0x8e10  4            0  Vendor specific\n0x8e11  4            0  Vendor specific\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7cyc6", "is_robot_indexable": true, "report_reasons": null, "author": "TheLeoDeveloper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7cyc6/can_i_use_this_hard_drive_for_raid_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7cyc6/can_i_use_this_hard_drive_for_raid_1/", "subreddit_subscribers": 736399, "created_utc": 1709664245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a photo storage option that meets these requirements:\n\n\\- Has a functional app for iOS (phone and iPad) and an app for Windows or web interface to upload/ organize\n\n\\- Allows for keyword tagging of photos to aid in searching\n\n\\- Decent face recognition/ AI and ability to tag people even if the AI doesn't pick up a face\n\n\\- Nice to have, not required - \"stacking\" or grouping of the same photo in JPEG and RAW so every photo isn't duplicated in the app/view.\n\nI was going to use Amazon Photos as it's unlimited and free with Prime which I already have, but I can't seem to figure out any way to tag photos with key words and the auto-tagging so far is awful (e.g., searching \"dog\" returns pictures of cats). The face recognition so far isn't great either and I can't figure out how to manually tag someone if a face isn't detected. Any experience using Amazon Photos and if the above is possible but I'm not seeing it?\n\nI can't really use iCloud for photo storage as my mobile is through work and they disable photo sync with iCloud, so that option is probably out.\n\nIs Google Photos materially better than Amazon Photos and does it have the capability I'm looking for? Are there any other options? What about an app with these features that works with storage providers? I saw this Photos+ app but not sure if the features match what I'm looking for.\n\n&amp;#x200B;", "author_fullname": "t2_jks02", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for photo storage with certain requirements?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b76qsu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709660053.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709649562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a photo storage option that meets these requirements:&lt;/p&gt;\n\n&lt;p&gt;- Has a functional app for iOS (phone and iPad) and an app for Windows or web interface to upload/ organize&lt;/p&gt;\n\n&lt;p&gt;- Allows for keyword tagging of photos to aid in searching&lt;/p&gt;\n\n&lt;p&gt;- Decent face recognition/ AI and ability to tag people even if the AI doesn&amp;#39;t pick up a face&lt;/p&gt;\n\n&lt;p&gt;- Nice to have, not required - &amp;quot;stacking&amp;quot; or grouping of the same photo in JPEG and RAW so every photo isn&amp;#39;t duplicated in the app/view.&lt;/p&gt;\n\n&lt;p&gt;I was going to use Amazon Photos as it&amp;#39;s unlimited and free with Prime which I already have, but I can&amp;#39;t seem to figure out any way to tag photos with key words and the auto-tagging so far is awful (e.g., searching &amp;quot;dog&amp;quot; returns pictures of cats). The face recognition so far isn&amp;#39;t great either and I can&amp;#39;t figure out how to manually tag someone if a face isn&amp;#39;t detected. Any experience using Amazon Photos and if the above is possible but I&amp;#39;m not seeing it?&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t really use iCloud for photo storage as my mobile is through work and they disable photo sync with iCloud, so that option is probably out.&lt;/p&gt;\n\n&lt;p&gt;Is Google Photos materially better than Amazon Photos and does it have the capability I&amp;#39;m looking for? Are there any other options? What about an app with these features that works with storage providers? I saw this Photos+ app but not sure if the features match what I&amp;#39;m looking for.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b76qsu", "is_robot_indexable": true, "report_reasons": null, "author": "j-rad4", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b76qsu/recommendations_for_photo_storage_with_certain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b76qsu/recommendations_for_photo_storage_with_certain/", "subreddit_subscribers": 736399, "created_utc": 1709649562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I hope this is the right place to ask this question. If not, my apologies. Also, I'm currently at work and can provide more specific information later in the day.\n\nI have a 90TB Plex server running off of 5 HDDs; 4 are Seagate Exos and the other is a WD shucked drive. Everything ran incredibly smooth before moving over to the DrivePool, having them all as individual drives. I was able to DirectPlay and Transcode 4K Remuxes without issue. I was tired of having 5 different drives and read about DrivePool and thought it met my needs. I set it up without issue and moved everything to the new drive....but now EVERYTHING struggles to be read/written to the drive.\n\nI've ran CrystalDiskMark on all the individual drives, and only one (the WD) has a slower speed (relative to the Exos). While attempting to stream, watching the StreamBit program, the read speed jumps all over and isn't a consistent speed AT ALL. It will start at 125MB/s, and then drop to something insanely low like 75KB/s. Reading isn't the only problem either, as I have SABnzbd set to download to the drive and it states the download speed is limited by writing to the Hard drive.\n\nAs I said, I'm at work and can certainly send screenshots of the benchmarks from CrystalDiskMark when I get home, or any other information to help shed some more light on this issue. But if there's any suggestions out there, I'd greatly appreciate it! ", "author_fullname": "t2_nj62oupr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DrivePool seems to be struggling to read movies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b76nf5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709649312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this is the right place to ask this question. If not, my apologies. Also, I&amp;#39;m currently at work and can provide more specific information later in the day.&lt;/p&gt;\n\n&lt;p&gt;I have a 90TB Plex server running off of 5 HDDs; 4 are Seagate Exos and the other is a WD shucked drive. Everything ran incredibly smooth before moving over to the DrivePool, having them all as individual drives. I was able to DirectPlay and Transcode 4K Remuxes without issue. I was tired of having 5 different drives and read about DrivePool and thought it met my needs. I set it up without issue and moved everything to the new drive....but now EVERYTHING struggles to be read/written to the drive.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve ran CrystalDiskMark on all the individual drives, and only one (the WD) has a slower speed (relative to the Exos). While attempting to stream, watching the StreamBit program, the read speed jumps all over and isn&amp;#39;t a consistent speed AT ALL. It will start at 125MB/s, and then drop to something insanely low like 75KB/s. Reading isn&amp;#39;t the only problem either, as I have SABnzbd set to download to the drive and it states the download speed is limited by writing to the Hard drive.&lt;/p&gt;\n\n&lt;p&gt;As I said, I&amp;#39;m at work and can certainly send screenshots of the benchmarks from CrystalDiskMark when I get home, or any other information to help shed some more light on this issue. But if there&amp;#39;s any suggestions out there, I&amp;#39;d greatly appreciate it! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b76nf5", "is_robot_indexable": true, "report_reasons": null, "author": "DiscussionNo226", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b76nf5/drivepool_seems_to_be_struggling_to_read_movies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b76nf5/drivepool_seems_to_be_struggling_to_read_movies/", "subreddit_subscribers": 736399, "created_utc": 1709649312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have few Tera of data but spread across external hard drives, remote clouds etc.\n\nI'm looking for some solution that allows me create an index of such resources that can be searched when devices are not connected or remote clouds are not reachable.\n\nEdit: I'm using mac/linux and it would be awesome if the solution would be web-based so that I can self-hosting it.\n\nIn other words I'd like to have a sofware where I can search for a specific filename or metadata and it shows where that file is stored.", "author_fullname": "t2_4mkbv07v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Keep track of content of remote archives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b703oo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709626766.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709626088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have few Tera of data but spread across external hard drives, remote clouds etc.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for some solution that allows me create an index of such resources that can be searched when devices are not connected or remote clouds are not reachable.&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;m using mac/linux and it would be awesome if the solution would be web-based so that I can self-hosting it.&lt;/p&gt;\n\n&lt;p&gt;In other words I&amp;#39;d like to have a sofware where I can search for a specific filename or metadata and it shows where that file is stored.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b703oo", "is_robot_indexable": true, "report_reasons": null, "author": "not-the-real-chopin", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b703oo/keep_track_of_content_of_remote_archives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b703oo/keep_track_of_content_of_remote_archives/", "subreddit_subscribers": 736399, "created_utc": 1709626088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI'm looking to purchase a Broadcom HBA 9500-8i Tri-Mode Storage Controller and came across a listing on eBay from seller \"hbr2015\" ([link to listing](https://www.ebay.com/itm/134734908059)).\n\nI'm interested in the controller but wanted to check with the community before making a purchase. Does anyone have any experience using this specific model (Broadcom HBA 9500-8i)?\n\nAdditionally, if anyone has purchased from seller \"hbr2015\" in the past, I'd appreciate any feedback you can share about your experience.\n\nThanks in advance for any insights!\n\n  \nEDIT:\n\nBroadcom GreyMarket: [https://www.broadcom.com/support/counterfeit-statement](https://www.broadcom.com/support/counterfeit-statement)", "author_fullname": "t2_9u74iu2x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone had experience with the Broadcom HBA 9500-8i Tri-Mode Controller (Seller: hbr2015)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6znfj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709647330.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709624230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to purchase a Broadcom HBA 9500-8i Tri-Mode Storage Controller and came across a listing on eBay from seller &amp;quot;hbr2015&amp;quot; (&lt;a href=\"https://www.ebay.com/itm/134734908059\"&gt;link to listing&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in the controller but wanted to check with the community before making a purchase. Does anyone have any experience using this specific model (Broadcom HBA 9500-8i)?&lt;/p&gt;\n\n&lt;p&gt;Additionally, if anyone has purchased from seller &amp;quot;hbr2015&amp;quot; in the past, I&amp;#39;d appreciate any feedback you can share about your experience.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any insights!&lt;/p&gt;\n\n&lt;p&gt;EDIT:&lt;/p&gt;\n\n&lt;p&gt;Broadcom GreyMarket: &lt;a href=\"https://www.broadcom.com/support/counterfeit-statement\"&gt;https://www.broadcom.com/support/counterfeit-statement&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uwXx2h_-cZPpDIzH7QGe0aC_lZryD6KRJqiO79ECHUY.jpg?auto=webp&amp;s=dee58aaacbeda15effcfd8c776d48481194bb1b4", "width": 400, "height": 318}, "resolutions": [{"url": "https://external-preview.redd.it/uwXx2h_-cZPpDIzH7QGe0aC_lZryD6KRJqiO79ECHUY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dece771ec241c18eaec3226a57a3eb9274be7710", "width": 108, "height": 85}, {"url": "https://external-preview.redd.it/uwXx2h_-cZPpDIzH7QGe0aC_lZryD6KRJqiO79ECHUY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b8326dd57b767e3a17502f2ca1dd01cc4636443", "width": 216, "height": 171}, {"url": "https://external-preview.redd.it/uwXx2h_-cZPpDIzH7QGe0aC_lZryD6KRJqiO79ECHUY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df51d643566c744835b0811202e840cccb2a6f6e", "width": 320, "height": 254}], "variants": {}, "id": "7ZhnX6MIBswCCj-bSDAGDsPD0HJd10Pi25BwdJxnfTA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b6znfj", "is_robot_indexable": true, "report_reasons": null, "author": "vkartk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b6znfj/has_anyone_had_experience_with_the_broadcom_hba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b6znfj/has_anyone_had_experience_with_the_broadcom_hba/", "subreddit_subscribers": 736399, "created_utc": 1709624230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Tried FreeFileSync and Rsync but this doesn't require any external programs.\n\nLast letter in the bat-file is the destination dir! F2 in Windows is rename, just in case.\n\nAny ideas for tweaks?\n\n\"Run as Administrator\" changes directory to c:\\\\windows\\\\system32\\\\ so can't use %cd% (CurrentDirectory) so %\\~dp0 has to be used and it works!\n\nDoesn't work without chopping of the last backslash hence the \\~0,-1%.\n\nSuper fast in windows just right-click and press \"e\" and \"unrem\" the /MIR **sober** for clearing out Cubase back-ups etc.\n\n\"robocopy 2 H.bat\":\n\n    @echo off\n    set filename=%~n0\n    set driveletter=%filename:~-1,1%\n    set batfiledir=%~dp0\n    set batfiledir=%batfiledir:~0,-1%\n    set destdir=%driveletter%%batfiledir:~1%\n    @echo on\n    \n    robocopy /E \"%batfiledir%\" \"%destdir%\" /ETA\n    rem robocopy /MIR \"%batfiledir%\" \"%destdir%\" /ETA\n    \n    pause", "author_fullname": "t2_a4cdqy3hc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "bat file for Robocopy syncing in windows.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6x9xg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709616151.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709615708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried FreeFileSync and Rsync but this doesn&amp;#39;t require any external programs.&lt;/p&gt;\n\n&lt;p&gt;Last letter in the bat-file is the destination dir! F2 in Windows is rename, just in case.&lt;/p&gt;\n\n&lt;p&gt;Any ideas for tweaks?&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Run as Administrator&amp;quot; changes directory to c:\\windows\\system32\\ so can&amp;#39;t use %cd% (CurrentDirectory) so %~dp0 has to be used and it works!&lt;/p&gt;\n\n&lt;p&gt;Doesn&amp;#39;t work without chopping of the last backslash hence the ~0,-1%.&lt;/p&gt;\n\n&lt;p&gt;Super fast in windows just right-click and press &amp;quot;e&amp;quot; and &amp;quot;unrem&amp;quot; the /MIR &lt;strong&gt;sober&lt;/strong&gt; for clearing out Cubase back-ups etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;robocopy 2 H.bat&amp;quot;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@echo off\nset filename=%~n0\nset driveletter=%filename:~-1,1%\nset batfiledir=%~dp0\nset batfiledir=%batfiledir:~0,-1%\nset destdir=%driveletter%%batfiledir:~1%\n@echo on\n\nrobocopy /E &amp;quot;%batfiledir%&amp;quot; &amp;quot;%destdir%&amp;quot; /ETA\nrem robocopy /MIR &amp;quot;%batfiledir%&amp;quot; &amp;quot;%destdir%&amp;quot; /ETA\n\npause\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b6x9xg", "is_robot_indexable": true, "report_reasons": null, "author": "scatkang", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b6x9xg/bat_file_for_robocopy_syncing_in_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b6x9xg/bat_file_for_robocopy_syncing_in_windows/", "subreddit_subscribers": 736399, "created_utc": 1709615708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I'd like to start hoarding some data :) ... \n\nI have an ASRock IMB-180 board (with i7 4810MQ) and a spare Corsair SF600 PSU) which I'd like to use solely for my home storage. For other purposes have a Lenovo M720q used as a Jellyfin server, Homebridge etc. (well actually currently it's out of order :D ) ...\n\nThere's not much space in the apartment where I live and therefore the only place where I can put my NAS would be on the shelf of my TV stand.\n\nThe problem is that there's just 165mm height clearance.\n\nI looked at some SilverStone cases but it doesn't seem like they can take 4 3.5\" HDDs.\n\nDoes anybody know about such case? Or is a DIY the only way here? \n\nAny help is appreciated", "author_fullname": "t2_773hioz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case with support for SFX PSU, 4x 3.5\" HDD, under 160mm in height", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7fts5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hardware", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709671000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I&amp;#39;d like to start hoarding some data :) ... &lt;/p&gt;\n\n&lt;p&gt;I have an ASRock IMB-180 board (with i7 4810MQ) and a spare Corsair SF600 PSU) which I&amp;#39;d like to use solely for my home storage. For other purposes have a Lenovo M720q used as a Jellyfin server, Homebridge etc. (well actually currently it&amp;#39;s out of order :D ) ...&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s not much space in the apartment where I live and therefore the only place where I can put my NAS would be on the shelf of my TV stand.&lt;/p&gt;\n\n&lt;p&gt;The problem is that there&amp;#39;s just 165mm height clearance.&lt;/p&gt;\n\n&lt;p&gt;I looked at some SilverStone cases but it doesn&amp;#39;t seem like they can take 4 3.5&amp;quot; HDDs.&lt;/p&gt;\n\n&lt;p&gt;Does anybody know about such case? Or is a DIY the only way here? &lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b7fts5", "is_robot_indexable": true, "report_reasons": null, "author": "SeaAbalone818", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7fts5/case_with_support_for_sfx_psu_4x_35_hdd_under/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7fts5/case_with_support_for_sfx_psu_4x_35_hdd_under/", "subreddit_subscribers": 736399, "created_utc": 1709671000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, my boyfriend and I have been poking the data storage gods in the eye for a lonnnnng time with a woefully inadequate backup solution, so we've decided to get two identical Dell PowerEdge R710 servers and put 6 Hitachi Ultrastar He12 12Tb drives in each of them. They'll be running Arch Linux.\n\nWhat we plan to do is copy everything we store to both of them, meaning, as one of our friendly worker bots acquires a file, it is copied to both the main server and the mirror/backup server. Ideally the two servers will be identical mirrors at all times.\n\nAny thoughts on how to do this? Thanks!", "author_fullname": "t2_jxrwr82a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some real-time file mirroring advice, please", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7f77w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709669508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, my boyfriend and I have been poking the data storage gods in the eye for a lonnnnng time with a woefully inadequate backup solution, so we&amp;#39;ve decided to get two identical Dell PowerEdge R710 servers and put 6 Hitachi Ultrastar He12 12Tb drives in each of them. They&amp;#39;ll be running Arch Linux.&lt;/p&gt;\n\n&lt;p&gt;What we plan to do is copy everything we store to both of them, meaning, as one of our friendly worker bots acquires a file, it is copied to both the main server and the mirror/backup server. Ideally the two servers will be identical mirrors at all times.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts on how to do this? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b7f77w", "is_robot_indexable": true, "report_reasons": null, "author": "renarde33", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b7f77w/need_some_realtime_file_mirroring_advice_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b7f77w/need_some_realtime_file_mirroring_advice_please/", "subreddit_subscribers": 736399, "created_utc": 1709669508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Datablocks.dev](https://Datablocks.dev) is selling a few 20TB Ironwolf Pro White label drives for 230 euro, making them quite cheap:\n\n[https://datablocks.dev/products/seagate-ironwolf-pro-20-tb-sata-white-label-hard-drive](https://datablocks.dev/products/seagate-ironwolf-pro-20-tb-sata-white-label-hard-drive)", "author_fullname": "t2_uo3ft", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20TB Ironwolf Pro White label 230 euro (11,5 euro pr TB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b72lf7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709636456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://Datablocks.dev\"&gt;Datablocks.dev&lt;/a&gt; is selling a few 20TB Ironwolf Pro White label drives for 230 euro, making them quite cheap:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://datablocks.dev/products/seagate-ironwolf-pro-20-tb-sata-white-label-hard-drive\"&gt;https://datablocks.dev/products/seagate-ironwolf-pro-20-tb-sata-white-label-hard-drive&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1b72lf7", "is_robot_indexable": true, "report_reasons": null, "author": "djandDK", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b72lf7/20tb_ironwolf_pro_white_label_230_euro_115_euro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b72lf7/20tb_ironwolf_pro_white_label_230_euro_115_euro/", "subreddit_subscribers": 736399, "created_utc": 1709636456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an ancient Seagate Business NASS (n090401), which has faithfully served me. I've replaced one of the 4TB drives and the firmware is up to date. But I was wondering if I can upgrade the drives to something larger but given the age I don't know what the largest drive it would support. Any suggestions?", "author_fullname": "t2_tkgut", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upgrading legacy SeaGate Business NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b70lw2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709628205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an ancient Seagate Business NASS (n090401), which has faithfully served me. I&amp;#39;ve replaced one of the 4TB drives and the firmware is up to date. But I was wondering if I can upgrade the drives to something larger but given the age I don&amp;#39;t know what the largest drive it would support. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b70lw2", "is_robot_indexable": true, "report_reasons": null, "author": "seanhvw", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b70lw2/upgrading_legacy_seagate_business_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b70lw2/upgrading_legacy_seagate_business_nas/", "subreddit_subscribers": 736399, "created_utc": 1709628205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone tried using a personal Dropbox/Gdrive solution? I'm not entirely sure how to set it up but I'm planning to learn and I'm not sure which to go with.\n\nMy colleague mentioned that he rents his own private vps for $5/month just for the sole purpose of hosting Seafile, but it seems that this sub seems to have a preference for NextCloud. Am curious as to why NextCloud is more popular here, what are some advantages/disadvantages to weigh?", "author_fullname": "t2_e1qv9dge", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NextCloud vs Seafile", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b70eij", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709627370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tried using a personal Dropbox/Gdrive solution? I&amp;#39;m not entirely sure how to set it up but I&amp;#39;m planning to learn and I&amp;#39;m not sure which to go with.&lt;/p&gt;\n\n&lt;p&gt;My colleague mentioned that he rents his own private vps for $5/month just for the sole purpose of hosting Seafile, but it seems that this sub seems to have a preference for NextCloud. Am curious as to why NextCloud is more popular here, what are some advantages/disadvantages to weigh?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b70eij", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed_Allele", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b70eij/nextcloud_vs_seafile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b70eij/nextcloud_vs_seafile/", "subreddit_subscribers": 736399, "created_utc": 1709627370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\nWith the recent events of Yuzu's repos being taken down (and tachiyomi before that), I see a lot of guys steuggling to get a full copy of their repos. So I just wanted to share this site (not an advertisement) that I discovered not so long ago, and that is similar to archive.org but for code repos:\n\nhttps://archive.softwareheritage.org/browse\n\nThe time needed to download data is quite slow but I always found any repo that I was searching for.", "author_fullname": "t2_70t22v0i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Git repos backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b6z5zu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709622269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nWith the recent events of Yuzu&amp;#39;s repos being taken down (and tachiyomi before that), I see a lot of guys steuggling to get a full copy of their repos. So I just wanted to share this site (not an advertisement) that I discovered not so long ago, and that is similar to archive.org but for code repos:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://archive.softwareheritage.org/browse\"&gt;https://archive.softwareheritage.org/browse&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The time needed to download data is quite slow but I always found any repo that I was searching for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b6z5zu", "is_robot_indexable": true, "report_reasons": null, "author": "drosoboeuf", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b6z5zu/git_repos_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b6z5zu/git_repos_backups/", "subreddit_subscribers": 736399, "created_utc": 1709622269.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}