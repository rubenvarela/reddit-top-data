{"kind": "Listing", "data": {"after": "t3_1b3ppn0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I (M20) just had a second round of 1 on 1 session for data engineer trainee in a company. \n\nI was asked to reverse a string in python and I forgot the syntax of while loop. And this one mistake just put me in a downward spiral for the entire hour of the session. So much so that once he asked me if two null values will be equal and I said no, and he asked why but I could not bring myself to be confident enough to say anything about memory addresses even after knowing about it, he asked me about indexing in database and I could only answer it in very simple terms.\n\nI feel really low right now, what can I do to improve and get better at interviewing.", "author_fullname": "t2_9pezqc7t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I bombed the interviuw and feel like the dumbest person in the world", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b34q4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 118, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 118, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709224773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I (M20) just had a second round of 1 on 1 session for data engineer trainee in a company. &lt;/p&gt;\n\n&lt;p&gt;I was asked to reverse a string in python and I forgot the syntax of while loop. And this one mistake just put me in a downward spiral for the entire hour of the session. So much so that once he asked me if two null values will be equal and I said no, and he asked why but I could not bring myself to be confident enough to say anything about memory addresses even after knowing about it, he asked me about indexing in database and I could only answer it in very simple terms.&lt;/p&gt;\n\n&lt;p&gt;I feel really low right now, what can I do to improve and get better at interviewing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b34q4i", "is_robot_indexable": true, "report_reasons": null, "author": "pmme_ur_titsandclits", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b34q4i/i_bombed_the_interviuw_and_feel_like_the_dumbest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b34q4i/i_bombed_the_interviuw_and_feel_like_the_dumbest/", "subreddit_subscribers": 164851, "created_utc": 1709224773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "2 YOE, 2022 CSE grad. Got placed into a data analyst job for 7.7lpa for which the tech stack is Informatica and Qliksense. My team never gave me a chance to work in Informatica projects, so I have been primarily working on just Qliksense which kills me with the shame everyday. Wanted to get SDE job, prepared for few months for it but felt like i need to achieve small goals first instead of aiming too high. Hence I changed the decision to Data engineering and preparing for it.\n\nBut currently my lead has been forcing me to get into functional domain (SAP) which is making me feel like resigning immediately. Also please note that in 2 hike cycles, I have been given only 6% and 4% hike respectively. I have considered myself a good student throughout my life and a pretty decent technical person. How do I deal with my situation now? I want to put papers and study and give interviews peacefully. But that seems not feasible since if I put papers, I still have to do functional work in my 3 months of Notice Period. How do I escape from this situation? Please help me. Please help me to get a direction. I have been thinking of putting papers everyday since last 1 year but I couldn't. This work, these people, this company just disgust me. Please advise. My current job is a waste of time and it's ruining my life.", "author_fullname": "t2_3v896k2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please help me with the urge of resigning every day. My current job is a waste of time.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b2zhni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 54, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 54, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709210755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;2 YOE, 2022 CSE grad. Got placed into a data analyst job for 7.7lpa for which the tech stack is Informatica and Qliksense. My team never gave me a chance to work in Informatica projects, so I have been primarily working on just Qliksense which kills me with the shame everyday. Wanted to get SDE job, prepared for few months for it but felt like i need to achieve small goals first instead of aiming too high. Hence I changed the decision to Data engineering and preparing for it.&lt;/p&gt;\n\n&lt;p&gt;But currently my lead has been forcing me to get into functional domain (SAP) which is making me feel like resigning immediately. Also please note that in 2 hike cycles, I have been given only 6% and 4% hike respectively. I have considered myself a good student throughout my life and a pretty decent technical person. How do I deal with my situation now? I want to put papers and study and give interviews peacefully. But that seems not feasible since if I put papers, I still have to do functional work in my 3 months of Notice Period. How do I escape from this situation? Please help me. Please help me to get a direction. I have been thinking of putting papers everyday since last 1 year but I couldn&amp;#39;t. This work, these people, this company just disgust me. Please advise. My current job is a waste of time and it&amp;#39;s ruining my life.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b2zhni", "is_robot_indexable": true, "report_reasons": null, "author": "pavip51", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b2zhni/please_help_me_with_the_urge_of_resigning_every/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b2zhni/please_help_me_with_the_urge_of_resigning_every/", "subreddit_subscribers": 164851, "created_utc": 1709210755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The data is in a table in redshift. On loading this data we plan to run few ML functions on the data (random forest etc), so doing it in python/pandas is essential. Any ideas/suggestions on how this can be performed ?\nEdit: Tech stack currently does not have spark. We are a small team that operates on pandas and sql", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I load a 70 million row table in a dataframe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3cb1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709243537.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709242979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The data is in a table in redshift. On loading this data we plan to run few ML functions on the data (random forest etc), so doing it in python/pandas is essential. Any ideas/suggestions on how this can be performed ?\nEdit: Tech stack currently does not have spark. We are a small team that operates on pandas and sql&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3cb1t", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3cb1t/how_do_i_load_a_70_million_row_table_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3cb1t/how_do_i_load_a_70_million_row_table_in_a/", "subreddit_subscribers": 164851, "created_utc": 1709242979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently, I did a task for a company. One of the questions was to write a SQL query to get number of recurring customers (those that had a purchase in the 12 months before the current month) vs new customers based on an orders table. What I did was use 2 CTEs with subqueries inside them, something like this:\n\n    -- count recurring customers\n    with recurring_customers as (select extract(year from order_date) as year,\n                                        extract(month from order_date) as month,\n                                        count(distinct (customer_id)) as recurring_customers\n                                 from orders o1\n                                 where (select count(*)\n                                        from orders o2\n                                        where o2.customer_id = o1.customer_id\n                                          and o2.order_date &lt; o1.order_date and o2.order_date &gt; (o1.order_date  - interval '12 months') ) &gt; 0\n                                 group by 1, 2),\n    -- count all customers\n        all_customers as (\n            select extract(year from order_date) as year,\n                                        extract(month from order_date) as month,\n                                        count(distinct (customer_id)) as all_customers\n                                 from orders\n                                 group by 1,2\n        )\n    \n    select a.year, a.month, recurring_customers, all_customers,\n           coalesce(recurring_customers, 0)/cast(all_customers as decimal) from all_customers a\n    left join recurring_customers r\n    on a.year = r.year and a.month = r.month\n\nI got the feedback that using subqueries in a CTEs wasn't incorrect per se, but they found it *strange*. I guess it's bad practice, but could someone explain why? And what would be a better way to do this? Using window functions?\n\nAlso, can you recommend a good resource to learn SQL best practices and to practice SQL? I never really worked in a position where I would get much feedback on my SQL - as long as it works and executes in a reasonable amount of time, it was alright, but I'm really looking for a way to improve my skills. \n\nThank you!!", "author_fullname": "t2_14oqfa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL help - why shouldn't I use subqueries inside CTEs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b2y3sd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709205975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, I did a task for a company. One of the questions was to write a SQL query to get number of recurring customers (those that had a purchase in the 12 months before the current month) vs new customers based on an orders table. What I did was use 2 CTEs with subqueries inside them, something like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;-- count recurring customers\nwith recurring_customers as (select extract(year from order_date) as year,\n                                    extract(month from order_date) as month,\n                                    count(distinct (customer_id)) as recurring_customers\n                             from orders o1\n                             where (select count(*)\n                                    from orders o2\n                                    where o2.customer_id = o1.customer_id\n                                      and o2.order_date &amp;lt; o1.order_date and o2.order_date &amp;gt; (o1.order_date  - interval &amp;#39;12 months&amp;#39;) ) &amp;gt; 0\n                             group by 1, 2),\n-- count all customers\n    all_customers as (\n        select extract(year from order_date) as year,\n                                    extract(month from order_date) as month,\n                                    count(distinct (customer_id)) as all_customers\n                             from orders\n                             group by 1,2\n    )\n\nselect a.year, a.month, recurring_customers, all_customers,\n       coalesce(recurring_customers, 0)/cast(all_customers as decimal) from all_customers a\nleft join recurring_customers r\non a.year = r.year and a.month = r.month\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I got the feedback that using subqueries in a CTEs wasn&amp;#39;t incorrect per se, but they found it &lt;em&gt;strange&lt;/em&gt;. I guess it&amp;#39;s bad practice, but could someone explain why? And what would be a better way to do this? Using window functions?&lt;/p&gt;\n\n&lt;p&gt;Also, can you recommend a good resource to learn SQL best practices and to practice SQL? I never really worked in a position where I would get much feedback on my SQL - as long as it works and executes in a reasonable amount of time, it was alright, but I&amp;#39;m really looking for a way to improve my skills. &lt;/p&gt;\n\n&lt;p&gt;Thank you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b2y3sd", "is_robot_indexable": true, "report_reasons": null, "author": "n_ex", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b2y3sd/sql_help_why_shouldnt_i_use_subqueries_inside_ctes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b2y3sd/sql_help_why_shouldnt_i_use_subqueries_inside_ctes/", "subreddit_subscribers": 164851, "created_utc": 1709205975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do companies in healthcare or climate tech hire data or distributed systems engineers?\n\nAre these jobs fulfilling careers? Are the people well renumerated.", "author_fullname": "t2_vqjalmbw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer jobs in healthcare or climate tech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b374j2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709230558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do companies in healthcare or climate tech hire data or distributed systems engineers?&lt;/p&gt;\n\n&lt;p&gt;Are these jobs fulfilling careers? Are the people well renumerated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b374j2", "is_robot_indexable": true, "report_reasons": null, "author": "diego-the-tortoise", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b374j2/data_engineer_jobs_in_healthcare_or_climate_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b374j2/data_engineer_jobs_in_healthcare_or_climate_tech/", "subreddit_subscribers": 164851, "created_utc": 1709230558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, asking for guidance as my company wanted to migrate to cloud. Currently our process is:\n1. Collect multiple excel files from sharepoint\n2. Transform it thru python code\n3. Store it as a parquet in sharepoint\n4. Feed the parquets to Power BI and more transformation is done in Power Query.\n\nNow, they want to use Talend as ETL solution then use SQL to connect it to PBI but I\u2019m not sure how well it will handle the job we do thru python. The process is done around once a week and it would be helpful if we can have those process automated. They have looked into procuring Azure but they said that Talend is more budget friendly. Are we doing the right thing? I thought what we would need the most is a data warehouse. ", "author_fullname": "t2_57luzhesi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Company wants to migrate to cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3mntn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709271381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, asking for guidance as my company wanted to migrate to cloud. Currently our process is:\n1. Collect multiple excel files from sharepoint\n2. Transform it thru python code\n3. Store it as a parquet in sharepoint\n4. Feed the parquets to Power BI and more transformation is done in Power Query.&lt;/p&gt;\n\n&lt;p&gt;Now, they want to use Talend as ETL solution then use SQL to connect it to PBI but I\u2019m not sure how well it will handle the job we do thru python. The process is done around once a week and it would be helpful if we can have those process automated. They have looked into procuring Azure but they said that Talend is more budget friendly. Are we doing the right thing? I thought what we would need the most is a data warehouse. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3mntn", "is_robot_indexable": true, "report_reasons": null, "author": "akanensan", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3mntn/company_wants_to_migrate_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3mntn/company_wants_to_migrate_to_cloud/", "subreddit_subscribers": 164851, "created_utc": 1709271381.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been doing this for my own enjoyment for a while now, something very close to what the guy(s) at [the pop foot](https://www.google.com/search?sca_esv=59998079312419c0&amp;q=thepopfoot&amp;tbm=isch&amp;source=lnms&amp;sa=X&amp;ved=2ahUKEwikqqyZ_9GEAxW9ANAFHd4nAioQ0pQJegQIDRAB&amp;biw=1920&amp;bih=953) have been doing (just to give you an example). I normally gather data of various teams/players of a certain sport, make some averages here and there and then just compare them out. Nothing too fancy but definitely not simply gathering data and organizing it. I don't think I'd be very good at making graphs/visuals so I just stick to good ol basic excel tables.\n\nYesterday I was having a conversation with some friends on some recent \"data comparison\" I made between teams (not sure if I should call it analysis nor \"insight\") and we all agreed that it made TOO MUCH sense in the end. Kind of made me realize I could be doing this sort of thing as a job; made some google searches and found out about data engineering.\n\nAm I in the right place? Or is analysis the thing I should be aiming for?", "author_fullname": "t2_drxgr3df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I gather, organize and make \"insights\" out of sport data for myself as a hobby. Is this the correct career path for me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3jcm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709261145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been doing this for my own enjoyment for a while now, something very close to what the guy(s) at &lt;a href=\"https://www.google.com/search?sca_esv=59998079312419c0&amp;amp;q=thepopfoot&amp;amp;tbm=isch&amp;amp;source=lnms&amp;amp;sa=X&amp;amp;ved=2ahUKEwikqqyZ_9GEAxW9ANAFHd4nAioQ0pQJegQIDRAB&amp;amp;biw=1920&amp;amp;bih=953\"&gt;the pop foot&lt;/a&gt; have been doing (just to give you an example). I normally gather data of various teams/players of a certain sport, make some averages here and there and then just compare them out. Nothing too fancy but definitely not simply gathering data and organizing it. I don&amp;#39;t think I&amp;#39;d be very good at making graphs/visuals so I just stick to good ol basic excel tables.&lt;/p&gt;\n\n&lt;p&gt;Yesterday I was having a conversation with some friends on some recent &amp;quot;data comparison&amp;quot; I made between teams (not sure if I should call it analysis nor &amp;quot;insight&amp;quot;) and we all agreed that it made TOO MUCH sense in the end. Kind of made me realize I could be doing this sort of thing as a job; made some google searches and found out about data engineering.&lt;/p&gt;\n\n&lt;p&gt;Am I in the right place? Or is analysis the thing I should be aiming for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3jcm4", "is_robot_indexable": true, "report_reasons": null, "author": "CarlosHnnz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3jcm4/i_gather_organize_and_make_insights_out_of_sport/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3jcm4/i_gather_organize_and_make_insights_out_of_sport/", "subreddit_subscribers": 164851, "created_utc": 1709261145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks! I am almost 9 months into my first DE job which is also my first programming job. I decided to switch to DE around 7 years after I graduated from uni and found my current job last summer.\n\nOver the past 9 months, I have helped in building 2 end to end pipelines applying both streaming and batch processing. I have also worked on a data viz project for internal use with another junior DE who joined around the same time but is much younger and a recent college grad.\n\nI am fearful that I am not pulling my weight at all even though my manager has never raised a performance issue and my two reviews so far have been quite good. I think this has something to do with me being closer in age to the mid level folks but being a rank amateur in competence compared to them. \n\nI am definitely asking for less help now and at least when I do I am able to show exactly what I did/why I did but I am keen to know what a very junior DE should be expected to do/know after 9 months. Thanks!", "author_fullname": "t2_chptq8qwh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I a handicap, deadweight to my team as a junior DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b2xftn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709203446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks! I am almost 9 months into my first DE job which is also my first programming job. I decided to switch to DE around 7 years after I graduated from uni and found my current job last summer.&lt;/p&gt;\n\n&lt;p&gt;Over the past 9 months, I have helped in building 2 end to end pipelines applying both streaming and batch processing. I have also worked on a data viz project for internal use with another junior DE who joined around the same time but is much younger and a recent college grad.&lt;/p&gt;\n\n&lt;p&gt;I am fearful that I am not pulling my weight at all even though my manager has never raised a performance issue and my two reviews so far have been quite good. I think this has something to do with me being closer in age to the mid level folks but being a rank amateur in competence compared to them. &lt;/p&gt;\n\n&lt;p&gt;I am definitely asking for less help now and at least when I do I am able to show exactly what I did/why I did but I am keen to know what a very junior DE should be expected to do/know after 9 months. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b2xftn", "is_robot_indexable": true, "report_reasons": null, "author": "jnrdataengineer2023", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b2xftn/am_i_a_handicap_deadweight_to_my_team_as_a_junior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b2xftn/am_i_a_handicap_deadweight_to_my_team_as_a_junior/", "subreddit_subscribers": 164851, "created_utc": 1709203446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm talking about something more in-depth than ERD diagrams. Possibly includes some graph theory analysis like the centrality of a table etc. Something that shows the flow of data linked by stored procedures, connections based on keys, list of triggers/keys/constraints. I've been working with SSMS/Azure Data Factory and I don't see anything on the market that currently fits my needs.   \n\n\n&amp;#x200B;", "author_fullname": "t2_xzn0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools do you guys use for schema analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3m3lh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709269528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talking about something more in-depth than ERD diagrams. Possibly includes some graph theory analysis like the centrality of a table etc. Something that shows the flow of data linked by stored procedures, connections based on keys, list of triggers/keys/constraints. I&amp;#39;ve been working with SSMS/Azure Data Factory and I don&amp;#39;t see anything on the market that currently fits my needs.   &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3m3lh", "is_robot_indexable": true, "report_reasons": null, "author": "richhoods", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3m3lh/what_tools_do_you_guys_use_for_schema_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3m3lh/what_tools_do_you_guys_use_for_schema_analysis/", "subreddit_subscribers": 164851, "created_utc": 1709269528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can someone please help me understand a use case that fits the usage of Redis? I have never worked on it just trying to understand its use cases before diving deep into it.\n\nMy friend recently went through a system design round for a company, and faced a scenario to build a system for displaying real time user count (logged in on a website) on a dashboard.\nWe were thinking of a an event based setup like kafka writing onto a nosql which renders data for dashboard.\nAnd we came to a conclusion that eventually persisting the events from queue to nosql would take all the time and defeat the solution.\n\nIs this a right use case for Redis?\n\n", "author_fullname": "t2_epbwyu83", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redis use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b2wcm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709198909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone please help me understand a use case that fits the usage of Redis? I have never worked on it just trying to understand its use cases before diving deep into it.&lt;/p&gt;\n\n&lt;p&gt;My friend recently went through a system design round for a company, and faced a scenario to build a system for displaying real time user count (logged in on a website) on a dashboard.\nWe were thinking of a an event based setup like kafka writing onto a nosql which renders data for dashboard.\nAnd we came to a conclusion that eventually persisting the events from queue to nosql would take all the time and defeat the solution.&lt;/p&gt;\n\n&lt;p&gt;Is this a right use case for Redis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b2wcm4", "is_robot_indexable": true, "report_reasons": null, "author": "gurmanavfc14", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b2wcm4/redis_use_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b2wcm4/redis_use_case/", "subreddit_subscribers": 164851, "created_utc": 1709198909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, havent seen any info on this, so perhaps its very dumb idea... would it make sense to run Polars on Databricks, using single node clusters.?\n\nReasoning being,... trying to get the advantages/features of databricks while at the same time using a small single node cluster with Polars instead of Spark/Pyspark to process data.   any opinions?\n\n", "author_fullname": "t2_3laxwg3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b37ca1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709231087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, havent seen any info on this, so perhaps its very dumb idea... would it make sense to run Polars on Databricks, using single node clusters.?&lt;/p&gt;\n\n&lt;p&gt;Reasoning being,... trying to get the advantages/features of databricks while at the same time using a small single node cluster with Polars instead of Spark/Pyspark to process data.   any opinions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b37ca1", "is_robot_indexable": true, "report_reasons": null, "author": "acelisalas", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b37ca1/polars_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b37ca1/polars_on_databricks/", "subreddit_subscribers": 164851, "created_utc": 1709231087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have set up scrapers to extract real estate data with the goal of storing this into a relational database to perform analysis on. I have more focus in data science so I am new to the field of data engineering. Assuming I parse 500 different houses, should i store my data directly into the relational database, or should I save the raw json files and then insert them manually. Also, these json files for each property can get relatively large, while scraping is it unwise to store all of the property json files into one larger json file?\n\nThanks for the help!", "author_fullname": "t2_i1oi9p4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner Need advice on intermediate storage of webscraped data.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3jjt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709261734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have set up scrapers to extract real estate data with the goal of storing this into a relational database to perform analysis on. I have more focus in data science so I am new to the field of data engineering. Assuming I parse 500 different houses, should i store my data directly into the relational database, or should I save the raw json files and then insert them manually. Also, these json files for each property can get relatively large, while scraping is it unwise to store all of the property json files into one larger json file?&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3jjt1", "is_robot_indexable": true, "report_reasons": null, "author": "No_Map3272", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3jjt1/beginner_need_advice_on_intermediate_storage_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3jjt1/beginner_need_advice_on_intermediate_storage_of/", "subreddit_subscribers": 164851, "created_utc": 1709261734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nI've got to make a report from our DW on customers and their transactions (for example). I am divided between having a single Spark job handle all the customers+transactions and give it lots of breathing room (in terms of cluster size), or whether I should have the Spark job handle a single customer+(all transactions) and rely on Airflow to get everything done.\n\nAdvice? Please and thank you.\n\nUpdate: The report is like a statement. One report per customer, including all transactions for the given time period.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Many \"customers\" in single Spark job, or many Spark jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3dkm3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709247485.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709245958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got to make a report from our DW on customers and their transactions (for example). I am divided between having a single Spark job handle all the customers+transactions and give it lots of breathing room (in terms of cluster size), or whether I should have the Spark job handle a single customer+(all transactions) and rely on Airflow to get everything done.&lt;/p&gt;\n\n&lt;p&gt;Advice? Please and thank you.&lt;/p&gt;\n\n&lt;p&gt;Update: The report is like a statement. One report per customer, including all transactions for the given time period.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3dkm3", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3dkm3/many_customers_in_single_spark_job_or_many_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3dkm3/many_customers_in_single_spark_job_or_many_spark/", "subreddit_subscribers": 164851, "created_utc": 1709245958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Question for all you that had entry level positions or started off with easier projects. What was the project? What tools did you use? And what was your day to day like?", "author_fullname": "t2_6cmmqb4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Entry Level\" or \"Junior\" if still exist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3afn7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709238584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question for all you that had entry level positions or started off with easier projects. What was the project? What tools did you use? And what was your day to day like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3afn7", "is_robot_indexable": true, "report_reasons": null, "author": "chainz3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3afn7/entry_level_or_junior_if_still_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3afn7/entry_level_or_junior_if_still_exist/", "subreddit_subscribers": 164851, "created_utc": 1709238584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to transfer the 365 data through api from 365 to big query, what would be the effective way to do this process !? Any suggestions ", "author_fullname": "t2_9r5qrevk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft 365 business central to big query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b34y1i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709225319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to transfer the 365 data through api from 365 to big query, what would be the effective way to do this process !? Any suggestions &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b34y1i", "is_robot_indexable": true, "report_reasons": null, "author": "Fabro_vaz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b34y1i/microsoft_365_business_central_to_big_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b34y1i/microsoft_365_business_central_to_big_query/", "subreddit_subscribers": 164851, "created_utc": 1709225319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In SQL I need to count occurrences of the date when a deal was passed to a salesperson, the meeting date, and the contract signing date, along with the sum of contracts. \n\nAll this data is in a single table with a unique ID as the primary key (PK).\n\nThe challenge is that I need to obtain the count of each of these dates by month without using any of the dates directly for grouping. This is because grouping by any of these dates directly would result in counting by that specific date. For example, if someone had a meeting in January and we group by the meeting date, we wouldn't capture their contract in February, etc.\n\nHow should I approach this?\"", "author_fullname": "t2_803gugjh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Count of different dates occurencies from one table ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3lump", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709268765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In SQL I need to count occurrences of the date when a deal was passed to a salesperson, the meeting date, and the contract signing date, along with the sum of contracts. &lt;/p&gt;\n\n&lt;p&gt;All this data is in a single table with a unique ID as the primary key (PK).&lt;/p&gt;\n\n&lt;p&gt;The challenge is that I need to obtain the count of each of these dates by month without using any of the dates directly for grouping. This is because grouping by any of these dates directly would result in counting by that specific date. For example, if someone had a meeting in January and we group by the meeting date, we wouldn&amp;#39;t capture their contract in February, etc.&lt;/p&gt;\n\n&lt;p&gt;How should I approach this?&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3lump", "is_robot_indexable": true, "report_reasons": null, "author": "Novel_Pattern8035", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3lump/count_of_different_dates_occurencies_from_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3lump/count_of_different_dates_occurencies_from_one/", "subreddit_subscribers": 164851, "created_utc": 1709268765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on people\u2019s thoughts\n\nContext: loading data to delta lake through spark jobs. \n\nFlow: \nRead incremental data from db\nMerge to Staging table\nValidate table\nOverwrite prod with stagjng\n\n\nQuestion is if staging tables are necessary here. The overwrite process is quite expensive since there is a 400gb table to overwrite daily. I could also merge to production after validating staging but that is also expensive and redundant.\n\n\nIf I just merged to a prod table and had a validation hook to time travel the table back in case of any errors, that doesn\u2019t seem that bad given the tables aren\u2019t being accessed during the load at all so data quality is not a huge concern\n\nAlso, no, the pattern of renaming a table doesn\u2019t work in our enterprise\n\nAnyways, curious what patterns people do for delta lake ", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake Staging Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3lipj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709267680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on people\u2019s thoughts&lt;/p&gt;\n\n&lt;p&gt;Context: loading data to delta lake through spark jobs. &lt;/p&gt;\n\n&lt;p&gt;Flow: \nRead incremental data from db\nMerge to Staging table\nValidate table\nOverwrite prod with stagjng&lt;/p&gt;\n\n&lt;p&gt;Question is if staging tables are necessary here. The overwrite process is quite expensive since there is a 400gb table to overwrite daily. I could also merge to production after validating staging but that is also expensive and redundant.&lt;/p&gt;\n\n&lt;p&gt;If I just merged to a prod table and had a validation hook to time travel the table back in case of any errors, that doesn\u2019t seem that bad given the tables aren\u2019t being accessed during the load at all so data quality is not a huge concern&lt;/p&gt;\n\n&lt;p&gt;Also, no, the pattern of renaming a table doesn\u2019t work in our enterprise&lt;/p&gt;\n\n&lt;p&gt;Anyways, curious what patterns people do for delta lake &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3lipj", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3lipj/delta_lake_staging_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3lipj/delta_lake_staging_tables/", "subreddit_subscribers": 164851, "created_utc": 1709267680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say you are dealing with multiple csv files, each with over 10 million rows. What kind of validation tests do you perform on the data itself and what type of tests do you write for dealing with database part? Which libraries do you use for it? \n\nAny reference material to read up on would be very helpful! ", "author_fullname": "t2_61uvorko", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of validation do you perform on the data and what tests do you write for correct data handling in to a dB? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3ip2a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709259266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say you are dealing with multiple csv files, each with over 10 million rows. What kind of validation tests do you perform on the data itself and what type of tests do you write for dealing with database part? Which libraries do you use for it? &lt;/p&gt;\n\n&lt;p&gt;Any reference material to read up on would be very helpful! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3ip2a", "is_robot_indexable": true, "report_reasons": null, "author": "jadedsprint", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3ip2a/what_kind_of_validation_do_you_perform_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3ip2a/what_kind_of_validation_do_you_perform_on_the/", "subreddit_subscribers": 164851, "created_utc": 1709259266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If DuckDb and dbt snapshot had a baby", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b34lag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "287cf772-ac9d-11eb-aa84-0ead36cb44af", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rfI1t9OkvDrHhQj1mW6PkEO7O6ES-_zHhH85_PMj4mM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709224443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/okfy1mrnxjlc1.gif", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?format=png8&amp;s=f3d0d3774f848b3e4b122db9c8cd8008ac461bdf", "width": 800, "height": 600}, "resolutions": [{"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=6bc4f5d39365b9ddf2279d6ea8c92df6af0e92f6", "width": 108, "height": 81}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=eb2aa40846c0019b47d166889b9ec31d32fdcff5", "width": 216, "height": 162}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=4b7cd4a7b5ee25bbc1a561596df48a28a62ddcdf", "width": 320, "height": 240}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=52594cbe54fc43cae1a99189591fc41dc846f7c9", "width": 640, "height": 480}], "variants": {"gif": {"source": {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?s=0987419796f35c807cdb61548584e7f30bb35ddd", "width": 800, "height": 600}, "resolutions": [{"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=108&amp;crop=smart&amp;s=59b599f6c8db7ea6685f94bf6c2c65cec9aa0f21", "width": 108, "height": 81}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=216&amp;crop=smart&amp;s=e331c2813a0adb69c01f1eb15e7e256971ed22a8", "width": 216, "height": 162}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=320&amp;crop=smart&amp;s=05e952f2124949eed70113c7a3be6e5b5a45a61f", "width": 320, "height": 240}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=640&amp;crop=smart&amp;s=d33c572e19921ecb002d0f18879666e7bac284bc", "width": 640, "height": 480}]}, "mp4": {"source": {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?format=mp4&amp;s=c96cfcf49b8c716100ce1ae504ec48c77b76ddbb", "width": 800, "height": 600}, "resolutions": [{"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=108&amp;format=mp4&amp;s=6e6734053a2d5f0a1e50bb2874aa88120d83c762", "width": 108, "height": 81}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=216&amp;format=mp4&amp;s=ba5786607830da36758615226237902bd0283279", "width": 216, "height": 162}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=320&amp;format=mp4&amp;s=ef67fc499fc4717eb3545379ab25c6560d7de420", "width": 320, "height": 240}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=640&amp;format=mp4&amp;s=8d5fcc65a1666f8bd84dfc24f903380ab01084fe", "width": 640, "height": 480}]}}, "id": "UklnMD9hFtQNuBI9rsBdDxN5wSNZjBccH9s7WJ4tHBk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Software Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b34lag", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b34lag/if_duckdb_and_dbt_snapshot_had_a_baby/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/okfy1mrnxjlc1.gif", "subreddit_subscribers": 164851, "created_utc": 1709224443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a newbie about data warehouse. I want to design a Warehouse using Star Schema for company problems.\n Different business processes will be designed with different schemas such as sales_schema, financial_schema. However, some schemas will use common dim tables such as dim_product. So how should the dim table be designed and stored in schema to be most effective and appropriate?\n- Should a table dim_product be created in a common schema (eg public_schema) and other schemas (sales_schema, financial_schema) can use data from the common table dim_product (public_schema)\n- Each schema (sales_schema, financial_schema) will create a dim_product table (duplicate data)\n\nPlease help me. Thank you !", "author_fullname": "t2_7fja37x0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Design schema use Star Schema in Data Warehouse Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b33ph1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709222262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a newbie about data warehouse. I want to design a Warehouse using Star Schema for company problems.\n Different business processes will be designed with different schemas such as sales_schema, financial_schema. However, some schemas will use common dim tables such as dim_product. So how should the dim table be designed and stored in schema to be most effective and appropriate?\n- Should a table dim_product be created in a common schema (eg public_schema) and other schemas (sales_schema, financial_schema) can use data from the common table dim_product (public_schema)\n- Each schema (sales_schema, financial_schema) will create a dim_product table (duplicate data)&lt;/p&gt;\n\n&lt;p&gt;Please help me. Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b33ph1", "is_robot_indexable": true, "report_reasons": null, "author": "Waste-Orchid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b33ph1/design_schema_use_star_schema_in_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b33ph1/design_schema_use_star_schema_in_data_warehouse/", "subreddit_subscribers": 164851, "created_utc": 1709222262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since January 31, 2024, Talend (acquired by Qlik) has been removed from the Talend Open Studio site. It's also impossible to find a version on Source forge.\n\nWho still has a copy of Talend Open Studio?", "author_fullname": "t2_634ju3is", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am looking for Talend Open Studio since it's not open source anymore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b319x2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709216028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since January 31, 2024, Talend (acquired by Qlik) has been removed from the Talend Open Studio site. It&amp;#39;s also impossible to find a version on Source forge.&lt;/p&gt;\n\n&lt;p&gt;Who still has a copy of Talend Open Studio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b319x2", "is_robot_indexable": true, "report_reasons": null, "author": "lilbuldogz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b319x2/i_am_looking_for_talend_open_studio_since_its_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b319x2/i_am_looking_for_talend_open_studio_since_its_not/", "subreddit_subscribers": 164851, "created_utc": 1709216028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to stream the changes made in my Postgresql DB using Debezium and Kafka. This is my connector configuration(pic 1)\n\nThe connector is created and I am able to consume the changes in Jupyter notebook using my Kafka consumer. But I am only getting the row data that was inserted or updated(pic 2). I am not able to get the operation that was performed(whether it was INSERT or UPDATE). Is there a way to get the operation performed data as well? also is it possible to get other operations data like TRUNCATE,DROP,ALTER because WAL logs store these information(pic 3- you can see the delete operation being logged but this doesnt get consumed by kafka).", "author_fullname": "t2_d37wcj6i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting additional data from Debezium connector", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b2xs94", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709204772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to stream the changes made in my Postgresql DB using Debezium and Kafka. This is my connector configuration(pic 1)&lt;/p&gt;\n\n&lt;p&gt;The connector is created and I am able to consume the changes in Jupyter notebook using my Kafka consumer. But I am only getting the row data that was inserted or updated(pic 2). I am not able to get the operation that was performed(whether it was INSERT or UPDATE). Is there a way to get the operation performed data as well? also is it possible to get other operations data like TRUNCATE,DROP,ALTER because WAL logs store these information(pic 3- you can see the delete operation being logged but this doesnt get consumed by kafka).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b2xs94", "is_robot_indexable": true, "report_reasons": null, "author": "Minute-Internal5628", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b2xs94/getting_additional_data_from_debezium_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b2xs94/getting_additional_data_from_debezium_connector/", "subreddit_subscribers": 164851, "created_utc": 1709204772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, everyone. I have question like this. I am trying to build datalake house with some components such as, Minio as a object storage, dreamio as SQL engine.  Requirements are like below;  \n\n\n* Migrate some tables (not high volume tables) from MSSQL to Minio, so we can simply join these tables with Dreamio and query these tables. This part is not the problematic. I already migrated all tables to Minio bucket with the help of Apache Spark.   \n\n\nProblem: Now, I need to bring only incremental changes to Minio. I think the way it can be done is Airbyte using enabled CDC on MSSQL to S3 (Minio bucket) connector. Lets say, one row got updated on source Sink (mssql) and CDC captured this change, and my airbyte job also migrated this incremental change as file to bucket. But then how can I implement this change to files there, so dreamio tables can also reflect these changes on tables. Source operations are insert, update, delete. Maybe someone will recommend to overwrite everything each time. So all the time there will be updated data. But I dont want to put this kind of strain, instead I want to use incremental changes. Is there anybody has idea about this ?", "author_fullname": "t2_m2wfe4pj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalakehouse pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b2wlrw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709199990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, everyone. I have question like this. I am trying to build datalake house with some components such as, Minio as a object storage, dreamio as SQL engine.  Requirements are like below;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Migrate some tables (not high volume tables) from MSSQL to Minio, so we can simply join these tables with Dreamio and query these tables. This part is not the problematic. I already migrated all tables to Minio bucket with the help of Apache Spark.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Problem: Now, I need to bring only incremental changes to Minio. I think the way it can be done is Airbyte using enabled CDC on MSSQL to S3 (Minio bucket) connector. Lets say, one row got updated on source Sink (mssql) and CDC captured this change, and my airbyte job also migrated this incremental change as file to bucket. But then how can I implement this change to files there, so dreamio tables can also reflect these changes on tables. Source operations are insert, update, delete. Maybe someone will recommend to overwrite everything each time. So all the time there will be updated data. But I dont want to put this kind of strain, instead I want to use incremental changes. Is there anybody has idea about this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b2wlrw", "is_robot_indexable": true, "report_reasons": null, "author": "Over_Ad_6186", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b2wlrw/datalakehouse_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b2wlrw/datalakehouse_pipeline/", "subreddit_subscribers": 164851, "created_utc": 1709199990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! Long time lurker of this sub. I've been working on a data quality tool to help data teams communicate their data requirements and enforce them on data sources. You can think of it as a data contracts development platform, or in other words, building data APIs. I would really love everyone's honest feedback on my MVP (please be honest but also helpful as possible). It'll go a long way in improving the product and eventually helping organizations manage and build scalable data infras. Big respect to all the folks here.   \n\n\nCan check it out here! [Loom Demo Link](https://www.loom.com/share/ec43b99a64094da0a8235cda060c6c10?sid=ba5043f2-3b20-4bb6-b20b-c9d00efdc981)  \n\n\nThanks a bunch homies. ", "author_fullname": "t2_9zilrtf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Honest Feedback on Data Quality Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b3psxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709283498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! Long time lurker of this sub. I&amp;#39;ve been working on a data quality tool to help data teams communicate their data requirements and enforce them on data sources. You can think of it as a data contracts development platform, or in other words, building data APIs. I would really love everyone&amp;#39;s honest feedback on my MVP (please be honest but also helpful as possible). It&amp;#39;ll go a long way in improving the product and eventually helping organizations manage and build scalable data infras. Big respect to all the folks here.   &lt;/p&gt;\n\n&lt;p&gt;Can check it out here! &lt;a href=\"https://www.loom.com/share/ec43b99a64094da0a8235cda060c6c10?sid=ba5043f2-3b20-4bb6-b20b-c9d00efdc981\"&gt;Loom Demo Link&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Thanks a bunch homies. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?auto=webp&amp;s=7c4310519f61ec4fb86f317eb64448af04517967", "width": 2400, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8962e2587966fe1151c1cb2d3da899e2f659ac4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b807a1ad51a8a9a4511e6405e312046216cca911", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0fe70178af2da36da999a8c85b57fee25b8e7920", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8b86d75c4ec9a0f6944cd21bbd5a3f2c9f5db31", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb231a58575d582c44c0d632b6a1b6a06ff33179", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e500d7385200026eef0e6bb2ea2c1b116635b8c1", "width": 1080, "height": 540}], "variants": {}, "id": "qeifkwwr4C5QSg58B_96VUqdBQ2DqQxNVOT5ptzpTrU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1b3psxn", "is_robot_indexable": true, "report_reasons": null, "author": "ParfaitRude229", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3psxn/honest_feedback_on_data_quality_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3psxn/honest_feedback_on_data_quality_tool/", "subreddit_subscribers": 164851, "created_utc": 1709283498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello r/dataengineering!\n\n&amp;#x200B;\n\nI'm part of the [Jargon.sh](https://Jargon.sh) team, a platform on a mission to bring the principles and tools of open-source software development to Domain Driven Design (DDD) and API design. Our scope is broad, but we've traditionally focused on providing just enough data modelling to meet our users' needs. However, we're seeing a growing demand from clients who are interested in leveraging our platform for general-purpose data modelling, marking a new and exciting direction for us.\n\n&amp;#x200B;\n\nOur clients have shared how much they value the DDD approach for breaking down large models into smaller, more manageable domain models, and how our method of calculating Semantic Version (SemVer) release numbers has been a game-changer for them. These strategies have proven effective for their API design and integration architecture. Additionally, they appreciate Jargon as a platform of reusable models that can be easily searched, discovered, and imported into other domains, all based on immutable SemVer versioned releases. This capability not only enhances the efficiency of their work by promoting reusability and consistency across different projects but also significantly reduces the time and effort required in developing new domain models from scratch. This feedback underscores why our clients are encouraging us to extend our support to include more comprehensive data modelling capabilities.\n\n&amp;#x200B;\n\nOur goal extends beyond offering generic tools; we aim to understand the unique challenges our users face and to develop innovative solutions. By engaging closely with the community, we believe we can customise our solutions to meet specific needs and challenges. This collaborative approach has served us well in the realms of DDD and API, and we're eager to apply it to data modelling for data engineering, hoping to add significant value.\n\n&amp;#x200B;\n\nAs a member of the Jargon team, I'm here to collect your feedback and insights on how an open-source software-inspired approach to data modelling might benefit you. Your input is incredibly important to us as we strive to evolve Jargon into not just a tool, but a community-driven solution that empowers practitioners to achieve what they're trying to do more efficiently.\n\n&amp;#x200B;\n\nI'm quite new to Reddit, having been stalking around for a little while before finding this great community. Our research indicated that this is the most active and engaged data engineering community around, so I decided to reach out. It's clear there's a wealth of knowledge and experience here, and I'm excited to learn from you all and maybe convert some of your knowledge into features of our freely available data modelling platform in return.\n\n&amp;#x200B;\n\nIf you're not familiar with Jargon yet, I invite you to explore our platform. We offer a free-forever tier packed with features that could be of interest to you.\n\n&amp;#x200B;\n\nThank you for your time and insights. I'm looking forward to your feedback, and happy to answer any questions you might have!\n\n&amp;#x200B;", "author_fullname": "t2_36d03l80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pivoting our product from API design to Data Modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b3ppn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709283093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m part of the &lt;a href=\"https://Jargon.sh\"&gt;Jargon.sh&lt;/a&gt; team, a platform on a mission to bring the principles and tools of open-source software development to Domain Driven Design (DDD) and API design. Our scope is broad, but we&amp;#39;ve traditionally focused on providing just enough data modelling to meet our users&amp;#39; needs. However, we&amp;#39;re seeing a growing demand from clients who are interested in leveraging our platform for general-purpose data modelling, marking a new and exciting direction for us.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Our clients have shared how much they value the DDD approach for breaking down large models into smaller, more manageable domain models, and how our method of calculating Semantic Version (SemVer) release numbers has been a game-changer for them. These strategies have proven effective for their API design and integration architecture. Additionally, they appreciate Jargon as a platform of reusable models that can be easily searched, discovered, and imported into other domains, all based on immutable SemVer versioned releases. This capability not only enhances the efficiency of their work by promoting reusability and consistency across different projects but also significantly reduces the time and effort required in developing new domain models from scratch. This feedback underscores why our clients are encouraging us to extend our support to include more comprehensive data modelling capabilities.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Our goal extends beyond offering generic tools; we aim to understand the unique challenges our users face and to develop innovative solutions. By engaging closely with the community, we believe we can customise our solutions to meet specific needs and challenges. This collaborative approach has served us well in the realms of DDD and API, and we&amp;#39;re eager to apply it to data modelling for data engineering, hoping to add significant value.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As a member of the Jargon team, I&amp;#39;m here to collect your feedback and insights on how an open-source software-inspired approach to data modelling might benefit you. Your input is incredibly important to us as we strive to evolve Jargon into not just a tool, but a community-driven solution that empowers practitioners to achieve what they&amp;#39;re trying to do more efficiently.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite new to Reddit, having been stalking around for a little while before finding this great community. Our research indicated that this is the most active and engaged data engineering community around, so I decided to reach out. It&amp;#39;s clear there&amp;#39;s a wealth of knowledge and experience here, and I&amp;#39;m excited to learn from you all and maybe convert some of your knowledge into features of our freely available data modelling platform in return.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re not familiar with Jargon yet, I invite you to explore our platform. We offer a free-forever tier packed with features that could be of interest to you.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time and insights. I&amp;#39;m looking forward to your feedback, and happy to answer any questions you might have!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3ppn0", "is_robot_indexable": true, "report_reasons": null, "author": "Jargon-sh", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3ppn0/pivoting_our_product_from_api_design_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3ppn0/pivoting_our_product_from_api_design_to_data/", "subreddit_subscribers": 164851, "created_utc": 1709283093.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}