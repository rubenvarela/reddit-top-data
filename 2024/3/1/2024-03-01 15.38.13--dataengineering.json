{"kind": "Listing", "data": {"after": "t3_1b3v71f", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I (M20) just had a second round of 1 on 1 session for data engineer trainee in a company. \n\nI was asked to reverse a string in python and I forgot the syntax of while loop. And this one mistake just put me in a downward spiral for the entire hour of the session. So much so that once he asked me if two null values will be equal and I said no, and he asked why but I could not bring myself to be confident enough to say anything about memory addresses even after knowing about it, he asked me about indexing in database and I could only answer it in very simple terms.\n\nI feel really low right now, what can I do to improve and get better at interviewing.", "author_fullname": "t2_9pezqc7t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I bombed the interviuw and feel like the dumbest person in the world", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b34q4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 138, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 138, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709224773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I (M20) just had a second round of 1 on 1 session for data engineer trainee in a company. &lt;/p&gt;\n\n&lt;p&gt;I was asked to reverse a string in python and I forgot the syntax of while loop. And this one mistake just put me in a downward spiral for the entire hour of the session. So much so that once he asked me if two null values will be equal and I said no, and he asked why but I could not bring myself to be confident enough to say anything about memory addresses even after knowing about it, he asked me about indexing in database and I could only answer it in very simple terms.&lt;/p&gt;\n\n&lt;p&gt;I feel really low right now, what can I do to improve and get better at interviewing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b34q4i", "is_robot_indexable": true, "report_reasons": null, "author": "pmme_ur_titsandclits", "discussion_type": null, "num_comments": 90, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b34q4i/i_bombed_the_interviuw_and_feel_like_the_dumbest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b34q4i/i_bombed_the_interviuw_and_feel_like_the_dumbest/", "subreddit_subscribers": 164916, "created_utc": 1709224773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The data is in a table in redshift. On loading this data we plan to run few ML functions on the data (random forest etc), so doing it in python/pandas is essential. Any ideas/suggestions on how this can be performed ?\nEdit: Tech stack currently does not have spark. We are a small team that operates on pandas and sql", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I load a 70 million row table in a dataframe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3cb1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709243537.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709242979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The data is in a table in redshift. On loading this data we plan to run few ML functions on the data (random forest etc), so doing it in python/pandas is essential. Any ideas/suggestions on how this can be performed ?\nEdit: Tech stack currently does not have spark. We are a small team that operates on pandas and sql&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3cb1t", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3cb1t/how_do_i_load_a_70_million_row_table_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3cb1t/how_do_i_load_a_70_million_row_table_in_a/", "subreddit_subscribers": 164916, "created_utc": 1709242979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, asking for guidance as my company wanted to migrate to cloud. Currently our process is:\n1. Collect multiple excel files from sharepoint\n2. Transform it thru python code\n3. Store it as a parquet in sharepoint\n4. Feed the parquets to Power BI and more transformation is done in Power Query.\n\nNow, they want to use Talend as ETL solution then use SQL to connect it to PBI but I\u2019m not sure how well it will handle the job we do thru python. The process is done around once a week and it would be helpful if we can have those process automated. They have looked into procuring Azure but they said that Talend is more budget friendly. Are we doing the right thing? I thought what we would need the most is a data warehouse. ", "author_fullname": "t2_57luzhesi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Company wants to migrate to cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3mntn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709271381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, asking for guidance as my company wanted to migrate to cloud. Currently our process is:\n1. Collect multiple excel files from sharepoint\n2. Transform it thru python code\n3. Store it as a parquet in sharepoint\n4. Feed the parquets to Power BI and more transformation is done in Power Query.&lt;/p&gt;\n\n&lt;p&gt;Now, they want to use Talend as ETL solution then use SQL to connect it to PBI but I\u2019m not sure how well it will handle the job we do thru python. The process is done around once a week and it would be helpful if we can have those process automated. They have looked into procuring Azure but they said that Talend is more budget friendly. Are we doing the right thing? I thought what we would need the most is a data warehouse. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3mntn", "is_robot_indexable": true, "report_reasons": null, "author": "akanensan", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3mntn/company_wants_to_migrate_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3mntn/company_wants_to_migrate_to_cloud/", "subreddit_subscribers": 164916, "created_utc": 1709271381.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do companies in healthcare or climate tech hire data or distributed systems engineers?\n\nAre these jobs fulfilling careers? Are the people well renumerated.", "author_fullname": "t2_vqjalmbw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer jobs in healthcare or climate tech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b374j2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709230558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do companies in healthcare or climate tech hire data or distributed systems engineers?&lt;/p&gt;\n\n&lt;p&gt;Are these jobs fulfilling careers? Are the people well renumerated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b374j2", "is_robot_indexable": true, "report_reasons": null, "author": "diego-the-tortoise", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b374j2/data_engineer_jobs_in_healthcare_or_climate_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b374j2/data_engineer_jobs_in_healthcare_or_climate_tech/", "subreddit_subscribers": 164916, "created_utc": 1709230558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey! I got some great help when asking about a similar topic on Reddit before so wanted to see some perspectives on this.   \n\n\nI am working in a databricks environment where my team feels like it's hard to get a good setup for the projects structure and good code quality in our notebooks. For some reason, even though the notebooks are basically run top-down when running processing jobs with them, it's harder to get good code in notebooks. Does anyone have the same experience?   \n\n\nAre there any given best practices when coding notebooks for spark jobs or similar processing? And how would we create a structure (like grouping folders with code by level of refinement in medallion for example, so one folder for the code used for creating bronze, one for silver and so on) to make the code make more sense? It's not a very large scale project either.\n\n&amp;#x200B;\n\nAny thoughts appreciated! ", "author_fullname": "t2_7fu4vx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices on notebook-based project structure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3p36n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709280526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I got some great help when asking about a similar topic on Reddit before so wanted to see some perspectives on this.   &lt;/p&gt;\n\n&lt;p&gt;I am working in a databricks environment where my team feels like it&amp;#39;s hard to get a good setup for the projects structure and good code quality in our notebooks. For some reason, even though the notebooks are basically run top-down when running processing jobs with them, it&amp;#39;s harder to get good code in notebooks. Does anyone have the same experience?   &lt;/p&gt;\n\n&lt;p&gt;Are there any given best practices when coding notebooks for spark jobs or similar processing? And how would we create a structure (like grouping folders with code by level of refinement in medallion for example, so one folder for the code used for creating bronze, one for silver and so on) to make the code make more sense? It&amp;#39;s not a very large scale project either.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any thoughts appreciated! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3p36n", "is_robot_indexable": true, "report_reasons": null, "author": "Maxxlax", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3p36n/best_practices_on_notebookbased_project_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3p36n/best_practices_on_notebookbased_project_structure/", "subreddit_subscribers": 164916, "created_utc": 1709280526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been doing this for my own enjoyment for a while now, something very close to what the guy(s) at [the pop foot](https://www.google.com/search?sca_esv=59998079312419c0&amp;q=thepopfoot&amp;tbm=isch&amp;source=lnms&amp;sa=X&amp;ved=2ahUKEwikqqyZ_9GEAxW9ANAFHd4nAioQ0pQJegQIDRAB&amp;biw=1920&amp;bih=953) have been doing (just to give you an example). I normally gather data of various teams/players of a certain sport, make some averages here and there and then just compare them out. Nothing too fancy but definitely not simply gathering data and organizing it. I don't think I'd be very good at making graphs/visuals so I just stick to good ol basic excel tables.\n\nYesterday I was having a conversation with some friends on some recent \"data comparison\" I made between teams (not sure if I should call it analysis nor \"insight\") and we all agreed that it made TOO MUCH sense in the end. Kind of made me realize I could be doing this sort of thing as a job; made some google searches and found out about data engineering.\n\nAm I in the right place? Or is analysis the thing I should be aiming for?", "author_fullname": "t2_drxgr3df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I gather, organize and make \"insights\" out of sport data for myself as a hobby. Is this the correct career path for me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3jcm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709261145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been doing this for my own enjoyment for a while now, something very close to what the guy(s) at &lt;a href=\"https://www.google.com/search?sca_esv=59998079312419c0&amp;amp;q=thepopfoot&amp;amp;tbm=isch&amp;amp;source=lnms&amp;amp;sa=X&amp;amp;ved=2ahUKEwikqqyZ_9GEAxW9ANAFHd4nAioQ0pQJegQIDRAB&amp;amp;biw=1920&amp;amp;bih=953\"&gt;the pop foot&lt;/a&gt; have been doing (just to give you an example). I normally gather data of various teams/players of a certain sport, make some averages here and there and then just compare them out. Nothing too fancy but definitely not simply gathering data and organizing it. I don&amp;#39;t think I&amp;#39;d be very good at making graphs/visuals so I just stick to good ol basic excel tables.&lt;/p&gt;\n\n&lt;p&gt;Yesterday I was having a conversation with some friends on some recent &amp;quot;data comparison&amp;quot; I made between teams (not sure if I should call it analysis nor &amp;quot;insight&amp;quot;) and we all agreed that it made TOO MUCH sense in the end. Kind of made me realize I could be doing this sort of thing as a job; made some google searches and found out about data engineering.&lt;/p&gt;\n\n&lt;p&gt;Am I in the right place? Or is analysis the thing I should be aiming for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3jcm4", "is_robot_indexable": true, "report_reasons": null, "author": "CarlosHnnz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3jcm4/i_gather_organize_and_make_insights_out_of_sport/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3jcm4/i_gather_organize_and_make_insights_out_of_sport/", "subreddit_subscribers": 164916, "created_utc": 1709261145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm talking about something more in-depth than ERD diagrams. Possibly includes some graph theory analysis like the centrality of a table etc. Something that shows the flow of data linked by stored procedures, connections based on keys, list of triggers/keys/constraints. I've been working with SSMS/Azure Data Factory and I don't see anything on the market that currently fits my needs.   \n\n\n&amp;#x200B;", "author_fullname": "t2_xzn0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools do you guys use for schema analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3m3lh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709269528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talking about something more in-depth than ERD diagrams. Possibly includes some graph theory analysis like the centrality of a table etc. Something that shows the flow of data linked by stored procedures, connections based on keys, list of triggers/keys/constraints. I&amp;#39;ve been working with SSMS/Azure Data Factory and I don&amp;#39;t see anything on the market that currently fits my needs.   &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3m3lh", "is_robot_indexable": true, "report_reasons": null, "author": "richhoods", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3m3lh/what_tools_do_you_guys_use_for_schema_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3m3lh/what_tools_do_you_guys_use_for_schema_analysis/", "subreddit_subscribers": 164916, "created_utc": 1709269528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, havent seen any info on this, so perhaps its very dumb idea... would it make sense to run Polars on Databricks, using single node clusters.?\n\nReasoning being,... trying to get the advantages/features of databricks while at the same time using a small single node cluster with Polars instead of Spark/Pyspark to process data.   any opinions?\n\n", "author_fullname": "t2_3laxwg3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b37ca1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709231087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, havent seen any info on this, so perhaps its very dumb idea... would it make sense to run Polars on Databricks, using single node clusters.?&lt;/p&gt;\n\n&lt;p&gt;Reasoning being,... trying to get the advantages/features of databricks while at the same time using a small single node cluster with Polars instead of Spark/Pyspark to process data.   any opinions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b37ca1", "is_robot_indexable": true, "report_reasons": null, "author": "acelisalas", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b37ca1/polars_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b37ca1/polars_on_databricks/", "subreddit_subscribers": 164916, "created_utc": 1709231087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nI've got to make a report from our DW on customers and their transactions (for example). I am divided between having a single Spark job handle all the customers+transactions and give it lots of breathing room (in terms of cluster size), or whether I should have the Spark job handle a single customer+(all transactions) and rely on Airflow to get everything done.\n\nAdvice? Please and thank you.\n\nUpdate: The report is like a statement. One report per customer, including all transactions for the given time period.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Many \"customers\" in single Spark job, or many Spark jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3dkm3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709247485.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709245958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got to make a report from our DW on customers and their transactions (for example). I am divided between having a single Spark job handle all the customers+transactions and give it lots of breathing room (in terms of cluster size), or whether I should have the Spark job handle a single customer+(all transactions) and rely on Airflow to get everything done.&lt;/p&gt;\n\n&lt;p&gt;Advice? Please and thank you.&lt;/p&gt;\n\n&lt;p&gt;Update: The report is like a statement. One report per customer, including all transactions for the given time period.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3dkm3", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3dkm3/many_customers_in_single_spark_job_or_many_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3dkm3/many_customers_in_single_spark_job_or_many_spark/", "subreddit_subscribers": 164916, "created_utc": 1709245958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If DuckDb and dbt snapshot had a baby", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1b34lag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "287cf772-ac9d-11eb-aa84-0ead36cb44af", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rfI1t9OkvDrHhQj1mW6PkEO7O6ES-_zHhH85_PMj4mM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709224443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/okfy1mrnxjlc1.gif", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?format=png8&amp;s=f3d0d3774f848b3e4b122db9c8cd8008ac461bdf", "width": 800, "height": 600}, "resolutions": [{"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=6bc4f5d39365b9ddf2279d6ea8c92df6af0e92f6", "width": 108, "height": 81}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=eb2aa40846c0019b47d166889b9ec31d32fdcff5", "width": 216, "height": 162}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=4b7cd4a7b5ee25bbc1a561596df48a28a62ddcdf", "width": 320, "height": 240}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=52594cbe54fc43cae1a99189591fc41dc846f7c9", "width": 640, "height": 480}], "variants": {"gif": {"source": {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?s=0987419796f35c807cdb61548584e7f30bb35ddd", "width": 800, "height": 600}, "resolutions": [{"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=108&amp;crop=smart&amp;s=59b599f6c8db7ea6685f94bf6c2c65cec9aa0f21", "width": 108, "height": 81}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=216&amp;crop=smart&amp;s=e331c2813a0adb69c01f1eb15e7e256971ed22a8", "width": 216, "height": 162}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=320&amp;crop=smart&amp;s=05e952f2124949eed70113c7a3be6e5b5a45a61f", "width": 320, "height": 240}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=640&amp;crop=smart&amp;s=d33c572e19921ecb002d0f18879666e7bac284bc", "width": 640, "height": 480}]}, "mp4": {"source": {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?format=mp4&amp;s=c96cfcf49b8c716100ce1ae504ec48c77b76ddbb", "width": 800, "height": 600}, "resolutions": [{"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=108&amp;format=mp4&amp;s=6e6734053a2d5f0a1e50bb2874aa88120d83c762", "width": 108, "height": 81}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=216&amp;format=mp4&amp;s=ba5786607830da36758615226237902bd0283279", "width": 216, "height": 162}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=320&amp;format=mp4&amp;s=ef67fc499fc4717eb3545379ab25c6560d7de420", "width": 320, "height": 240}, {"url": "https://preview.redd.it/okfy1mrnxjlc1.gif?width=640&amp;format=mp4&amp;s=8d5fcc65a1666f8bd84dfc24f903380ab01084fe", "width": 640, "height": 480}]}}, "id": "UklnMD9hFtQNuBI9rsBdDxN5wSNZjBccH9s7WJ4tHBk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Software Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1b34lag", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1b34lag/if_duckdb_and_dbt_snapshot_had_a_baby/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/okfy1mrnxjlc1.gif", "subreddit_subscribers": 164916, "created_utc": 1709224443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am investigating current ELT/ETL tools like airbyte, cloudquery, fivetran etc. and all of them seems to be very complicated to setup and maintain locally due to their architecture with different types of containers and moving parts.\n\nIs this complexity really worth it? As a user do you think a simpler alternative like a single executable be better? What are your thoughts about the usage experience and maintainability?", "author_fullname": "t2_20n43h6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts about current ELT/ETL tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b3vdae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709302750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am investigating current ELT/ETL tools like airbyte, cloudquery, fivetran etc. and all of them seems to be very complicated to setup and maintain locally due to their architecture with different types of containers and moving parts.&lt;/p&gt;\n\n&lt;p&gt;Is this complexity really worth it? As a user do you think a simpler alternative like a single executable be better? What are your thoughts about the usage experience and maintainability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3vdae", "is_robot_indexable": true, "report_reasons": null, "author": "gibriyagi", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3vdae/thoughts_about_current_eltetl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3vdae/thoughts_about_current_eltetl_tools/", "subreddit_subscribers": 164916, "created_utc": 1709302750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have set up scrapers to extract real estate data with the goal of storing this into a relational database to perform analysis on. I have more focus in data science so I am new to the field of data engineering. Assuming I parse 500 different houses, should i store my data directly into the relational database, or should I save the raw json files and then insert them manually. Also, these json files for each property can get relatively large, while scraping is it unwise to store all of the property json files into one larger json file?\n\nThanks for the help!", "author_fullname": "t2_i1oi9p4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner Need advice on intermediate storage of webscraped data.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3jjt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709261734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have set up scrapers to extract real estate data with the goal of storing this into a relational database to perform analysis on. I have more focus in data science so I am new to the field of data engineering. Assuming I parse 500 different houses, should i store my data directly into the relational database, or should I save the raw json files and then insert them manually. Also, these json files for each property can get relatively large, while scraping is it unwise to store all of the property json files into one larger json file?&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3jjt1", "is_robot_indexable": true, "report_reasons": null, "author": "No_Map3272", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3jjt1/beginner_need_advice_on_intermediate_storage_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3jjt1/beginner_need_advice_on_intermediate_storage_of/", "subreddit_subscribers": 164916, "created_utc": 1709261734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Question for all you that had entry level positions or started off with easier projects. What was the project? What tools did you use? And what was your day to day like?", "author_fullname": "t2_6cmmqb4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Entry Level\" or \"Junior\" if still exist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3afn7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709238584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question for all you that had entry level positions or started off with easier projects. What was the project? What tools did you use? And what was your day to day like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3afn7", "is_robot_indexable": true, "report_reasons": null, "author": "chainz3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3afn7/entry_level_or_junior_if_still_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3afn7/entry_level_or_junior_if_still_exist/", "subreddit_subscribers": 164916, "created_utc": 1709238584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New library out of airbyte - [\u201cPyAirbyte\u201d](https://docs.airbyte.com/using-airbyte/pyairbyte/getting-started). Excited to use airbyte connectors without a UI or platform, I generally stick to writing scripts over no-code tools\n\n&amp;#x200B;\n\nDo you think you will try this in your ELT/ETL pipelines? ", "author_fullname": "t2_5h5nqi7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python library for ELT pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b36rlx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709229680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New library out of airbyte - &lt;a href=\"https://docs.airbyte.com/using-airbyte/pyairbyte/getting-started\"&gt;\u201cPyAirbyte\u201d&lt;/a&gt;. Excited to use airbyte connectors without a UI or platform, I generally stick to writing scripts over no-code tools&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do you think you will try this in your ELT/ETL pipelines? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b36rlx", "is_robot_indexable": true, "report_reasons": null, "author": "Chemical-Treat6596", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b36rlx/python_library_for_elt_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b36rlx/python_library_for_elt_pipelines/", "subreddit_subscribers": 164916, "created_utc": 1709229680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to transfer the 365 data through api from 365 to big query, what would be the effective way to do this process !? Any suggestions ", "author_fullname": "t2_9r5qrevk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft 365 business central to big query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b34y1i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709225319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to transfer the 365 data through api from 365 to big query, what would be the effective way to do this process !? Any suggestions &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b34y1i", "is_robot_indexable": true, "report_reasons": null, "author": "Fabro_vaz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b34y1i/microsoft_365_business_central_to_big_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b34y1i/microsoft_365_business_central_to_big_query/", "subreddit_subscribers": 164916, "created_utc": 1709225319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello.  I am looking to find demo data that I can use to practice some data modeling techniques.  Ideally, this data would be very normalized, such as how it would look when sourced from an application system.  I also hope to find data that includes snapshots of dimensional data to support constructed SCD.  Any suggestions on where I can find such a data set?", "author_fullname": "t2_l04by", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demo data for data modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3u70d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709299513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.  I am looking to find demo data that I can use to practice some data modeling techniques.  Ideally, this data would be very normalized, such as how it would look when sourced from an application system.  I also hope to find data that includes snapshots of dimensional data to support constructed SCD.  Any suggestions on where I can find such a data set?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3u70d", "is_robot_indexable": true, "report_reasons": null, "author": "kentmaxwell", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3u70d/demo_data_for_data_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3u70d/demo_data_for_data_modeling/", "subreddit_subscribers": 164916, "created_utc": 1709299513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,  \nI'm currently being offered a role as the data migration lead for a healthcare organisation changing electronic patient record systems. I've been unofficially doing this for a while, but they're talking about making this more official for the next 8 months or so. My background is really in business intelligence, data analysis, etc, so this is somewhat new to me. Is there any resources I should read about best practice for this kind of thing?   \n\n\nBoth of the systems have a SQL Server back end in case that's relevant, although technically the process basically involves creating a bunch of CSVs and loading them into a tool provided by the software supplier.  \nThanks", "author_fullname": "t2_bfginlr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Migration best practice, book/resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3qzi7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709288388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nI&amp;#39;m currently being offered a role as the data migration lead for a healthcare organisation changing electronic patient record systems. I&amp;#39;ve been unofficially doing this for a while, but they&amp;#39;re talking about making this more official for the next 8 months or so. My background is really in business intelligence, data analysis, etc, so this is somewhat new to me. Is there any resources I should read about best practice for this kind of thing?   &lt;/p&gt;\n\n&lt;p&gt;Both of the systems have a SQL Server back end in case that&amp;#39;s relevant, although technically the process basically involves creating a bunch of CSVs and loading them into a tool provided by the software supplier.&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3qzi7", "is_robot_indexable": true, "report_reasons": null, "author": "MakingNumbersBehave", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3qzi7/data_migration_best_practice_bookresources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3qzi7/data_migration_best_practice_bookresources/", "subreddit_subscribers": 164916, "created_utc": 1709288388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! Long time lurker of this sub. I've been working on a data quality tool to help data teams communicate their data requirements and enforce them on data sources. You can think of it as a data contracts development platform, or in other words, building data APIs. I would really love everyone's honest feedback on my MVP (please be honest but also helpful as possible). Leave a short comment on the post if anything!\n\nCan check it out here! [Loom Demo Link](https://www.loom.com/share/ec43b99a64094da0a8235cda060c6c10?sid=ba5043f2-3b20-4bb6-b20b-c9d00efdc981)\n\nIt'll go a long way in improving the product and eventually helping organizations manage and build scalable data infras.\n\nI'd also love to talk to people and all things data quality! Awesome way to reach me: [https://calendly.com/ryan-closure/30min?month=2024-03](https://calendly.com/ryan-closure/30min?month=2024-03)\n\nBig respect to all the folks here and thanks!", "author_fullname": "t2_9zilrtf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Honest Feedback on Data Quality Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3psxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709302080.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709283498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! Long time lurker of this sub. I&amp;#39;ve been working on a data quality tool to help data teams communicate their data requirements and enforce them on data sources. You can think of it as a data contracts development platform, or in other words, building data APIs. I would really love everyone&amp;#39;s honest feedback on my MVP (please be honest but also helpful as possible). Leave a short comment on the post if anything!&lt;/p&gt;\n\n&lt;p&gt;Can check it out here! &lt;a href=\"https://www.loom.com/share/ec43b99a64094da0a8235cda060c6c10?sid=ba5043f2-3b20-4bb6-b20b-c9d00efdc981\"&gt;Loom Demo Link&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;ll go a long way in improving the product and eventually helping organizations manage and build scalable data infras.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d also love to talk to people and all things data quality! Awesome way to reach me: &lt;a href=\"https://calendly.com/ryan-closure/30min?month=2024-03\"&gt;https://calendly.com/ryan-closure/30min?month=2024-03&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Big respect to all the folks here and thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?auto=webp&amp;s=7c4310519f61ec4fb86f317eb64448af04517967", "width": 2400, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8962e2587966fe1151c1cb2d3da899e2f659ac4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b807a1ad51a8a9a4511e6405e312046216cca911", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0fe70178af2da36da999a8c85b57fee25b8e7920", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8b86d75c4ec9a0f6944cd21bbd5a3f2c9f5db31", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb231a58575d582c44c0d632b6a1b6a06ff33179", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/fIzE_b_cjOXYF24YK4ZLDsxYib3uuXriW4KXgyLKB8k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e500d7385200026eef0e6bb2ea2c1b116635b8c1", "width": 1080, "height": 540}], "variants": {}, "id": "qeifkwwr4C5QSg58B_96VUqdBQ2DqQxNVOT5ptzpTrU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3psxn", "is_robot_indexable": true, "report_reasons": null, "author": "ParfaitRude229", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3psxn/honest_feedback_on_data_quality_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3psxn/honest_feedback_on_data_quality_tool/", "subreddit_subscribers": 164916, "created_utc": 1709283498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello r/dataengineering!\n\n&amp;#x200B;\n\nI'm part of the [Jargon.sh](https://Jargon.sh) team, a platform on a mission to bring the principles and tools of open-source software development to Domain Driven Design (DDD) and API design. Our scope is broad, but we've traditionally focused on providing just enough data modelling to meet our users' needs. However, we're seeing a growing demand from clients who are interested in leveraging our platform for general-purpose data modelling, marking a new and exciting direction for us.\n\n&amp;#x200B;\n\nOur clients have shared how much they value the DDD approach for breaking down large models into smaller, more manageable domain models, and how our method of calculating Semantic Version (SemVer) release numbers has been a game-changer for them. These strategies have proven effective for their API design and integration architecture. Additionally, they appreciate Jargon as a platform of reusable models that can be easily searched, discovered, and imported into other domains, all based on immutable SemVer versioned releases. This capability not only enhances the efficiency of their work by promoting reusability and consistency across different projects but also significantly reduces the time and effort required in developing new domain models from scratch. This feedback underscores why our clients are encouraging us to extend our support to include more comprehensive data modelling capabilities.\n\n&amp;#x200B;\n\nOur goal extends beyond offering generic tools; we aim to understand the unique challenges our users face and to develop innovative solutions. By engaging closely with the community, we believe we can customise our solutions to meet specific needs and challenges. This collaborative approach has served us well in the realms of DDD and API, and we're eager to apply it to data modelling for data engineering, hoping to add significant value.\n\n&amp;#x200B;\n\nAs a member of the Jargon team, I'm here to collect your feedback and insights on how an open-source software-inspired approach to data modelling might benefit you. Your input is incredibly important to us as we strive to evolve Jargon into not just a tool, but a community-driven solution that empowers practitioners to achieve what they're trying to do more efficiently.\n\n&amp;#x200B;\n\nI'm quite new to Reddit, having been stalking around for a little while before finding this great community. Our research indicated that this is the most active and engaged data engineering community around, so I decided to reach out. It's clear there's a wealth of knowledge and experience here, and I'm excited to learn from you all and maybe convert some of your knowledge into features of our freely available data modelling platform in return.\n\n&amp;#x200B;\n\nIf you're not familiar with Jargon yet, I invite you to explore our platform. We offer a free-forever tier packed with features that could be of interest to you.\n\n&amp;#x200B;\n\nThank you for your time and insights. I'm looking forward to your feedback, and happy to answer any questions you might have!\n\n&amp;#x200B;", "author_fullname": "t2_36d03l80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pivoting our product from API design to Data Modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3ppn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709283093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m part of the &lt;a href=\"https://Jargon.sh\"&gt;Jargon.sh&lt;/a&gt; team, a platform on a mission to bring the principles and tools of open-source software development to Domain Driven Design (DDD) and API design. Our scope is broad, but we&amp;#39;ve traditionally focused on providing just enough data modelling to meet our users&amp;#39; needs. However, we&amp;#39;re seeing a growing demand from clients who are interested in leveraging our platform for general-purpose data modelling, marking a new and exciting direction for us.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Our clients have shared how much they value the DDD approach for breaking down large models into smaller, more manageable domain models, and how our method of calculating Semantic Version (SemVer) release numbers has been a game-changer for them. These strategies have proven effective for their API design and integration architecture. Additionally, they appreciate Jargon as a platform of reusable models that can be easily searched, discovered, and imported into other domains, all based on immutable SemVer versioned releases. This capability not only enhances the efficiency of their work by promoting reusability and consistency across different projects but also significantly reduces the time and effort required in developing new domain models from scratch. This feedback underscores why our clients are encouraging us to extend our support to include more comprehensive data modelling capabilities.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Our goal extends beyond offering generic tools; we aim to understand the unique challenges our users face and to develop innovative solutions. By engaging closely with the community, we believe we can customise our solutions to meet specific needs and challenges. This collaborative approach has served us well in the realms of DDD and API, and we&amp;#39;re eager to apply it to data modelling for data engineering, hoping to add significant value.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As a member of the Jargon team, I&amp;#39;m here to collect your feedback and insights on how an open-source software-inspired approach to data modelling might benefit you. Your input is incredibly important to us as we strive to evolve Jargon into not just a tool, but a community-driven solution that empowers practitioners to achieve what they&amp;#39;re trying to do more efficiently.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite new to Reddit, having been stalking around for a little while before finding this great community. Our research indicated that this is the most active and engaged data engineering community around, so I decided to reach out. It&amp;#39;s clear there&amp;#39;s a wealth of knowledge and experience here, and I&amp;#39;m excited to learn from you all and maybe convert some of your knowledge into features of our freely available data modelling platform in return.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re not familiar with Jargon yet, I invite you to explore our platform. We offer a free-forever tier packed with features that could be of interest to you.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time and insights. I&amp;#39;m looking forward to your feedback, and happy to answer any questions you might have!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3ppn0", "is_robot_indexable": true, "report_reasons": null, "author": "Jargon-sh", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3ppn0/pivoting_our_product_from_api_design_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3ppn0/pivoting_our_product_from_api_design_to_data/", "subreddit_subscribers": 164916, "created_utc": 1709283093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In SQL I need to count occurrences of the date when a deal was passed to a salesperson, the meeting date, and the contract signing date, along with the sum of contracts. \n\nAll this data is in a single table with a unique ID as the primary key (PK).\n\nThe challenge is that I need to obtain the count of each of these dates by month without using any of the dates directly for grouping. This is because grouping by any of these dates directly would result in counting by that specific date. For example, if someone had a meeting in January and we group by the meeting date, we wouldn't capture their contract in February, etc.\n\nHow should I approach this?\"", "author_fullname": "t2_803gugjh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Count of different dates occurencies from one table ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3lump", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709268765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In SQL I need to count occurrences of the date when a deal was passed to a salesperson, the meeting date, and the contract signing date, along with the sum of contracts. &lt;/p&gt;\n\n&lt;p&gt;All this data is in a single table with a unique ID as the primary key (PK).&lt;/p&gt;\n\n&lt;p&gt;The challenge is that I need to obtain the count of each of these dates by month without using any of the dates directly for grouping. This is because grouping by any of these dates directly would result in counting by that specific date. For example, if someone had a meeting in January and we group by the meeting date, we wouldn&amp;#39;t capture their contract in February, etc.&lt;/p&gt;\n\n&lt;p&gt;How should I approach this?&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3lump", "is_robot_indexable": true, "report_reasons": null, "author": "Novel_Pattern8035", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3lump/count_of_different_dates_occurencies_from_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3lump/count_of_different_dates_occurencies_from_one/", "subreddit_subscribers": 164916, "created_utc": 1709268765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious on people\u2019s thoughts\n\nContext: loading data to delta lake through spark jobs. \n\nFlow: \nRead incremental data from db\nMerge to Staging table\nValidate table\nOverwrite prod with stagjng\n\n\nQuestion is if staging tables are necessary here. The overwrite process is quite expensive since there is a 400gb table to overwrite daily. I could also merge to production after validating staging but that is also expensive and redundant.\n\n\nIf I just merged to a prod table and had a validation hook to time travel the table back in case of any errors, that doesn\u2019t seem that bad given the tables aren\u2019t being accessed during the load at all so data quality is not a huge concern\n\nAlso, no, the pattern of renaming a table doesn\u2019t work in our enterprise\n\nAnyways, curious what patterns people do for delta lake ", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake Staging Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3lipj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709267680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious on people\u2019s thoughts&lt;/p&gt;\n\n&lt;p&gt;Context: loading data to delta lake through spark jobs. &lt;/p&gt;\n\n&lt;p&gt;Flow: \nRead incremental data from db\nMerge to Staging table\nValidate table\nOverwrite prod with stagjng&lt;/p&gt;\n\n&lt;p&gt;Question is if staging tables are necessary here. The overwrite process is quite expensive since there is a 400gb table to overwrite daily. I could also merge to production after validating staging but that is also expensive and redundant.&lt;/p&gt;\n\n&lt;p&gt;If I just merged to a prod table and had a validation hook to time travel the table back in case of any errors, that doesn\u2019t seem that bad given the tables aren\u2019t being accessed during the load at all so data quality is not a huge concern&lt;/p&gt;\n\n&lt;p&gt;Also, no, the pattern of renaming a table doesn\u2019t work in our enterprise&lt;/p&gt;\n\n&lt;p&gt;Anyways, curious what patterns people do for delta lake &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3lipj", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3lipj/delta_lake_staging_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3lipj/delta_lake_staging_tables/", "subreddit_subscribers": 164916, "created_utc": 1709267680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say you are dealing with multiple csv files, each with over 10 million rows. What kind of validation tests do you perform on the data itself and what type of tests do you write for dealing with database part? Which libraries do you use for it? \n\nAny reference material to read up on would be very helpful! ", "author_fullname": "t2_61uvorko", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of validation do you perform on the data and what tests do you write for correct data handling in to a dB? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3ip2a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709259266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say you are dealing with multiple csv files, each with over 10 million rows. What kind of validation tests do you perform on the data itself and what type of tests do you write for dealing with database part? Which libraries do you use for it? &lt;/p&gt;\n\n&lt;p&gt;Any reference material to read up on would be very helpful! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3ip2a", "is_robot_indexable": true, "report_reasons": null, "author": "jadedsprint", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3ip2a/what_kind_of_validation_do_you_perform_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3ip2a/what_kind_of_validation_do_you_perform_on_the/", "subreddit_subscribers": 164916, "created_utc": 1709259266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I own a small business and I have about two years of sales records, our data stack looks like this:  \n\n\nNotion database where we keep the sales records +\n\nPull daily sales into Excel via Power Query through Notion API+\n\nGenerate Excel Pivot Tables and Pivot charts for analysis. The Year/Quarter/Month grouping feature has worked very well so far.\n\nAs you can see these are mostly free and basic tools (I use the single-user, free version of Notion). \n\n  \nNow I would love to migrate this to something where I can do SQL, generate some graphs and perhaps s3 storage. Would like to keep it free as well. \n\nI was thinking of setting up a MySQL and MinIO server in my computer and work from there but I don't know what kind of limitations MinIO has as of today.\n\nDoes anyone have any advice?\n\n&amp;#x200B;", "author_fullname": "t2_5xd8fkm8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for small business data stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b3fthq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709251466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I own a small business and I have about two years of sales records, our data stack looks like this:  &lt;/p&gt;\n\n&lt;p&gt;Notion database where we keep the sales records +&lt;/p&gt;\n\n&lt;p&gt;Pull daily sales into Excel via Power Query through Notion API+&lt;/p&gt;\n\n&lt;p&gt;Generate Excel Pivot Tables and Pivot charts for analysis. The Year/Quarter/Month grouping feature has worked very well so far.&lt;/p&gt;\n\n&lt;p&gt;As you can see these are mostly free and basic tools (I use the single-user, free version of Notion). &lt;/p&gt;\n\n&lt;p&gt;Now I would love to migrate this to something where I can do SQL, generate some graphs and perhaps s3 storage. Would like to keep it free as well. &lt;/p&gt;\n\n&lt;p&gt;I was thinking of setting up a MySQL and MinIO server in my computer and work from there but I don&amp;#39;t know what kind of limitations MinIO has as of today.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any advice?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b3fthq", "is_robot_indexable": true, "report_reasons": null, "author": "carlsLobato", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3fthq/advice_for_small_business_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3fthq/advice_for_small_business_data_stack/", "subreddit_subscribers": 164916, "created_utc": 1709251466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a newbie about data warehouse. I want to design a Warehouse using Star Schema for company problems.\n Different business processes will be designed with different schemas such as sales_schema, financial_schema. However, some schemas will use common dim tables such as dim_product. So how should the dim table be designed and stored in schema to be most effective and appropriate?\n- Should a table dim_product be created in a common schema (eg public_schema) and other schemas (sales_schema, financial_schema) can use data from the common table dim_product (public_schema)\n- Each schema (sales_schema, financial_schema) will create a dim_product table (duplicate data)\n\nPlease help me. Thank you !", "author_fullname": "t2_7fja37x0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Design schema use Star Schema in Data Warehouse Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b33ph1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709222262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a newbie about data warehouse. I want to design a Warehouse using Star Schema for company problems.\n Different business processes will be designed with different schemas such as sales_schema, financial_schema. However, some schemas will use common dim tables such as dim_product. So how should the dim table be designed and stored in schema to be most effective and appropriate?\n- Should a table dim_product be created in a common schema (eg public_schema) and other schemas (sales_schema, financial_schema) can use data from the common table dim_product (public_schema)\n- Each schema (sales_schema, financial_schema) will create a dim_product table (duplicate data)&lt;/p&gt;\n\n&lt;p&gt;Please help me. Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b33ph1", "is_robot_indexable": true, "report_reasons": null, "author": "Waste-Orchid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b33ph1/design_schema_use_star_schema_in_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b33ph1/design_schema_use_star_schema_in_data_warehouse/", "subreddit_subscribers": 164916, "created_utc": 1709222262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m the lone Product Analyst at a small SaaS company. We are embarking on a data warehouse project this year and since we don\u2019t have dedicated DE\u2019s, I\u2019m kind of playing a data PM role where I\u2019m scoping out the work, writing reqs, and getting as close to the building as possible before passing off to Eng to do the work. \n\nWe have a lot of different data sources but are looking to get it all into BigQuery at the end of the day. \n\nWorking on my SQL and python skills and interested in tackling a project this weekend that can uplevel those and also get me some DE experience so I can better understand what our Eng Team will be working with. ", "author_fullname": "t2_lbq0k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s a good weekend project to uplevel my DE skills as a Product Analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b3v71f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709302291.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m the lone Product Analyst at a small SaaS company. We are embarking on a data warehouse project this year and since we don\u2019t have dedicated DE\u2019s, I\u2019m kind of playing a data PM role where I\u2019m scoping out the work, writing reqs, and getting as close to the building as possible before passing off to Eng to do the work. &lt;/p&gt;\n\n&lt;p&gt;We have a lot of different data sources but are looking to get it all into BigQuery at the end of the day. &lt;/p&gt;\n\n&lt;p&gt;Working on my SQL and python skills and interested in tackling a project this weekend that can uplevel those and also get me some DE experience so I can better understand what our Eng Team will be working with. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b3v71f", "is_robot_indexable": true, "report_reasons": null, "author": "BillyE53", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b3v71f/whats_a_good_weekend_project_to_uplevel_my_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b3v71f/whats_a_good_weekend_project_to_uplevel_my_de/", "subreddit_subscribers": 164916, "created_utc": 1709302291.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}