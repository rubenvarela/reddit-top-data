{"kind": "Listing", "data": {"after": "t3_1bnl9gw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working in a large Healthcare company , there are critical legacy ETL processing that pull data from two source systems built by someone over 15 years ago. The ETL is done in layers of SQL to output maybe 400 fields across 15 tables. The layered complex SQL manipulates data from the source system with literally hundreds of if else statements, window functions, apply lookups etc across 5 or 6 layers of staging tables. \n\nRecently the business has come to us to say a number looks incorrect possibly. I was tasked with investigation and fixing any issues.\n\nThe problem is the sql has much logic pulling from random tables and columns across both systems, applying crazy amounts of conditional transformations that I don't know how to even begin.\n\nI set up a meeting with the business who explained the business logic around what the field should be, the problem is I have no idea how to get to that in source systems with thousands of tables and fields. \n\nAnyone faced such an issue?", "author_fullname": "t2_ew0arl93h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to maintain complex SQL ETL with no understanding of the source system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn1ioq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711330003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working in a large Healthcare company , there are critical legacy ETL processing that pull data from two source systems built by someone over 15 years ago. The ETL is done in layers of SQL to output maybe 400 fields across 15 tables. The layered complex SQL manipulates data from the source system with literally hundreds of if else statements, window functions, apply lookups etc across 5 or 6 layers of staging tables. &lt;/p&gt;\n\n&lt;p&gt;Recently the business has come to us to say a number looks incorrect possibly. I was tasked with investigation and fixing any issues.&lt;/p&gt;\n\n&lt;p&gt;The problem is the sql has much logic pulling from random tables and columns across both systems, applying crazy amounts of conditional transformations that I don&amp;#39;t know how to even begin.&lt;/p&gt;\n\n&lt;p&gt;I set up a meeting with the business who explained the business logic around what the field should be, the problem is I have no idea how to get to that in source systems with thousands of tables and fields. &lt;/p&gt;\n\n&lt;p&gt;Anyone faced such an issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn1ioq", "is_robot_indexable": true, "report_reasons": null, "author": "quartzsmelt", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn1ioq/trying_to_maintain_complex_sql_etl_with_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn1ioq/trying_to_maintain_complex_sql_etl_with_no/", "subreddit_subscribers": 171702, "created_utc": 1711330003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all!\nI feel kinda stuck and lost career wise. I broke into DE and related fields by accident, taking an internship assembling the DW of a large company right when I was finishing my bachelor in Economics. 4.5y later, I'm very proficient with SQL and Python, I've built a few ETLs and done a fair share of KPIs, dashboards and fact tables and the like. I'm good with dbt but no senior level (failed a recent technical) and don't know much about PySpark, Hadoop, etc. I'm very insecure as to what to do, which technologies to learn (and how to prove I know them in my CV). I'm occasionally called an AE, DA or DE because the positions have interchangable titles here in my country. What do I try next?\n", "author_fullname": "t2_bdy92fyd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mid level and stuck", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bmxjhe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711319479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all!\nI feel kinda stuck and lost career wise. I broke into DE and related fields by accident, taking an internship assembling the DW of a large company right when I was finishing my bachelor in Economics. 4.5y later, I&amp;#39;m very proficient with SQL and Python, I&amp;#39;ve built a few ETLs and done a fair share of KPIs, dashboards and fact tables and the like. I&amp;#39;m good with dbt but no senior level (failed a recent technical) and don&amp;#39;t know much about PySpark, Hadoop, etc. I&amp;#39;m very insecure as to what to do, which technologies to learn (and how to prove I know them in my CV). I&amp;#39;m occasionally called an AE, DA or DE because the positions have interchangable titles here in my country. What do I try next?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bmxjhe", "is_robot_indexable": true, "report_reasons": null, "author": "BelugaGolfinho", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bmxjhe/mid_level_and_stuck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bmxjhe/mid_level_and_stuck/", "subreddit_subscribers": 171702, "created_utc": 1711319479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company recently launched their data engineering team, and I'm their newest hire (a self-taught fresh grad, &lt;1 month in the company). I have two seniors and we're the only data engineers here.  \n\nWe use Snowflake as our central data warehouse, and we do most of the SQL transformations there. Currently planning to use dbt Core instead and then orchestrate it using Apache Airflow. We're weighing the pros and cons of using Amazon MWAA vs self-hosting in an EC2 server considering the price, ease of setting up, scalability, and potential bugs/issues. If we self-host, we're gonna use Astronomer's Astro CLI and Cosmos to set up dbt with Airflow.  \n\nHas anyone successfully deployed dbt + Airflow on EC2? What are some of the issues you encountered? How's Cosmos for integrating dbt with Airflow? Should we just go with MWAA? I've heard many had success with it and cost is the only concern. We're trying to minimize costs, so as much as possible we're opting for open-source solutions (I believe Astro CLI and Cosmos are free if you're not using their cloud/IDE version).\n\nAny insights/advice would be highly appreciated. TYIA!\n\n&amp;#x200B;", "author_fullname": "t2_ts2pktzo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrating dbt Core with Apache Airflow: MWAA vs Self-Hosted EC2 Deployment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnebbr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711375051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company recently launched their data engineering team, and I&amp;#39;m their newest hire (a self-taught fresh grad, &amp;lt;1 month in the company). I have two seniors and we&amp;#39;re the only data engineers here.  &lt;/p&gt;\n\n&lt;p&gt;We use Snowflake as our central data warehouse, and we do most of the SQL transformations there. Currently planning to use dbt Core instead and then orchestrate it using Apache Airflow. We&amp;#39;re weighing the pros and cons of using Amazon MWAA vs self-hosting in an EC2 server considering the price, ease of setting up, scalability, and potential bugs/issues. If we self-host, we&amp;#39;re gonna use Astronomer&amp;#39;s Astro CLI and Cosmos to set up dbt with Airflow.  &lt;/p&gt;\n\n&lt;p&gt;Has anyone successfully deployed dbt + Airflow on EC2? What are some of the issues you encountered? How&amp;#39;s Cosmos for integrating dbt with Airflow? Should we just go with MWAA? I&amp;#39;ve heard many had success with it and cost is the only concern. We&amp;#39;re trying to minimize costs, so as much as possible we&amp;#39;re opting for open-source solutions (I believe Astro CLI and Cosmos are free if you&amp;#39;re not using their cloud/IDE version).&lt;/p&gt;\n\n&lt;p&gt;Any insights/advice would be highly appreciated. TYIA!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnebbr", "is_robot_indexable": true, "report_reasons": null, "author": "zmxavier", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnebbr/orchestrating_dbt_core_with_apache_airflow_mwaa/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnebbr/orchestrating_dbt_core_with_apache_airflow_mwaa/", "subreddit_subscribers": 171702, "created_utc": 1711375051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the behavioral and technical questions would be the best to identify if a candidate a good fit or not, and if he/she actually really good data engineers? ", "author_fullname": "t2_7chglbbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question to ask senior data engineers during interviews?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn843b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711353211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the behavioral and technical questions would be the best to identify if a candidate a good fit or not, and if he/she actually really good data engineers? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn843b", "is_robot_indexable": true, "report_reasons": null, "author": "Computingss", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn843b/question_to_ask_senior_data_engineers_during/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn843b/question_to_ask_senior_data_engineers_during/", "subreddit_subscribers": 171702, "created_utc": 1711353211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nSo, I just smashed the DP-203 exam yesterday (24th Mar 2024), and I gotta get real about something. You know all those YouTubers claiming they aced it in just seven days? Yeah, total BS. There's no way you can prep for this exam in a week and come out unscathed.\n\nHere's the deal: the syllabus is massive, like trying to eat a whole pizza in one sitting massive. Azure's got a bazillion products and concepts, and mastering them takes time. So, if you're thinking about cramming, think again.\n\nBut fear not, fellow Azure adventurers! I've got some advice that'll steer you in the right direction:\n\n1. **MS Learn is Your Friend:** Don't waste precious time scribbling notes. Blast through the self-paced MS Learn syllabus to get a bird's-eye view of Azure cloud. Do all the exercises like your cloud career depends on it.\n\n2. **YouTube Isn't Gospel:** Sure, YouTube is a treasure trove of exam Q&amp;A guides, but don't take everything at face value. A good chunk of those answers is as reliable as a chocolate teapot. Crosscheck with your own logic and, hey, even bounce ideas off ChatGPT, Gemini, or Copilot.\n\nAnd lastly, here's a pro tip: book your exam date before you even crack open the books. It's like setting a deadline for yourself, but with a bit of added pressure...and excitement!\n\nSo, there you have it, folks. Go forth, conquer the Azure skies, and remember: Rome wasn't built in a day, and neither is Azure expertise. \ud83d\ude01\u2728\n", "author_fullname": "t2_t67twgnr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Justs Cleared DP-203 Exam - Some Real Talk and Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn1yl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711331255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;So, I just smashed the DP-203 exam yesterday (24th Mar 2024), and I gotta get real about something. You know all those YouTubers claiming they aced it in just seven days? Yeah, total BS. There&amp;#39;s no way you can prep for this exam in a week and come out unscathed.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the deal: the syllabus is massive, like trying to eat a whole pizza in one sitting massive. Azure&amp;#39;s got a bazillion products and concepts, and mastering them takes time. So, if you&amp;#39;re thinking about cramming, think again.&lt;/p&gt;\n\n&lt;p&gt;But fear not, fellow Azure adventurers! I&amp;#39;ve got some advice that&amp;#39;ll steer you in the right direction:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;MS Learn is Your Friend:&lt;/strong&gt; Don&amp;#39;t waste precious time scribbling notes. Blast through the self-paced MS Learn syllabus to get a bird&amp;#39;s-eye view of Azure cloud. Do all the exercises like your cloud career depends on it.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;YouTube Isn&amp;#39;t Gospel:&lt;/strong&gt; Sure, YouTube is a treasure trove of exam Q&amp;amp;A guides, but don&amp;#39;t take everything at face value. A good chunk of those answers is as reliable as a chocolate teapot. Crosscheck with your own logic and, hey, even bounce ideas off ChatGPT, Gemini, or Copilot.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;And lastly, here&amp;#39;s a pro tip: book your exam date before you even crack open the books. It&amp;#39;s like setting a deadline for yourself, but with a bit of added pressure...and excitement!&lt;/p&gt;\n\n&lt;p&gt;So, there you have it, folks. Go forth, conquer the Azure skies, and remember: Rome wasn&amp;#39;t built in a day, and neither is Azure expertise. \ud83d\ude01\u2728&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bn1yl3", "is_robot_indexable": true, "report_reasons": null, "author": "Square_Thing_4613", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn1yl3/justs_cleared_dp203_exam_some_real_talk_and_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn1yl3/justs_cleared_dp203_exam_some_real_talk_and_advice/", "subreddit_subscribers": 171702, "created_utc": 1711331255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Someone I know who is into DE role &gt;20yrs, he says there is absolutely no need to work on algorithms if you are a DE.  \n\n\nIn DE, to what extent are algorithms (ML algo or fine tuning related stuff) integrated into your day-to-day tasks or do you even deal with any sort of algorithm in your role? And if yes, are there particular ML algorithms or algorithmic concepts that you frequently encounter or utilize? ", "author_fullname": "t2_8i81bbqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much of you guys have to deal with algorithms or ML algorithms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnda5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711378244.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711372229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Someone I know who is into DE role &amp;gt;20yrs, he says there is absolutely no need to work on algorithms if you are a DE.  &lt;/p&gt;\n\n&lt;p&gt;In DE, to what extent are algorithms (ML algo or fine tuning related stuff) integrated into your day-to-day tasks or do you even deal with any sort of algorithm in your role? And if yes, are there particular ML algorithms or algorithmic concepts that you frequently encounter or utilize? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnda5x", "is_robot_indexable": true, "report_reasons": null, "author": "trafalgar28", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnda5x/how_much_of_you_guys_have_to_deal_with_algorithms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnda5x/how_much_of_you_guys_have_to_deal_with_algorithms/", "subreddit_subscribers": 171702, "created_utc": 1711372229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all - I\u2019m looking for a little bit of guidance regarding learning. I searched the subreddit and wasn\u2019t sure which resources to focus on.\n\nFor context - I\u2019m the head of data at a midsized company. My background is mostly in the analytics/data science space: building ML models, visuals, insights etc to drive business strategy. I am keenly aware that I have a pretty big gap around data engineering. I have picked a few things up here and there working with data engineers, but I really want to learn more to make me more effective. The way I\u2019m currently thinking about this (and let me know if this seems off base):\n\n1. Getting a deeper understanding of the problems that data engineering is solving for (technology agnostic) - basically the \u201cwhat\u201d and the \u201cwhy\u201d.\n\n2. Current industry best practices in terms of technologies, frameworks, approaches, etc to solve those problems - what are the major methods currently used to solve #1 above?\n\n3. Some level of hands on practice - I\u2019m someone who learns best doing it by hand in a relatively structured environment. Now, since my day to day isn\u2019t going to be doing data engineering work day in day out (have actual technical people to do it and run the teams), I don\u2019t (think..) I need to have advanced hands on practice, just enough to understand the high level approach and things to watch out for so I can advocate for my teams. \n\nIdeal scenario for me is to have a course to cover most of these to build a base, and then augment from there with reading/other items as necessary. Any suggestions? Does something like this exist? ", "author_fullname": "t2_u9cx9z9b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning resources and courses - data leader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bmz1rx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711323286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - I\u2019m looking for a little bit of guidance regarding learning. I searched the subreddit and wasn\u2019t sure which resources to focus on.&lt;/p&gt;\n\n&lt;p&gt;For context - I\u2019m the head of data at a midsized company. My background is mostly in the analytics/data science space: building ML models, visuals, insights etc to drive business strategy. I am keenly aware that I have a pretty big gap around data engineering. I have picked a few things up here and there working with data engineers, but I really want to learn more to make me more effective. The way I\u2019m currently thinking about this (and let me know if this seems off base):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Getting a deeper understanding of the problems that data engineering is solving for (technology agnostic) - basically the \u201cwhat\u201d and the \u201cwhy\u201d.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Current industry best practices in terms of technologies, frameworks, approaches, etc to solve those problems - what are the major methods currently used to solve #1 above?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Some level of hands on practice - I\u2019m someone who learns best doing it by hand in a relatively structured environment. Now, since my day to day isn\u2019t going to be doing data engineering work day in day out (have actual technical people to do it and run the teams), I don\u2019t (think..) I need to have advanced hands on practice, just enough to understand the high level approach and things to watch out for so I can advocate for my teams. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ideal scenario for me is to have a course to cover most of these to build a base, and then augment from there with reading/other items as necessary. Any suggestions? Does something like this exist? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bmz1rx", "is_robot_indexable": true, "report_reasons": null, "author": "RandomRandomPenguin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bmz1rx/learning_resources_and_courses_data_leader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bmz1rx/learning_resources_and_courses_data_leader/", "subreddit_subscribers": 171702, "created_utc": 1711323286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI currently work as an aerospace engineer (nominally), although a lot of my actual work has been to do with data and software. This is definitely what interests me, and I want to move into this sort of role full-time.\n\nHowever, my experience has been a bit of a mess across several different disciplines, so I'm really not sure what I'm qualified to go for or what's expected. I have some software development experience (creating custom analysis software in Python), some data analysis / data science experience, and then also some data engineering experience.\n\nHowever, a lot of the stuff that I have done has been using Palantir Foundry, while all of the jobs I see seem to want things like Azure, Databricks, AWS etc.\n\nIs Palantir Foundry an \"easier\" version of these tools, and I will struggle to move to something like Azure? Or is it broadly similar, and I should I look to do some self-learning on Azure, for example, and then I will be job ready for a data engineering role?\n\nThe work I have done in Palantir Foundry definitely seems quite data engineering related - I use Python / PySpark SQL and have created some custom dashboards using HTML / CSS / JavaScript. However, I haven't actually been involved in the ingestion of any raw data, I have usually taken existing data sets that were in a poor state and cleaned, transformed, joined multiple datasets to get the result I wanted.", "author_fullname": "t2_iemkebmei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Palantir Foundry Count as Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnbg99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711366574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I currently work as an aerospace engineer (nominally), although a lot of my actual work has been to do with data and software. This is definitely what interests me, and I want to move into this sort of role full-time.&lt;/p&gt;\n\n&lt;p&gt;However, my experience has been a bit of a mess across several different disciplines, so I&amp;#39;m really not sure what I&amp;#39;m qualified to go for or what&amp;#39;s expected. I have some software development experience (creating custom analysis software in Python), some data analysis / data science experience, and then also some data engineering experience.&lt;/p&gt;\n\n&lt;p&gt;However, a lot of the stuff that I have done has been using Palantir Foundry, while all of the jobs I see seem to want things like Azure, Databricks, AWS etc.&lt;/p&gt;\n\n&lt;p&gt;Is Palantir Foundry an &amp;quot;easier&amp;quot; version of these tools, and I will struggle to move to something like Azure? Or is it broadly similar, and I should I look to do some self-learning on Azure, for example, and then I will be job ready for a data engineering role?&lt;/p&gt;\n\n&lt;p&gt;The work I have done in Palantir Foundry definitely seems quite data engineering related - I use Python / PySpark SQL and have created some custom dashboards using HTML / CSS / JavaScript. However, I haven&amp;#39;t actually been involved in the ingestion of any raw data, I have usually taken existing data sets that were in a poor state and cleaned, transformed, joined multiple datasets to get the result I wanted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnbg99", "is_robot_indexable": true, "report_reasons": null, "author": "anonymous_lurker_01", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnbg99/does_palantir_foundry_count_as_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnbg99/does_palantir_foundry_count_as_data_engineering/", "subreddit_subscribers": 171702, "created_utc": 1711366574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nHello everyone , we have different source systems sitting in Amazon rds , Mongo db instances and so on. We are migrating all the data to redshift for single source of truth. For rds instances, we are using AWS dms to transfer the data. For mongo we have hourly scripts to transfer the data. Dms is not suitable for mongo in our usecase because of nature of the data we have . \n\nNow the problem is sometimes the data is not complete like missing data, sometimes it is not fresh due to various reasons in the dms, sometimes we are getting duplicate rows.\n\nNow we have to convey the SLA's to our downstream systems about the freshness like how much time this table or database will take to get th latest incremental data from source . And also we have to be confident enough to say like our data is complete , we are not missing anything. \n\n\nI have brainstormed several approaches but didn't get the concrete solution yet . One approach we decided was to have the list of important tables . Query the source and target every 15 mins to check the latest record in both the systems and the no of rows. This approach looks promising to me.  But the problem is our source db's are somewhat fragile and it requires a lot of approvals from the stake holders . If we fire count(*) query with our time range , to fetch the total no of records, it will take 10 mins in the worst case . \n\n\nNow how to tackle this problem and convey the freshness and SLA's to downstream systems.\n\nAny suggestions or external tools will be helpful. \n\nThanks in advance ", "author_fullname": "t2_bpwbtfnd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data freshness and completeness", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn74g3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711348926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone , we have different source systems sitting in Amazon rds , Mongo db instances and so on. We are migrating all the data to redshift for single source of truth. For rds instances, we are using AWS dms to transfer the data. For mongo we have hourly scripts to transfer the data. Dms is not suitable for mongo in our usecase because of nature of the data we have . &lt;/p&gt;\n\n&lt;p&gt;Now the problem is sometimes the data is not complete like missing data, sometimes it is not fresh due to various reasons in the dms, sometimes we are getting duplicate rows.&lt;/p&gt;\n\n&lt;p&gt;Now we have to convey the SLA&amp;#39;s to our downstream systems about the freshness like how much time this table or database will take to get th latest incremental data from source . And also we have to be confident enough to say like our data is complete , we are not missing anything. &lt;/p&gt;\n\n&lt;p&gt;I have brainstormed several approaches but didn&amp;#39;t get the concrete solution yet . One approach we decided was to have the list of important tables . Query the source and target every 15 mins to check the latest record in both the systems and the no of rows. This approach looks promising to me.  But the problem is our source db&amp;#39;s are somewhat fragile and it requires a lot of approvals from the stake holders . If we fire count(*) query with our time range , to fetch the total no of records, it will take 10 mins in the worst case . &lt;/p&gt;\n\n&lt;p&gt;Now how to tackle this problem and convey the freshness and SLA&amp;#39;s to downstream systems.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or external tools will be helpful. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bn74g3", "is_robot_indexable": true, "report_reasons": null, "author": "oofla_mey_goofla", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn74g3/data_freshness_and_completeness/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn74g3/data_freshness_and_completeness/", "subreddit_subscribers": 171702, "created_utc": 1711348926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I've been working hard to learn these, but I'm stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven't seen many jobs on Upwork. Is full-time freelancing even a possibility in this field? ", "author_fullname": "t2_w54oah5ij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Guidance on Advancing Skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnnfq8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711397087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I&amp;#39;ve been working hard to learn these, but I&amp;#39;m stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven&amp;#39;t seen many jobs on Upwork. Is full-time freelancing even a possibility in this field? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnnfq8", "is_robot_indexable": true, "report_reasons": null, "author": "abhannan980", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnnfq8/seeking_guidance_on_advancing_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnnfq8/seeking_guidance_on_advancing_skills/", "subreddit_subscribers": 171702, "created_utc": 1711397087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHello everyone,\n\nI have just completed learning Snowflake and dbt. Is building a project the only way to become proficient?", "author_fullname": "t2_ic15ya1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "snowflake+dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnkubv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711391040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I have just completed learning Snowflake and dbt. Is building a project the only way to become proficient?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnkubv", "is_robot_indexable": true, "report_reasons": null, "author": "satwikchandra", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnkubv/snowflakedbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnkubv/snowflakedbt/", "subreddit_subscribers": 171702, "created_utc": 1711391040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. \n\nOne of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. \n\nThe prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don't want to move anything to a new storage. So I'm a little worried if we are biting more than we can chew, but I don't want to lose a business that is already at the doorsteps. \n\nWhat are some implementation techniques you guys used during virtualization to address speed and performance issues?", "author_fullname": "t2_a1jxqx9hs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Potential Issues with Data Virtualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnis0n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711386134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. &lt;/p&gt;\n\n&lt;p&gt;One of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. &lt;/p&gt;\n\n&lt;p&gt;The prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don&amp;#39;t want to move anything to a new storage. So I&amp;#39;m a little worried if we are biting more than we can chew, but I don&amp;#39;t want to lose a business that is already at the doorsteps. &lt;/p&gt;\n\n&lt;p&gt;What are some implementation techniques you guys used during virtualization to address speed and performance issues?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnis0n", "is_robot_indexable": true, "report_reasons": null, "author": "turboline-ai", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnis0n/potential_issues_with_data_virtualization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnis0n/potential_issues_with_data_virtualization/", "subreddit_subscribers": 171702, "created_utc": 1711386134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Python package to help Databricks Unity Catalog users read and query Delta Lake tables with Polars, DuckDb, or PyArrow.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnf67w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DRcjTpTyb9GkuKMvyPZE89uYrxg2Ons7iy7JhO_rnWI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711377212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/danielbeach/lakescum", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?auto=webp&amp;s=6d21a764643609310ba3389f80673261cc8025e5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c996873227f58c286c4db36ef4d7cfd67c76e37c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4e02ecb0c65b217928fff4c91e98189640791a5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a9aad197a1c0cd0477a498cae2ab9a8fdf370fd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=02b63710d92488c40bbc932cfbf13f1f7dc4db67", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af8cec4826bda3fabf081007c22a673572befb61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b89038e7fcb5beaa1bef04c9cf48c2ee7e3c930", "width": 1080, "height": 540}], "variants": {}, "id": "u586jNjYI52ADVaOP025fVjfx6Pn53ZVWD_9GRGNRBg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bnf67w", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnf67w/a_python_package_to_help_databricks_unity_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/danielbeach/lakescum", "subreddit_subscribers": 171702, "created_utc": 1711377212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I've researched the topic a good amount and still haven't been able to verify whether my idea is possible and is the right approach or not.\n\n**The problem:** at my start-up, we have a mongodb database used for recording different events. We have a postgresql database used for analytics. The company is very immature, and also has cost and regional restrictions making it impossible to use something like aws or gcp for pub/sub.\n\nOne (of many) issues we had before I joined was lack of synchronization between one dashboard (in which calculations are in realtime and based on mongodb db), and another dashboard (which uses postgres db which is updated every night via apache airflow). So whatever happens during the day will not necessarily be captured by the postgres-based dashboard.\n\nWhat I'd really like to do is implement a more CDC-based ETL pipeline. I don't have much experience with this but essentially my idea is to use MongoDB change streams to trigger certain Airflow DAGs (used for transforming data and writing into postgres db), as opposed to triggering such dags daily.\n\nThis would only be used on certain tables, all of which don't have millions of daily changes.\n\nI've found lots of change stream examples but none using Airflow. Is this even a good approach? If so, can someone direct me towards how to go about writing the relevant DAGs? I'm confused as to how to use a change stream-based sensor operator?", "author_fullname": "t2_gcq3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trigger airflow dag based on real-time changes to MongoDB database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bne5wy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711374657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I&amp;#39;ve researched the topic a good amount and still haven&amp;#39;t been able to verify whether my idea is possible and is the right approach or not.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; at my start-up, we have a mongodb database used for recording different events. We have a postgresql database used for analytics. The company is very immature, and also has cost and regional restrictions making it impossible to use something like aws or gcp for pub/sub.&lt;/p&gt;\n\n&lt;p&gt;One (of many) issues we had before I joined was lack of synchronization between one dashboard (in which calculations are in realtime and based on mongodb db), and another dashboard (which uses postgres db which is updated every night via apache airflow). So whatever happens during the day will not necessarily be captured by the postgres-based dashboard.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;d really like to do is implement a more CDC-based ETL pipeline. I don&amp;#39;t have much experience with this but essentially my idea is to use MongoDB change streams to trigger certain Airflow DAGs (used for transforming data and writing into postgres db), as opposed to triggering such dags daily.&lt;/p&gt;\n\n&lt;p&gt;This would only be used on certain tables, all of which don&amp;#39;t have millions of daily changes.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found lots of change stream examples but none using Airflow. Is this even a good approach? If so, can someone direct me towards how to go about writing the relevant DAGs? I&amp;#39;m confused as to how to use a change stream-based sensor operator?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bne5wy", "is_robot_indexable": true, "report_reasons": null, "author": "johnsonfrusciante", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bne5wy/trigger_airflow_dag_based_on_realtime_changes_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bne5wy/trigger_airflow_dag_based_on_realtime_changes_to/", "subreddit_subscribers": 171702, "created_utc": 1711374657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently started working on a project where the source data is delivered in CSV or fixed-length file format, with individual files approximately 1 GB in size. The total dataset size could be large, and the data quality and transformation on this dataset are minimal. However, some of these source columns need to be mapped to another data model in PostgreSQL. Currently, there are COPY scripts to import the data into the database and Python/Java scripts to map it to the new data model with minimal transformations.\n\nSo, my question is, what's the ideal data pipeline for this scenario? Should I continue using the same scripting methods or are there any efficient ways to do this? This is a small team in a large organization, so computational resources are fine, but we have to rely on open-source data tools. Additionally, a small POC revealed that we don't necessarily have to rely on a relational database; data stored in Parquet format is equally efficient. Any advise is appreciated.", "author_fullname": "t2_7j05ermei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnb4p5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711365473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently started working on a project where the source data is delivered in CSV or fixed-length file format, with individual files approximately 1 GB in size. The total dataset size could be large, and the data quality and transformation on this dataset are minimal. However, some of these source columns need to be mapped to another data model in PostgreSQL. Currently, there are COPY scripts to import the data into the database and Python/Java scripts to map it to the new data model with minimal transformations.&lt;/p&gt;\n\n&lt;p&gt;So, my question is, what&amp;#39;s the ideal data pipeline for this scenario? Should I continue using the same scripting methods or are there any efficient ways to do this? This is a small team in a large organization, so computational resources are fine, but we have to rely on open-source data tools. Additionally, a small POC revealed that we don&amp;#39;t necessarily have to rely on a relational database; data stored in Parquet format is equally efficient. Any advise is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnb4p5", "is_robot_indexable": true, "report_reasons": null, "author": "careerrant", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnb4p5/tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnb4p5/tools/", "subreddit_subscribers": 171702, "created_utc": 1711365473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am curious to poll if anyone has implemented RBAC on Fivetran itself, via the usage of groups, teams, roles, etc.  \n\n\nHave you found it necessary for your team, and if so, how does the general RBAC design look like from your side?\n\n&amp;#x200B;\n\nThanks.", "author_fullname": "t2_7erz2gzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran RBAC - Terraform or REST API based implementation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn9edz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711358843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious to poll if anyone has implemented RBAC on Fivetran itself, via the usage of groups, teams, roles, etc.  &lt;/p&gt;\n\n&lt;p&gt;Have you found it necessary for your team, and if so, how does the general RBAC design look like from your side?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn9edz", "is_robot_indexable": true, "report_reasons": null, "author": "TinkermanN7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn9edz/fivetran_rbac_terraform_or_rest_api_based/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn9edz/fivetran_rbac_terraform_or_rest_api_based/", "subreddit_subscribers": 171702, "created_utc": 1711358843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dealing with a large unwieldy code base that has no documentation. Looking for tools that can generate lineage and help with data governance/cataloging. \n", "author_fullname": "t2_4yzg0zz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools to generate lineage for Oracle PL/SQL Stored Procedures and Packages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn0gvk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711327065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dealing with a large unwieldy code base that has no documentation. Looking for tools that can generate lineage and help with data governance/cataloging. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn0gvk", "is_robot_indexable": true, "report_reasons": null, "author": "StandardForever4", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn0gvk/any_tools_to_generate_lineage_for_oracle_plsql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn0gvk/any_tools_to_generate_lineage_for_oracle_plsql/", "subreddit_subscribers": 171702, "created_utc": 1711327065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey All,\n\nI have launched my own channel where I'll be talking about DE related stuff, if you guys found the previous blogs to be useful please do consider visiting and subbing to my channel. \n\nThis is the link to it: [https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ](https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ)", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Continuation to structured blogs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bnpwgz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711402774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;I have launched my own channel where I&amp;#39;ll be talking about DE related stuff, if you guys found the previous blogs to be useful please do consider visiting and subbing to my channel. &lt;/p&gt;\n\n&lt;p&gt;This is the link to it: &lt;a href=\"https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ\"&gt;https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?auto=webp&amp;s=a4be3076916d800bb3a8eb096c5789d1eaa97d1c", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66404e8501d319d0e15d2f3a1659a2786a8f9643", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38673cf42a869d7ded60f2972a3855ad6aecdd79", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e3e7065e11363a6d0505cfc4be54b168c8dce12", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/OwLbjijGi2b_UAKV0Aq_Cy-XE2HRgf7C84cFuZbKtbU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f9b6bf60319f1d3533cc3da27c216d9a465de96", "width": 640, "height": 640}], "variants": {}, "id": "Kcq4v7vShDHigIbd-Y5Fpp501_MvubPvf7chWti_PUg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bnpwgz", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnpwgz/continuation_to_structured_blogs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnpwgz/continuation_to_structured_blogs/", "subreddit_subscribers": 171702, "created_utc": 1711402774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, so Parquet is famouse for being more efficient than csv in terms of storage sapce and query times. I wanted to check that, so I created a dummy csv file with 1 million records. It takes up 4.34 MB. I then converted it to Parquet using pandas and the newly cerated file takes up  35.9 MB! Almost 10 times more! Why? I expected the opposite.\n\n&amp;#x200B;\n\nCode for reference:\n\n    import csv\n    import random\n    from faker import Faker\n    \n    # Initialize Faker to generate fake data\n    fake = Faker()\n    \n    # Define the number of rows to generate\n    num_rows = 1000000\n    \n    # Define field names for the CSV\n    field_names = ['ID', 'Name', 'Age', 'Email', 'Address']\n    \n    # Function to generate random data for a row\n    def generate_row():\n        return {\n            'ID': random.randint(1, 1000000),\n            'Name': fake.name(),\n            'Age': random.randint(18, 80),\n            'Email': fake.email(),\n            'Address': fake.address()\n        }\n    \n    # Generate and write data to CSV file\n    with open('dummy_data.csv', 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n        writer.writeheader()\n        for _ in range(num_rows):\n            writer.writerow(generate_row())\n    \n    print(\"CSV file generation complete.\")\n    \n    ##########################################\n    \n    import pandas as pd\n    \n    # Read CSV file into pandas DataFrame\n    df = pd.read_csv('dummy_data.csv')\n    \n    # Write DataFrame to Parquet file\n    df.to_parquet('dummy_data.parquet', index=False)\n    \n    print(\"Parquet file generation complete.\")\n    \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My parquet file takes up MORE space than original csv. Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bnps6k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711402498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, so Parquet is famouse for being more efficient than csv in terms of storage sapce and query times. I wanted to check that, so I created a dummy csv file with 1 million records. It takes up 4.34 MB. I then converted it to Parquet using pandas and the newly cerated file takes up  35.9 MB! Almost 10 times more! Why? I expected the opposite.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Code for reference:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import csv\nimport random\nfrom faker import Faker\n\n# Initialize Faker to generate fake data\nfake = Faker()\n\n# Define the number of rows to generate\nnum_rows = 1000000\n\n# Define field names for the CSV\nfield_names = [&amp;#39;ID&amp;#39;, &amp;#39;Name&amp;#39;, &amp;#39;Age&amp;#39;, &amp;#39;Email&amp;#39;, &amp;#39;Address&amp;#39;]\n\n# Function to generate random data for a row\ndef generate_row():\n    return {\n        &amp;#39;ID&amp;#39;: random.randint(1, 1000000),\n        &amp;#39;Name&amp;#39;: fake.name(),\n        &amp;#39;Age&amp;#39;: random.randint(18, 80),\n        &amp;#39;Email&amp;#39;: fake.email(),\n        &amp;#39;Address&amp;#39;: fake.address()\n    }\n\n# Generate and write data to CSV file\nwith open(&amp;#39;dummy_data.csv&amp;#39;, &amp;#39;w&amp;#39;, newline=&amp;#39;&amp;#39;) as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n    writer.writeheader()\n    for _ in range(num_rows):\n        writer.writerow(generate_row())\n\nprint(&amp;quot;CSV file generation complete.&amp;quot;)\n\n##########################################\n\nimport pandas as pd\n\n# Read CSV file into pandas DataFrame\ndf = pd.read_csv(&amp;#39;dummy_data.csv&amp;#39;)\n\n# Write DataFrame to Parquet file\ndf.to_parquet(&amp;#39;dummy_data.parquet&amp;#39;, index=False)\n\nprint(&amp;quot;Parquet file generation complete.&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnps6k", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnps6k/my_parquet_file_takes_up_more_space_than_original/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnps6k/my_parquet_file_takes_up_more_space_than_original/", "subreddit_subscribers": 171702, "created_utc": 1711402498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys,\nI am working for a particular research lab. That uses an application to collect survey data. This survey data has multiple fields and they keep adding question even after creation.\nThe survey are partitioned into different group and each group has different different survey with similar fields. \n\nThe current work flow is as follows; \n1. The data is stored in mysql database with one of the field taking majority of the data from the survey in JSON form. \n2. A script is writing to pick the data from mysql to mongoDB as documents and flat file. \nSo all the fields from the the different survey are all put as a single flat file with a lot of missing values. \n\n3. Using Tableau to connect through mysql connector to mongoDB BI connector and pull the data to tableau as a single file. \n\nProblem;\n\n1. The fields size increases with increase in survey information from the application and Tableau has a limitation of 700 columns. \nWhich posed a problem for some fields missing. \n\n2. We try to partition the data based on the groups so we can use the Union method of blending, we encounter a problem of having two values on  a particular field for a single instance which is a problem. \n\n3. The modeling of the application is not in a relational approach that we can use joints. \n\nWhat is the best approach to solve this  problem? \n\n", "author_fullname": "t2_djf2dg5p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on the best way to store and process the large volume of data. ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bnpp91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711402311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,\nI am working for a particular research lab. That uses an application to collect survey data. This survey data has multiple fields and they keep adding question even after creation.\nThe survey are partitioned into different group and each group has different different survey with similar fields. &lt;/p&gt;\n\n&lt;p&gt;The current work flow is as follows; \n1. The data is stored in mysql database with one of the field taking majority of the data from the survey in JSON form. \n2. A script is writing to pick the data from mysql to mongoDB as documents and flat file. \nSo all the fields from the the different survey are all put as a single flat file with a lot of missing values. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Using Tableau to connect through mysql connector to mongoDB BI connector and pull the data to tableau as a single file. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Problem;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;The fields size increases with increase in survey information from the application and Tableau has a limitation of 700 columns. \nWhich posed a problem for some fields missing. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We try to partition the data based on the groups so we can use the Union method of blending, we encounter a problem of having two values on  a particular field for a single instance which is a problem. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The modeling of the application is not in a relational approach that we can use joints. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What is the best approach to solve this  problem? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnpp91", "is_robot_indexable": true, "report_reasons": null, "author": "ayockishayaa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnpp91/advice_on_the_best_way_to_store_and_process_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnpp91/advice_on_the_best_way_to_store_and_process_the/", "subreddit_subscribers": 171702, "created_utc": 1711402311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I went in with trying to compare Databricks and SnowFlake thinking they were more or less competitors to each other. However, it seems from some comments I've read on other posts, am I correct to assume they complement each other?\n\n&amp;#x200B;\n\nI'm not familiar with either product so an ELI5 to these would be appreciated. What are the use cases for going for one vs the other? What are the use cases to implement both? Are there other services worth looking into?", "author_fullname": "t2_62leo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks, SnowFlake, others, are they competitors or complement each other.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bnoiq5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711399600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I went in with trying to compare Databricks and SnowFlake thinking they were more or less competitors to each other. However, it seems from some comments I&amp;#39;ve read on other posts, am I correct to assume they complement each other?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not familiar with either product so an ELI5 to these would be appreciated. What are the use cases for going for one vs the other? What are the use cases to implement both? Are there other services worth looking into?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnoiq5", "is_robot_indexable": true, "report_reasons": null, "author": "crypticsage", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnoiq5/databricks_snowflake_others_are_they_competitors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnoiq5/databricks_snowflake_others_are_they_competitors/", "subreddit_subscribers": 171702, "created_utc": 1711399600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you call the curated dataset that you use to validate or unit test your transformation?\n\nExample, say you have a manual process that you are automating. The users have their way of extracting the raw data, and doing their thing in Excel with procV's galore, or something like that and you are automating all the business rules and making it run daily. Now, you're going to validate if your pipeline recreates exactly what the users used to do in Excel.\n\nHow do you call the dataset that you have to mirror? baseline, ground truth, gold standard, template?", "author_fullname": "t2_rpj2htsu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ground Truth vs Gold Standard vs Baseline vs ???", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnm4w0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711394092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you call the curated dataset that you use to validate or unit test your transformation?&lt;/p&gt;\n\n&lt;p&gt;Example, say you have a manual process that you are automating. The users have their way of extracting the raw data, and doing their thing in Excel with procV&amp;#39;s galore, or something like that and you are automating all the business rules and making it run daily. Now, you&amp;#39;re going to validate if your pipeline recreates exactly what the users used to do in Excel.&lt;/p&gt;\n\n&lt;p&gt;How do you call the dataset that you have to mirror? baseline, ground truth, gold standard, template?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnm4w0", "is_robot_indexable": true, "report_reasons": null, "author": "taciom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnm4w0/ground_truth_vs_gold_standard_vs_baseline_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnm4w0/ground_truth_vs_gold_standard_vs_baseline_vs/", "subreddit_subscribers": 171702, "created_utc": 1711394092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Too specific of a question but would like to know the nuances", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the difference between exadata database vs exadata warehouse? Also, can we migrate both of them to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnljan", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Too specific of a question but would like to know the nuances&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnljan", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnljan/what_is_the_difference_between_exadata_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnljan/what_is_the_difference_between_exadata_database/", "subreddit_subscribers": 171702, "created_utc": 1711392678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have multiple data sources that are integrated into single Data Warehouse on Redshift. Those modeled data is then used by other teams. So far batch approach was more then enough but, new stakeholder, new requirements (5 min latency)\n\nNow: ELT We ingest data with glue, DMS or Api Gateway depending on source to S3. Then we have Glue Crawler that creates Glue Catalog. With Redshift spectrum we create external schemas in Redshift. Whole processing is Done in Redshift/DBT. Json explodes, types assertion, Creating current state of Fact / Dimension. group by, window functions.\n\nWe incorporating lambda approach for the biggest Fact Tables but it is not fast enough for Near Real Time since procession of those in Redshift takes \\~30min.\n\nI am wondering where should I move data processing especially Aggregations and Window functions to make it as fast as possible\u2026\n\nI want to move heavy lifting to something suitable for Near Real Time and parallelization. But at the end whole data must be available in Redshift.\n\nI have seen something like this:  Kinesis Data Stream + Lambda + DynomDB: [https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/](https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/)\n\n  \nI was also thinking If I shouldn't learn something about Spark ?", "author_fullname": "t2_45aob9b94", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline- Near Real Time + Agregations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnldjq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711392782.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have multiple data sources that are integrated into single Data Warehouse on Redshift. Those modeled data is then used by other teams. So far batch approach was more then enough but, new stakeholder, new requirements (5 min latency)&lt;/p&gt;\n\n&lt;p&gt;Now: ELT We ingest data with glue, DMS or Api Gateway depending on source to S3. Then we have Glue Crawler that creates Glue Catalog. With Redshift spectrum we create external schemas in Redshift. Whole processing is Done in Redshift/DBT. Json explodes, types assertion, Creating current state of Fact / Dimension. group by, window functions.&lt;/p&gt;\n\n&lt;p&gt;We incorporating lambda approach for the biggest Fact Tables but it is not fast enough for Near Real Time since procession of those in Redshift takes ~30min.&lt;/p&gt;\n\n&lt;p&gt;I am wondering where should I move data processing especially Aggregations and Window functions to make it as fast as possible\u2026&lt;/p&gt;\n\n&lt;p&gt;I want to move heavy lifting to something suitable for Near Real Time and parallelization. But at the end whole data must be available in Redshift.&lt;/p&gt;\n\n&lt;p&gt;I have seen something like this:  Kinesis Data Stream + Lambda + DynomDB: &lt;a href=\"https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/\"&gt;https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was also thinking If I shouldn&amp;#39;t learn something about Spark ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?auto=webp&amp;s=6ad82f47a3d6a62b2bbe5139e4841e5f0a6f75a2", "width": 1260, "height": 538}, "resolutions": [{"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3cf1736c6a16d2b8cd7ae93df5a8d59f54909c22", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c62cac93a4e0545197bc7cdcf4e203283c22695f", "width": 216, "height": 92}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d9a108f4f6adba13443f83dbc51271d64b50413", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dfa5dd36d6440ea7044664a3a3083b3f421c8a86", "width": 640, "height": 273}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f170c95f2d034549ce56aabfa593dc38339a5153", "width": 960, "height": 409}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95d6bba35de071bfe9f817a1b91afe4fe031299e", "width": 1080, "height": 461}], "variants": {}, "id": "uGRpiyPtZEwnzNrVdL72uXc059bnigMEAwohB9qPa2A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnldjq", "is_robot_indexable": true, "report_reasons": null, "author": "Deep-Shape-323", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnldjq/data_pipeline_near_real_time_agregations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnldjq/data_pipeline_near_real_time_agregations/", "subreddit_subscribers": 171702, "created_utc": 1711392308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the prime use case behind migrating oltp/ods on prem systems to databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnl9gw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnl9gw", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnl9gw/what_is_the_prime_use_case_behind_migrating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnl9gw/what_is_the_prime_use_case_behind_migrating/", "subreddit_subscribers": 171702, "created_utc": 1711392036.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}