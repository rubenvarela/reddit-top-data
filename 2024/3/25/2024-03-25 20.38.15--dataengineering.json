{"kind": "Listing", "data": {"after": "t3_1bnbtkc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working in a large Healthcare company , there are critical legacy ETL processing that pull data from two source systems built by someone over 15 years ago. The ETL is done in layers of SQL to output maybe 400 fields across 15 tables. The layered complex SQL manipulates data from the source system with literally hundreds of if else statements, window functions, apply lookups etc across 5 or 6 layers of staging tables. \n\nRecently the business has come to us to say a number looks incorrect possibly. I was tasked with investigation and fixing any issues.\n\nThe problem is the sql has much logic pulling from random tables and columns across both systems, applying crazy amounts of conditional transformations that I don't know how to even begin.\n\nI set up a meeting with the business who explained the business logic around what the field should be, the problem is I have no idea how to get to that in source systems with thousands of tables and fields. \n\nAnyone faced such an issue?", "author_fullname": "t2_ew0arl93h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to maintain complex SQL ETL with no understanding of the source system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn1ioq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 68, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 68, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711330003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working in a large Healthcare company , there are critical legacy ETL processing that pull data from two source systems built by someone over 15 years ago. The ETL is done in layers of SQL to output maybe 400 fields across 15 tables. The layered complex SQL manipulates data from the source system with literally hundreds of if else statements, window functions, apply lookups etc across 5 or 6 layers of staging tables. &lt;/p&gt;\n\n&lt;p&gt;Recently the business has come to us to say a number looks incorrect possibly. I was tasked with investigation and fixing any issues.&lt;/p&gt;\n\n&lt;p&gt;The problem is the sql has much logic pulling from random tables and columns across both systems, applying crazy amounts of conditional transformations that I don&amp;#39;t know how to even begin.&lt;/p&gt;\n\n&lt;p&gt;I set up a meeting with the business who explained the business logic around what the field should be, the problem is I have no idea how to get to that in source systems with thousands of tables and fields. &lt;/p&gt;\n\n&lt;p&gt;Anyone faced such an issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn1ioq", "is_robot_indexable": true, "report_reasons": null, "author": "quartzsmelt", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn1ioq/trying_to_maintain_complex_sql_etl_with_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn1ioq/trying_to_maintain_complex_sql_etl_with_no/", "subreddit_subscribers": 171676, "created_utc": 1711330003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all!\nI feel kinda stuck and lost career wise. I broke into DE and related fields by accident, taking an internship assembling the DW of a large company right when I was finishing my bachelor in Economics. 4.5y later, I'm very proficient with SQL and Python, I've built a few ETLs and done a fair share of KPIs, dashboards and fact tables and the like. I'm good with dbt but no senior level (failed a recent technical) and don't know much about PySpark, Hadoop, etc. I'm very insecure as to what to do, which technologies to learn (and how to prove I know them in my CV). I'm occasionally called an AE, DA or DE because the positions have interchangable titles here in my country. What do I try next?\n", "author_fullname": "t2_bdy92fyd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mid level and stuck", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bmxjhe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711319479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all!\nI feel kinda stuck and lost career wise. I broke into DE and related fields by accident, taking an internship assembling the DW of a large company right when I was finishing my bachelor in Economics. 4.5y later, I&amp;#39;m very proficient with SQL and Python, I&amp;#39;ve built a few ETLs and done a fair share of KPIs, dashboards and fact tables and the like. I&amp;#39;m good with dbt but no senior level (failed a recent technical) and don&amp;#39;t know much about PySpark, Hadoop, etc. I&amp;#39;m very insecure as to what to do, which technologies to learn (and how to prove I know them in my CV). I&amp;#39;m occasionally called an AE, DA or DE because the positions have interchangable titles here in my country. What do I try next?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bmxjhe", "is_robot_indexable": true, "report_reasons": null, "author": "BelugaGolfinho", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bmxjhe/mid_level_and_stuck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bmxjhe/mid_level_and_stuck/", "subreddit_subscribers": 171676, "created_utc": 1711319479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the behavioral and technical questions would be the best to identify if a candidate a good fit or not, and if he/she actually really good data engineers? ", "author_fullname": "t2_7chglbbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question to ask senior data engineers during interviews?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn843b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711353211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the behavioral and technical questions would be the best to identify if a candidate a good fit or not, and if he/she actually really good data engineers? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn843b", "is_robot_indexable": true, "report_reasons": null, "author": "Computingss", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn843b/question_to_ask_senior_data_engineers_during/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn843b/question_to_ask_senior_data_engineers_during/", "subreddit_subscribers": 171676, "created_utc": 1711353211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company recently launched their data engineering team, and I'm their newest hire (a self-taught fresh grad, &lt;1 month in the company). I have two seniors and we're the only data engineers here.  \n\nWe use Snowflake as our central data warehouse, and we do most of the SQL transformations there. Currently planning to use dbt Core instead and then orchestrate it using Apache Airflow. We're weighing the pros and cons of using Amazon MWAA vs self-hosting in an EC2 server considering the price, ease of setting up, scalability, and potential bugs/issues. If we self-host, we're gonna use Astronomer's Astro CLI and Cosmos to set up dbt with Airflow.  \n\nHas anyone successfully deployed dbt + Airflow on EC2? What are some of the issues you encountered? How's Cosmos for integrating dbt with Airflow? Should we just go with MWAA? I've heard many had success with it and cost is the only concern. We're trying to minimize costs, so as much as possible we're opting for open-source solutions (I believe Astro CLI and Cosmos are free if you're not using their cloud/IDE version).\n\nAny insights/advice would be highly appreciated. TYIA!\n\n&amp;#x200B;", "author_fullname": "t2_ts2pktzo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrating dbt Core with Apache Airflow: MWAA vs Self-Hosted EC2 Deployment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnebbr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711375051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company recently launched their data engineering team, and I&amp;#39;m their newest hire (a self-taught fresh grad, &amp;lt;1 month in the company). I have two seniors and we&amp;#39;re the only data engineers here.  &lt;/p&gt;\n\n&lt;p&gt;We use Snowflake as our central data warehouse, and we do most of the SQL transformations there. Currently planning to use dbt Core instead and then orchestrate it using Apache Airflow. We&amp;#39;re weighing the pros and cons of using Amazon MWAA vs self-hosting in an EC2 server considering the price, ease of setting up, scalability, and potential bugs/issues. If we self-host, we&amp;#39;re gonna use Astronomer&amp;#39;s Astro CLI and Cosmos to set up dbt with Airflow.  &lt;/p&gt;\n\n&lt;p&gt;Has anyone successfully deployed dbt + Airflow on EC2? What are some of the issues you encountered? How&amp;#39;s Cosmos for integrating dbt with Airflow? Should we just go with MWAA? I&amp;#39;ve heard many had success with it and cost is the only concern. We&amp;#39;re trying to minimize costs, so as much as possible we&amp;#39;re opting for open-source solutions (I believe Astro CLI and Cosmos are free if you&amp;#39;re not using their cloud/IDE version).&lt;/p&gt;\n\n&lt;p&gt;Any insights/advice would be highly appreciated. TYIA!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnebbr", "is_robot_indexable": true, "report_reasons": null, "author": "zmxavier", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnebbr/orchestrating_dbt_core_with_apache_airflow_mwaa/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnebbr/orchestrating_dbt_core_with_apache_airflow_mwaa/", "subreddit_subscribers": 171676, "created_utc": 1711375051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nSo, I just smashed the DP-203 exam yesterday (24th Mar 2024), and I gotta get real about something. You know all those YouTubers claiming they aced it in just seven days? Yeah, total BS. There's no way you can prep for this exam in a week and come out unscathed.\n\nHere's the deal: the syllabus is massive, like trying to eat a whole pizza in one sitting massive. Azure's got a bazillion products and concepts, and mastering them takes time. So, if you're thinking about cramming, think again.\n\nBut fear not, fellow Azure adventurers! I've got some advice that'll steer you in the right direction:\n\n1. **MS Learn is Your Friend:** Don't waste precious time scribbling notes. Blast through the self-paced MS Learn syllabus to get a bird's-eye view of Azure cloud. Do all the exercises like your cloud career depends on it.\n\n2. **YouTube Isn't Gospel:** Sure, YouTube is a treasure trove of exam Q&amp;A guides, but don't take everything at face value. A good chunk of those answers is as reliable as a chocolate teapot. Crosscheck with your own logic and, hey, even bounce ideas off ChatGPT, Gemini, or Copilot.\n\nAnd lastly, here's a pro tip: book your exam date before you even crack open the books. It's like setting a deadline for yourself, but with a bit of added pressure...and excitement!\n\nSo, there you have it, folks. Go forth, conquer the Azure skies, and remember: Rome wasn't built in a day, and neither is Azure expertise. \ud83d\ude01\u2728\n", "author_fullname": "t2_t67twgnr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Justs Cleared DP-203 Exam - Some Real Talk and Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn1yl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711331255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;So, I just smashed the DP-203 exam yesterday (24th Mar 2024), and I gotta get real about something. You know all those YouTubers claiming they aced it in just seven days? Yeah, total BS. There&amp;#39;s no way you can prep for this exam in a week and come out unscathed.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the deal: the syllabus is massive, like trying to eat a whole pizza in one sitting massive. Azure&amp;#39;s got a bazillion products and concepts, and mastering them takes time. So, if you&amp;#39;re thinking about cramming, think again.&lt;/p&gt;\n\n&lt;p&gt;But fear not, fellow Azure adventurers! I&amp;#39;ve got some advice that&amp;#39;ll steer you in the right direction:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;MS Learn is Your Friend:&lt;/strong&gt; Don&amp;#39;t waste precious time scribbling notes. Blast through the self-paced MS Learn syllabus to get a bird&amp;#39;s-eye view of Azure cloud. Do all the exercises like your cloud career depends on it.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;YouTube Isn&amp;#39;t Gospel:&lt;/strong&gt; Sure, YouTube is a treasure trove of exam Q&amp;amp;A guides, but don&amp;#39;t take everything at face value. A good chunk of those answers is as reliable as a chocolate teapot. Crosscheck with your own logic and, hey, even bounce ideas off ChatGPT, Gemini, or Copilot.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;And lastly, here&amp;#39;s a pro tip: book your exam date before you even crack open the books. It&amp;#39;s like setting a deadline for yourself, but with a bit of added pressure...and excitement!&lt;/p&gt;\n\n&lt;p&gt;So, there you have it, folks. Go forth, conquer the Azure skies, and remember: Rome wasn&amp;#39;t built in a day, and neither is Azure expertise. \ud83d\ude01\u2728&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bn1yl3", "is_robot_indexable": true, "report_reasons": null, "author": "Square_Thing_4613", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn1yl3/justs_cleared_dp203_exam_some_real_talk_and_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn1yl3/justs_cleared_dp203_exam_some_real_talk_and_advice/", "subreddit_subscribers": 171676, "created_utc": 1711331255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Someone I know who is into DE role &gt;20yrs, he says there is absolutely no need to work on algorithms if you are a DE.  \n\n\nIn DE, to what extent are algorithms (ML algo or fine tuning related stuff) integrated into your day-to-day tasks or do you even deal with any sort of algorithm in your role? And if yes, are there particular ML algorithms or algorithmic concepts that you frequently encounter or utilize? ", "author_fullname": "t2_8i81bbqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much of you guys have to deal with algorithms or ML algorithms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnda5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711378244.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711372229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Someone I know who is into DE role &amp;gt;20yrs, he says there is absolutely no need to work on algorithms if you are a DE.  &lt;/p&gt;\n\n&lt;p&gt;In DE, to what extent are algorithms (ML algo or fine tuning related stuff) integrated into your day-to-day tasks or do you even deal with any sort of algorithm in your role? And if yes, are there particular ML algorithms or algorithmic concepts that you frequently encounter or utilize? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnda5x", "is_robot_indexable": true, "report_reasons": null, "author": "trafalgar28", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnda5x/how_much_of_you_guys_have_to_deal_with_algorithms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnda5x/how_much_of_you_guys_have_to_deal_with_algorithms/", "subreddit_subscribers": 171676, "created_utc": 1711372229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI currently work as an aerospace engineer (nominally), although a lot of my actual work has been to do with data and software. This is definitely what interests me, and I want to move into this sort of role full-time.\n\nHowever, my experience has been a bit of a mess across several different disciplines, so I'm really not sure what I'm qualified to go for or what's expected. I have some software development experience (creating custom analysis software in Python), some data analysis / data science experience, and then also some data engineering experience.\n\nHowever, a lot of the stuff that I have done has been using Palantir Foundry, while all of the jobs I see seem to want things like Azure, Databricks, AWS etc.\n\nIs Palantir Foundry an \"easier\" version of these tools, and I will struggle to move to something like Azure? Or is it broadly similar, and I should I look to do some self-learning on Azure, for example, and then I will be job ready for a data engineering role?\n\nThe work I have done in Palantir Foundry definitely seems quite data engineering related - I use Python / PySpark SQL and have created some custom dashboards using HTML / CSS / JavaScript. However, I haven't actually been involved in the ingestion of any raw data, I have usually taken existing data sets that were in a poor state and cleaned, transformed, joined multiple datasets to get the result I wanted.", "author_fullname": "t2_iemkebmei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Palantir Foundry Count as Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnbg99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711366574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I currently work as an aerospace engineer (nominally), although a lot of my actual work has been to do with data and software. This is definitely what interests me, and I want to move into this sort of role full-time.&lt;/p&gt;\n\n&lt;p&gt;However, my experience has been a bit of a mess across several different disciplines, so I&amp;#39;m really not sure what I&amp;#39;m qualified to go for or what&amp;#39;s expected. I have some software development experience (creating custom analysis software in Python), some data analysis / data science experience, and then also some data engineering experience.&lt;/p&gt;\n\n&lt;p&gt;However, a lot of the stuff that I have done has been using Palantir Foundry, while all of the jobs I see seem to want things like Azure, Databricks, AWS etc.&lt;/p&gt;\n\n&lt;p&gt;Is Palantir Foundry an &amp;quot;easier&amp;quot; version of these tools, and I will struggle to move to something like Azure? Or is it broadly similar, and I should I look to do some self-learning on Azure, for example, and then I will be job ready for a data engineering role?&lt;/p&gt;\n\n&lt;p&gt;The work I have done in Palantir Foundry definitely seems quite data engineering related - I use Python / PySpark SQL and have created some custom dashboards using HTML / CSS / JavaScript. However, I haven&amp;#39;t actually been involved in the ingestion of any raw data, I have usually taken existing data sets that were in a poor state and cleaned, transformed, joined multiple datasets to get the result I wanted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnbg99", "is_robot_indexable": true, "report_reasons": null, "author": "anonymous_lurker_01", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnbg99/does_palantir_foundry_count_as_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnbg99/does_palantir_foundry_count_as_data_engineering/", "subreddit_subscribers": 171676, "created_utc": 1711366574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all - I\u2019m looking for a little bit of guidance regarding learning. I searched the subreddit and wasn\u2019t sure which resources to focus on.\n\nFor context - I\u2019m the head of data at a midsized company. My background is mostly in the analytics/data science space: building ML models, visuals, insights etc to drive business strategy. I am keenly aware that I have a pretty big gap around data engineering. I have picked a few things up here and there working with data engineers, but I really want to learn more to make me more effective. The way I\u2019m currently thinking about this (and let me know if this seems off base):\n\n1. Getting a deeper understanding of the problems that data engineering is solving for (technology agnostic) - basically the \u201cwhat\u201d and the \u201cwhy\u201d.\n\n2. Current industry best practices in terms of technologies, frameworks, approaches, etc to solve those problems - what are the major methods currently used to solve #1 above?\n\n3. Some level of hands on practice - I\u2019m someone who learns best doing it by hand in a relatively structured environment. Now, since my day to day isn\u2019t going to be doing data engineering work day in day out (have actual technical people to do it and run the teams), I don\u2019t (think..) I need to have advanced hands on practice, just enough to understand the high level approach and things to watch out for so I can advocate for my teams. \n\nIdeal scenario for me is to have a course to cover most of these to build a base, and then augment from there with reading/other items as necessary. Any suggestions? Does something like this exist? ", "author_fullname": "t2_u9cx9z9b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning resources and courses - data leader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bmz1rx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711323286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - I\u2019m looking for a little bit of guidance regarding learning. I searched the subreddit and wasn\u2019t sure which resources to focus on.&lt;/p&gt;\n\n&lt;p&gt;For context - I\u2019m the head of data at a midsized company. My background is mostly in the analytics/data science space: building ML models, visuals, insights etc to drive business strategy. I am keenly aware that I have a pretty big gap around data engineering. I have picked a few things up here and there working with data engineers, but I really want to learn more to make me more effective. The way I\u2019m currently thinking about this (and let me know if this seems off base):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Getting a deeper understanding of the problems that data engineering is solving for (technology agnostic) - basically the \u201cwhat\u201d and the \u201cwhy\u201d.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Current industry best practices in terms of technologies, frameworks, approaches, etc to solve those problems - what are the major methods currently used to solve #1 above?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Some level of hands on practice - I\u2019m someone who learns best doing it by hand in a relatively structured environment. Now, since my day to day isn\u2019t going to be doing data engineering work day in day out (have actual technical people to do it and run the teams), I don\u2019t (think..) I need to have advanced hands on practice, just enough to understand the high level approach and things to watch out for so I can advocate for my teams. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ideal scenario for me is to have a course to cover most of these to build a base, and then augment from there with reading/other items as necessary. Any suggestions? Does something like this exist? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bmz1rx", "is_robot_indexable": true, "report_reasons": null, "author": "RandomRandomPenguin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bmz1rx/learning_resources_and_courses_data_leader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bmz1rx/learning_resources_and_courses_data_leader/", "subreddit_subscribers": 171676, "created_utc": 1711323286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nHello everyone , we have different source systems sitting in Amazon rds , Mongo db instances and so on. We are migrating all the data to redshift for single source of truth. For rds instances, we are using AWS dms to transfer the data. For mongo we have hourly scripts to transfer the data. Dms is not suitable for mongo in our usecase because of nature of the data we have . \n\nNow the problem is sometimes the data is not complete like missing data, sometimes it is not fresh due to various reasons in the dms, sometimes we are getting duplicate rows.\n\nNow we have to convey the SLA's to our downstream systems about the freshness like how much time this table or database will take to get th latest incremental data from source . And also we have to be confident enough to say like our data is complete , we are not missing anything. \n\n\nI have brainstormed several approaches but didn't get the concrete solution yet . One approach we decided was to have the list of important tables . Query the source and target every 15 mins to check the latest record in both the systems and the no of rows. This approach looks promising to me.  But the problem is our source db's are somewhat fragile and it requires a lot of approvals from the stake holders . If we fire count(*) query with our time range , to fetch the total no of records, it will take 10 mins in the worst case . \n\n\nNow how to tackle this problem and convey the freshness and SLA's to downstream systems.\n\nAny suggestions or external tools will be helpful. \n\nThanks in advance ", "author_fullname": "t2_bpwbtfnd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data freshness and completeness", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn74g3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711348926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone , we have different source systems sitting in Amazon rds , Mongo db instances and so on. We are migrating all the data to redshift for single source of truth. For rds instances, we are using AWS dms to transfer the data. For mongo we have hourly scripts to transfer the data. Dms is not suitable for mongo in our usecase because of nature of the data we have . &lt;/p&gt;\n\n&lt;p&gt;Now the problem is sometimes the data is not complete like missing data, sometimes it is not fresh due to various reasons in the dms, sometimes we are getting duplicate rows.&lt;/p&gt;\n\n&lt;p&gt;Now we have to convey the SLA&amp;#39;s to our downstream systems about the freshness like how much time this table or database will take to get th latest incremental data from source . And also we have to be confident enough to say like our data is complete , we are not missing anything. &lt;/p&gt;\n\n&lt;p&gt;I have brainstormed several approaches but didn&amp;#39;t get the concrete solution yet . One approach we decided was to have the list of important tables . Query the source and target every 15 mins to check the latest record in both the systems and the no of rows. This approach looks promising to me.  But the problem is our source db&amp;#39;s are somewhat fragile and it requires a lot of approvals from the stake holders . If we fire count(*) query with our time range , to fetch the total no of records, it will take 10 mins in the worst case . &lt;/p&gt;\n\n&lt;p&gt;Now how to tackle this problem and convey the freshness and SLA&amp;#39;s to downstream systems.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or external tools will be helpful. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bn74g3", "is_robot_indexable": true, "report_reasons": null, "author": "oofla_mey_goofla", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn74g3/data_freshness_and_completeness/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn74g3/data_freshness_and_completeness/", "subreddit_subscribers": 171676, "created_utc": 1711348926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Parquet is better for read operations, while avro is more efficient for write operations. Do you ever  decide to save some data as avro because you think it will be written to more often in your data lake or do you prefer to have all files in one file format like parquet? What are your experiences here?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parquet for reads, avro for writes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bnlwjj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711393542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Parquet is better for read operations, while avro is more efficient for write operations. Do you ever  decide to save some data as avro because you think it will be written to more often in your data lake or do you prefer to have all files in one file format like parquet? What are your experiences here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnlwjj", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnlwjj/parquet_for_reads_avro_for_writes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnlwjj/parquet_for_reads_avro_for_writes/", "subreddit_subscribers": 171676, "created_utc": 1711393542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im currently a student (business informatics - masters) and im having a horrific experience with job searching (either part time or full time). Do you guys know where i could look for internships or jobs? Im located in Slovenia and here they are only looking for senior roles (+ 7 years experience). Im open to work from home or moving abroad. \n\nMy biggest problem is that there is no work market for an entry or junior position. Even when I find something that seem to good it is. Usually they want me to be DE, DA and DS at the same time. ", "author_fullname": "t2_qmhxkds80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job search ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnjycg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711388913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im currently a student (business informatics - masters) and im having a horrific experience with job searching (either part time or full time). Do you guys know where i could look for internships or jobs? Im located in Slovenia and here they are only looking for senior roles (+ 7 years experience). Im open to work from home or moving abroad. &lt;/p&gt;\n\n&lt;p&gt;My biggest problem is that there is no work market for an entry or junior position. Even when I find something that seem to good it is. Usually they want me to be DE, DA and DS at the same time. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bnjycg", "is_robot_indexable": true, "report_reasons": null, "author": "Comment_Error", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnjycg/job_search/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnjycg/job_search/", "subreddit_subscribers": 171676, "created_utc": 1711388913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. \n\nOne of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. \n\nThe prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don't want to move anything to a new storage. So I'm a little worried if we are biting more than we can chew, but I don't want to lose a business that is already at the doorsteps. \n\nWhat are some implementation techniques you guys used during virtualization to address speed and performance issues?", "author_fullname": "t2_a1jxqx9hs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Potential Issues with Data Virtualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnis0n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711386134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. &lt;/p&gt;\n\n&lt;p&gt;One of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. &lt;/p&gt;\n\n&lt;p&gt;The prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don&amp;#39;t want to move anything to a new storage. So I&amp;#39;m a little worried if we are biting more than we can chew, but I don&amp;#39;t want to lose a business that is already at the doorsteps. &lt;/p&gt;\n\n&lt;p&gt;What are some implementation techniques you guys used during virtualization to address speed and performance issues?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnis0n", "is_robot_indexable": true, "report_reasons": null, "author": "turboline-ai", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnis0n/potential_issues_with_data_virtualization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnis0n/potential_issues_with_data_virtualization/", "subreddit_subscribers": 171676, "created_utc": 1711386134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Python package to help Databricks Unity Catalog users read and query Delta Lake tables with Polars, DuckDb, or PyArrow.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnf67w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DRcjTpTyb9GkuKMvyPZE89uYrxg2Ons7iy7JhO_rnWI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711377212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/danielbeach/lakescum", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?auto=webp&amp;s=6d21a764643609310ba3389f80673261cc8025e5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c996873227f58c286c4db36ef4d7cfd67c76e37c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4e02ecb0c65b217928fff4c91e98189640791a5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a9aad197a1c0cd0477a498cae2ab9a8fdf370fd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=02b63710d92488c40bbc932cfbf13f1f7dc4db67", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af8cec4826bda3fabf081007c22a673572befb61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7UWGY1qzJv9FUaYApWOkgcI4zsoK-NyUn8vVyHoPJNw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b89038e7fcb5beaa1bef04c9cf48c2ee7e3c930", "width": 1080, "height": 540}], "variants": {}, "id": "u586jNjYI52ADVaOP025fVjfx6Pn53ZVWD_9GRGNRBg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bnf67w", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnf67w/a_python_package_to_help_databricks_unity_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/danielbeach/lakescum", "subreddit_subscribers": 171676, "created_utc": 1711377212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I've researched the topic a good amount and still haven't been able to verify whether my idea is possible and is the right approach or not.\n\n**The problem:** at my start-up, we have a mongodb database used for recording different events. We have a postgresql database used for analytics. The company is very immature, and also has cost and regional restrictions making it impossible to use something like aws or gcp for pub/sub.\n\nOne (of many) issues we had before I joined was lack of synchronization between one dashboard (in which calculations are in realtime and based on mongodb db), and another dashboard (which uses postgres db which is updated every night via apache airflow). So whatever happens during the day will not necessarily be captured by the postgres-based dashboard.\n\nWhat I'd really like to do is implement a more CDC-based ETL pipeline. I don't have much experience with this but essentially my idea is to use MongoDB change streams to trigger certain Airflow DAGs (used for transforming data and writing into postgres db), as opposed to triggering such dags daily.\n\nThis would only be used on certain tables, all of which don't have millions of daily changes.\n\nI've found lots of change stream examples but none using Airflow. Is this even a good approach? If so, can someone direct me towards how to go about writing the relevant DAGs? I'm confused as to how to use a change stream-based sensor operator?", "author_fullname": "t2_gcq3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trigger airflow dag based on real-time changes to MongoDB database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bne5wy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711374657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I&amp;#39;ve researched the topic a good amount and still haven&amp;#39;t been able to verify whether my idea is possible and is the right approach or not.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; at my start-up, we have a mongodb database used for recording different events. We have a postgresql database used for analytics. The company is very immature, and also has cost and regional restrictions making it impossible to use something like aws or gcp for pub/sub.&lt;/p&gt;\n\n&lt;p&gt;One (of many) issues we had before I joined was lack of synchronization between one dashboard (in which calculations are in realtime and based on mongodb db), and another dashboard (which uses postgres db which is updated every night via apache airflow). So whatever happens during the day will not necessarily be captured by the postgres-based dashboard.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;d really like to do is implement a more CDC-based ETL pipeline. I don&amp;#39;t have much experience with this but essentially my idea is to use MongoDB change streams to trigger certain Airflow DAGs (used for transforming data and writing into postgres db), as opposed to triggering such dags daily.&lt;/p&gt;\n\n&lt;p&gt;This would only be used on certain tables, all of which don&amp;#39;t have millions of daily changes.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found lots of change stream examples but none using Airflow. Is this even a good approach? If so, can someone direct me towards how to go about writing the relevant DAGs? I&amp;#39;m confused as to how to use a change stream-based sensor operator?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bne5wy", "is_robot_indexable": true, "report_reasons": null, "author": "johnsonfrusciante", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bne5wy/trigger_airflow_dag_based_on_realtime_changes_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bne5wy/trigger_airflow_dag_based_on_realtime_changes_to/", "subreddit_subscribers": 171676, "created_utc": 1711374657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nAs many of you reached out to me and said to continue the blogs I have decided to get back at it. However, 1 glaring issue was that all my blogs would be lost with time on reddit hence I have decided to post them on YouTube going forward. This will ensure all of you and the future folks can access it whenever you want as well as I can teach with more creative freedom than being restricted to writing.\n\nI have posted my first video today which is about Spark Architecture: [https://youtu.be/8JPu1CECtPE](https://youtu.be/8JPu1CECtPE)\n\nNote: As this is my first video there are some problems which I am aware of like maybe sound, getting stuck for a short time while explaining at times, flow may seem a bit off, rest assured I will keep improving, for now goal is to get these videos out to world and improve myself in the process rather than waiting for perfection and never getting started :) ", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnbtne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711367855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;As many of you reached out to me and said to continue the blogs I have decided to get back at it. However, 1 glaring issue was that all my blogs would be lost with time on reddit hence I have decided to post them on YouTube going forward. This will ensure all of you and the future folks can access it whenever you want as well as I can teach with more creative freedom than being restricted to writing.&lt;/p&gt;\n\n&lt;p&gt;I have posted my first video today which is about Spark Architecture: &lt;a href=\"https://youtu.be/8JPu1CECtPE\"&gt;https://youtu.be/8JPu1CECtPE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: As this is my first video there are some problems which I am aware of like maybe sound, getting stuck for a short time while explaining at times, flow may seem a bit off, rest assured I will keep improving, for now goal is to get these videos out to world and improve myself in the process rather than waiting for perfection and never getting started :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bEJM6C_nDGqyNvaCtPbBLoasdHzwiTh5-AQ39F8H3Pk.jpg?auto=webp&amp;s=e9aeb3c75eb54cbdbeee58e530a54f4ce8c1023d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/bEJM6C_nDGqyNvaCtPbBLoasdHzwiTh5-AQ39F8H3Pk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6deffc0caf1132ee83fa142820772ed0cbceb572", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/bEJM6C_nDGqyNvaCtPbBLoasdHzwiTh5-AQ39F8H3Pk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b299507ca0c917622c0a9deb22a983f8eb8452bb", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/bEJM6C_nDGqyNvaCtPbBLoasdHzwiTh5-AQ39F8H3Pk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e96b444650aa0a272f5b45780a325b79b504c05a", "width": 320, "height": 240}], "variants": {}, "id": "Y-PsXGrq_V2X6IBudqfiOtlS8pTQQJ3l6onGNzrLGYk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bnbtne", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnbtne/spark_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnbtne/spark_architecture/", "subreddit_subscribers": 171676, "created_utc": 1711367855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently started working on a project where the source data is delivered in CSV or fixed-length file format, with individual files approximately 1 GB in size. The total dataset size could be large, and the data quality and transformation on this dataset are minimal. However, some of these source columns need to be mapped to another data model in PostgreSQL. Currently, there are COPY scripts to import the data into the database and Python/Java scripts to map it to the new data model with minimal transformations.\n\nSo, my question is, what's the ideal data pipeline for this scenario? Should I continue using the same scripting methods or are there any efficient ways to do this? This is a small team in a large organization, so computational resources are fine, but we have to rely on open-source data tools. Additionally, a small POC revealed that we don't necessarily have to rely on a relational database; data stored in Parquet format is equally efficient. Any advise is appreciated.", "author_fullname": "t2_7j05ermei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnb4p5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711365473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently started working on a project where the source data is delivered in CSV or fixed-length file format, with individual files approximately 1 GB in size. The total dataset size could be large, and the data quality and transformation on this dataset are minimal. However, some of these source columns need to be mapped to another data model in PostgreSQL. Currently, there are COPY scripts to import the data into the database and Python/Java scripts to map it to the new data model with minimal transformations.&lt;/p&gt;\n\n&lt;p&gt;So, my question is, what&amp;#39;s the ideal data pipeline for this scenario? Should I continue using the same scripting methods or are there any efficient ways to do this? This is a small team in a large organization, so computational resources are fine, but we have to rely on open-source data tools. Additionally, a small POC revealed that we don&amp;#39;t necessarily have to rely on a relational database; data stored in Parquet format is equally efficient. Any advise is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnb4p5", "is_robot_indexable": true, "report_reasons": null, "author": "careerrant", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnb4p5/tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnb4p5/tools/", "subreddit_subscribers": 171676, "created_utc": 1711365473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am curious to poll if anyone has implemented RBAC on Fivetran itself, via the usage of groups, teams, roles, etc.  \n\n\nHave you found it necessary for your team, and if so, how does the general RBAC design look like from your side?\n\n&amp;#x200B;\n\nThanks.", "author_fullname": "t2_7erz2gzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran RBAC - Terraform or REST API based implementation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn9edz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711358843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious to poll if anyone has implemented RBAC on Fivetran itself, via the usage of groups, teams, roles, etc.  &lt;/p&gt;\n\n&lt;p&gt;Have you found it necessary for your team, and if so, how does the general RBAC design look like from your side?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn9edz", "is_robot_indexable": true, "report_reasons": null, "author": "TinkermanN7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn9edz/fivetran_rbac_terraform_or_rest_api_based/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn9edz/fivetran_rbac_terraform_or_rest_api_based/", "subreddit_subscribers": 171676, "created_utc": 1711358843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dealing with a large unwieldy code base that has no documentation. Looking for tools that can generate lineage and help with data governance/cataloging. \n", "author_fullname": "t2_4yzg0zz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools to generate lineage for Oracle PL/SQL Stored Procedures and Packages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bn0gvk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711327065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dealing with a large unwieldy code base that has no documentation. Looking for tools that can generate lineage and help with data governance/cataloging. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bn0gvk", "is_robot_indexable": true, "report_reasons": null, "author": "StandardForever4", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bn0gvk/any_tools_to_generate_lineage_for_oracle_plsql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bn0gvk/any_tools_to_generate_lineage_for_oracle_plsql/", "subreddit_subscribers": 171676, "created_utc": 1711327065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Too specific of a question but would like to know the nuances", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the difference between exadata database vs exadata warehouse? Also, can we migrate both of them to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bnljan", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Too specific of a question but would like to know the nuances&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnljan", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnljan/what_is_the_difference_between_exadata_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnljan/what_is_the_difference_between_exadata_database/", "subreddit_subscribers": 171676, "created_utc": 1711392678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have multiple data sources that are integrated into single Data Warehouse on Redshift. Those modeled data is then used by other teams. So far batch approach was more then enough but, new stakeholder, new requirements (5 min latency)\n\nNow: ELT We ingest data with glue, DMS or Api Gateway depending on source to S3. Then we have Glue Crawler that creates Glue Catalog. With Redshift spectrum we create external schemas in Redshift. Whole processing is Done in Redshift/DBT. Json explodes, types assertion, Creating current state of Fact / Dimension. group by, window functions.\n\nWe incorporating lambda approach for the biggest Fact Tables but it is not fast enough for Near Real Time since procession of those in Redshift takes \\~30min.\n\nI am wondering where should I move data processing especially Aggregations and Window functions to make it as fast as possible\u2026\n\nI want to move heavy lifting to something suitable for Near Real Time and parallelization. But at the end whole data must be available in Redshift.\n\nI have seen something like this:  Kinesis Data Stream + Lambda + DynomDB: [https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/](https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/)\n\n  \nI was also thinking If I shouldn't learn something about Spark ?", "author_fullname": "t2_45aob9b94", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline- Near Real Time + Agregations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bnldjq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711392782.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have multiple data sources that are integrated into single Data Warehouse on Redshift. Those modeled data is then used by other teams. So far batch approach was more then enough but, new stakeholder, new requirements (5 min latency)&lt;/p&gt;\n\n&lt;p&gt;Now: ELT We ingest data with glue, DMS or Api Gateway depending on source to S3. Then we have Glue Crawler that creates Glue Catalog. With Redshift spectrum we create external schemas in Redshift. Whole processing is Done in Redshift/DBT. Json explodes, types assertion, Creating current state of Fact / Dimension. group by, window functions.&lt;/p&gt;\n\n&lt;p&gt;We incorporating lambda approach for the biggest Fact Tables but it is not fast enough for Near Real Time since procession of those in Redshift takes ~30min.&lt;/p&gt;\n\n&lt;p&gt;I am wondering where should I move data processing especially Aggregations and Window functions to make it as fast as possible\u2026&lt;/p&gt;\n\n&lt;p&gt;I want to move heavy lifting to something suitable for Near Real Time and parallelization. But at the end whole data must be available in Redshift.&lt;/p&gt;\n\n&lt;p&gt;I have seen something like this:  Kinesis Data Stream + Lambda + DynomDB: &lt;a href=\"https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/\"&gt;https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was also thinking If I shouldn&amp;#39;t learn something about Spark ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?auto=webp&amp;s=6ad82f47a3d6a62b2bbe5139e4841e5f0a6f75a2", "width": 1260, "height": 538}, "resolutions": [{"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3cf1736c6a16d2b8cd7ae93df5a8d59f54909c22", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c62cac93a4e0545197bc7cdcf4e203283c22695f", "width": 216, "height": 92}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d9a108f4f6adba13443f83dbc51271d64b50413", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dfa5dd36d6440ea7044664a3a3083b3f421c8a86", "width": 640, "height": 273}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f170c95f2d034549ce56aabfa593dc38339a5153", "width": 960, "height": 409}, {"url": "https://external-preview.redd.it/m5sh7aDem7ZblCx0JDhUbAFp-ODWsy_6UgVjlqRyS4k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95d6bba35de071bfe9f817a1b91afe4fe031299e", "width": 1080, "height": 461}], "variants": {}, "id": "uGRpiyPtZEwnzNrVdL72uXc059bnigMEAwohB9qPa2A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnldjq", "is_robot_indexable": true, "report_reasons": null, "author": "Deep-Shape-323", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnldjq/data_pipeline_near_real_time_agregations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnldjq/data_pipeline_near_real_time_agregations/", "subreddit_subscribers": 171676, "created_utc": 1711392308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is the prime use case behind migrating oltp/ods on prem systems to databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bnl9gw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711392036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bnl9gw", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnl9gw/what_is_the_prime_use_case_behind_migrating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnl9gw/what_is_the_prime_use_case_behind_migrating/", "subreddit_subscribers": 171676, "created_utc": 1711392036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New job, and I'm kinda new at being in this capacity.\n\nI'm planning on refactoring a lot of our data infrastructure, at least insofar as a few teams are concerned. I was thinking of going into this by having DevOps provision me an EC2, S3, and Redshift instance and I'd just set things up myself on an as-needed basis; i.e., start with dbt, Airflow, and the means to connect to our S3 and Redshift, then anything else I may happen to come across.\n\nDevOps is currently trying to just give me something set up in a cluster and they're asking for what services and relevant configs/requirements I need set up. So, I guess this is a two-part question:\n\n1. What other services and relevant configs should I consider adding to this setup?\n2. Should I insist that I be given an EC2 mostly because DevOps has admitted to being understaffed and I wouldn't want to have to wait a while for additional services to be added if I come across some that would help?", "author_fullname": "t2_svn12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any requirements/configs that I should give to DevOps for setting up services in a cluster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bngsi2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711381262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New job, and I&amp;#39;m kinda new at being in this capacity.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning on refactoring a lot of our data infrastructure, at least insofar as a few teams are concerned. I was thinking of going into this by having DevOps provision me an EC2, S3, and Redshift instance and I&amp;#39;d just set things up myself on an as-needed basis; i.e., start with dbt, Airflow, and the means to connect to our S3 and Redshift, then anything else I may happen to come across.&lt;/p&gt;\n\n&lt;p&gt;DevOps is currently trying to just give me something set up in a cluster and they&amp;#39;re asking for what services and relevant configs/requirements I need set up. So, I guess this is a two-part question:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What other services and relevant configs should I consider adding to this setup?&lt;/li&gt;\n&lt;li&gt;Should I insist that I be given an EC2 mostly because DevOps has admitted to being understaffed and I wouldn&amp;#39;t want to have to wait a while for additional services to be added if I come across some that would help?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bngsi2", "is_robot_indexable": true, "report_reasons": null, "author": "paxmlank", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bngsi2/any_requirementsconfigs_that_i_should_give_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bngsi2/any_requirementsconfigs_that_i_should_give_to/", "subreddit_subscribers": 171676, "created_utc": 1711381262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,  \nWe are considering dbt core for transformations. I have a question, When we modify a model in the development environment, is there a way to ensure that it won't cause any issues with downstream tables before we merge it into production? ", "author_fullname": "t2_g2837t6l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downstream model testing in the dbt dev environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnf899", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711377355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;br/&gt;\nWe are considering dbt core for transformations. I have a question, When we modify a model in the development environment, is there a way to ensure that it won&amp;#39;t cause any issues with downstream tables before we merge it into production? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnf899", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting-Fun8932", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnf899/downstream_model_testing_in_the_dbt_dev/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnf899/downstream_model_testing_in_the_dbt_dev/", "subreddit_subscribers": 171676, "created_utc": 1711377355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\n\n&amp;#x200B;\n\nMy New video on the topic mentioned in title is out here: [https://youtu.be/-UPzFFiC\\_BQ](https://youtu.be/-UPzFFiC_BQ).  \nIf you are interested in learning about the topic pls do watch and subscribe for further videos.", "author_fullname": "t2_f86nbjeq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Catalyst Optimizer or Spark SQL Engine.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnf0jm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711376813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My New video on the topic mentioned in title is out here: &lt;a href=\"https://youtu.be/-UPzFFiC_BQ\"&gt;https://youtu.be/-UPzFFiC_BQ&lt;/a&gt;.&lt;br/&gt;\nIf you are interested in learning about the topic pls do watch and subscribe for further videos.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U7K0C35KAiRxaUg2e7s23efVn8GQDiIrjeNDvpEnzgM.jpg?auto=webp&amp;s=e7fc3922b1fe919b99219f558dac17f8f14937d6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/U7K0C35KAiRxaUg2e7s23efVn8GQDiIrjeNDvpEnzgM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d47cf49d37361f3c9e5fbcf93d8ccd18c5676d6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/U7K0C35KAiRxaUg2e7s23efVn8GQDiIrjeNDvpEnzgM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bfe6093cb97ef8511526ad7a4647cb0aa49d284a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/U7K0C35KAiRxaUg2e7s23efVn8GQDiIrjeNDvpEnzgM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a794425113d3637345c6b45e9a2e9f99d96a876", "width": 320, "height": 240}], "variants": {}, "id": "RohhFZZr0wZOySlS19JOIKSvLQ5DhdpUqXAh6AZtp-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bnf0jm", "is_robot_indexable": true, "report_reasons": null, "author": "Vikinghehe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnf0jm/catalyst_optimizer_or_spark_sql_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnf0jm/catalyst_optimizer_or_spark_sql_engine/", "subreddit_subscribers": 171676, "created_utc": 1711376813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not having used DBT for anything other than demos, this is likely a 'basic mindset' question.\n\nIs it normal to have a destination final target in your data warehouse that is defined outside of dbt?  \n\nIn other words: would it be the \"dbt way\" to for an organization to say: \"the schema of ACCOUNTS\\_DB.HVAC.PERRY is maintained by some team other than DEs, and DEs use dbt to update that table, but dbt user does not have sufficient grants to modify the table's metadata\"\n\n&amp;#x200B;", "author_fullname": "t2_abfpw4qq7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt basics - final model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bnbtkc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711367847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not having used DBT for anything other than demos, this is likely a &amp;#39;basic mindset&amp;#39; question.&lt;/p&gt;\n\n&lt;p&gt;Is it normal to have a destination final target in your data warehouse that is defined outside of dbt?  &lt;/p&gt;\n\n&lt;p&gt;In other words: would it be the &amp;quot;dbt way&amp;quot; to for an organization to say: &amp;quot;the schema of ACCOUNTS_DB.HVAC.PERRY is maintained by some team other than DEs, and DEs use dbt to update that table, but dbt user does not have sufficient grants to modify the table&amp;#39;s metadata&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bnbtkc", "is_robot_indexable": true, "report_reasons": null, "author": "levintennine", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bnbtkc/dbt_basics_final_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bnbtkc/dbt_basics_final_model/", "subreddit_subscribers": 171676, "created_utc": 1711367847.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}