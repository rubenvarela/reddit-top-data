{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working as a DE for about a year and a half the following is my opinion on basis for my experience and looking at mu friends\u2019 experiences. 90% of the roles in data are usually analytics, BI, data science. Even if it is a DE role it usually falls into one of the above. These roles typically exist in orgs which are not mature with and in data and execs work on excel. If this is the case, then the \u2018data\u2019 team\u2019s priority is making a case for itself /selling itself with its initiatives add value to the execs. In my opinion this is very close to consulting. This causes a de-prioritization of DE work which can be lack of data modeling, no focus on data infra, data quality sucks etc. This makes DE a support role and a visibility lacking role. On the other hand, orgs which are mature with data, say Netflix, few mid sized startups and maybe few companies actually have real DE roles where focus is equal on infra, data quality, analytics, DS. If I want to get into these roles, it makes it tougher as there are so few of these. Would like to know thoughts of DEs/Senior DEs here who have been thru this/navigated/transitioned into something else from DE", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u2018Data\u2019 is essentially a consulting field in most companies ? Thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1basru9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 80, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710024687.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710019796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a DE for about a year and a half the following is my opinion on basis for my experience and looking at mu friends\u2019 experiences. 90% of the roles in data are usually analytics, BI, data science. Even if it is a DE role it usually falls into one of the above. These roles typically exist in orgs which are not mature with and in data and execs work on excel. If this is the case, then the \u2018data\u2019 team\u2019s priority is making a case for itself /selling itself with its initiatives add value to the execs. In my opinion this is very close to consulting. This causes a de-prioritization of DE work which can be lack of data modeling, no focus on data infra, data quality sucks etc. This makes DE a support role and a visibility lacking role. On the other hand, orgs which are mature with data, say Netflix, few mid sized startups and maybe few companies actually have real DE roles where focus is equal on infra, data quality, analytics, DS. If I want to get into these roles, it makes it tougher as there are so few of these. Would like to know thoughts of DEs/Senior DEs here who have been thru this/navigated/transitioned into something else from DE&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1basru9", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1basru9/data_is_essentially_a_consulting_field_in_most/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1basru9/data_is_essentially_a_consulting_field_in_most/", "subreddit_subscribers": 167309, "created_utc": 1710019796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learn the simple yet powerful optimization techniques that helped me reduce BigQuery spend by $70,000 a month.\n\nI think lot of folks can take help from this one:\nhttps://www.junaideffendi.com/p/how-i-saved-70k-a-month-in-bigquery\n\nThese techniques can be applied to most of the data warehouses in the market today.\n\nLet me know what else have you done to save $$$.\n\nThanks for reading :)\n", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving $70k a month in DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bapzzr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710013765.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710012772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learn the simple yet powerful optimization techniques that helped me reduce BigQuery spend by $70,000 a month.&lt;/p&gt;\n\n&lt;p&gt;I think lot of folks can take help from this one:\n&lt;a href=\"https://www.junaideffendi.com/p/how-i-saved-70k-a-month-in-bigquery\"&gt;https://www.junaideffendi.com/p/how-i-saved-70k-a-month-in-bigquery&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;These techniques can be applied to most of the data warehouses in the market today.&lt;/p&gt;\n\n&lt;p&gt;Let me know what else have you done to save $$$.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?auto=webp&amp;s=d28ff71c6b5bd71061c223703a1a03ec24994d59", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23aeef673fefebb88d5345c3c1a8852dd7c5e097", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=976978b5380b045c766822cf9fd75448d0834cf6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b76d2509f73cbfa97eefd22d554d78f20d1648c1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=05329556dea12385f46545b45e7bb139c6efdd99", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3be9e1a3ffd4133c17b3046cf032eb77f448839", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/j98jsWRaDb2Syuhjm9wvh_xgTZQv1gr50beHYZu9ulQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8281fed66fd14c2a45dc709b0fc6f9fa7eebb99a", "width": 1080, "height": 540}], "variants": {}, "id": "57zTR0op2xToniSMTCu-C9Fq6-ZMfwz5tXxPBNdNYTE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bapzzr", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bapzzr/saving_70k_a_month_in_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bapzzr/saving_70k_a_month_in_dwh/", "subreddit_subscribers": 167309, "created_utc": 1710012772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit fam,\n\n5-year Data Analyst here feeling like I've hit a wall. I'm a CS grad with proficiency in SQL, Tableau, Metabase, dbt, dimensional modeling, and a sprinkle of Python. My experience spans across Fintech, Health, and Telecom.\n\nWhile I appreciate data analysis, I crave a more challenging and technical role. With my programming background, I feel underutilized. Is it too late to switch gears?\n\nI'm torn between Data Engineering and Data Science for upskilling. My goal? Advance to a more technical position within data.\n\nHere's the kicker: I don't just want courses. I crave practical learning that integrates with my current skillset and is job-market relevant.\n\nAny advice on the best path forward? Open to all suggestions! Happy to share more details about my experience\n\nThanks in advance!", "author_fullname": "t2_mpil1bv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analyst (SQL &amp; Viz ONLY!) - Feeling Stuck. What's the Next Step?!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1baknx0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709999226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit fam,&lt;/p&gt;\n\n&lt;p&gt;5-year Data Analyst here feeling like I&amp;#39;ve hit a wall. I&amp;#39;m a CS grad with proficiency in SQL, Tableau, Metabase, dbt, dimensional modeling, and a sprinkle of Python. My experience spans across Fintech, Health, and Telecom.&lt;/p&gt;\n\n&lt;p&gt;While I appreciate data analysis, I crave a more challenging and technical role. With my programming background, I feel underutilized. Is it too late to switch gears?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m torn between Data Engineering and Data Science for upskilling. My goal? Advance to a more technical position within data.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the kicker: I don&amp;#39;t just want courses. I crave practical learning that integrates with my current skillset and is job-market relevant.&lt;/p&gt;\n\n&lt;p&gt;Any advice on the best path forward? Open to all suggestions! Happy to share more details about my experience&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1baknx0", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Avocado-226", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1baknx0/data_analyst_sql_viz_only_feeling_stuck_whats_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1baknx0/data_analyst_sql_viz_only_feeling_stuck_whats_the/", "subreddit_subscribers": 167309, "created_utc": 1709999226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you orchestrating AI calls in your pipelines. I often come across the case that I have to run classifications on subsets of data, which is then processed further based on these classifications.   \n\n\nThe classification might run on raw documents (e.g. decide whether it is an invoice, an email, or something else) or on text. Mostly the data is either reinserted into the database or another stage of the lakehouse, but sometimes it is also passed to the next processing step.   \n\n\nI would love to know what tools you are using and how you are orchestrating these AI calls. I personally often have to rely on serverless AI deployments, which I autoscale, so I try wherever possible trigger it with a simple Lambda, which calls the different AI endpoints and alters the data and then reinserts it wherever necessary. I would love to know whether you deem that a bad approach.  \n\n\nIf you know or have seen any other interesting libraries or tools, drop them below.  ", "author_fullname": "t2_as93aiie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the tools you use to integrate AI in your dags?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bb3alv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710051060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you orchestrating AI calls in your pipelines. I often come across the case that I have to run classifications on subsets of data, which is then processed further based on these classifications.   &lt;/p&gt;\n\n&lt;p&gt;The classification might run on raw documents (e.g. decide whether it is an invoice, an email, or something else) or on text. Mostly the data is either reinserted into the database or another stage of the lakehouse, but sometimes it is also passed to the next processing step.   &lt;/p&gt;\n\n&lt;p&gt;I would love to know what tools you are using and how you are orchestrating these AI calls. I personally often have to rely on serverless AI deployments, which I autoscale, so I try wherever possible trigger it with a simple Lambda, which calls the different AI endpoints and alters the data and then reinserts it wherever necessary. I would love to know whether you deem that a bad approach.  &lt;/p&gt;\n\n&lt;p&gt;If you know or have seen any other interesting libraries or tools, drop them below.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bb3alv", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedAd895", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bb3alv/what_are_the_tools_you_use_to_integrate_ai_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bb3alv/what_are_the_tools_you_use_to_integrate_ai_in/", "subreddit_subscribers": 167309, "created_utc": 1710051060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a one man data team who is responsible for too much. In an effort to make our reporting tool easier to use the data model needs to be simplified. Our analytics system is a bloated mess in PowerBI and I am planning on rebuilding it, and planning to hire a consultant to help me build a best practice retail data model for reporting using DBT core + metric flow (open source semantic layer). The source data is already being served in BigQuery with an active DBT project so it\u2019s purely a modeling design exercise.\n\nFor serious data professionals where might be a good place to search for consultants in this domain?", "author_fullname": "t2_ronx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to find good data modeling consultants?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bahyli", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "8cf4f390-e787-11ed-81a4-ca7b65282907", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709991590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a one man data team who is responsible for too much. In an effort to make our reporting tool easier to use the data model needs to be simplified. Our analytics system is a bloated mess in PowerBI and I am planning on rebuilding it, and planning to hire a consultant to help me build a best practice retail data model for reporting using DBT core + metric flow (open source semantic layer). The source data is already being served in BigQuery with an active DBT project so it\u2019s purely a modeling design exercise.&lt;/p&gt;\n\n&lt;p&gt;For serious data professionals where might be a good place to search for consultants in this domain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Lead Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bahyli", "is_robot_indexable": true, "report_reasons": null, "author": "seanpool3", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bahyli/where_to_find_good_data_modeling_consultants/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bahyli/where_to_find_good_data_modeling_consultants/", "subreddit_subscribers": 167309, "created_utc": 1709991590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1bx2p34m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roast my data project &amp; video editing skills. I made a python script that acts like a Google Tasks plugin on Obsidian, the note-taking app. Works with the watchdog library to watch for file changes in your Obsidian vault's daily note. Code in comments!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1bazt75", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/jywlhev28fnc1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jywlhev28fnc1/DASH_96.mp4", "dash_url": "https://v.redd.it/jywlhev28fnc1/DASHPlaylist.mpd?a=1712669886%2CMGMzYjE1ZWEwZDVlYzNlNzJjN2Y1NDAxOTA5YWYwZTJkOWZhNzA1NWI5YTdlZDA0N2NkYjdlNzk1MWJmZWM2Mg%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/jywlhev28fnc1/HLSPlaylist.m3u8?a=1712669886%2CMjIyNmNhOWRmNmRjMmJhMjA4ZmE1M2Q5NGNlY2EyZmQ1ZTE0MjQ0YmE4NmM0OWZlMWE3N2M5NDYxNTA0MmNmMw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=e7b1dfb3eaff091841eac51109a8323a51acbce8", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710039316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/jywlhev28fnc1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?format=pjpg&amp;auto=webp&amp;s=e52ff659189eed228a1c74e3bd75e07ef0eb491d", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e810fda3fdfe6d0276a7e26fd40812cce197c5f1", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a43cb69274f0205c680b3eadb43bca5be24e655a", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e2ef78bac704831c6e3fe4b3a45d593a71047219", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=844f3360c52fbfce67cf4b092b4b7177419b0a34", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b47e35f6d386c07cadc4bf61c46813ce0943d5ee", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bd0a9daf229e11028a53cc98a1ce2e436dbe0571", "width": 1080, "height": 607}], "variants": {}, "id": "bTJ5YTYybXU4Zm5jMeLTSlyXiR3V6N6StUXMKx-SQmv_UHV8VNtOEnjGKOXl"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bazt75", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrapez", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bazt75/roast_my_data_project_video_editing_skills_i_made/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/jywlhev28fnc1", "subreddit_subscribers": 167309, "created_utc": 1710039316.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 5000, "fallback_url": "https://v.redd.it/jywlhev28fnc1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 1920, "scrubber_media_url": "https://v.redd.it/jywlhev28fnc1/DASH_96.mp4", "dash_url": "https://v.redd.it/jywlhev28fnc1/DASHPlaylist.mpd?a=1712669886%2CMGMzYjE1ZWEwZDVlYzNlNzJjN2Y1NDAxOTA5YWYwZTJkOWZhNzA1NWI5YTdlZDA0N2NkYjdlNzk1MWJmZWM2Mg%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/jywlhev28fnc1/HLSPlaylist.m3u8?a=1712669886%2CMjIyNmNhOWRmNmRjMmJhMjA4ZmE1M2Q5NGNlY2EyZmQ1ZTE0MjQ0YmE4NmM0OWZlMWE3N2M5NDYxNTA0MmNmMw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building systems that business users will eventually rely upon to do their job. Data will be collected via Webapp, asynchronously from hundreds of remote super intendants at any time of day. This will get cleaned and funneled into an analytical database so that project managers can review and make changes over time. In a way, it transitions the state of their project from implicit to explicit, as the most current state of any project is now tracked and represented via the database. Eventually, project managers should rely completely on my system for the service it provides.\n\nThis means that if super intendants can't access my web app for ANY reason, they can't do their job. That's unacceptable to me.\n\nAs a failsafe, I'm thinking of creating a physical version of the data collection form via Scantron. I've never done this though ..\n\nWould anyone have some insight into the kinds of problems I'm likely to face and what sort of tradeoffs are made when choosing a solution to those problems?", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used Scantron sheets yet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bapiy7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710011535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building systems that business users will eventually rely upon to do their job. Data will be collected via Webapp, asynchronously from hundreds of remote super intendants at any time of day. This will get cleaned and funneled into an analytical database so that project managers can review and make changes over time. In a way, it transitions the state of their project from implicit to explicit, as the most current state of any project is now tracked and represented via the database. Eventually, project managers should rely completely on my system for the service it provides.&lt;/p&gt;\n\n&lt;p&gt;This means that if super intendants can&amp;#39;t access my web app for ANY reason, they can&amp;#39;t do their job. That&amp;#39;s unacceptable to me.&lt;/p&gt;\n\n&lt;p&gt;As a failsafe, I&amp;#39;m thinking of creating a physical version of the data collection form via Scantron. I&amp;#39;ve never done this though ..&lt;/p&gt;\n\n&lt;p&gt;Would anyone have some insight into the kinds of problems I&amp;#39;m likely to face and what sort of tradeoffs are made when choosing a solution to those problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bapiy7", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bapiy7/has_anyone_used_scantron_sheets_yet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bapiy7/has_anyone_used_scantron_sheets_yet/", "subreddit_subscribers": 167309, "created_utc": 1710011535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! \nGiven any hypothetical data (you may assume we have to scrape for the data ourselves, or have access to an API, or anything - up to you), what would you do with it to ensure it\u2019s a project that teaches you the most crucial data engineering concepts, and one you can display on your portfolio?\n\nPlease help me ideate! \n\nP.S. The cheaper the better!", "author_fullname": "t2_7gvb151", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should an end to end personal project encompass for you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bb3wa2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710053259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! \nGiven any hypothetical data (you may assume we have to scrape for the data ourselves, or have access to an API, or anything - up to you), what would you do with it to ensure it\u2019s a project that teaches you the most crucial data engineering concepts, and one you can display on your portfolio?&lt;/p&gt;\n\n&lt;p&gt;Please help me ideate! &lt;/p&gt;\n\n&lt;p&gt;P.S. The cheaper the better!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bb3wa2", "is_robot_indexable": true, "report_reasons": null, "author": "SueTupp", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bb3wa2/what_should_an_end_to_end_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bb3wa2/what_should_an_end_to_end_personal_project/", "subreddit_subscribers": 167309, "created_utc": 1710053259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working on a patent analysis project and I'm seeking insights on how to efficiently scrape data from the United States Patent and Trademark Office (USPTO) website. We aim to stream this data into a data lake for further analysis. I'm particularly interested in methods or tools that can help streamline the process and ensure accuracy in collecting patent information, while also facilitating the integration of the scraped data into our data lake infrastructure. Any advice or recommendations on best practices, relevant libraries, or specific techniques would be greatly appreciated. Thank you in advance for your insights!", "author_fullname": "t2_7eqe0vfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering student in need of help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1banv68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710007357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a patent analysis project and I&amp;#39;m seeking insights on how to efficiently scrape data from the United States Patent and Trademark Office (USPTO) website. We aim to stream this data into a data lake for further analysis. I&amp;#39;m particularly interested in methods or tools that can help streamline the process and ensure accuracy in collecting patent information, while also facilitating the integration of the scraped data into our data lake infrastructure. Any advice or recommendations on best practices, relevant libraries, or specific techniques would be greatly appreciated. Thank you in advance for your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1banv68", "is_robot_indexable": true, "report_reasons": null, "author": "Positive_Temporary77", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1banv68/data_engineering_student_in_need_of_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1banv68/data_engineering_student_in_need_of_help/", "subreddit_subscribers": 167309, "created_utc": 1710007357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nI want to create a parallel copy activity for 5 components to the datalake. Three of them are from legacy database to datalake, two of them from sharepoint to datalake. Out of those three from legacy, two are from same db and schema, one is from different. From sharepoint everything is in the same directory just different folders\n\nSo far we dont use something like metadata pipeline but it will be soon either config from db/databricks or so however for the time being its not there. And I wonder what is the best approach for that?\n\nShould I create one executor pipeline and in the sub pipeline 5 copy activity with directly set db/schema and query for each copy activity and with bearer tokens to get sharepoint access and ingest files from there?  I know most sense makes split by source, atleast by db and sharepoint but then, how it is parallel? I believe what they want is to click debug or trigger will get all 5 copy activities running at the same time, so \"for each\" is not the case? Do I really need this master pipeline?\n\nEDIT:okay I figured out that if I have \"IsSequential\" set to false and I run via Trigger it will be parallel, not sequential, but still, how to create one properly and set the metadata including what query to run\n\nAlso if I were to create a json file with config metadata for each, where should I store it? in the raw directory in datalake, where all the files/tables should go after ingestion?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qpx7gcnj3inc1.png?width=1345&amp;format=png&amp;auto=webp&amp;s=77fa499c16575f1da01c393fc4646c8f9db979a3", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my \"architecture\" ok?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 43, "top_awarded_type": null, "hide_score": true, "media_metadata": {"qpx7gcnj3inc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7918510c51e106c80f2c121cc08f1c6c0a5eccbc"}, {"y": 66, "x": 216, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7327008d21956a77d0ff5a5c6bfe0ba79109205"}, {"y": 99, "x": 320, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=93c27d7223b4385ab4a876e3d747cc50ca746a4c"}, {"y": 198, "x": 640, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84775eaaada206f8ee36594bb8ac27bf77b309fc"}, {"y": 297, "x": 960, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cfde75ab0f72a8e2b3cc273021bef945bc8f575"}, {"y": 334, "x": 1080, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=032ca72cf2fcc67403e78e8ffc96ebbf3f717c26"}], "s": {"y": 417, "x": 1345, "u": "https://preview.redd.it/qpx7gcnj3inc1.png?width=1345&amp;format=png&amp;auto=webp&amp;s=77fa499c16575f1da01c393fc4646c8f9db979a3"}, "id": "qpx7gcnj3inc1"}}, "name": "t3_1bb9am5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/THUhHFK-q07g153t_SPIWLCXvX2Pkc4BCKTFXvlzaqY.jpg", "edited": 1710074905.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710073955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I want to create a parallel copy activity for 5 components to the datalake. Three of them are from legacy database to datalake, two of them from sharepoint to datalake. Out of those three from legacy, two are from same db and schema, one is from different. From sharepoint everything is in the same directory just different folders&lt;/p&gt;\n\n&lt;p&gt;So far we dont use something like metadata pipeline but it will be soon either config from db/databricks or so however for the time being its not there. And I wonder what is the best approach for that?&lt;/p&gt;\n\n&lt;p&gt;Should I create one executor pipeline and in the sub pipeline 5 copy activity with directly set db/schema and query for each copy activity and with bearer tokens to get sharepoint access and ingest files from there?  I know most sense makes split by source, atleast by db and sharepoint but then, how it is parallel? I believe what they want is to click debug or trigger will get all 5 copy activities running at the same time, so &amp;quot;for each&amp;quot; is not the case? Do I really need this master pipeline?&lt;/p&gt;\n\n&lt;p&gt;EDIT:okay I figured out that if I have &amp;quot;IsSequential&amp;quot; set to false and I run via Trigger it will be parallel, not sequential, but still, how to create one properly and set the metadata including what query to run&lt;/p&gt;\n\n&lt;p&gt;Also if I were to create a json file with config metadata for each, where should I store it? in the raw directory in datalake, where all the files/tables should go after ingestion?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qpx7gcnj3inc1.png?width=1345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77fa499c16575f1da01c393fc4646c8f9db979a3\"&gt;https://preview.redd.it/qpx7gcnj3inc1.png?width=1345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77fa499c16575f1da01c393fc4646c8f9db979a3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bb9am5", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bb9am5/is_my_architecture_ok/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bb9am5/is_my_architecture_ok/", "subreddit_subscribers": 167309, "created_utc": 1710073955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI am a CS masters student. Have 2 years of experience in Cloud technologies along with Newtork and security background. These days\nMy interest is in Data engineering kind of stuff like I am learning Sql and Python and even feel enthusiastic and getting better everyday.Is it the right step and what could be a best approach here to crack a job especially in the US for a fresher like me. Any resources ?? Any help is appreciated.\nThanks!", "author_fullname": "t2_stesjwdoq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused in choosing my career.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1baypfj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710035941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a CS masters student. Have 2 years of experience in Cloud technologies along with Newtork and security background. These days\nMy interest is in Data engineering kind of stuff like I am learning Sql and Python and even feel enthusiastic and getting better everyday.Is it the right step and what could be a best approach here to crack a job especially in the US for a fresher like me. Any resources ?? Any help is appreciated.\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1baypfj", "is_robot_indexable": true, "report_reasons": null, "author": "Stunning_Program_968", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1baypfj/confused_in_choosing_my_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1baypfj/confused_in_choosing_my_career/", "subreddit_subscribers": 167309, "created_utc": 1710035941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI would really appreciate if a fellow data engineer could help me out.\n\nI have been doing training in Databricks, as we plan to use it at work for our ML tasks.\n\nOne thing we use is YOLO for object detection. I\u2019ve managed to run YOLO by loading data from the blob storage, but I\u2019ve seen that the best way to do deep learning tasks in Databricks is to train your ML models on Delta Live Tables.\n\nI currently have my training dataset as a Delta table, and I was wondering if anyone has managed to train computer vision models on Delta tables.\n\nI\u2019ve read the documentations and have seen repos such as petastorm that try to implement training on delta tables, but I can\u2019t for the life of me understand how to actually run yolo this way, especially since YOLO uses yaml for config.\n\nThank in advance for your help! \ud83d\ude07", "author_fullname": "t2_3ln911x9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Delta tables and YOLO computer vision", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bax86r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710031595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate if a fellow data engineer could help me out.&lt;/p&gt;\n\n&lt;p&gt;I have been doing training in Databricks, as we plan to use it at work for our ML tasks.&lt;/p&gt;\n\n&lt;p&gt;One thing we use is YOLO for object detection. I\u2019ve managed to run YOLO by loading data from the blob storage, but I\u2019ve seen that the best way to do deep learning tasks in Databricks is to train your ML models on Delta Live Tables.&lt;/p&gt;\n\n&lt;p&gt;I currently have my training dataset as a Delta table, and I was wondering if anyone has managed to train computer vision models on Delta tables.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve read the documentations and have seen repos such as petastorm that try to implement training on delta tables, but I can\u2019t for the life of me understand how to actually run yolo this way, especially since YOLO uses yaml for config.&lt;/p&gt;\n\n&lt;p&gt;Thank in advance for your help! \ud83d\ude07&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bax86r", "is_robot_indexable": true, "report_reasons": null, "author": "wsb_crazytrader", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bax86r/databricks_delta_tables_and_yolo_computer_vision/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bax86r/databricks_delta_tables_and_yolo_computer_vision/", "subreddit_subscribers": 167309, "created_utc": 1710031595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We all know that good quality data is needed for analytics, machine learning and artificial intelligence to really be useful. \n\nI think the modern data team resembles a superhero team up of The Key Three (data owner, steward and architect) who implement governance and management with the analytics and engineering team known as The Core Four (data engineer, analytics engineer, data analyst, machine learning to engineer). \n\nDo you agree? \nWha about data science? What about business intelligence? I wrote a post here... Check it out of interested \nhttps://medium.com/@robdoesdata/what-roles-exist-in-the-modern-data-team-571f934f78c1", "author_fullname": "t2_jso5n8jj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roles in the modern data team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bawk3l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.35, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710029724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We all know that good quality data is needed for analytics, machine learning and artificial intelligence to really be useful. &lt;/p&gt;\n\n&lt;p&gt;I think the modern data team resembles a superhero team up of The Key Three (data owner, steward and architect) who implement governance and management with the analytics and engineering team known as The Core Four (data engineer, analytics engineer, data analyst, machine learning to engineer). &lt;/p&gt;\n\n&lt;p&gt;Do you agree? \nWha about data science? What about business intelligence? I wrote a post here... Check it out of interested \n&lt;a href=\"https://medium.com/@robdoesdata/what-roles-exist-in-the-modern-data-team-571f934f78c1\"&gt;https://medium.com/@robdoesdata/what-roles-exist-in-the-modern-data-team-571f934f78c1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x0FxOCovoJn_p4wVSijUF1r66BBCN8fOsRBjIZYVhJo.jpg?auto=webp&amp;s=1162157499fdbb8911b2808d8f3d2a207c32d296", "width": 1200, "height": 422}, "resolutions": [{"url": "https://external-preview.redd.it/x0FxOCovoJn_p4wVSijUF1r66BBCN8fOsRBjIZYVhJo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7fbd376ef42442f29928ed967717d7bdd06f7d9d", "width": 108, "height": 37}, {"url": "https://external-preview.redd.it/x0FxOCovoJn_p4wVSijUF1r66BBCN8fOsRBjIZYVhJo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d153b701808d4c7eabf0232d282446116ab25045", "width": 216, "height": 75}, {"url": "https://external-preview.redd.it/x0FxOCovoJn_p4wVSijUF1r66BBCN8fOsRBjIZYVhJo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0e909a7a568d193638cf54c2a2e6287f3c8c579", "width": 320, "height": 112}, {"url": "https://external-preview.redd.it/x0FxOCovoJn_p4wVSijUF1r66BBCN8fOsRBjIZYVhJo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ae95a7873b9ab96be06c648236c46fbc1b687f6", "width": 640, "height": 225}, {"url": "https://external-preview.redd.it/x0FxOCovoJn_p4wVSijUF1r66BBCN8fOsRBjIZYVhJo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9243d477a9101f0a521ca401bbaf817cb58430df", "width": 960, "height": 337}, {"url": "https://external-preview.redd.it/x0FxOCovoJn_p4wVSijUF1r66BBCN8fOsRBjIZYVhJo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=54ee03412b28ab45d2e63a32e7abbe11724219eb", "width": 1080, "height": 379}], "variants": {}, "id": "vJynZ3MpgC1EUXrMkAsLC7NGDN4BrhcQSMUZH75QA1Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bawk3l", "is_robot_indexable": true, "report_reasons": null, "author": "RobDoesData", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bawk3l/roles_in_the_modern_data_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bawk3l/roles_in_the_modern_data_team/", "subreddit_subscribers": 167309, "created_utc": 1710029724.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}