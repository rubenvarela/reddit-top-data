{"kind": "Listing", "data": {"after": "t3_1b5vxxa", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As you know there are rumors about shutting down subscene.\n\nThis is a FULL Subscene database, it has every single subtitle file has been uploaded, even the deleted ones, they have the same structure of subscene with all the metadata\n\nThere is also nzb file if you want to download through usenet provider.\n\nYou might notice V2 in the name, because there was V1 but was not publicly published, but since subscene is playing games, they have should published everything instead of scaring everyone and make them worried about losing their valuable work.\n\nSubscene didn't make a control panel for uploaders to download their old files, not make a way to get to them, it is like subscene is blackmailing people for many, they say the website does not make enough money, so why now share the files? is it a tool to make people pay?\n\nanyway this torrent solves the problem, Subscene shutting down? so be it!\n\nTorrent file:  \n[https://gofile.io/d/OEBWLu](https://gofile.io/d/OEBWLu)\n\nUsenet file:\n\n[https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/kt9qtcx/](https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/kt9qtcx/)\n\nMagnet:\n\n[https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/kta5ras/](https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/kta5ras/)\n\n**UPDATE 1:**\n\nThanks for everyone seeding the file, I have supported the file with 10gb seedbox from the beginning, but didn't expect others to suppoert it this much\n\nOnly torrent seeded near 3TB untill now: [https://i.ibb.co/8j9VvFR/image.png](https://i.ibb.co/8j9VvFR/image.png)\n\nbeside Usenet and cached real-debrid users.\n\n  \n**UPDATE 2:**\n\nsubdl is a potential alternative!\n\n[https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/ktaft6h/](https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/ktaft6h/)", "author_fullname": "t2_vf2ui7k8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Subscene.com full Dump", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5rxc2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 164, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Dump", "can_mod_post": false, "score": 164, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709556542.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709499677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As you know there are rumors about shutting down subscene.&lt;/p&gt;\n\n&lt;p&gt;This is a FULL Subscene database, it has every single subtitle file has been uploaded, even the deleted ones, they have the same structure of subscene with all the metadata&lt;/p&gt;\n\n&lt;p&gt;There is also nzb file if you want to download through usenet provider.&lt;/p&gt;\n\n&lt;p&gt;You might notice V2 in the name, because there was V1 but was not publicly published, but since subscene is playing games, they have should published everything instead of scaring everyone and make them worried about losing their valuable work.&lt;/p&gt;\n\n&lt;p&gt;Subscene didn&amp;#39;t make a control panel for uploaders to download their old files, not make a way to get to them, it is like subscene is blackmailing people for many, they say the website does not make enough money, so why now share the files? is it a tool to make people pay?&lt;/p&gt;\n\n&lt;p&gt;anyway this torrent solves the problem, Subscene shutting down? so be it!&lt;/p&gt;\n\n&lt;p&gt;Torrent file:&lt;br/&gt;\n&lt;a href=\"https://gofile.io/d/OEBWLu\"&gt;https://gofile.io/d/OEBWLu&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Usenet file:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/kt9qtcx/\"&gt;https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/kt9qtcx/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Magnet:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/kta5ras/\"&gt;https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/kta5ras/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;UPDATE 1:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for everyone seeding the file, I have supported the file with 10gb seedbox from the beginning, but didn&amp;#39;t expect others to suppoert it this much&lt;/p&gt;\n\n&lt;p&gt;Only torrent seeded near 3TB untill now: &lt;a href=\"https://i.ibb.co/8j9VvFR/image.png\"&gt;https://i.ibb.co/8j9VvFR/image.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;beside Usenet and cached real-debrid users.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;UPDATE 2:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;subdl is a potential alternative!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/ktaft6h/\"&gt;https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/comment/ktaft6h/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OW2SujYPKmLMNGUkLGwk8ntCusunWI3OPjR5SIOxFf8.png?auto=webp&amp;s=f78112adee0d060aaa8e4522c027c8ba284ecc8e", "width": 560, "height": 136}, "resolutions": [{"url": "https://external-preview.redd.it/OW2SujYPKmLMNGUkLGwk8ntCusunWI3OPjR5SIOxFf8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd0c8b9dd1a9693b191e8724542d51e7e3210662", "width": 108, "height": 26}, {"url": "https://external-preview.redd.it/OW2SujYPKmLMNGUkLGwk8ntCusunWI3OPjR5SIOxFf8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=60b9ae90fe152758c600402b4106801658da3d12", "width": 216, "height": 52}, {"url": "https://external-preview.redd.it/OW2SujYPKmLMNGUkLGwk8ntCusunWI3OPjR5SIOxFf8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c46586ccf421e823b562046ad5e7fce1334c04c", "width": 320, "height": 77}], "variants": {}, "id": "U9lwe5moQkLgfZ7izEyookJm86FxFBrR4EM3SZNmWvA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b5rxc2", "is_robot_indexable": true, "report_reasons": null, "author": "subscene_V2", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5rxc2/subscenecom_full_dump/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5rxc2/subscenecom_full_dump/", "subreddit_subscribers": 736025, "created_utc": 1709499677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hy everyone, greetings from Peru. my grandfather lived in the USA, he spent most of his time recording musical performances on his TV, over 1025 performances on DVDR, 704x 480, I am willing to share this publicly. There a lot of good rock bands, grunge, pop and more\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7ppjthv3r5mc1.png?width=642&amp;format=png&amp;auto=webp&amp;s=5eeb54822cc44893b97b20d26ff594589787240c\n\nhttps://preview.redd.it/99pujm7dr5mc1.png?width=642&amp;format=png&amp;auto=webp&amp;s=5e670ad75b889580344557a7afba29549c4ea713\n\n&amp;#x200B;\n\nhttps://preview.redd.it/f8s3snymr5mc1.png?width=642&amp;format=png&amp;auto=webp&amp;s=9e299ca226c62be6dbf5caf1da8a758706151a14\n\nI am just looking for these live performances. If you send me i sent you everything (Good Quality please)\n\nThe Strokes - The Modern Age - Conan Obrien (November 1, 2001)\n\nThe Strokes - Soma - Conan Obrien (May 17, 2002)\n\nThe Strokes - Someday (August 16, 2002)\n\nThe Strokes - You Only Live Once (May 3, 2006)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8dlfe2zf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have 98DVDR of Conan Obrien, Letterman, Jay Leno, Carson performances (1025 performances)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 134, "top_awarded_type": null, "hide_score": false, "media_metadata": {"f8s3snymr5mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 103, "x": 108, "u": "https://preview.redd.it/f8s3snymr5mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=95192f005297dc982e022e3ef9f456cd89399fb8"}, {"y": 206, "x": 216, "u": "https://preview.redd.it/f8s3snymr5mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1626bf1207b02e0af8e8f6c7acb0ef93fc068cae"}, {"y": 306, "x": 320, "u": "https://preview.redd.it/f8s3snymr5mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b6cbd2bc8c5c8063bd09ed0877f5bc0164b40fa"}, {"y": 613, "x": 640, "u": "https://preview.redd.it/f8s3snymr5mc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a3d7f008bbacfaeefc8663967769d35876ac6ee"}], "s": {"y": 615, "x": 642, "u": "https://preview.redd.it/f8s3snymr5mc1.png?width=642&amp;format=png&amp;auto=webp&amp;s=9e299ca226c62be6dbf5caf1da8a758706151a14"}, "id": "f8s3snymr5mc1"}, "7ppjthv3r5mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 103, "x": 108, "u": "https://preview.redd.it/7ppjthv3r5mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c128895872cd22f9db5c4e718b353e0c803cc58"}, {"y": 206, "x": 216, "u": "https://preview.redd.it/7ppjthv3r5mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a905faab8486d90d4b92d67076939285d3e2d8e"}, {"y": 306, "x": 320, "u": "https://preview.redd.it/7ppjthv3r5mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d570d047221e8188d8ed81432f821cfd8e630dab"}, {"y": 613, "x": 640, "u": "https://preview.redd.it/7ppjthv3r5mc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=488096444766de792ad0d7579ede49b23087489c"}], "s": {"y": 615, "x": 642, "u": "https://preview.redd.it/7ppjthv3r5mc1.png?width=642&amp;format=png&amp;auto=webp&amp;s=5eeb54822cc44893b97b20d26ff594589787240c"}, "id": "7ppjthv3r5mc1"}, "99pujm7dr5mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 103, "x": 108, "u": "https://preview.redd.it/99pujm7dr5mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2bffcf820012054ed936f84a473e211ada810cbc"}, {"y": 206, "x": 216, "u": "https://preview.redd.it/99pujm7dr5mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f468b68bfe39a755d49583808144ff9d080df4fa"}, {"y": 306, "x": 320, "u": "https://preview.redd.it/99pujm7dr5mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4125ea1940f81238ab47b1fe61f911e64fd3dcf4"}, {"y": 613, "x": 640, "u": "https://preview.redd.it/99pujm7dr5mc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=848e42f843daf92f5c19e9ffafe5ddf93cd5b197"}], "s": {"y": 615, "x": 642, "u": "https://preview.redd.it/99pujm7dr5mc1.png?width=642&amp;format=png&amp;auto=webp&amp;s=5e670ad75b889580344557a7afba29549c4ea713"}, "id": "99pujm7dr5mc1"}}, "name": "t3_1b5nd6p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 82, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 82, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/VexbHj06oe6dB46eeoI5VLAiHbnChtvXUTR1mhkCmR4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709488796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hy everyone, greetings from Peru. my grandfather lived in the USA, he spent most of his time recording musical performances on his TV, over 1025 performances on DVDR, 704x 480, I am willing to share this publicly. There a lot of good rock bands, grunge, pop and more&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7ppjthv3r5mc1.png?width=642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5eeb54822cc44893b97b20d26ff594589787240c\"&gt;https://preview.redd.it/7ppjthv3r5mc1.png?width=642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5eeb54822cc44893b97b20d26ff594589787240c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/99pujm7dr5mc1.png?width=642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e670ad75b889580344557a7afba29549c4ea713\"&gt;https://preview.redd.it/99pujm7dr5mc1.png?width=642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e670ad75b889580344557a7afba29549c4ea713&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/f8s3snymr5mc1.png?width=642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e299ca226c62be6dbf5caf1da8a758706151a14\"&gt;https://preview.redd.it/f8s3snymr5mc1.png?width=642&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e299ca226c62be6dbf5caf1da8a758706151a14&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am just looking for these live performances. If you send me i sent you everything (Good Quality please)&lt;/p&gt;\n\n&lt;p&gt;The Strokes - The Modern Age - Conan Obrien (November 1, 2001)&lt;/p&gt;\n\n&lt;p&gt;The Strokes - Soma - Conan Obrien (May 17, 2002)&lt;/p&gt;\n\n&lt;p&gt;The Strokes - Someday (August 16, 2002)&lt;/p&gt;\n\n&lt;p&gt;The Strokes - You Only Live Once (May 3, 2006)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b5nd6p", "is_robot_indexable": true, "report_reasons": null, "author": "Greedy_Ad_8624", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5nd6p/i_have_98dvdr_of_conan_obrien_letterman_jay_leno/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5nd6p/i_have_98dvdr_of_conan_obrien_letterman_jay_leno/", "subreddit_subscribers": 736025, "created_utc": 1709488796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5dtrzurd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "I need to compress and archive 5k+ old videos for archive, Is there a compressor mac software (not handbrake) that retains date metadata? I've been using handbrake but while searching for answers, few of their community members is somewhat against it, I cant rename each file like this guys. Thanks.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 136, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5g9kx4edh5mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 105, "x": 108, "u": "https://preview.redd.it/5g9kx4edh5mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=63cddee773cffb6c7d5d0411fab3a84b32a625ae"}, {"y": 210, "x": 216, "u": "https://preview.redd.it/5g9kx4edh5mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecab9cccf4566307846ef736bb61602cf391077d"}, {"y": 311, "x": 320, "u": "https://preview.redd.it/5g9kx4edh5mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f10ee60f181a35cf256ac2887b0d5b413adc5bf0"}, {"y": 623, "x": 640, "u": "https://preview.redd.it/5g9kx4edh5mc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4586b0662581c4216424dd55b775d461ba13df48"}, {"y": 935, "x": 960, "u": "https://preview.redd.it/5g9kx4edh5mc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c61e6129f067650366d86160ec12b3461cc53d8b"}, {"y": 1052, "x": 1080, "u": "https://preview.redd.it/5g9kx4edh5mc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=412368c0acbcb7471b455d73d97ce2b132b91e64"}], "s": {"y": 1436, "x": 1474, "u": "https://preview.redd.it/5g9kx4edh5mc1.png?width=1474&amp;format=png&amp;auto=webp&amp;s=2daffa99822042f7647f440512068330f2db30ca"}, "id": "5g9kx4edh5mc1"}, "sff9jwddh5mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 41, "x": 108, "u": "https://preview.redd.it/sff9jwddh5mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb8d722906e828278a3dc54d2386ae10590963c0"}, {"y": 82, "x": 216, "u": "https://preview.redd.it/sff9jwddh5mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b87824f64480fe7902dcd20c817b09c54aeb1dc"}, {"y": 121, "x": 320, "u": "https://preview.redd.it/sff9jwddh5mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1a21f5e92a62edde2662f835cbd3e0499bd09a9"}], "s": {"y": 152, "x": 400, "u": "https://preview.redd.it/sff9jwddh5mc1.png?width=400&amp;format=png&amp;auto=webp&amp;s=9854daa2efcf55f24a317da3718b4c39b5aa2e88"}, "id": "sff9jwddh5mc1"}}, "name": "t3_1b5m1di", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 36, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "5g9kx4edh5mc1", "id": 414626193}, {"media_id": "sff9jwddh5mc1", "id": 414626194}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/5Aid5ocP70hGubTbFwdw_XLrz2EDCfW4hVrY1OxkSb8.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709485456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/1b5m1di", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5m1di", "is_robot_indexable": true, "report_reasons": null, "author": "sagunmdr", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b5m1di/i_need_to_compress_and_archive_5k_old_videos_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/1b5m1di", "subreddit_subscribers": 736025, "created_utc": 1709485456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# Lists\n\n1. [https://www.reddit.com/r/DataHoarder/wiki/software/#wiki\\_website\\_archiving\\_tools](https://www.reddit.com/r/DataHoarder/wiki/software/#wiki_website_archiving_tools)\n2. [https://github.com/iipc/awesome-web-archiving](https://github.com/iipc/awesome-web-archiving)\n3. [https://github.com/ArchiveBox/ArchiveBox/wiki/Web-Archiving-Community](https://github.com/ArchiveBox/ArchiveBox/wiki/Web-Archiving-Community)\n4. [https://wiki.archiveteam.org/index.php?title=Software](https://wiki.archiveteam.org/index.php?title=Software)\n\n# Command\n\n1. [Wget](http://www.gnu.org/software/wget/) \\- An open source file retrieval utility that of [version 1.14 supports writing warcs](http://www.archiveteam.org/index.php?title=Wget_with_WARC_output). *(Stable)*\n2. [Wget-lua](https://github.com/alard/wget-lua) \\- Wget with Lua extension. *(Stable)*\n3. [Heritrix](https://github.com/internetarchive/heritrix3/wiki) \\- An open source, extensible, web-scale, archival quality web crawler. *(Stable)* [Heritrix Q&amp;A](https://github.com/internetarchive/heritrix3/discussions/categories/q-a) \\- A discussion forum for asking questions and getting answers about using Heritrix. [Heritrix Walkthrough](https://github.com/web-archive-group/heritrix-walkthrough) *(In Development)*\n4. [archivenow](https://github.com/oduwsdl/archivenow) \\- A [Python library](http://ws-dl.blogspot.com/2017/02/2017-02-22-archive-now-archivenow.html) to push web resources into on-demand web archives. *(Stable)*\n5. [StormCrawler](http://stormcrawler.net/) ([https://github.com/DigitalPebble/storm-crawler](https://github.com/DigitalPebble/storm-crawler)) - A collection of resources for building low-latency, scalable web crawlers on Apache Storm. *(Stable)*\n6. [DiskerNet](https://github.com/dosyago/DiskerNet) ([https://github.com/dosyago/DownloadNet](https://github.com/dosyago/DownloadNet)) - A non-WARC-based tool which hooks into the Chrome browser and archives everything you browse making it available for offline replay. *(In Development)*\n7. [Cairn](https://github.com/wabarc/cairn) \\- A npm package and CLI tool for saving webpages. *(Stable)*\n8. [Crawl](https://git.autistici.org/ale/crawl) \\- A simple web crawler in Golang. *(Stable) in WARC*\n\n# Docker\n\n* webrecorder based [Browsertrix Crawler](https://github.com/webrecorder/browsertrix-crawler) \\- A Chrome based high-fidelity crawling system, designed to run a complex, customizable browser-based crawl in a single Docker container.\n\n# UI\n\n1. [HTTrack](http://www.httrack.com/) \\- An open source website copying utility. *(Stable) Have to compile yourself only GitHub updated not Website downloads.*\n2. [ArchiveBox](https://github.com/pirate/ArchiveBox) \\- A tool which maintains an additive archive from RSS feeds, bookmarks, and links using wget, Chrome headless, and other methods (formerly Bookmark Archiver). *(In Development)*\n3. [ArchiveWeb.Page](https://archiveweb.page/) \\- A plugin for Chrome and other Chromium based browsers that lets you interactively archive web pages, replay them, and export them as WARC data. Also available as an Electron based desktop application.\n4. [Auto Archiver](https://github.com/bellingcat/auto-archiver) \\- Python script to automatically archive social media posts, videos, and images from a Google Sheets document. Read the [article about Auto Archiver on bellingcat.com](https://www.bellingcat.com/resources/2022/09/22/preserve-vital-online-content-with-bellingcats-auto-archiver-tool/).\n5. [Webrecorder](https://webrecorder.io/) \\- Create high-fidelity, interactive recordings of any web site you browse. *(Stable)*\n6. [Brozzler](https://github.com/internetarchive/brozzler) \\- A distributed web crawler that uses a real browser (Chrome or Chromium) to fetch pages and embedded urls and to extract links. *(Stable)*\n7. [https://github.com/linkwarden/linkwarden](https://github.com/linkwarden/linkwarden)\n8. [https://github.com/wallabag/wallabag](https://github.com/wallabag/wallabag)\n\n# HTML Format\n\n1. [SingleFile](https://github.com/gildas-lormeau/SingleFile) \\- Browser extension for Firefox/Chrome and CLI tool to save a faithful copy of a complete page as a single HTML file. *(Stable)*\n2. [monolith](https://github.com/Y2Z/monolith) \\- CLI tool to save a web page as a single HTML file. *(Stable)*\n3. [Obelisk](https://github.com/go-shiori/obelisk) \\- Go package and CLI tool for saving web page as single HTML file. *(Stable)*\n\n# Special Purpose\n\n1. [Social Feed Manager](https://gwu-libraries.github.io/sfm-ui/) \\- Open source software that enables users to create social media collections from Twitter, Tumblr, Flickr, and Sina Weibo public APIs. *(Stable)*\n2. [F(b)arc](https://github.com/justinlittman/fbarc) \\- A commandline tool and Python library for archiving data from [Facebook](https://www.facebook.com/) using the [Graph API](https://developers.facebook.com/docs/graph-api). *(Stable)*\n\n# Read-it-later\n\n1. [https://github.com/omnivore-app/omnivore](https://github.com/omnivore-app/omnivore)\n\n# Random ones from [github.com/iipc/awesome-web-archiving](https://github.com/iipc/awesome-web-archiving)\n\n1. [crocoite](https://github.com/promyloph/crocoite) \\- Crawl websites using headless Google Chrome/Chromium and save resources, static DOM snapshot and page screenshots to WARC files. *(In Development)*\n2. [html2warc](https://github.com/steffenfritz/html2warc) \\- A simple script to convert offline data into a single WARC file. *(Stable)*\n3. [Scoop](https://github.com/harvard-lil/scoop) \\- High-fidelity, browser-based, single-page web archiving library and CLI for witnessing the web. *(Stable)*\n4. [SiteStory](http://mementoweb.github.com/SiteStory/) \\- A transactional archive that selectively captures and stores transactions that take place between a web client (browser) and a web server. *(Stable)*\n5. [Squidwarc](https://github.com/N0taN3rd/Squidwarc) \\- An [open source, high-fidelity, page interacting](http://ws-dl.blogspot.com/2017/07/2017-07-24-replacing-heritrix-with.html) archival crawler that uses Chrome or Chrome Headless directly. *(In Development)*\n6. [twarc](https://github.com/docnow/twarc) \\- A command line tool and Python library for archiving Twitter JSON data. *(Stable)*\n7. [WAIL](https://github.com/machawk1/wail) \\- A graphical user interface (GUI) atop multiple web archiving tools intended to be used as an easy way for anyone to preserve and replay web pages; [Python](https://machawk1.github.io/wail/), [Electron](https://github.com/n0tan3rd/wail). *(Stable)*\n8. [Warcprox](https://github.com/internetarchive/warcprox) \\- WARC-writing MITM HTTP/S proxy. *(Stable)*\n9. [WARCreate](http://matkelly.com/warcreate/) \\- A [Google Chrome](https://www.google.com/intl/en/chrome/browser/) extension for archiving an individual webpage or website to a WARC file. *(Stable)*\n10. [Warcworker](https://github.com/peterk/warcworker) \\- An open source, dockerized, queued, high fidelity web archiver based on Squidwarc with a simple web GUI. *(Stable)*\n11. [Wayback](https://github.com/wabarc/wayback) \\- A toolkit for snapshot webpage to Internet Archive, archive.today, IPFS and beyond. *(Stable)*\n12. [Waybackpy](https://github.com/akamhy/waybackpy) \\- Wayback Machine Save, CDX and availability API interface in Python and a command-line tool *(Stable)*\n13. [Web2Warc](https://github.com/helgeho/Web2Warc) \\- An easy-to-use and highly customizable crawler that enables anyone to create their own little Web archives (WARC/CDX). *(Stable)*\n14. [Web Curator Tool](https://webcuratortool.org/) \\- Open-source workflow management for selective web archiving. *(Stable)*\n15. [WebMemex](https://github.com/WebMemex) \\- Browser extension for Firefox and Chrome which lets you archive web pages you visit. *(In Development)*\n\n# Not Updated\n\n1. [grab-site](https://github.com/ArchiveTeam/grab-site) \\- The archivist's web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns. *(Stable)*\n2. [freeze-dry](https://github.com/WebMemex/freeze-dry) \\- JavaScript library to turn page into static, self-contained HTML document; useful for browser extensions. *(In Development)*\n3. [quadsucker.com](https://quadsucker.com)\n4. [Chronicler](https://github.com/CGamesPlay/chronicler) \\- Web browser with record and replay functionality. *(In Development)*\n5. [crau](https://github.com/turicas/crau) \\- crau is the way (most) Brazilians pronounce crawl, it's the easiest command-line tool for archiving the Web and playing archives: you just need a list of URLs. *(Stable)*\n6. [Wpull](https://github.com/chfoo/wpull) \\- A Wget-compatible (or remake/clone/replacement/alternative) web downloader and crawler. *(Stable)*", "author_fullname": "t2_4jurunac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to Archive Website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5ng90", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709530995.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709488984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Lists&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/wiki/software/#wiki_website_archiving_tools\"&gt;https://www.reddit.com/r/DataHoarder/wiki/software/#wiki_website_archiving_tools&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/iipc/awesome-web-archiving\"&gt;https://github.com/iipc/awesome-web-archiving&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Web-Archiving-Community\"&gt;https://github.com/ArchiveBox/ArchiveBox/wiki/Web-Archiving-Community&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php?title=Software\"&gt;https://wiki.archiveteam.org/index.php?title=Software&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Command&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"http://www.gnu.org/software/wget/\"&gt;Wget&lt;/a&gt; - An open source file retrieval utility that of &lt;a href=\"http://www.archiveteam.org/index.php?title=Wget_with_WARC_output\"&gt;version 1.14 supports writing warcs&lt;/a&gt;. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/alard/wget-lua\"&gt;Wget-lua&lt;/a&gt; - Wget with Lua extension. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/internetarchive/heritrix3/wiki\"&gt;Heritrix&lt;/a&gt; - An open source, extensible, web-scale, archival quality web crawler. &lt;em&gt;(Stable)&lt;/em&gt; &lt;a href=\"https://github.com/internetarchive/heritrix3/discussions/categories/q-a\"&gt;Heritrix Q&amp;amp;A&lt;/a&gt; - A discussion forum for asking questions and getting answers about using Heritrix. &lt;a href=\"https://github.com/web-archive-group/heritrix-walkthrough\"&gt;Heritrix Walkthrough&lt;/a&gt; &lt;em&gt;(In Development)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/oduwsdl/archivenow\"&gt;archivenow&lt;/a&gt; - A &lt;a href=\"http://ws-dl.blogspot.com/2017/02/2017-02-22-archive-now-archivenow.html\"&gt;Python library&lt;/a&gt; to push web resources into on-demand web archives. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"http://stormcrawler.net/\"&gt;StormCrawler&lt;/a&gt; (&lt;a href=\"https://github.com/DigitalPebble/storm-crawler\"&gt;https://github.com/DigitalPebble/storm-crawler&lt;/a&gt;) - A collection of resources for building low-latency, scalable web crawlers on Apache Storm. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/dosyago/DiskerNet\"&gt;DiskerNet&lt;/a&gt; (&lt;a href=\"https://github.com/dosyago/DownloadNet\"&gt;https://github.com/dosyago/DownloadNet&lt;/a&gt;) - A non-WARC-based tool which hooks into the Chrome browser and archives everything you browse making it available for offline replay. &lt;em&gt;(In Development)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wabarc/cairn\"&gt;Cairn&lt;/a&gt; - A npm package and CLI tool for saving webpages. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://git.autistici.org/ale/crawl\"&gt;Crawl&lt;/a&gt; - A simple web crawler in Golang. &lt;em&gt;(Stable) in WARC&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Docker&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;webrecorder based &lt;a href=\"https://github.com/webrecorder/browsertrix-crawler\"&gt;Browsertrix Crawler&lt;/a&gt; - A Chrome based high-fidelity crawling system, designed to run a complex, customizable browser-based crawl in a single Docker container.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;UI&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"http://www.httrack.com/\"&gt;HTTrack&lt;/a&gt; - An open source website copying utility. &lt;em&gt;(Stable) Have to compile yourself only GitHub updated not Website downloads.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/pirate/ArchiveBox\"&gt;ArchiveBox&lt;/a&gt; - A tool which maintains an additive archive from RSS feeds, bookmarks, and links using wget, Chrome headless, and other methods (formerly Bookmark Archiver). &lt;em&gt;(In Development)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://archiveweb.page/\"&gt;ArchiveWeb.Page&lt;/a&gt; - A plugin for Chrome and other Chromium based browsers that lets you interactively archive web pages, replay them, and export them as WARC data. Also available as an Electron based desktop application.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/bellingcat/auto-archiver\"&gt;Auto Archiver&lt;/a&gt; - Python script to automatically archive social media posts, videos, and images from a Google Sheets document. Read the &lt;a href=\"https://www.bellingcat.com/resources/2022/09/22/preserve-vital-online-content-with-bellingcats-auto-archiver-tool/\"&gt;article about Auto Archiver on bellingcat.com&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://webrecorder.io/\"&gt;Webrecorder&lt;/a&gt; - Create high-fidelity, interactive recordings of any web site you browse. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/internetarchive/brozzler\"&gt;Brozzler&lt;/a&gt; - A distributed web crawler that uses a real browser (Chrome or Chromium) to fetch pages and embedded urls and to extract links. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/linkwarden/linkwarden\"&gt;https://github.com/linkwarden/linkwarden&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wallabag/wallabag\"&gt;https://github.com/wallabag/wallabag&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;HTML Format&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/gildas-lormeau/SingleFile\"&gt;SingleFile&lt;/a&gt; - Browser extension for Firefox/Chrome and CLI tool to save a faithful copy of a complete page as a single HTML file. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Y2Z/monolith\"&gt;monolith&lt;/a&gt; - CLI tool to save a web page as a single HTML file. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/go-shiori/obelisk\"&gt;Obelisk&lt;/a&gt; - Go package and CLI tool for saving web page as single HTML file. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Special Purpose&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://gwu-libraries.github.io/sfm-ui/\"&gt;Social Feed Manager&lt;/a&gt; - Open source software that enables users to create social media collections from Twitter, Tumblr, Flickr, and Sina Weibo public APIs. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/justinlittman/fbarc\"&gt;F(b)arc&lt;/a&gt; - A commandline tool and Python library for archiving data from &lt;a href=\"https://www.facebook.com/\"&gt;Facebook&lt;/a&gt; using the &lt;a href=\"https://developers.facebook.com/docs/graph-api\"&gt;Graph API&lt;/a&gt;. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Read-it-later&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/omnivore-app/omnivore\"&gt;https://github.com/omnivore-app/omnivore&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Random ones from &lt;a href=\"https://github.com/iipc/awesome-web-archiving\"&gt;github.com/iipc/awesome-web-archiving&lt;/a&gt;&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/promyloph/crocoite\"&gt;crocoite&lt;/a&gt; - Crawl websites using headless Google Chrome/Chromium and save resources, static DOM snapshot and page screenshots to WARC files. &lt;em&gt;(In Development)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/steffenfritz/html2warc\"&gt;html2warc&lt;/a&gt; - A simple script to convert offline data into a single WARC file. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/harvard-lil/scoop\"&gt;Scoop&lt;/a&gt; - High-fidelity, browser-based, single-page web archiving library and CLI for witnessing the web. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"http://mementoweb.github.com/SiteStory/\"&gt;SiteStory&lt;/a&gt; - A transactional archive that selectively captures and stores transactions that take place between a web client (browser) and a web server. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/N0taN3rd/Squidwarc\"&gt;Squidwarc&lt;/a&gt; - An &lt;a href=\"http://ws-dl.blogspot.com/2017/07/2017-07-24-replacing-heritrix-with.html\"&gt;open source, high-fidelity, page interacting&lt;/a&gt; archival crawler that uses Chrome or Chrome Headless directly. &lt;em&gt;(In Development)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/docnow/twarc\"&gt;twarc&lt;/a&gt; - A command line tool and Python library for archiving Twitter JSON data. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/machawk1/wail\"&gt;WAIL&lt;/a&gt; - A graphical user interface (GUI) atop multiple web archiving tools intended to be used as an easy way for anyone to preserve and replay web pages; &lt;a href=\"https://machawk1.github.io/wail/\"&gt;Python&lt;/a&gt;, &lt;a href=\"https://github.com/n0tan3rd/wail\"&gt;Electron&lt;/a&gt;. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/internetarchive/warcprox\"&gt;Warcprox&lt;/a&gt; - WARC-writing MITM HTTP/S proxy. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"http://matkelly.com/warcreate/\"&gt;WARCreate&lt;/a&gt; - A &lt;a href=\"https://www.google.com/intl/en/chrome/browser/\"&gt;Google Chrome&lt;/a&gt; extension for archiving an individual webpage or website to a WARC file. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/peterk/warcworker\"&gt;Warcworker&lt;/a&gt; - An open source, dockerized, queued, high fidelity web archiver based on Squidwarc with a simple web GUI. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/wabarc/wayback\"&gt;Wayback&lt;/a&gt; - A toolkit for snapshot webpage to Internet Archive, archive.today, IPFS and beyond. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/akamhy/waybackpy\"&gt;Waybackpy&lt;/a&gt; - Wayback Machine Save, CDX and availability API interface in Python and a command-line tool &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/helgeho/Web2Warc\"&gt;Web2Warc&lt;/a&gt; - An easy-to-use and highly customizable crawler that enables anyone to create their own little Web archives (WARC/CDX). &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://webcuratortool.org/\"&gt;Web Curator Tool&lt;/a&gt; - Open-source workflow management for selective web archiving. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/WebMemex\"&gt;WebMemex&lt;/a&gt; - Browser extension for Firefox and Chrome which lets you archive web pages you visit. &lt;em&gt;(In Development)&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Not Updated&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ArchiveTeam/grab-site\"&gt;grab-site&lt;/a&gt; - The archivist&amp;#39;s web crawler: WARC output, dashboard for all crawls, dynamic ignore patterns. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/WebMemex/freeze-dry\"&gt;freeze-dry&lt;/a&gt; - JavaScript library to turn page into static, self-contained HTML document; useful for browser extensions. &lt;em&gt;(In Development)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://quadsucker.com\"&gt;quadsucker.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/CGamesPlay/chronicler\"&gt;Chronicler&lt;/a&gt; - Web browser with record and replay functionality. &lt;em&gt;(In Development)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/turicas/crau\"&gt;crau&lt;/a&gt; - crau is the way (most) Brazilians pronounce crawl, it&amp;#39;s the easiest command-line tool for archiving the Web and playing archives: you just need a list of URLs. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/chfoo/wpull\"&gt;Wpull&lt;/a&gt; - A Wget-compatible (or remake/clone/replacement/alternative) web downloader and crawler. &lt;em&gt;(Stable)&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ex_WlqzVlTSAjwqtklYsEwQXwNC4RUpEoIqxRKO68U4.jpg?auto=webp&amp;s=ee84b5ae34b6c4994473e8e092432d4e065aa17a", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ex_WlqzVlTSAjwqtklYsEwQXwNC4RUpEoIqxRKO68U4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=649211963fdc47cf6d2f32f29b8e0a5966600c9e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ex_WlqzVlTSAjwqtklYsEwQXwNC4RUpEoIqxRKO68U4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc304320c3a72db0a5ebca34c5497ff8853dcda6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ex_WlqzVlTSAjwqtklYsEwQXwNC4RUpEoIqxRKO68U4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4781ce51e54638975e216da073afcf8e724d4af2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ex_WlqzVlTSAjwqtklYsEwQXwNC4RUpEoIqxRKO68U4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=94de2dd62a048bdaa5105ed002935b470e767940", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ex_WlqzVlTSAjwqtklYsEwQXwNC4RUpEoIqxRKO68U4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=47a59eda9b308d445d18bb7ddbf8a53fd8b85cd9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ex_WlqzVlTSAjwqtklYsEwQXwNC4RUpEoIqxRKO68U4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b1549db19e200d967188dfc2e55497a43e78eb55", "width": 1080, "height": 540}], "variants": {}, "id": "NNfgBHa7qmSoSmTgLFkbWQwgzwKdj9ayFDbzB9yMWVQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b5ng90", "is_robot_indexable": true, "report_reasons": null, "author": "RedditNoobie777", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5ng90/best_way_to_archive_website/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5ng90/best_way_to_archive_website/", "subreddit_subscribers": 736025, "created_utc": 1709488984.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The Title made people think this is only for Anime, so I reposted with correct title\n\nIT IS FULL DUMP OF SUBSCENE\n\n[Subscene.com full Dump : r/DataHoarder (reddit.com)](https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/subscenecom_full_dump/)", "author_fullname": "t2_vf2ui7k8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Subscene V2 - Full backup of every anime subtitle Torrent &amp; NZB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5ikif", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709499810.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709476469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Title made people think this is only for Anime, so I reposted with correct title&lt;/p&gt;\n\n&lt;p&gt;IT IS FULL DUMP OF SUBSCENE&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1b5rxc2/subscenecom_full_dump/\"&gt;Subscene.com full Dump : r/DataHoarder (reddit.com)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5ikif", "is_robot_indexable": true, "report_reasons": null, "author": "subscene_V2", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5ikif/subscene_v2_full_backup_of_every_anime_subtitle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5ikif/subscene_v2_full_backup_of_every_anime_subtitle/", "subreddit_subscribers": 736025, "created_utc": 1709476469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\nAfter having refined my processes for checking for fake disks, I wanted to share my process.\n\n**BEFORE BEGINNING** most of these tests can be destructive, particularly for capacity tests. This means you *may* lose any data already on the flash drive. Make a backup of any files you wish to keep or an image of the drive prior to starting any tests. This guide is best-suited for new drives recently-purchased to ensure they meet expectations.\n\nAlso, I hope this goes without saying, but don't use the disk or otherwise cause any reads/writes with another program during testing...\n\n# Ensuring Your Rig is Isn't Bottlenecked\n\nPrior to running tests, you need to make sure you don't have any bottlenecks preventing speed transfers (e.g., USB 2.0 somewhere preventing optimal transfers).\n\nTo do this, click the Apple icon at the top-left of the screen and choose **About this Mac:**\n\n[Select \\\\\"About This Mac\\\\\" from the Apple main menu](https://preview.redd.it/z9na0hb0l6mc1.png?width=564&amp;format=png&amp;auto=webp&amp;s=fabf1e7f2d23454b026f95e7be1af16467187bcc)\n\nIn the **About** modal that appears, choose the **More Info...** button.\n\n&amp;#x200B;\n\n[Click the \\\\\"More Info...\\\\\" button.](https://preview.redd.it/bds1fgfcl6mc1.png?width=560&amp;format=png&amp;auto=webp&amp;s=0e9d18885a466f108e5551450df2a8a53ac5b5e3)\n\nAt the bottom of the Preferences pane that appears, click **System Report**:\n\n[Click the \\\\\"System Report...\\\\\" button at the bottom of this page.](https://preview.redd.it/m1fiehogl6mc1.png?width=658&amp;format=png&amp;auto=webp&amp;s=c3f26d9c7451778693a7baddde5e9a0c84fd8709)\n\nIn the System Report pane, choose **USB** from the left and navigate to your selected drive:\n\n[Ensure your \\\\\"Speed\\\\\" doesn't bottleneck your disk through any of the USB hubs. Notice the SanDisk disk is USB 3.2, but the parent USB 3.1 Bus may further-limit speeds.](https://preview.redd.it/roleuofxr6mc1.png?width=1396&amp;format=png&amp;auto=webp&amp;s=474db6bef0dd7ab27fede361d7740059d6a0579a)\n\nWhat you want to look for here are any devices in the USB chain (including all parent hubs, connection points, etc - all the way up) that limit the USB speeds. You can see this as **Up to 5 Gb/s** in my example above. If you see something that is slower than your drive is capable of, you may have a part of your chain limited to USB 2.0 or similar and need to find a better adapter.\n\n*Don't skip this step!* You may think you have a USB 3.2 5Gb/s drive when it is actually limited to 400MB/s or less. You need to check every link in the chain before running a test.\n\n# Read and Write Speed Stress Testing\n\n&gt;**Note** \\- if you are also testing capacity, the section, below will also provide read- and write-speeds; however, capacity tests take significantly longer to complete. This section is useful if you are *only* looking to stress test a drive relatively-quickly. If you plan on testing capacity, as well as transfer speeds, you can likely skip this section.\n\nNearly all disks have caches wherein simply writing a 1GB file to and fro will give unrealistic speeds. Additionally, most manufacturers only list their read speeds, as they are often an order of magnitude faster than write transfers. That's where a realism-test is important.\n\nAt time of writing, the best, free utility for this is the [BlackMagic Disk Utility](https://apps.apple.com/us/app/blackmagic-disk-speed-test/id425264550?mt=12), which is intended for stress-testing media for high-throughput video. This is freeware, and BlackMagic is a well-respected company with great software. As BlackMagic specifically caters to high-end video processing customers, this utility is specifically designed to stress-test drives in realistic environments that will bypass many cache scenarios.\n\nI recommend running their default **5GB** test - that is, you don't need to do anything other than select your correct drive and run the test. Simply choose your disk (Click the **Gear** icon **Select Target Drive...**) under the `/Volumes` directory and press **Start**. Occasionally, for new drives, I've found that the first test, for whatever reason, is often a bit slower than subsequent tests that more-accurately represent the read- and write-speeds of the disk. I generally let it repeat the test 2-3 times to get a realistic result. The test will repeat over and over until you click the **Start** button, again:\n\n[A very-expected result for a USB 3.2 basic SanDisk \\\\\"Up to 400MB\\/s\\\\\" flash key.](https://preview.redd.it/qc966z0zn6mc1.png?width=1392&amp;format=png&amp;auto=webp&amp;s=3b970b8d07f518aa9839c179e66cedac1e835159)\n\nThe lower portion of the results really only apply to video producers. The read and write speed are of note for this disk testing.\n\n# Capacity Testing\n\nWhile speeds can be faked, capacity is almost always the area of concern for most phony drives. Fake drives will report they have larger capacity than they actually do and perform one of two actions when you try to write more than what they physically can contain:\n\n1. They will simply \"drop\" the data you write to it past a certain point, and simply not tell you\n2. They will \"loop around\" and start overwriting data at the front of the disk, also without telling you\n\nEither way, the result is corrupted or missing data on the disk. We will use a tool called `f3` to check for both of these. You will need to be comfortable using the macOS terminal for the remainder of this guide.\n\n&amp;#x200B;\n\n&gt;**Note** \\- if you will be testing a large number of drives, or are otherwise Linux-savvy, comfortable with configuring and using Docker, and seeking a more efficient, advanced approach, I recommend reading up on how to use [`f3probe`](https://fight-flash-fraud.readthedocs.io/en/latest/introduction.html) in a Docker container on the targeted device.  Unfortunately, this tool is not available as part of the macOS f3 package.\n&gt;  \n&gt;Take note, however, that the Docker Machine used by macOS is part of a virtual machine that cannot have host devices attached to it, so it takes some [advanced configuration](https://fight-flash-fraud.readthedocs.io/en/latest/introduction.html#drive-permissions-passthrough) via VirtualBox (or another VM provider) to make this work. While this is a much more efficient test, it also is best-suited for more-advanced users and will not be covered in this guide.\n\n&amp;#x200B;\n\nFor the sake of completeness, I highly recommend completely [erasing and reformatting](https://support.apple.com/guide/disk-utility/erase-and-reformat-a-storage-device-dskutl14079/mac) your disk prior to starting an **f3** test. Use whatever filesystem type you expect to use on the disk in real life. A simple erase is more than sufficient.\n\nThe way **f3** works is to fill the drive with data and then read that data back and test it for various failure modes for fake disks. This can take some time, depending on the write/read speeds of your device and the size of the drive, but is very thorough.\n\nThe easiest way to install **f3** is via Homebrew, but there are [other methods](https://fight-flash-fraud.readthedocs.io/en/latest/introduction.html#compile-stable-software-on-apple-mac) if you do not have Homebrew installed:\n\n    ~ $ brew install f3\n\nThis will install the **f3write** and **f3read** tools we will use to write and read data to and from the disk.\n\nTo start the test, navigate to the mounted Volume and run **f3write**, which will fill the drive with 1GB files of a particular, checkable format:\n\n    ~ $ cd /Volumes/TestDisk\n    \n    /Volumes/TestDisk $ f3write ./\n    \n    F3 write 8.0\n    Copyright (C) 2010 Digirati Internet LTDA.\n    This is free software; see the source for copying conditions.\n    \n    Free space: 116.55 GB\n    Creating file 1.h2w ... OK!\n    Creating file 2.h2w ... OK!\n    Creating file 3.h2w ... OK!\n    Creating file 4.h2w ... OK!\n    Creating file 5.h2w ... OK!\n    \n    ...\n    \n    Creating file 115.h2w ... OK!\n    Creating file 116.h2w ... OK!\n    Creating file 117.h2w ... OK!\n    Free space: 1.62 MB\n    Average writing speed: 82.57 MB/s\n    \n    /Volumes/TestDisk $\n\nFor the 128GB USB 3.2 disk I showed above, because of \\~90MB/s write speeds, this took about 20-25 minutes. Also note that I realized \\~90MB/s in this utility because the files were smaller, while 5GB \"stress\" files above limited speeds to \\~65MB/s. This is where both utilities can be useful, depending on the types of writes you will be making to your disk\n\nUltimately, this will write a number of files **\\*.h2w** files to the disk that we will then verify with the **f3read** utility, which should run much more quickly for most disks:\n\n    /Volumes/TestDisk $ f3read ./\n    F3 read 8.0\n    Copyright (C) 2010 Digirati Internet LTDA.\n    This is free software; see the source for copying conditions.\n    \n                      SECTORS      ok/corrupted/changed/overwritten\n    Validating file 1.h2w ... 2097152/        0/      0/      0\n    Validating file 2.h2w ... 2097152/        0/      0/      0\n    Validating file 3.h2w ... 2097152/        0/      0/      0\n    Validating file 4.h2w ... 2097152/        0/      0/      0\n    Validating file 5.h2w ... 2097152/        0/      0/      0\n    \n    ...\n    \n    Validating file 115.h2w ... 2097152/        0/      0/      0\n    Validating file 116.h2w ... 2097152/        0/      0/      0\n    Validating file 117.h2w ... 1112633/        0/      0/      0\n    \n      Data OK: 116.53 GB (244382265 sectors)\n    Data LOST: 0.00 Byte (0 sectors)\n    \t       Corrupted: 0.00 Byte (0 sectors)\n    \tSlightly changed: 0.00 Byte (0 sectors)\n    \t     Overwritten: 0.00 Byte (0 sectors)\n    Average reading speed: 290.67 MB/s\n    \n    /Volumes/TestDisk $\n\nIf your disk is healthy and accurate, you should receive no overwrite or missing errors. If **f3read** shows any problems, you likely have a fake disk.\n\nUnfortunately, I do not have any phony drives to show what a fake result would look like; but, if you happen to run across one, please add what the **f3read** output looks like in the comments!\n\nBest of luck, and may the odds be ever in your favor.", "author_fullname": "t2_43498", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guide for Identifying Fake Flash Disks (macOS)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"z9na0hb0l6mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 94, "x": 108, "u": "https://preview.redd.it/z9na0hb0l6mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae0a1e59adbec233383c47327bdc17e2ed5039f1"}, {"y": 189, "x": 216, "u": "https://preview.redd.it/z9na0hb0l6mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d6731b59db0abac9055f11fcbb759b37bd079b0"}, {"y": 280, "x": 320, "u": "https://preview.redd.it/z9na0hb0l6mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7128aa5132a51f4cc9f969957eabb13b9eb1ce10"}], "s": {"y": 494, "x": 564, "u": "https://preview.redd.it/z9na0hb0l6mc1.png?width=564&amp;format=png&amp;auto=webp&amp;s=fabf1e7f2d23454b026f95e7be1af16467187bcc"}, "id": "z9na0hb0l6mc1"}, "m1fiehogl6mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 36, "x": 108, "u": "https://preview.redd.it/m1fiehogl6mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d7d34cd6072e10989ba3d0b126ad48dfab0c6f0"}, {"y": 72, "x": 216, "u": "https://preview.redd.it/m1fiehogl6mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25b47a974bc41a4d554b7e16cca83d4b809cef5f"}, {"y": 107, "x": 320, "u": "https://preview.redd.it/m1fiehogl6mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=803b2223f96dd7fe0d28078942eb8b57fd6f2847"}, {"y": 215, "x": 640, "u": "https://preview.redd.it/m1fiehogl6mc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=02109d91582fac999f01b615beb87eb1afafe08d"}], "s": {"y": 222, "x": 658, "u": "https://preview.redd.it/m1fiehogl6mc1.png?width=658&amp;format=png&amp;auto=webp&amp;s=c3f26d9c7451778693a7baddde5e9a0c84fd8709"}, "id": "m1fiehogl6mc1"}, "bds1fgfcl6mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/bds1fgfcl6mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b98892396ce03ef4059a7b59375c1b6702b695f2"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/bds1fgfcl6mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=edb37737490a2844c4889f50a194e75cda1775ce"}, {"y": 570, "x": 320, "u": "https://preview.redd.it/bds1fgfcl6mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38483f41dcc041fdfe72d7cdc44b5a0b27da904d"}], "s": {"y": 998, "x": 560, "u": "https://preview.redd.it/bds1fgfcl6mc1.png?width=560&amp;format=png&amp;auto=webp&amp;s=0e9d18885a466f108e5551450df2a8a53ac5b5e3"}, "id": "bds1fgfcl6mc1"}, "roleuofxr6mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 100, "x": 108, "u": "https://preview.redd.it/roleuofxr6mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36adf51e70159ca006e2be20e4de17f585a10b85"}, {"y": 201, "x": 216, "u": "https://preview.redd.it/roleuofxr6mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a07720da49f8d08c8442b1795cf1fbef83432bc"}, {"y": 298, "x": 320, "u": "https://preview.redd.it/roleuofxr6mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a3fe8d4b525d07786419cecd5a43c17a3083b476"}, {"y": 597, "x": 640, "u": "https://preview.redd.it/roleuofxr6mc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc921de21711072593ad8a42ccc05e2756e881ef"}, {"y": 896, "x": 960, "u": "https://preview.redd.it/roleuofxr6mc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=91fecc299752ba5ec5a8547280339a7b3aa46b59"}, {"y": 1008, "x": 1080, "u": "https://preview.redd.it/roleuofxr6mc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71dfbd275cc5d946fe3b148990143a427e3d5d3e"}], "s": {"y": 1304, "x": 1396, "u": "https://preview.redd.it/roleuofxr6mc1.png?width=1396&amp;format=png&amp;auto=webp&amp;s=474db6bef0dd7ab27fede361d7740059d6a0579a"}, "id": "roleuofxr6mc1"}, "qc966z0zn6mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 111, "x": 108, "u": "https://preview.redd.it/qc966z0zn6mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=873ab675e0ac7ce0042fa903dded2c098ef54a46"}, {"y": 223, "x": 216, "u": "https://preview.redd.it/qc966z0zn6mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=241c51195b9aafa22df682356a4a169981c83713"}, {"y": 330, "x": 320, "u": "https://preview.redd.it/qc966z0zn6mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=303e33de9e1119d7a57c0a3b80e13c47cb72b6da"}, {"y": 661, "x": 640, "u": "https://preview.redd.it/qc966z0zn6mc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=03ffb28e42c761683d05474fc3f3c2c7e6391387"}, {"y": 991, "x": 960, "u": "https://preview.redd.it/qc966z0zn6mc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f09822da8db7e96308e2ab7ec9c3b86dba9e013b"}, {"y": 1115, "x": 1080, "u": "https://preview.redd.it/qc966z0zn6mc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69651ac5c010b8aa0b14b1c30d237aaccff0e863"}], "s": {"y": 1438, "x": 1392, "u": "https://preview.redd.it/qc966z0zn6mc1.png?width=1392&amp;format=png&amp;auto=webp&amp;s=3b970b8d07f518aa9839c179e66cedac1e835159"}, "id": "qc966z0zn6mc1"}}, "name": "t3_1b5t837", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wkYBw_eVdFrFKUohAW3AhZfmWQPRj49mG4-tMvQ7Bbk.jpg", "edited": 1709509329.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1709502813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;After having refined my processes for checking for fake disks, I wanted to share my process.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BEFORE BEGINNING&lt;/strong&gt; most of these tests can be destructive, particularly for capacity tests. This means you &lt;em&gt;may&lt;/em&gt; lose any data already on the flash drive. Make a backup of any files you wish to keep or an image of the drive prior to starting any tests. This guide is best-suited for new drives recently-purchased to ensure they meet expectations.&lt;/p&gt;\n\n&lt;p&gt;Also, I hope this goes without saying, but don&amp;#39;t use the disk or otherwise cause any reads/writes with another program during testing...&lt;/p&gt;\n\n&lt;h1&gt;Ensuring Your Rig is Isn&amp;#39;t Bottlenecked&lt;/h1&gt;\n\n&lt;p&gt;Prior to running tests, you need to make sure you don&amp;#39;t have any bottlenecks preventing speed transfers (e.g., USB 2.0 somewhere preventing optimal transfers).&lt;/p&gt;\n\n&lt;p&gt;To do this, click the Apple icon at the top-left of the screen and choose &lt;strong&gt;About this Mac:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z9na0hb0l6mc1.png?width=564&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fabf1e7f2d23454b026f95e7be1af16467187bcc\"&gt;Select \\&amp;quot;About This Mac\\&amp;quot; from the Apple main menu&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the &lt;strong&gt;About&lt;/strong&gt; modal that appears, choose the &lt;strong&gt;More Info...&lt;/strong&gt; button.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bds1fgfcl6mc1.png?width=560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e9d18885a466f108e5551450df2a8a53ac5b5e3\"&gt;Click the \\&amp;quot;More Info...\\&amp;quot; button.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;At the bottom of the Preferences pane that appears, click &lt;strong&gt;System Report&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/m1fiehogl6mc1.png?width=658&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3f26d9c7451778693a7baddde5e9a0c84fd8709\"&gt;Click the \\&amp;quot;System Report...\\&amp;quot; button at the bottom of this page.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the System Report pane, choose &lt;strong&gt;USB&lt;/strong&gt; from the left and navigate to your selected drive:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/roleuofxr6mc1.png?width=1396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=474db6bef0dd7ab27fede361d7740059d6a0579a\"&gt;Ensure your \\&amp;quot;Speed\\&amp;quot; doesn&amp;#39;t bottleneck your disk through any of the USB hubs. Notice the SanDisk disk is USB 3.2, but the parent USB 3.1 Bus may further-limit speeds.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What you want to look for here are any devices in the USB chain (including all parent hubs, connection points, etc - all the way up) that limit the USB speeds. You can see this as &lt;strong&gt;Up to 5 Gb/s&lt;/strong&gt; in my example above. If you see something that is slower than your drive is capable of, you may have a part of your chain limited to USB 2.0 or similar and need to find a better adapter.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Don&amp;#39;t skip this step!&lt;/em&gt; You may think you have a USB 3.2 5Gb/s drive when it is actually limited to 400MB/s or less. You need to check every link in the chain before running a test.&lt;/p&gt;\n\n&lt;h1&gt;Read and Write Speed Stress Testing&lt;/h1&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; - if you are also testing capacity, the section, below will also provide read- and write-speeds; however, capacity tests take significantly longer to complete. This section is useful if you are &lt;em&gt;only&lt;/em&gt; looking to stress test a drive relatively-quickly. If you plan on testing capacity, as well as transfer speeds, you can likely skip this section.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Nearly all disks have caches wherein simply writing a 1GB file to and fro will give unrealistic speeds. Additionally, most manufacturers only list their read speeds, as they are often an order of magnitude faster than write transfers. That&amp;#39;s where a realism-test is important.&lt;/p&gt;\n\n&lt;p&gt;At time of writing, the best, free utility for this is the &lt;a href=\"https://apps.apple.com/us/app/blackmagic-disk-speed-test/id425264550?mt=12\"&gt;BlackMagic Disk Utility&lt;/a&gt;, which is intended for stress-testing media for high-throughput video. This is freeware, and BlackMagic is a well-respected company with great software. As BlackMagic specifically caters to high-end video processing customers, this utility is specifically designed to stress-test drives in realistic environments that will bypass many cache scenarios.&lt;/p&gt;\n\n&lt;p&gt;I recommend running their default &lt;strong&gt;5GB&lt;/strong&gt; test - that is, you don&amp;#39;t need to do anything other than select your correct drive and run the test. Simply choose your disk (Click the &lt;strong&gt;Gear&lt;/strong&gt; icon &lt;strong&gt;Select Target Drive...&lt;/strong&gt;) under the &lt;code&gt;/Volumes&lt;/code&gt; directory and press &lt;strong&gt;Start&lt;/strong&gt;. Occasionally, for new drives, I&amp;#39;ve found that the first test, for whatever reason, is often a bit slower than subsequent tests that more-accurately represent the read- and write-speeds of the disk. I generally let it repeat the test 2-3 times to get a realistic result. The test will repeat over and over until you click the &lt;strong&gt;Start&lt;/strong&gt; button, again:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qc966z0zn6mc1.png?width=1392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b970b8d07f518aa9839c179e66cedac1e835159\"&gt;A very-expected result for a USB 3.2 basic SanDisk \\&amp;quot;Up to 400MB/s\\&amp;quot; flash key.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The lower portion of the results really only apply to video producers. The read and write speed are of note for this disk testing.&lt;/p&gt;\n\n&lt;h1&gt;Capacity Testing&lt;/h1&gt;\n\n&lt;p&gt;While speeds can be faked, capacity is almost always the area of concern for most phony drives. Fake drives will report they have larger capacity than they actually do and perform one of two actions when you try to write more than what they physically can contain:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;They will simply &amp;quot;drop&amp;quot; the data you write to it past a certain point, and simply not tell you&lt;/li&gt;\n&lt;li&gt;They will &amp;quot;loop around&amp;quot; and start overwriting data at the front of the disk, also without telling you&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Either way, the result is corrupted or missing data on the disk. We will use a tool called &lt;code&gt;f3&lt;/code&gt; to check for both of these. You will need to be comfortable using the macOS terminal for the remainder of this guide.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; - if you will be testing a large number of drives, or are otherwise Linux-savvy, comfortable with configuring and using Docker, and seeking a more efficient, advanced approach, I recommend reading up on how to use &lt;a href=\"https://fight-flash-fraud.readthedocs.io/en/latest/introduction.html\"&gt;&lt;code&gt;f3probe&lt;/code&gt;&lt;/a&gt; in a Docker container on the targeted device.  Unfortunately, this tool is not available as part of the macOS f3 package.&lt;/p&gt;\n\n&lt;p&gt;Take note, however, that the Docker Machine used by macOS is part of a virtual machine that cannot have host devices attached to it, so it takes some &lt;a href=\"https://fight-flash-fraud.readthedocs.io/en/latest/introduction.html#drive-permissions-passthrough\"&gt;advanced configuration&lt;/a&gt; via VirtualBox (or another VM provider) to make this work. While this is a much more efficient test, it also is best-suited for more-advanced users and will not be covered in this guide.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For the sake of completeness, I highly recommend completely &lt;a href=\"https://support.apple.com/guide/disk-utility/erase-and-reformat-a-storage-device-dskutl14079/mac\"&gt;erasing and reformatting&lt;/a&gt; your disk prior to starting an &lt;strong&gt;f3&lt;/strong&gt; test. Use whatever filesystem type you expect to use on the disk in real life. A simple erase is more than sufficient.&lt;/p&gt;\n\n&lt;p&gt;The way &lt;strong&gt;f3&lt;/strong&gt; works is to fill the drive with data and then read that data back and test it for various failure modes for fake disks. This can take some time, depending on the write/read speeds of your device and the size of the drive, but is very thorough.&lt;/p&gt;\n\n&lt;p&gt;The easiest way to install &lt;strong&gt;f3&lt;/strong&gt; is via Homebrew, but there are &lt;a href=\"https://fight-flash-fraud.readthedocs.io/en/latest/introduction.html#compile-stable-software-on-apple-mac\"&gt;other methods&lt;/a&gt; if you do not have Homebrew installed:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;~ $ brew install f3\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This will install the &lt;strong&gt;f3write&lt;/strong&gt; and &lt;strong&gt;f3read&lt;/strong&gt; tools we will use to write and read data to and from the disk.&lt;/p&gt;\n\n&lt;p&gt;To start the test, navigate to the mounted Volume and run &lt;strong&gt;f3write&lt;/strong&gt;, which will fill the drive with 1GB files of a particular, checkable format:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;~ $ cd /Volumes/TestDisk\n\n/Volumes/TestDisk $ f3write ./\n\nF3 write 8.0\nCopyright (C) 2010 Digirati Internet LTDA.\nThis is free software; see the source for copying conditions.\n\nFree space: 116.55 GB\nCreating file 1.h2w ... OK!\nCreating file 2.h2w ... OK!\nCreating file 3.h2w ... OK!\nCreating file 4.h2w ... OK!\nCreating file 5.h2w ... OK!\n\n...\n\nCreating file 115.h2w ... OK!\nCreating file 116.h2w ... OK!\nCreating file 117.h2w ... OK!\nFree space: 1.62 MB\nAverage writing speed: 82.57 MB/s\n\n/Volumes/TestDisk $\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;For the 128GB USB 3.2 disk I showed above, because of ~90MB/s write speeds, this took about 20-25 minutes. Also note that I realized ~90MB/s in this utility because the files were smaller, while 5GB &amp;quot;stress&amp;quot; files above limited speeds to ~65MB/s. This is where both utilities can be useful, depending on the types of writes you will be making to your disk&lt;/p&gt;\n\n&lt;p&gt;Ultimately, this will write a number of files &lt;strong&gt;*.h2w&lt;/strong&gt; files to the disk that we will then verify with the &lt;strong&gt;f3read&lt;/strong&gt; utility, which should run much more quickly for most disks:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/Volumes/TestDisk $ f3read ./\nF3 read 8.0\nCopyright (C) 2010 Digirati Internet LTDA.\nThis is free software; see the source for copying conditions.\n\n                  SECTORS      ok/corrupted/changed/overwritten\nValidating file 1.h2w ... 2097152/        0/      0/      0\nValidating file 2.h2w ... 2097152/        0/      0/      0\nValidating file 3.h2w ... 2097152/        0/      0/      0\nValidating file 4.h2w ... 2097152/        0/      0/      0\nValidating file 5.h2w ... 2097152/        0/      0/      0\n\n...\n\nValidating file 115.h2w ... 2097152/        0/      0/      0\nValidating file 116.h2w ... 2097152/        0/      0/      0\nValidating file 117.h2w ... 1112633/        0/      0/      0\n\n  Data OK: 116.53 GB (244382265 sectors)\nData LOST: 0.00 Byte (0 sectors)\n           Corrupted: 0.00 Byte (0 sectors)\n    Slightly changed: 0.00 Byte (0 sectors)\n         Overwritten: 0.00 Byte (0 sectors)\nAverage reading speed: 290.67 MB/s\n\n/Volumes/TestDisk $\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;If your disk is healthy and accurate, you should receive no overwrite or missing errors. If &lt;strong&gt;f3read&lt;/strong&gt; shows any problems, you likely have a fake disk.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, I do not have any phony drives to show what a fake result would look like; but, if you happen to run across one, please add what the &lt;strong&gt;f3read&lt;/strong&gt; output looks like in the comments!&lt;/p&gt;\n\n&lt;p&gt;Best of luck, and may the odds be ever in your favor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kxUUUE6yFOZUUeJMRYvL5J3lNsSD4lg4-7kOBzMapgU.jpg?auto=webp&amp;s=08e4a0ea5b11bcb3b8e3f2b57418a07b096e685c", "width": 630, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/kxUUUE6yFOZUUeJMRYvL5J3lNsSD4lg4-7kOBzMapgU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae93335b6ef642e6a3cc45abb75dc1b6b9a2d8ca", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/kxUUUE6yFOZUUeJMRYvL5J3lNsSD4lg4-7kOBzMapgU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92dd1fad255e0a44d74ef7209b1500b172cc1113", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/kxUUUE6yFOZUUeJMRYvL5J3lNsSD4lg4-7kOBzMapgU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=566be2cd4386c8f8197517c5be60f6067eff1e1a", "width": 320, "height": 320}], "variants": {}, "id": "UFT3rIqq282EOrZLQX8F9hWeb8DaXYU6jmDc7DmwgRI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5t837", "is_robot_indexable": true, "report_reasons": null, "author": "wspnut", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5t837/guide_for_identifying_fake_flash_disks_macos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5t837/guide_for_identifying_fake_flash_disks_macos/", "subreddit_subscribers": 736025, "created_utc": 1709502813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My 3TB drive decided to show some faults and bad sectors. I managed to get all the important data off the drive.\n\nI then chose to run badblock using destructive read-write to get a more accurate report on the drive. I ran the following command:\n\n`badblocks -v -s -w /dev/sdb &gt; badblock_results.txt 2&gt;&amp;1 &amp; disown`\n\nThis is the current output of the scan if i use `tail -f badblock_results.txt`\n\nhttps://preview.redd.it/7itge7bhl5mc1.png?width=1173&amp;format=png&amp;auto=webp&amp;s=ec8e7145c46e5fe421f9499f82f767a6ecaf669f\n\nis it normal for badblocks to be running for 24hours+\n\nIs there a way to speed this up if i ever need to do this again in the future with another drive ?\n\n&amp;#x200B;\n\nUPDATE EDIT: Its on the second round of testing with `0x55`\n\n&amp;#x200B;\n\nhttps://preview.redd.it/k9ze73zdkbmc1.png?width=1173&amp;format=png&amp;auto=webp&amp;s=3a3f106289a53042d5a701a0f3b2679f769dfadc\n\nProbably another 24hrs left of read-write.", "author_fullname": "t2_h6ra3gen", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First time running badblock scan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "media_metadata": {"k9ze73zdkbmc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/k9ze73zdkbmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc1394fafd36e045527493207f98a715b5ed26eb"}, {"y": 106, "x": 216, "u": "https://preview.redd.it/k9ze73zdkbmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c6cc00e7354254f549699f68f1ecd0d213d8400"}, {"y": 158, "x": 320, "u": "https://preview.redd.it/k9ze73zdkbmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a356d82331c4cf624ffbbb7b973ccf9acf8130a"}, {"y": 316, "x": 640, "u": "https://preview.redd.it/k9ze73zdkbmc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d34a63b845a1cc7b920139bff0db975660030e5a"}, {"y": 475, "x": 960, "u": "https://preview.redd.it/k9ze73zdkbmc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b20cae0665ac4ce9c4423c45124f4162a8a9a86"}, {"y": 534, "x": 1080, "u": "https://preview.redd.it/k9ze73zdkbmc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=298d0b8a1f0021bb4d9ea2ac23985e6e7d056a50"}], "s": {"y": 581, "x": 1173, "u": "https://preview.redd.it/k9ze73zdkbmc1.png?width=1173&amp;format=png&amp;auto=webp&amp;s=3a3f106289a53042d5a701a0f3b2679f769dfadc"}, "id": "k9ze73zdkbmc1"}, "7itge7bhl5mc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/7itge7bhl5mc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88499a483682d6d6f1c9a6d6e19e8add379350b5"}, {"y": 112, "x": 216, "u": "https://preview.redd.it/7itge7bhl5mc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fce23a93a08b6c886dc0845f9b3ade2b104bb53b"}, {"y": 166, "x": 320, "u": "https://preview.redd.it/7itge7bhl5mc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de5688c8c1872b0ecbc433c57b51feb09352eb76"}, {"y": 332, "x": 640, "u": "https://preview.redd.it/7itge7bhl5mc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27d5064b04283a66406c32b533a0551a410a16d9"}, {"y": 498, "x": 960, "u": "https://preview.redd.it/7itge7bhl5mc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d03b1634aa95a49f34eda7f9f89157961a248a7"}, {"y": 560, "x": 1080, "u": "https://preview.redd.it/7itge7bhl5mc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=181250a635472529bf07a83e1b266f225f15d2e3"}], "s": {"y": 609, "x": 1173, "u": "https://preview.redd.it/7itge7bhl5mc1.png?width=1173&amp;format=png&amp;auto=webp&amp;s=ec8e7145c46e5fe421f9499f82f767a6ecaf669f"}, "id": "7itge7bhl5mc1"}}, "name": "t3_1b5mjni", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Xo0156HpkRql34_0rP0opJcmYdlprG4GWF-ljTGe7SE.jpg", "edited": 1709558957.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709486744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My 3TB drive decided to show some faults and bad sectors. I managed to get all the important data off the drive.&lt;/p&gt;\n\n&lt;p&gt;I then chose to run badblock using destructive read-write to get a more accurate report on the drive. I ran the following command:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;badblocks -v -s -w /dev/sdb &amp;gt; badblock_results.txt 2&amp;gt;&amp;amp;1 &amp;amp; disown&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the current output of the scan if i use &lt;code&gt;tail -f badblock_results.txt&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7itge7bhl5mc1.png?width=1173&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec8e7145c46e5fe421f9499f82f767a6ecaf669f\"&gt;https://preview.redd.it/7itge7bhl5mc1.png?width=1173&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec8e7145c46e5fe421f9499f82f767a6ecaf669f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;is it normal for badblocks to be running for 24hours+&lt;/p&gt;\n\n&lt;p&gt;Is there a way to speed this up if i ever need to do this again in the future with another drive ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;UPDATE EDIT: Its on the second round of testing with &lt;code&gt;0x55&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/k9ze73zdkbmc1.png?width=1173&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a3f106289a53042d5a701a0f3b2679f769dfadc\"&gt;https://preview.redd.it/k9ze73zdkbmc1.png?width=1173&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a3f106289a53042d5a701a0f3b2679f769dfadc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Probably another 24hrs left of read-write.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b5mjni", "is_robot_indexable": true, "report_reasons": null, "author": "Responsible_Plane379", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5mjni/first_time_running_badblock_scan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5mjni/first_time_running_badblock_scan/", "subreddit_subscribers": 736025, "created_utc": 1709486744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello folks, plis what are my options? Can't stand HDDs spinning in my small room.\n\nHad my eyes on those 8TB Samsung SSDs but god are those expensive.\n\n&amp;#x200B;\n\nI was thinking about routing a USB cable to the HDD enclosure and placing it somewhere else maybe? Cloud is an option but not having the data ''physically'' kinda makes me worried probably schizo.\n\nTy\n\n&amp;#x200B;", "author_fullname": "t2_10knf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quiet decently fast 8TB+ storage (Plex)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b66vb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709544979.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709544779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks, plis what are my options? Can&amp;#39;t stand HDDs spinning in my small room.&lt;/p&gt;\n\n&lt;p&gt;Had my eyes on those 8TB Samsung SSDs but god are those expensive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was thinking about routing a USB cable to the HDD enclosure and placing it somewhere else maybe? Cloud is an option but not having the data &amp;#39;&amp;#39;physically&amp;#39;&amp;#39; kinda makes me worried probably schizo.&lt;/p&gt;\n\n&lt;p&gt;Ty&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b66vb2", "is_robot_indexable": true, "report_reasons": null, "author": "Zeriepam", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b66vb2/quiet_decently_fast_8tb_storage_plex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b66vb2/quiet_decently_fast_8tb_storage_plex/", "subreddit_subscribers": 736025, "created_utc": 1709544779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So logged into my unraid and was presented with this.. I'm guessing this Seagate 8TB drive is destined for the bin?  \nAlthough at the same time, a second 8TB Seagate drive popped up with 3x Reallocated Sector Counts.\n\n&amp;#x200B;\n\nIs it just a coincidence the've both had these errors? They're both about 5 years old or maybe more with a ton of hours.  \n\n\nHappy to replace them, just making sure that I don't have an external issue that needs rectification before I throw some new shiny drives in\n\nhttps://preview.redd.it/7o7ba1q459mc1.jpg?width=1945&amp;format=pjpg&amp;auto=webp&amp;s=94b19eefd73bbd58a82b506197421c2db5d60eb5", "author_fullname": "t2_7zq0yx1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UNRAID. All of a sudden have some SMART errors. Replace?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 47, "top_awarded_type": null, "hide_score": false, "media_metadata": {"7o7ba1q459mc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 36, "x": 108, "u": "https://preview.redd.it/7o7ba1q459mc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ebb2a23b0cf273d82bc1c2bb13bd210d7f128bb"}, {"y": 73, "x": 216, "u": "https://preview.redd.it/7o7ba1q459mc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=003867fee7eef4b8a8f95d91b24908bd926157ed"}, {"y": 109, "x": 320, "u": "https://preview.redd.it/7o7ba1q459mc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c2f16c5667a8a1051baf98d232f562f89e4c21df"}, {"y": 219, "x": 640, "u": "https://preview.redd.it/7o7ba1q459mc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4da1e2e7a70329d1b4bebeb6a30bcda484262d9"}, {"y": 328, "x": 960, "u": "https://preview.redd.it/7o7ba1q459mc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e74d3cbb9522a74edbe39a0992d90163de74c35"}, {"y": 369, "x": 1080, "u": "https://preview.redd.it/7o7ba1q459mc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa84d01bb792b2691d42e38c3791127847c483b7"}], "s": {"y": 666, "x": 1945, "u": "https://preview.redd.it/7o7ba1q459mc1.jpg?width=1945&amp;format=pjpg&amp;auto=webp&amp;s=94b19eefd73bbd58a82b506197421c2db5d60eb5"}, "id": "7o7ba1q459mc1"}}, "name": "t3_1b62wn2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/nPAiR6qz254CU7fV-L_eIHl3Tqa_Sm7O4Dp5gEd_qO0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709529792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So logged into my unraid and was presented with this.. I&amp;#39;m guessing this Seagate 8TB drive is destined for the bin?&lt;br/&gt;\nAlthough at the same time, a second 8TB Seagate drive popped up with 3x Reallocated Sector Counts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is it just a coincidence the&amp;#39;ve both had these errors? They&amp;#39;re both about 5 years old or maybe more with a ton of hours.  &lt;/p&gt;\n\n&lt;p&gt;Happy to replace them, just making sure that I don&amp;#39;t have an external issue that needs rectification before I throw some new shiny drives in&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7o7ba1q459mc1.jpg?width=1945&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=94b19eefd73bbd58a82b506197421c2db5d60eb5\"&gt;https://preview.redd.it/7o7ba1q459mc1.jpg?width=1945&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=94b19eefd73bbd58a82b506197421c2db5d60eb5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b62wn2", "is_robot_indexable": true, "report_reasons": null, "author": "Few_Ad_1079", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b62wn2/unraid_all_of_a_sudden_have_some_smart_errors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b62wn2/unraid_all_of_a_sudden_have_some_smart_errors/", "subreddit_subscribers": 736025, "created_utc": 1709529792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So NordLocker basically says: you cannot upload anything that breaks any laws / you can't use our service to aid you in breaking the law. But what law is this exactly? Panama law, or the law wherever the user happens to be, or the law in the user's country of citizenship, or residence, or what? And also, do they actually check what you upload to see if it breaks the law / can they check? Wouldn't that break their privacy policy that says they don't have access to the data uploaded by user beyond anonimized technical data and such? Asking for a friend.\n\nSorry if this is not the place to ask this. ", "author_fullname": "t2_4ph2a7gm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Terms of use NordLocker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5wcnk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709510452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So NordLocker basically says: you cannot upload anything that breaks any laws / you can&amp;#39;t use our service to aid you in breaking the law. But what law is this exactly? Panama law, or the law wherever the user happens to be, or the law in the user&amp;#39;s country of citizenship, or residence, or what? And also, do they actually check what you upload to see if it breaks the law / can they check? Wouldn&amp;#39;t that break their privacy policy that says they don&amp;#39;t have access to the data uploaded by user beyond anonimized technical data and such? Asking for a friend.&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is not the place to ask this. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5wcnk", "is_robot_indexable": true, "report_reasons": null, "author": "RunDiscombobulated67", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5wcnk/terms_of_use_nordlocker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5wcnk/terms_of_use_nordlocker/", "subreddit_subscribers": 736025, "created_utc": 1709510452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to set up my diy nas with 4 18tb drives in raid 6 for future proofing however am struggling with making sure I am setting up drivepool and then snapraid in the right way.\n\nI've looked online and resources are either offline or don't seem quite like I'm looking for", "author_fullname": "t2_zoyy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guide for Drivepool and Snapraid set up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5ks7z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709482287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to set up my diy nas with 4 18tb drives in raid 6 for future proofing however am struggling with making sure I am setting up drivepool and then snapraid in the right way.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked online and resources are either offline or don&amp;#39;t seem quite like I&amp;#39;m looking for&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5ks7z", "is_robot_indexable": true, "report_reasons": null, "author": "awkwardreader", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5ks7z/guide_for_drivepool_and_snapraid_set_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5ks7z/guide_for_drivepool_and_snapraid_set_up/", "subreddit_subscribers": 736025, "created_utc": 1709482287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know nowadays of a good cross-platform disk catalog/search applications they could recommend?\n\nI have a bunch of spare drives with content over them. I mainly use them for long term archives and I regularly use them to get stuff out. They follow a specific logic but I still need to be able to know what content is on them. So I need a UI so I can browse and search content. No need for fancy metadata or thumbnails / previews.\n\nWe're talking about 50TB across 12 drives with mixed media content. I use Mac for work, Linux, Mac and Windows at home so crossplatform is a must (else I'd have Everything on Windows, Neofinder on a Mac for example).\n\nI\u2019ve tried VVV, but it sadly keeps crashing on a regular basis (too much files?). Thanks for any recommendations", "author_fullname": "t2_12etmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Disk indexing Recommendations (FOSS, cross platform)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b69oya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709555169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know nowadays of a good cross-platform disk catalog/search applications they could recommend?&lt;/p&gt;\n\n&lt;p&gt;I have a bunch of spare drives with content over them. I mainly use them for long term archives and I regularly use them to get stuff out. They follow a specific logic but I still need to be able to know what content is on them. So I need a UI so I can browse and search content. No need for fancy metadata or thumbnails / previews.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re talking about 50TB across 12 drives with mixed media content. I use Mac for work, Linux, Mac and Windows at home so crossplatform is a must (else I&amp;#39;d have Everything on Windows, Neofinder on a Mac for example).&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried VVV, but it sadly keeps crashing on a regular basis (too much files?). Thanks for any recommendations&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b69oya", "is_robot_indexable": true, "report_reasons": null, "author": "I-need-a-proper-nick", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b69oya/disk_indexing_recommendations_foss_cross_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b69oya/disk_indexing_recommendations_foss_cross_platform/", "subreddit_subscribers": 736025, "created_utc": 1709555169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I never gave it much thought until today- had a bit of a shower thought. Is the caption and/or girl from an anime or comic series I take it? I looked up \"what do you mean delete\", in quotes, on that ducky search engine and found nothing.", "author_fullname": "t2_zfbqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Meta] What's the sauce of this sub's banner image?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b68s2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709552077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I never gave it much thought until today- had a bit of a shower thought. Is the caption and/or girl from an anime or comic series I take it? I looked up &amp;quot;what do you mean delete&amp;quot;, in quotes, on that ducky search engine and found nothing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "I got 99 movies, but I ain't watched one.", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b68s2k", "is_robot_indexable": true, "report_reasons": null, "author": "tapdancingwhale", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b68s2k/meta_whats_the_sauce_of_this_subs_banner_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b68s2k/meta_whats_the_sauce_of_this_subs_banner_image/", "subreddit_subscribers": 736025, "created_utc": 1709552077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a bunch of Amazon gift cards recently and want to get a new drive. I've seen on here a lot of people have issues on amazon getting bad drives. Wondering I'd yall have any tips on how to avoid. ", "author_fullname": "t2_5rjdm3bk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for buying 12 tb external off amazon?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b67wtc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709548824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a bunch of Amazon gift cards recently and want to get a new drive. I&amp;#39;ve seen on here a lot of people have issues on amazon getting bad drives. Wondering I&amp;#39;d yall have any tips on how to avoid. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b67wtc", "is_robot_indexable": true, "report_reasons": null, "author": "Alicedoll02", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b67wtc/advice_for_buying_12_tb_external_off_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b67wtc/advice_for_buying_12_tb_external_off_amazon/", "subreddit_subscribers": 736025, "created_utc": 1709548824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently my Plex library sits on a measly total of 4 TB, with 1 TB in my PC HDD, another TB in an external HDD and the rest 2 TB in the cloud. I now want to centralize all this data to one place and I've also realized that 4 TB will **not** suffice if I want to expand my library.\n\nFor this, I'm looking to get two 8 TB drives for the time being, one as the main storage and the other for backup, and set them up in a dual bay enclosure. Once I hit the 8 TB limit, I plan to get another dual bay with two more 8 TB drives, and I'll then separate my main storage and backup storage (both of 16 TB chunks) into separate bays. Before I even move to product recommendations.... is my plan of how I want to expand stupid? I'm taking a pretty simple-minded approach to this, so please tell me if there are better and/or more effective/efficient way of doing this.\n\nNow for product recommendations, these are the ones I chose. The enclosure is nice and cheap and it's just under $100, which is my budget for the enclosure. The drives are also quite cheap and just under $200 each. Do let me know if there are better alternatives, as long as the price is near my mentioned budget ($500 in total), or if you have any experience/review on these products:\n\nHDD: [Western Digital WD60PURZ 8TB 3.5\" Purple Hard Disk](https://www.techlandbd.com/western-digital-8tb-hdd)  \nEnclosure: [ORICO NS200C3 3.5 inch 2 Bay Type-C Hard Drive Enclosure](https://www.startech.com.bd/orico-ns200c3-2-bay-type-c-hard-drive-enclosure)\n\nAlso tell me if the enclosure I've chosen is overkill and if I could do away with something cheaper for my use case. Looking forward to your feedback!", "author_fullname": "t2_k77hz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stepping into the world of data hoarding, need advice on which hard drive enclosure to get!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b67en3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709547135.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709546880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently my Plex library sits on a measly total of 4 TB, with 1 TB in my PC HDD, another TB in an external HDD and the rest 2 TB in the cloud. I now want to centralize all this data to one place and I&amp;#39;ve also realized that 4 TB will &lt;strong&gt;not&lt;/strong&gt; suffice if I want to expand my library.&lt;/p&gt;\n\n&lt;p&gt;For this, I&amp;#39;m looking to get two 8 TB drives for the time being, one as the main storage and the other for backup, and set them up in a dual bay enclosure. Once I hit the 8 TB limit, I plan to get another dual bay with two more 8 TB drives, and I&amp;#39;ll then separate my main storage and backup storage (both of 16 TB chunks) into separate bays. Before I even move to product recommendations.... is my plan of how I want to expand stupid? I&amp;#39;m taking a pretty simple-minded approach to this, so please tell me if there are better and/or more effective/efficient way of doing this.&lt;/p&gt;\n\n&lt;p&gt;Now for product recommendations, these are the ones I chose. The enclosure is nice and cheap and it&amp;#39;s just under $100, which is my budget for the enclosure. The drives are also quite cheap and just under $200 each. Do let me know if there are better alternatives, as long as the price is near my mentioned budget ($500 in total), or if you have any experience/review on these products:&lt;/p&gt;\n\n&lt;p&gt;HDD: &lt;a href=\"https://www.techlandbd.com/western-digital-8tb-hdd\"&gt;Western Digital WD60PURZ 8TB 3.5&amp;quot; Purple Hard Disk&lt;/a&gt;&lt;br/&gt;\nEnclosure: &lt;a href=\"https://www.startech.com.bd/orico-ns200c3-2-bay-type-c-hard-drive-enclosure\"&gt;ORICO NS200C3 3.5 inch 2 Bay Type-C Hard Drive Enclosure&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also tell me if the enclosure I&amp;#39;ve chosen is overkill and if I could do away with something cheaper for my use case. Looking forward to your feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mwcllsd2gh8eNzswtCRvmEopQoLVSl800LfCzpY5sqc.jpg?auto=webp&amp;s=1e445ada6d56bff3d4408cee4232e0ffac29cc7d", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/Mwcllsd2gh8eNzswtCRvmEopQoLVSl800LfCzpY5sqc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4acccc7d044128c48bda17456ae5b48f203c28ba", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Mwcllsd2gh8eNzswtCRvmEopQoLVSl800LfCzpY5sqc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ae04f808cd3a2a874dadb8e7cc1a1c334a3ac1e8", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Mwcllsd2gh8eNzswtCRvmEopQoLVSl800LfCzpY5sqc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5bbfd9d66be68c70fb95f70856713660e1aefa98", "width": 320, "height": 168}], "variants": {}, "id": "UR5UErOOEmGg29ZfzRiEvbe4iBNosdtaw4qnojlJqdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b67en3", "is_robot_indexable": true, "report_reasons": null, "author": "ninjapotato59", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b67en3/stepping_into_the_world_of_data_hoarding_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b67en3/stepping_into_the_world_of_data_hoarding_need/", "subreddit_subscribers": 736025, "created_utc": 1709546880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I'm looking to send all my OneDrive photos (1TB+) to Amazon Photos, to take advantage of the latter's unlimited space. Currently the only option I've found is to download from OneDrive to my PC, then reupload into Amazon Photos (this is manual and slow). \n\nIs there a better way, such as connecting directly cloud to cloud? I've used [https://app.mover.io/](https://app.mover.io/) successfully before, however it doesn't seem to support OneDrive to Amazon Photos (or vice versa). Thanks!", "author_fullname": "t2_vedhl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sending photos from OneDrive to Amazon Photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b66b39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709542499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m looking to send all my OneDrive photos (1TB+) to Amazon Photos, to take advantage of the latter&amp;#39;s unlimited space. Currently the only option I&amp;#39;ve found is to download from OneDrive to my PC, then reupload into Amazon Photos (this is manual and slow). &lt;/p&gt;\n\n&lt;p&gt;Is there a better way, such as connecting directly cloud to cloud? I&amp;#39;ve used &lt;a href=\"https://app.mover.io/\"&gt;https://app.mover.io/&lt;/a&gt; successfully before, however it doesn&amp;#39;t seem to support OneDrive to Amazon Photos (or vice versa). Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b66b39", "is_robot_indexable": true, "report_reasons": null, "author": "niner4nine", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b66b39/sending_photos_from_onedrive_to_amazon_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b66b39/sending_photos_from_onedrive_to_amazon_photos/", "subreddit_subscribers": 736025, "created_utc": 1709542499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As a passionate book collector and data hoarder, I worry about the possibility of losing access to certain books as they disappear from the internet and libraries. However, as my collection grows, I've come to realize that some books hold little meaning or value, such as poorly written novels or explicit content. I'm seeking a way to efficiently eliminate these books from my vast collection without risking the removal of books that I may actually need or enjoy. While I understand that I won't be able to read all the books in my lifetime, I still have a strong desire to collect them and hesitate to give up any. The manual process of sorting through millions of books is time-consuming and inefficient. I would greatly appreciate any suggestions or advice on how to approach this problem.", "author_fullname": "t2_409c23x3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Striking a Balance Between Meaningful Content and Eliminating Unwanted Books", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b62b1u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709527857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a passionate book collector and data hoarder, I worry about the possibility of losing access to certain books as they disappear from the internet and libraries. However, as my collection grows, I&amp;#39;ve come to realize that some books hold little meaning or value, such as poorly written novels or explicit content. I&amp;#39;m seeking a way to efficiently eliminate these books from my vast collection without risking the removal of books that I may actually need or enjoy. While I understand that I won&amp;#39;t be able to read all the books in my lifetime, I still have a strong desire to collect them and hesitate to give up any. The manual process of sorting through millions of books is time-consuming and inefficient. I would greatly appreciate any suggestions or advice on how to approach this problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b62b1u", "is_robot_indexable": true, "report_reasons": null, "author": "NavyandEnvy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b62b1u/striking_a_balance_between_meaningful_content_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b62b1u/striking_a_balance_between_meaningful_content_and/", "subreddit_subscribers": 736025, "created_utc": 1709527857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I expect a lot of the files are the same.  Is there a good program that can help me consolidate files?  I expect a lot of repeat files and/or photos of various resolutions but being the same photo.\n\nWindows OS", "author_fullname": "t2_irmir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a bunch of backup CDs/DVDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5yfn9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709516252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I expect a lot of the files are the same.  Is there a good program that can help me consolidate files?  I expect a lot of repeat files and/or photos of various resolutions but being the same photo.&lt;/p&gt;\n\n&lt;p&gt;Windows OS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5yfn9", "is_robot_indexable": true, "report_reasons": null, "author": "Ottomatica", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5yfn9/i_have_a_bunch_of_backup_cdsdvds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5yfn9/i_have_a_bunch_of_backup_cdsdvds/", "subreddit_subscribers": 736025, "created_utc": 1709516252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m moving to Unraid for my server from Ubuntu &amp; am transferring about ~20TB of movies/tv shows to a new drive. Doing all of this locally on Ubuntu so not doing anything over the network but both rsync jobs for both my movies &amp; tv shows (currently have 2 going at the same time for both folders) are stuck at \u201cbuilding file list\u2026\u201d\n\nThe command I used is the following \u201crsync -acviz \u2014no-i-r \u2014info=progress2 \u201cyour source\u201d \u201cyour destination\u201d\u201d\n\nJust wondering if it\u2019s normal for it to take several hours on just the building file list portion or not. I used the -c because I\u2019ll be clearing the original drive afterwards &amp; want to make sure everything transfers all the way before erasing original drive. If there\u2019s another way I should go about it, please feel free to let me know.", "author_fullname": "t2_18bv7jlr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "rsync stuck on \u201cBuilding File List\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5y3vr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709515305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m moving to Unraid for my server from Ubuntu &amp;amp; am transferring about ~20TB of movies/tv shows to a new drive. Doing all of this locally on Ubuntu so not doing anything over the network but both rsync jobs for both my movies &amp;amp; tv shows (currently have 2 going at the same time for both folders) are stuck at \u201cbuilding file list\u2026\u201d&lt;/p&gt;\n\n&lt;p&gt;The command I used is the following \u201crsync -acviz \u2014no-i-r \u2014info=progress2 \u201cyour source\u201d \u201cyour destination\u201d\u201d&lt;/p&gt;\n\n&lt;p&gt;Just wondering if it\u2019s normal for it to take several hours on just the building file list portion or not. I used the -c because I\u2019ll be clearing the original drive afterwards &amp;amp; want to make sure everything transfers all the way before erasing original drive. If there\u2019s another way I should go about it, please feel free to let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5y3vr", "is_robot_indexable": true, "report_reasons": null, "author": "lilcowboy", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5y3vr/rsync_stuck_on_building_file_list/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5y3vr/rsync_stuck_on_building_file_list/", "subreddit_subscribers": 736025, "created_utc": 1709515305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Guys,  \ni found several posts about exporting snapchat memories and connecting it with time/date.  \nNow i want to go a step further and do the same thing to the snapchat \"chat\\_media\".  \nI literally find nothing to this topic and am not good enough in coding to change something myself.  \nAnyone ever did this?\n\nThank you for any Answers", "author_fullname": "t2_w70n1ofx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to export snapchat \"chat_media\" with date/time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5vx7j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709509329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys,&lt;br/&gt;\ni found several posts about exporting snapchat memories and connecting it with time/date.&lt;br/&gt;\nNow i want to go a step further and do the same thing to the snapchat &amp;quot;chat_media&amp;quot;.&lt;br/&gt;\nI literally find nothing to this topic and am not good enough in coding to change something myself.&lt;br/&gt;\nAnyone ever did this?&lt;/p&gt;\n\n&lt;p&gt;Thank you for any Answers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5vx7j", "is_robot_indexable": true, "report_reasons": null, "author": "smallbignutz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5vx7j/how_to_export_snapchat_chat_media_with_datetime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5vx7j/how_to_export_snapchat_chat_media_with_datetime/", "subreddit_subscribers": 736025, "created_utc": 1709509329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title.", "author_fullname": "t2_v83vicz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need HDD with around 100-200GB. Everything l see is either 1 TB or 500 GB. Does anyone know any HDD with low storage capacity?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5r2r3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709497657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5r2r3", "is_robot_indexable": true, "report_reasons": null, "author": "Rangoq", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5r2r3/i_need_hdd_with_around_100200gb_everything_l_see/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5r2r3/i_need_hdd_with_around_100200gb_everything_l_see/", "subreddit_subscribers": 736025, "created_utc": 1709497657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am aware I am going to bottleneck with this setup, but it gives me the room for upgrading in the future. \n\nFor my small NAS setup, I have a Pi4 2GB and am looking at getting a Mediasonic Probox HF7-SU31C.\n\nThe Pi4 only has USB 3.0 Type A and the Enclosure has 3.2 Gen 2 Type C.\n\nWould [this](https://www.monoprice.com/product?p_id=24287) (or similar) 3.1 Gen 2 Type C to A work? \n\nI assume usb has backwards comparability but just wanted to check! Also if this is the wrong, subreddit, please direct me to the correct one! \n", "author_fullname": "t2_tei9c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will a 3.1 Gen 2 Type C to A cable work with Mediasonic Probox HF7-SU31C?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b665xb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709541921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am aware I am going to bottleneck with this setup, but it gives me the room for upgrading in the future. &lt;/p&gt;\n\n&lt;p&gt;For my small NAS setup, I have a Pi4 2GB and am looking at getting a Mediasonic Probox HF7-SU31C.&lt;/p&gt;\n\n&lt;p&gt;The Pi4 only has USB 3.0 Type A and the Enclosure has 3.2 Gen 2 Type C.&lt;/p&gt;\n\n&lt;p&gt;Would &lt;a href=\"https://www.monoprice.com/product?p_id=24287\"&gt;this&lt;/a&gt; (or similar) 3.1 Gen 2 Type C to A work? &lt;/p&gt;\n\n&lt;p&gt;I assume usb has backwards comparability but just wanted to check! Also if this is the wrong, subreddit, please direct me to the correct one! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b665xb", "is_robot_indexable": true, "report_reasons": null, "author": "Lazzizzle", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b665xb/will_a_31_gen_2_type_c_to_a_cable_work_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b665xb/will_a_31_gen_2_type_c_to_a_cable_work_with/", "subreddit_subscribers": 736025, "created_utc": 1709541921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# Format Specs\n\n[https://replayweb.page/docs/wacz-format](https://replayweb.page/docs/wacz-format)\n\n[https://specs.webrecorder.net/wacz/1.1.1/](https://specs.webrecorder.net/wacz/1.1.1/)\n\n[https://webrecorder.net/2021/01/18/wacz-format-1-0.html](https://webrecorder.net/2021/01/18/wacz-format-1-0.html)\n\n# Recorder\n\n[https://pypi.org/project/wacz/](https://pypi.org/project/wacz/)\n\n# WARC vs WACZ\n\n    WACZ serves as a zipped package format for WARCS. Normally WARC files contain mostly the raw network data. WACZ files take the raw WARC files and zip them up, along with a CDX or compressed CDX index, and a full text index.", "author_fullname": "t2_4jurunac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zim vs WACZ ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b656xo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709538070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Format Specs&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://replayweb.page/docs/wacz-format\"&gt;https://replayweb.page/docs/wacz-format&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://specs.webrecorder.net/wacz/1.1.1/\"&gt;https://specs.webrecorder.net/wacz/1.1.1/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://webrecorder.net/2021/01/18/wacz-format-1-0.html\"&gt;https://webrecorder.net/2021/01/18/wacz-format-1-0.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Recorder&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://pypi.org/project/wacz/\"&gt;https://pypi.org/project/wacz/&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;WARC vs WACZ&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;WACZ serves as a zipped package format for WARCS. Normally WARC files contain mostly the raw network data. WACZ files take the raw WARC files and zip them up, along with a CDX or compressed CDX index, and a full text index.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b656xo", "is_robot_indexable": true, "report_reasons": null, "author": "RedditNoobie777", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b656xo/zim_vs_wacz/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b656xo/zim_vs_wacz/", "subreddit_subscribers": 736025, "created_utc": 1709538070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,  \n\n\nSo im losing access to my Ed stem account soon,  \nI was enrolled in various courses, which have their own forums\n\nEach course has multiple lessons, threads, and discussions,   \nThe lessons are divided into different parts which have a description ,code base, and images  \n\n\nAny ideas what i could do to export / backup them?   \n\n\nThanks guys", "author_fullname": "t2_3it0gav2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup /export an entire Ed forum", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b64l93", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709535711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,  &lt;/p&gt;\n\n&lt;p&gt;So im losing access to my Ed stem account soon,&lt;br/&gt;\nI was enrolled in various courses, which have their own forums&lt;/p&gt;\n\n&lt;p&gt;Each course has multiple lessons, threads, and discussions,&lt;br/&gt;\nThe lessons are divided into different parts which have a description ,code base, and images  &lt;/p&gt;\n\n&lt;p&gt;Any ideas what i could do to export / backup them?   &lt;/p&gt;\n\n&lt;p&gt;Thanks guys&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b64l93", "is_robot_indexable": true, "report_reasons": null, "author": "Big_Virus_", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b64l93/how_to_backup_export_an_entire_ed_forum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b64l93/how_to_backup_export_an_entire_ed_forum/", "subreddit_subscribers": 736025, "created_utc": 1709535711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, I had some Twitter drama saved as mthml files about a week ago, and for some reason they're opening to blank pages as new windows. I've had this issue sometimes with Twitter threads saved as mhtml before, so I check them immediately after they're saved to verify the integrity - they all opened up just fine before. I save them using the Windows Explorer tool, right click &gt; save as mhtml.\n\nSo I'm stuck here wondering why the hell those mthml files are now opening up to....nothing? I open them using Brave Browser, and I just get a completely new window?\n\nI'm not aware that mhtml files have an expiry date. Is this just Twitter backend fuckery or something?\n\nQuick edit: this issue appears to only be happening to Twitter threads I've saved as mhtml. Reddit and other websites work just fine.", "author_fullname": "t2_9ukhkee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mhtml files...stopped working???", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b5vxxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709509381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I had some Twitter drama saved as mthml files about a week ago, and for some reason they&amp;#39;re opening to blank pages as new windows. I&amp;#39;ve had this issue sometimes with Twitter threads saved as mhtml before, so I check them immediately after they&amp;#39;re saved to verify the integrity - they all opened up just fine before. I save them using the Windows Explorer tool, right click &amp;gt; save as mhtml.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m stuck here wondering why the hell those mthml files are now opening up to....nothing? I open them using Brave Browser, and I just get a completely new window?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not aware that mhtml files have an expiry date. Is this just Twitter backend fuckery or something?&lt;/p&gt;\n\n&lt;p&gt;Quick edit: this issue appears to only be happening to Twitter threads I&amp;#39;ve saved as mhtml. Reddit and other websites work just fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b5vxxa", "is_robot_indexable": true, "report_reasons": null, "author": "Nani_The_Fock", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b5vxxa/mhtml_filesstopped_working/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b5vxxa/mhtml_filesstopped_working/", "subreddit_subscribers": 736025, "created_utc": 1709509381.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}