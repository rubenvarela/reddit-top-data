{"kind": "Listing", "data": {"after": "t3_1bhxugv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I usually work with Databricks and I just started learning how Data Factory works. From my understanding, Data Factory can be used for data transformations, as well as for the Extract and Load parts of an ETL process. But I don\u2019t see it used for transformations by my client.\n\nMe and my colleagues use Data Factory for this client, but from what I can see (since this project started years before me arriving in the company) the pipelines 90% of the time run notebooks and send emails when the notebooks fail. Is this the norm?", "author_fullname": "t2_k97u3vqd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhnh1m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710757729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually work with Databricks and I just started learning how Data Factory works. From my understanding, Data Factory can be used for data transformations, as well as for the Extract and Load parts of an ETL process. But I don\u2019t see it used for transformations by my client.&lt;/p&gt;\n\n&lt;p&gt;Me and my colleagues use Data Factory for this client, but from what I can see (since this project started years before me arriving in the company) the pipelines 90% of the time run notebooks and send emails when the notebooks fail. Is this the norm?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhnh1m", "is_robot_indexable": true, "report_reasons": null, "author": "IlMagodelLusso", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhnh1m/azure_data_factory_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhnh1m/azure_data_factory_use/", "subreddit_subscribers": 169964, "created_utc": 1710757729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently hosted the \"NBA Data Modeling Challenge,\" where over 100 participants modeled\u2014yes, you guessed it\u2014historical NBA data!\n\nLeveraging SQL and dbt, participants went above and beyond to uncover NBA insights and compete for a big prize: $1,500!\n\nIn this blog post, I've compiled my favorite insights generated by the participants, such as:\n\n* The dramatic impact of the 3-pointer on the NBA over the last decade\n* The most consistent playoff performers of all time\n* The players who should have been awarded MVP in each season\n* The most clutch NBA players of all time\n* After adjusting for inflation, the highest-paid NBA players ever\n* The most overvalued players in the 2022-23 season\n\nIt's a must-read if you're an NBA fan or just love high-quality SQL, dbt, data analysis, and data visualization!\n\n[Check out the blog here!](https://www.paradime.io/blog/basketball-by-the-numbers-insights-from-paradimes-nba-data-modeling-challenge)", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key Insights from NBA Data Modeling Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhutin", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710778985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently hosted the &amp;quot;NBA Data Modeling Challenge,&amp;quot; where over 100 participants modeled\u2014yes, you guessed it\u2014historical NBA data!&lt;/p&gt;\n\n&lt;p&gt;Leveraging SQL and dbt, participants went above and beyond to uncover NBA insights and compete for a big prize: $1,500!&lt;/p&gt;\n\n&lt;p&gt;In this blog post, I&amp;#39;ve compiled my favorite insights generated by the participants, such as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The dramatic impact of the 3-pointer on the NBA over the last decade&lt;/li&gt;\n&lt;li&gt;The most consistent playoff performers of all time&lt;/li&gt;\n&lt;li&gt;The players who should have been awarded MVP in each season&lt;/li&gt;\n&lt;li&gt;The most clutch NBA players of all time&lt;/li&gt;\n&lt;li&gt;After adjusting for inflation, the highest-paid NBA players ever&lt;/li&gt;\n&lt;li&gt;The most overvalued players in the 2022-23 season&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s a must-read if you&amp;#39;re an NBA fan or just love high-quality SQL, dbt, data analysis, and data visualization!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.paradime.io/blog/basketball-by-the-numbers-insights-from-paradimes-nba-data-modeling-challenge\"&gt;Check out the blog here!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?auto=webp&amp;s=25f017d68d0cf4c258efa3e609d72159ed14aac6", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6fbac571ba0ce966cbcec335f160a91ac7ced94", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb36f934fd4b8620ed8f27f64b10972193ec4f35", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0be76566051f49336667a35d8613b7e9e075336e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6b200f5d42ed3944e618b8233d94f7bcc0dc47f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=65158b0818951a2c049382d98f68aa710e693c4c", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d6aaf46c2cc14d1be0078f00af273aa676c29fa", "width": 1080, "height": 607}], "variants": {}, "id": "hdEr2Ol9ohYczAT_H4Yxt1eeFjwqm5W_afbNmG9q8NQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhutin", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhutin/key_insights_from_nba_data_modeling_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhutin/key_insights_from_nba_data_modeling_challenge/", "subreddit_subscribers": 169964, "created_utc": 1710778985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey reddit, I'm a data architect with 10 years of experience and for the past few years I've been [mentoring data engineers](https://mentorcruise.com/mentor/lewisgavin/) on a site called MentorCruise.\n\nI've not long become a new dad so I can't take on as many mentees as I used to. But a lot of the advice I'm giving applies to most people looking to progress in their DE career. (I even saw a post here yesterday asking [about becoming a good engineer](https://www.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/))\n\nI get an overwhelming number of applications but can't accept as many as I'd like due to just becoming a dad for the first time. So I've decided to create a course.\n\n**This is where you come in.** I'd love to get your feedback on the course as a fellow data engineer to ensure I'm on the right track.\n\nHere is the link to the course site: [https://next-level-data.mailchimpsites.com/](https://next-level-data.mailchimpsites.com/)\n\nI'd love your feedback on:\n\n* the course contents\n* the course price\n* the website/experience in general\n\nThis may seem like I'm farming for signups but I'm genuinely just getting started with this and all I'm looking for is feedback.\n\nHowever, if you do look at the course and think it will be of interest to you then let me know. I can definitely setup some special referral discounts for Redditors who bring friends/colleagues along too.\n\nThanks in advance :)", "author_fullname": "t2_9kwrl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking feedback on data engineering career course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhm35q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710797404.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710751932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey reddit, I&amp;#39;m a data architect with 10 years of experience and for the past few years I&amp;#39;ve been &lt;a href=\"https://mentorcruise.com/mentor/lewisgavin/\"&gt;mentoring data engineers&lt;/a&gt; on a site called MentorCruise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve not long become a new dad so I can&amp;#39;t take on as many mentees as I used to. But a lot of the advice I&amp;#39;m giving applies to most people looking to progress in their DE career. (I even saw a post here yesterday asking &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/\"&gt;about becoming a good engineer&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;I get an overwhelming number of applications but can&amp;#39;t accept as many as I&amp;#39;d like due to just becoming a dad for the first time. So I&amp;#39;ve decided to create a course.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This is where you come in.&lt;/strong&gt; I&amp;#39;d love to get your feedback on the course as a fellow data engineer to ensure I&amp;#39;m on the right track.&lt;/p&gt;\n\n&lt;p&gt;Here is the link to the course site: &lt;a href=\"https://next-level-data.mailchimpsites.com/\"&gt;https://next-level-data.mailchimpsites.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love your feedback on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the course contents&lt;/li&gt;\n&lt;li&gt;the course price&lt;/li&gt;\n&lt;li&gt;the website/experience in general&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This may seem like I&amp;#39;m farming for signups but I&amp;#39;m genuinely just getting started with this and all I&amp;#39;m looking for is feedback.&lt;/p&gt;\n\n&lt;p&gt;However, if you do look at the course and think it will be of interest to you then let me know. I can definitely setup some special referral discounts for Redditors who bring friends/colleagues along too.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?auto=webp&amp;s=826577a6dff9c2822defccc6abeb8a3791a4f378", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5f6652f61b1a0e56833d08f0686d30e2329eb05", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f365e7495e0645aa0c8774a5cfed8409b5674b4f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16de7d5daef6611323ac7642d9f5f287849b64a3", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f0b36b08ab5346200cd441068e98c3ac852d9ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c93bf08bf7fa6625c90d6c3f183341ce83f477e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6cb598cce81c0d8d0949be2acb444bf84ad2b82", "width": 1080, "height": 567}], "variants": {}, "id": "zCh4-DKpuT8tl8-SzPXCdRevHjz5DJZ0d8U0sLHbmXw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bhm35q", "is_robot_indexable": true, "report_reasons": null, "author": "gavlaaaaaaaa", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhm35q/seeking_feedback_on_data_engineering_career_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhm35q/seeking_feedback_on_data_engineering_career_course/", "subreddit_subscribers": 169964, "created_utc": 1710751932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nTo be very up front: I run a small saas focused on real-time metrics. I recently announced a new feature/product I'd love some feedback on (and potentially beta testers).  [Blog Post](https://aggregations.io/blog/autodocs-coming-soon).\n\nThe idea is simple: you forward your event stream and you get a searchable schema of your events, &amp; their properties along with statistics/distributions of the field values. \n\nThe other element comes in the form of a per-version changelog, with alerting for things like type changes, cardinality fluctuations, etc. \n\nI won't go too far into the technical details, unless people are interested -- but building this is obviously has been a very intricate and complex project, I'm pretty happy with the result so far :)\n\n&amp;#x200B;\n\nI've built a system like this multiple times in the past at larger companies, so I know the value it can provide -- I'm just not sure (1) how to express it well and (2) what other scenarios/ features might be useful.\n\nFor example, post launch I know I want to add in more collaboration/annotation features (to make it more of a \"documentation hub\" for analysts, data producers, etc) -- but other things I've built before, may not be super applicable. \n\nIn the past, I enabled \"generate SQL to fetch this property in different languages\" because JSON functions can be tricky and some payloads are gnarly. I don't know if that is widely applicable? \n\n&amp;#x200B;\n\nIs this a thing that would help you? What struggles do you have with documenting your events, etc?\n\nAny thoughts or feedback would be greatly appreciated! ", "author_fullname": "t2_vpdufq3pm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics Events Documentation &amp; Monitoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhpxx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710766234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;To be very up front: I run a small saas focused on real-time metrics. I recently announced a new feature/product I&amp;#39;d love some feedback on (and potentially beta testers).  &lt;a href=\"https://aggregations.io/blog/autodocs-coming-soon\"&gt;Blog Post&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The idea is simple: you forward your event stream and you get a searchable schema of your events, &amp;amp; their properties along with statistics/distributions of the field values. &lt;/p&gt;\n\n&lt;p&gt;The other element comes in the form of a per-version changelog, with alerting for things like type changes, cardinality fluctuations, etc. &lt;/p&gt;\n\n&lt;p&gt;I won&amp;#39;t go too far into the technical details, unless people are interested -- but building this is obviously has been a very intricate and complex project, I&amp;#39;m pretty happy with the result so far :)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a system like this multiple times in the past at larger companies, so I know the value it can provide -- I&amp;#39;m just not sure (1) how to express it well and (2) what other scenarios/ features might be useful.&lt;/p&gt;\n\n&lt;p&gt;For example, post launch I know I want to add in more collaboration/annotation features (to make it more of a &amp;quot;documentation hub&amp;quot; for analysts, data producers, etc) -- but other things I&amp;#39;ve built before, may not be super applicable. &lt;/p&gt;\n\n&lt;p&gt;In the past, I enabled &amp;quot;generate SQL to fetch this property in different languages&amp;quot; because JSON functions can be tricky and some payloads are gnarly. I don&amp;#39;t know if that is widely applicable? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this a thing that would help you? What struggles do you have with documenting your events, etc?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts or feedback would be greatly appreciated! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?auto=webp&amp;s=d82e0bdcb768bd94828b24f022cf6710f5fd134f", "width": 1024, "height": 649}, "resolutions": [{"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df99c7f6cc2a6e1b07f115e66572f9d72a9b8c92", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b006cd92fff54c42cf968e704fd21d6aaedd845d", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=03d22aa565ec345cb9aa8586fb78133fe3868106", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8739eb0b61a84385b26c321db0d6125c4c1f1835", "width": 640, "height": 405}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=216181d41f4abfddef7cbbcc1b76ad8259974862", "width": 960, "height": 608}], "variants": {}, "id": "-IguNufVGIqW9ENdLi7KpG31ptOTdRhW0AL9zjARo1g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhpxx2", "is_robot_indexable": true, "report_reasons": null, "author": "jsneedles", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhpxx2/analytics_events_documentation_monitoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhpxx2/analytics_events_documentation_monitoring/", "subreddit_subscribers": 169964, "created_utc": 1710766234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the past few months I have been learning about data strategies that employ Apache Iceberg with Snowflake DB and Apache Spark &amp; I have compiled my learnings into a short article.\n\n[https://medium.com/@pbd\\_94/skiing-with-snowflake-b196e8f7e2e6](https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6)\n\nFire away.", "author_fullname": "t2_11mmk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Supercharge your compute strategy with Apache Iceberg, Snowflake, Apache Spark, AWS Glue &amp; Project Nessie", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhhxs0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710734969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the past few months I have been learning about data strategies that employ Apache Iceberg with Snowflake DB and Apache Spark &amp;amp; I have compiled my learnings into a short article.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6\"&gt;https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Fire away.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?auto=webp&amp;s=0380b1add45a87f89967d78ac69b9471fa1758ec", "width": 1200, "height": 915}, "resolutions": [{"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df7f210d8f4b10b04cc8590372be4515d0dc2af6", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=770379812d74ee166858c6c46f32e994adb05d37", "width": 216, "height": 164}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f3dee8a276d36599de5bf5f8205968fc92ddb05", "width": 320, "height": 244}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd25d5968eb8136f3f1109b294c9cd66b8b6d3bc", "width": 640, "height": 488}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a3ba06688cc1992e79c2ea280839cd13e4b0063", "width": 960, "height": 732}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5f371a9a1991838b4c850e5adb62d389bde796a", "width": 1080, "height": 823}], "variants": {}, "id": "csQ4U8qiLyOe4t7uz2oyBKWIrTrW4Z3Y939nxE1g1Bg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhhxs0", "is_robot_indexable": true, "report_reasons": null, "author": "Pbd1194", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhhxs0/supercharge_your_compute_strategy_with_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhhxs0/supercharge_your_compute_strategy_with_apache/", "subreddit_subscribers": 169964, "created_utc": 1710734969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "agenda is to extract the data from Redshift and need to load into SQL server\n\n\nWhat are someone of the best approachs to do this , fast and cost efficient.", "author_fullname": "t2_iiiqo30a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestion on etl", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhv9to", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710780087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;agenda is to extract the data from Redshift and need to load into SQL server&lt;/p&gt;\n\n&lt;p&gt;What are someone of the best approachs to do this , fast and cost efficient.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhv9to", "is_robot_indexable": true, "report_reasons": null, "author": "BOOBINDERxKK", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhv9to/suggestion_on_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhv9to/suggestion_on_etl/", "subreddit_subscribers": 169964, "created_utc": 1710780087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if anyone here moved from a SAP BW background into core Data Engineering field? Is this even possible? Looks like SAP experience is not strongly considered as a transferable skill to other tools. I have been learning data structures, streaming tools, databases etc. because its a very interesting space but unsure if I could ever switch to a DE job complementing with my SAP BW exp. ", "author_fullname": "t2_7qvd84fd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move from SAP BW to Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhfmka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710727789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if anyone here moved from a SAP BW background into core Data Engineering field? Is this even possible? Looks like SAP experience is not strongly considered as a transferable skill to other tools. I have been learning data structures, streaming tools, databases etc. because its a very interesting space but unsure if I could ever switch to a DE job complementing with my SAP BW exp. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhfmka", "is_robot_indexable": true, "report_reasons": null, "author": "SignificantOpinion92", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhfmka/how_to_move_from_sap_bw_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhfmka/how_to_move_from_sap_bw_to_data_engineering/", "subreddit_subscribers": 169964, "created_utc": 1710727789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Get the knowledge required to excel in the realms of data engineering, data science, and a host of related in-demand fields with this collection of books from O\u2019Reilly! Deciphering Data Architectures provides a guided tour of today\u2019s most common architectures\u2014from data lakehouses to data meshes\u2014to help you understand the pros and cons of each. Data Science: The Hard Parts is a handy guidebook of techniques and best practices that are generally overlooked when teaching this wide-ranging discipline.\n\n\n", "author_fullname": "t2_6b9o0e5i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Humble Tech Book Bundle: Pipelines and NoSQL by O'Reilly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhzg06", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kRC4x_HcbhsgiwleL2XqINxbXymSn_3C1EO3omqF0og.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1710790063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "humblebundle.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Get the knowledge required to excel in the realms of data engineering, data science, and a host of related in-demand fields with this collection of books from O\u2019Reilly! Deciphering Data Architectures provides a guided tour of today\u2019s most common architectures\u2014from data lakehouses to data meshes\u2014to help you understand the pros and cons of each. Data Science: The Hard Parts is a handy guidebook of techniques and best practices that are generally overlooked when teaching this wide-ranging discipline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_threes_tile_index_1_c_pipelinesandnosqloreilly_bookbundle", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?auto=webp&amp;s=b50f3a7282d32dcfa0e059b0b506a19daa6b7df9", "width": 1120, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8df569a17eb586e2b0feb844c8ddd80e2c53523e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc1d0f218d319d03ff9fb045ea5a017098e1c402", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4db0e3979f4fd989e036859dfa1257d15a01f34", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0c5aa5216466749f2ffd89e4f6dda46502974b1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92b050a7f148a6395bed48099df1cca2307178a1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abb37e101932d1c5791c9ad599433a01b56b311e", "width": 1080, "height": 607}], "variants": {}, "id": "FbykrjGMgm2863qufl2xVlkGdUUd4rQbUPblShEhFV4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1bhzg06", "is_robot_indexable": true, "report_reasons": null, "author": "serious_frank", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhzg06/humble_tech_book_bundle_pipelines_and_nosql_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_threes_tile_index_1_c_pipelinesandnosqloreilly_bookbundle", "subreddit_subscribers": 169964, "created_utc": 1710790063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Team,\n\nlike all data engineers, we want to have the modern data stack for a data transformation initiative. My context is that I'm having trouble getting management approval on this (mostly due to costs) and I think it is mostly due to data literacy gaps. We're doing a lot of that (teaching them) but it seems it would take more than just informing them to get them to invest. It's a large enterprise that doesn't have a lot of consumer data, but lots of other data opportunities to explore for analytics use cases in the form of supplier, vendor, partner, location data and varying industries. They currently don't have an enterprise wide implementation of anything. ALSO, they don't trust the cloud and currently is a heavy MS user.\n\nso my question to the group is, given this context, what would you advise me to do as next step? Here are my thoughts but I'm open to ideas and stories. I would recommend to at least start with building a business intelligence platform with datawarehouse first. I'm open to using ms products if it will help speed things up a long e.g. power bi and ms sql as the datawarehouse. Then I'll probably just look for an open source tool (maybe - i'm not sure what ms has in terms of on-prem integration tool) to use for ingestion then a separate server for machine learning deployments (we can use local laptops for development). my only problem as well is the encryption/ hashing tools for PII, i think. use this for a few years to prove the use cases, then hopefully gain enough approval to invest in an on-prem mds.   \n  \nI was thinking the tech debt would be worth it just to get things started and prove that having these things in place works to get more buy in later on. would really appreciate everyone's thoughts thank you!", "author_fullname": "t2_56myc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having trouble moving to MDS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhdssw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710722815.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710722568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Team,&lt;/p&gt;\n\n&lt;p&gt;like all data engineers, we want to have the modern data stack for a data transformation initiative. My context is that I&amp;#39;m having trouble getting management approval on this (mostly due to costs) and I think it is mostly due to data literacy gaps. We&amp;#39;re doing a lot of that (teaching them) but it seems it would take more than just informing them to get them to invest. It&amp;#39;s a large enterprise that doesn&amp;#39;t have a lot of consumer data, but lots of other data opportunities to explore for analytics use cases in the form of supplier, vendor, partner, location data and varying industries. They currently don&amp;#39;t have an enterprise wide implementation of anything. ALSO, they don&amp;#39;t trust the cloud and currently is a heavy MS user.&lt;/p&gt;\n\n&lt;p&gt;so my question to the group is, given this context, what would you advise me to do as next step? Here are my thoughts but I&amp;#39;m open to ideas and stories. I would recommend to at least start with building a business intelligence platform with datawarehouse first. I&amp;#39;m open to using ms products if it will help speed things up a long e.g. power bi and ms sql as the datawarehouse. Then I&amp;#39;ll probably just look for an open source tool (maybe - i&amp;#39;m not sure what ms has in terms of on-prem integration tool) to use for ingestion then a separate server for machine learning deployments (we can use local laptops for development). my only problem as well is the encryption/ hashing tools for PII, i think. use this for a few years to prove the use cases, then hopefully gain enough approval to invest in an on-prem mds.   &lt;/p&gt;\n\n&lt;p&gt;I was thinking the tech debt would be worth it just to get things started and prove that having these things in place works to get more buy in later on. would really appreciate everyone&amp;#39;s thoughts thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhdssw", "is_robot_indexable": true, "report_reasons": null, "author": "saintmichel", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhdssw/having_trouble_moving_to_mds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhdssw/having_trouble_moving_to_mds/", "subreddit_subscribers": 169964, "created_utc": 1710722568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey!\n\nNot sure if this is the best place for this question, but I thought it would be interesting to hear how people are building models with require joining across large impression and conversion tables to calculate stats.\n\nThe main issue I am trying to figure out best practice for right now is how to handle the fact that conversions can happen up 30 days days in the future past impressions. This means each time I run my jobs, I need to select a whole 30 days of data from the tables. This seems extremely inefficient, especially when running this hourly.\n\nWould love to hear how people are tackling this!\n\nI am using DBT + Bigquery.", "author_fullname": "t2_8eusc3v3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are people building a impression and conversion tracking system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhwr9o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710783670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;Not sure if this is the best place for this question, but I thought it would be interesting to hear how people are building models with require joining across large impression and conversion tables to calculate stats.&lt;/p&gt;\n\n&lt;p&gt;The main issue I am trying to figure out best practice for right now is how to handle the fact that conversions can happen up 30 days days in the future past impressions. This means each time I run my jobs, I need to select a whole 30 days of data from the tables. This seems extremely inefficient, especially when running this hourly.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how people are tackling this!&lt;/p&gt;\n\n&lt;p&gt;I am using DBT + Bigquery.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhwr9o", "is_robot_indexable": true, "report_reasons": null, "author": "Suspicious_Dress_350", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhwr9o/how_are_people_building_a_impression_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhwr9o/how_are_people_building_a_impression_and/", "subreddit_subscribers": 169964, "created_utc": 1710783670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to integrate OpenLineage to my airflow server, but I wanna know if I can use other metadata visualization tools instead of marquez", "author_fullname": "t2_dwxrwyrv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to use DataHub as UI for OpenLineage backend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhsa88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710772659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to integrate OpenLineage to my airflow server, but I wanna know if I can use other metadata visualization tools instead of marquez&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhsa88", "is_robot_indexable": true, "report_reasons": null, "author": "nhawlao", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhsa88/is_it_possible_to_use_datahub_as_ui_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhsa88/is_it_possible_to_use_datahub_as_ui_for/", "subreddit_subscribers": 169964, "created_utc": 1710772659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE folks,\n\nCreated this account to gather feedback and insights from y'all. I'm fairly new to data engineering/governance. My company has been using Informatica Cloud for quite some time now.\n\n&amp;#x200B;\n\nMy organization wants to dive into the whole Generative AI segment. This is at a very nascent stage where nothing is really decided or planned, beyond the database. And the DB of choice for this initiative is Pinecone. Other than this, nothing else has been decided yet, what the architecture would be like, how and where we are planning to use Pinecone. I'm aware vector DBs are great for extending the capabilities of LLMs especially since they're infrequently trained, but again.. I have no idea if that is the use case here, or something else entirely since the company has not revealed anything around that.\n\n&amp;#x200B;\n\nThat being said, I am trying to figure out what would be the best way to catalog the metadata from Pinecone. Like I said, we're on Informatica and are using the Cloud Data Governance and Catalog tool for our operations - however this tool does not have any vector DB cataloging capabilities, nor does Informatica have any Pinecone connectors planned for the near future.\n\n&amp;#x200B;\n\nCan you share some insights, maybe articles or just your thoughts on how I should be approaching this problem given the context around it? Apologies for such a broad question.", "author_fullname": "t2_wer0lc400", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cataloging vector databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhgung", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710731530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE folks,&lt;/p&gt;\n\n&lt;p&gt;Created this account to gather feedback and insights from y&amp;#39;all. I&amp;#39;m fairly new to data engineering/governance. My company has been using Informatica Cloud for quite some time now.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My organization wants to dive into the whole Generative AI segment. This is at a very nascent stage where nothing is really decided or planned, beyond the database. And the DB of choice for this initiative is Pinecone. Other than this, nothing else has been decided yet, what the architecture would be like, how and where we are planning to use Pinecone. I&amp;#39;m aware vector DBs are great for extending the capabilities of LLMs especially since they&amp;#39;re infrequently trained, but again.. I have no idea if that is the use case here, or something else entirely since the company has not revealed anything around that.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;That being said, I am trying to figure out what would be the best way to catalog the metadata from Pinecone. Like I said, we&amp;#39;re on Informatica and are using the Cloud Data Governance and Catalog tool for our operations - however this tool does not have any vector DB cataloging capabilities, nor does Informatica have any Pinecone connectors planned for the near future.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can you share some insights, maybe articles or just your thoughts on how I should be approaching this problem given the context around it? Apologies for such a broad question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhgung", "is_robot_indexable": true, "report_reasons": null, "author": "Key-Imagination-9090", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhgung/cataloging_vector_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhgung/cataloging_vector_databases/", "subreddit_subscribers": 169964, "created_utc": 1710731530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently PoCing the new AWS Zero ETL DynamoDB/Redshift integration, and infrastructure-wise, it seems very straightforward and simple. However, the table created by the integration holds all columns of the DynamoDB source table as a single column as a large [super type](https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html) named `value`, which I have zero experience with!\n\nMy first thought here is to create a view where I extract elements of `value` into distinct, normalized columns:\n\nFor example, suppose the value of one row of `value` is:\n\n    {\n      \"id\": {\n        \"S\": \"RcVpEFPNIAMFytg=\"\n      },\n      \"timestamp\": {\n        \"N\": \"1705090208167\"\n      },\n      \"files\": {\n        \"L\": [\n          {\n            \"S\": \"s3://example/file1.raw\"\n          },\n          {\n            \"S\": \"s3://example/file2.raw\"\n          },\n          {\n            \"S\": \"s3://example/file3.raw\"\n          }\n        ]\n      }\n    }\n\nI could theoretically create a table/view to query this like:\n\n    with s1 as\n    (\n        select\n            json_extract_path_text(json_serialize(e.value), 'id', 'S')::varchar as id,\n            nullif(json_extract_path_text(json_serialize(e.value), 'timestamp', 'N'), '')::bigint as timestamp,\n            json_extract_path_text(json_serialize(e.value), 'files', 'L') as files\n        from \"example\".\"public\".\"example\" e\n    )\n    select *\n    from s1\n    order by s1.timestamp desc;\n\nHowever, I've never worked with super variables before, and I have two big questions:\n\n1. Am I right to be extracting `super` type columns into normalized columns like this, or should I be querying them as they are?\n2. This is a bit more of a detailed question, but how do I successfully extract arrays of `super` type columns? For example, in the above, the new `data` column would still be a `super` (`[{\"S\":\"s3://example/file1.raw\"},{\"S\":\"s3://example/file2.raw\"},{\"S\":\"s3://example/file3.raw\"}`), and it's not clear both how this should be extracted and 3N normalized into separate columns?\n\nAny help is appreciated, thanks!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should Redshift columns of type super be queried?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhexl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710725792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently PoCing the new AWS Zero ETL DynamoDB/Redshift integration, and infrastructure-wise, it seems very straightforward and simple. However, the table created by the integration holds all columns of the DynamoDB source table as a single column as a large &lt;a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html\"&gt;super type&lt;/a&gt; named &lt;code&gt;value&lt;/code&gt;, which I have zero experience with!&lt;/p&gt;\n\n&lt;p&gt;My first thought here is to create a view where I extract elements of &lt;code&gt;value&lt;/code&gt; into distinct, normalized columns:&lt;/p&gt;\n\n&lt;p&gt;For example, suppose the value of one row of &lt;code&gt;value&lt;/code&gt; is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;id&amp;quot;: {\n    &amp;quot;S&amp;quot;: &amp;quot;RcVpEFPNIAMFytg=&amp;quot;\n  },\n  &amp;quot;timestamp&amp;quot;: {\n    &amp;quot;N&amp;quot;: &amp;quot;1705090208167&amp;quot;\n  },\n  &amp;quot;files&amp;quot;: {\n    &amp;quot;L&amp;quot;: [\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file1.raw&amp;quot;\n      },\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file2.raw&amp;quot;\n      },\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file3.raw&amp;quot;\n      }\n    ]\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I could theoretically create a table/view to query this like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;with s1 as\n(\n    select\n        json_extract_path_text(json_serialize(e.value), &amp;#39;id&amp;#39;, &amp;#39;S&amp;#39;)::varchar as id,\n        nullif(json_extract_path_text(json_serialize(e.value), &amp;#39;timestamp&amp;#39;, &amp;#39;N&amp;#39;), &amp;#39;&amp;#39;)::bigint as timestamp,\n        json_extract_path_text(json_serialize(e.value), &amp;#39;files&amp;#39;, &amp;#39;L&amp;#39;) as files\n    from &amp;quot;example&amp;quot;.&amp;quot;public&amp;quot;.&amp;quot;example&amp;quot; e\n)\nselect *\nfrom s1\norder by s1.timestamp desc;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;However, I&amp;#39;ve never worked with super variables before, and I have two big questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Am I right to be extracting &lt;code&gt;super&lt;/code&gt; type columns into normalized columns like this, or should I be querying them as they are?&lt;/li&gt;\n&lt;li&gt;This is a bit more of a detailed question, but how do I successfully extract arrays of &lt;code&gt;super&lt;/code&gt; type columns? For example, in the above, the new &lt;code&gt;data&lt;/code&gt; column would still be a &lt;code&gt;super&lt;/code&gt; (&lt;code&gt;[{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file1.raw&amp;quot;},{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file2.raw&amp;quot;},{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file3.raw&amp;quot;}&lt;/code&gt;), and it&amp;#39;s not clear both how this should be extracted and 3N normalized into separate columns?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help is appreciated, thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhexl3", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhexl3/how_should_redshift_columns_of_type_super_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhexl3/how_should_redshift_columns_of_type_super_be/", "subreddit_subscribers": 169964, "created_utc": 1710725792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a client that insists they want to see normalized data in a denormalized csv. If I were to do this it would be hundreds of columns and the number of columns could change so they would need to be dynamically generated.  \n\n\nI don't suppose a tool exists to do this?", "author_fullname": "t2_mo4lb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Denormalize Tables into CSV?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bi1uwp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710795746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a client that insists they want to see normalized data in a denormalized csv. If I were to do this it would be hundreds of columns and the number of columns could change so they would need to be dynamically generated.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t suppose a tool exists to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bi1uwp", "is_robot_indexable": true, "report_reasons": null, "author": "Phantazein", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1uwp/denormalize_tables_into_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1uwp/denormalize_tables_into_csv/", "subreddit_subscribers": 169964, "created_utc": 1710795746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have athena partitions for date like:\n\nyear=2012/month=01/day=01 and inside each folder around 30 parquet files\n\nThe schema of the parquet files does not have the columns year, month, and day, it has a single column called 'date' which is a timestamp, when I created the folders just did it by filtering on the spark df and then writing on the S3 folder. Does this affect performance in how Athena queries this data? Should I have within the .parquet files a schema with the year, month, and day? should I change my partitions to be 2012/01/01 2012/01/02 and create a column within the .parquet files called just \"actual\\_date\" and keep \"date\" timestamp? any thoughts? :) TIA!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena Partitions and .parquet schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bi1qnn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710795935.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710795487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have athena partitions for date like:&lt;/p&gt;\n\n&lt;p&gt;year=2012/month=01/day=01 and inside each folder around 30 parquet files&lt;/p&gt;\n\n&lt;p&gt;The schema of the parquet files does not have the columns year, month, and day, it has a single column called &amp;#39;date&amp;#39; which is a timestamp, when I created the folders just did it by filtering on the spark df and then writing on the S3 folder. Does this affect performance in how Athena queries this data? Should I have within the .parquet files a schema with the year, month, and day? should I change my partitions to be 2012/01/01 2012/01/02 and create a column within the .parquet files called just &amp;quot;actual_date&amp;quot; and keep &amp;quot;date&amp;quot; timestamp? any thoughts? :) TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi1qnn", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1qnn/athena_partitions_and_parquet_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1qnn/athena_partitions_and_parquet_schema/", "subreddit_subscribers": 169964, "created_utc": 1710795487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**TL;DR**\nWhat processes or tools do you use to create logging specs, get the specs implemented, validate their implementation, and then maintain documentation for that logging?\n\n**Background**\n\nAs a Data Engineer, I spend a lot of time with my software engineers asking them to implement new or modify existing logging. In order to do so, I provide a logging spec to them with instructions on what I'm looking for.  \n\n**My Process**\n\nMy company currently uses unstructured google docs to collaborate on the spec, but then there's no way to validate the implementation and google docs are easily lost over time and documentation for these events become non-existent.", "author_fullname": "t2_fo0y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the most effective way to create, implement, validate, and document logging specifications?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bi1mny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710795238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;\nWhat processes or tools do you use to create logging specs, get the specs implemented, validate their implementation, and then maintain documentation for that logging?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As a Data Engineer, I spend a lot of time with my software engineers asking them to implement new or modify existing logging. In order to do so, I provide a logging spec to them with instructions on what I&amp;#39;m looking for.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Process&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My company currently uses unstructured google docs to collaborate on the spec, but then there&amp;#39;s no way to validate the implementation and google docs are easily lost over time and documentation for these events become non-existent.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi1mny", "is_robot_indexable": true, "report_reasons": null, "author": "sharpchicity", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1mny/what_is_the_most_effective_way_to_create/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1mny/what_is_the_most_effective_way_to_create/", "subreddit_subscribers": 169964, "created_utc": 1710795238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is optimized by DBMS: logical or physical plans? or both? If only logical plans, how DBMS can calculate costs without knowing of exact physical operations behind nodes of query tree? I also doubt that only physical plans are optimized.  \nI have this picture (CMU DB group lectures) where given logical plan is optimized but I dunno how this may happen if logical plan is to my knowledge high-level DAG of operators without any physical specifics.\n\nTo me it seems that optimizer should optimize both logical plans (iterate through different query plan trees and choose best plan using *heuristics*) and physical plans (iterate through different options of executing the same query plan tree (different joins, access methods, etc.) and choose best plan using *costs-based approach*) but I'm definitely not sure about that.\n\nhttps://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;format=png&amp;auto=webp&amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a", "author_fullname": "t2_b3ayn5hsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When does query optimization in DBMS happen?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "media_metadata": {"3p9o5illm5pc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f31e57669dba4a7a2b7b285720441bb9fdb4f7f"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c9c84776ba8b8b2e4e94043039331ca3d0a690e"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=631e3dae7cbd3a70a36f697e530ad1d9dcd4185b"}], "s": {"y": 270, "x": 537, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;format=png&amp;auto=webp&amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a"}, "id": "3p9o5illm5pc1"}}, "name": "t3_1bi1et9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BxOyR28XQl3uMXoBOQxQsvgves95Gy5v-9Un_5vmKxM.jpg", "edited": 1710795638.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710794728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is optimized by DBMS: logical or physical plans? or both? If only logical plans, how DBMS can calculate costs without knowing of exact physical operations behind nodes of query tree? I also doubt that only physical plans are optimized.&lt;br/&gt;\nI have this picture (CMU DB group lectures) where given logical plan is optimized but I dunno how this may happen if logical plan is to my knowledge high-level DAG of operators without any physical specifics.&lt;/p&gt;\n\n&lt;p&gt;To me it seems that optimizer should optimize both logical plans (iterate through different query plan trees and choose best plan using &lt;em&gt;heuristics&lt;/em&gt;) and physical plans (iterate through different options of executing the same query plan tree (different joins, access methods, etc.) and choose best plan using &lt;em&gt;costs-based approach&lt;/em&gt;) but I&amp;#39;m definitely not sure about that.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a\"&gt;https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bi1et9", "is_robot_indexable": true, "report_reasons": null, "author": "isk14yo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1et9/when_does_query_optimization_in_dbms_happen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1et9/when_does_query_optimization_in_dbms_happen/", "subreddit_subscribers": 169964, "created_utc": 1710794728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So i have a project where basically data needs to be ingested into a big snowflake table from multiple csv files that are sent to AWS SFTP (and forwarded to S3). The project itself is pretty simple tbh. The high level architecture and workflow is as follows: CSV gets send by customer applications into S3 --&gt; loading into snowflake load table --&gt; some small/basic transformations (joins across data of multiple files) --&gt; database with clean data --&gt; some customers will require export jobs like nested json or csv to their own s3 or whatever.\n\nTo be a little bit more specific: the data we are talking about here is very small like 30mb etc. the ingestion is very basic as well maybe some edge cases but overall just parsing CSV.\n\nNow we have a debate in the team. Our AWS architect made an architecture i will call \"aws first\": Basically whenever anything has to be done he wants me to do it with AWS Lambda and python3. Meaning: Ingestion made with s3 events that triggers lambda that will do simple ingestion. Then the copy process and joins also in a lambda and the export jobs as well.\n\nThe second approach i would call snowflake first is skipping AWS Lambda all together and just using Snowpipe that will listen to s3 events and then some UFD functions or stored procedures. For the Export jobs we can use snowpipe and snowflake tasks as well maybe in some cases we would need external functions and then some kind of AWS lambda service but it's probably unlikely this will be a real use-case. The snowpipes and storage integration i would create with terraform snowflake provider.\n\nSo i would like to ask the community what approach they would choose here. In my opinion both approaches have disadvantages and advantages:\n\nadvantages aws first approach (aws lambda and python)\n\n- most of the project is in aws so aws lambda would integrate very well (IaC) etc.\n\n- patching of software is required anyways since our project has some other tools and apps\n\n- we could implement some additional stuff in the lambda like idempotency\n\n- very easy to understand if the project wants to hire low cost devs in the future for programing integrations\n\n- data is pretty small probably even fits into a python3 dataframe so all the optimisation we would get from focusing on snowflake features probably not worth it\n\nadvantages \"snowflake first\" approach (terraform and snowpipe, tasks, ufd etc.)\n\n- leveraging the tools of a platform we already paying for anyways\n\n- less operational overhead, less changes needed in the future\n\nMaybe i can get some guidance on this topic. It just feels counterintuitive to me to build so many custom lambda functions on the other hand there isn't really a good argument to do most of the workloads in snowflake. What do you think? :) Thanks!\n\n", "author_fullname": "t2_ficwvf44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance: Snowflake first approach vs AWS first approach data ingestion and export jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bi195y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710794665.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710794349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i have a project where basically data needs to be ingested into a big snowflake table from multiple csv files that are sent to AWS SFTP (and forwarded to S3). The project itself is pretty simple tbh. The high level architecture and workflow is as follows: CSV gets send by customer applications into S3 --&amp;gt; loading into snowflake load table --&amp;gt; some small/basic transformations (joins across data of multiple files) --&amp;gt; database with clean data --&amp;gt; some customers will require export jobs like nested json or csv to their own s3 or whatever.&lt;/p&gt;\n\n&lt;p&gt;To be a little bit more specific: the data we are talking about here is very small like 30mb etc. the ingestion is very basic as well maybe some edge cases but overall just parsing CSV.&lt;/p&gt;\n\n&lt;p&gt;Now we have a debate in the team. Our AWS architect made an architecture i will call &amp;quot;aws first&amp;quot;: Basically whenever anything has to be done he wants me to do it with AWS Lambda and python3. Meaning: Ingestion made with s3 events that triggers lambda that will do simple ingestion. Then the copy process and joins also in a lambda and the export jobs as well.&lt;/p&gt;\n\n&lt;p&gt;The second approach i would call snowflake first is skipping AWS Lambda all together and just using Snowpipe that will listen to s3 events and then some UFD functions or stored procedures. For the Export jobs we can use snowpipe and snowflake tasks as well maybe in some cases we would need external functions and then some kind of AWS lambda service but it&amp;#39;s probably unlikely this will be a real use-case. The snowpipes and storage integration i would create with terraform snowflake provider.&lt;/p&gt;\n\n&lt;p&gt;So i would like to ask the community what approach they would choose here. In my opinion both approaches have disadvantages and advantages:&lt;/p&gt;\n\n&lt;p&gt;advantages aws first approach (aws lambda and python)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;most of the project is in aws so aws lambda would integrate very well (IaC) etc.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;patching of software is required anyways since our project has some other tools and apps&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;we could implement some additional stuff in the lambda like idempotency&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;very easy to understand if the project wants to hire low cost devs in the future for programing integrations&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;data is pretty small probably even fits into a python3 dataframe so all the optimisation we would get from focusing on snowflake features probably not worth it&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;advantages &amp;quot;snowflake first&amp;quot; approach (terraform and snowpipe, tasks, ufd etc.)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;leveraging the tools of a platform we already paying for anyways&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;less operational overhead, less changes needed in the future&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Maybe i can get some guidance on this topic. It just feels counterintuitive to me to build so many custom lambda functions on the other hand there isn&amp;#39;t really a good argument to do most of the workloads in snowflake. What do you think? :) Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi195y", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Interaction_5701", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi195y/guidance_snowflake_first_approach_vs_aws_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi195y/guidance_snowflake_first_approach_vs_aws_first/", "subreddit_subscribers": 169964, "created_utc": 1710794349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain\\_like\\_im\\_5\\_databricks\\_photon/](https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/)\n\nHey guys I saw this post the other day and we have a similar set up. except I want something open source instead of a full fledge product like Photon. from what ive read, spark3 already converts data frames into columnar formatting in the catalytic optimizer. \n\nso im even wondering apart from the C++ translation if I were to use photon would there be any other benefits? \n\n&amp;#x200B;\n\nalso if you know of any open source products that can be used in OP's tech stack to accelerate my project lmk please\n\n&amp;#x200B;", "author_fullname": "t2_i28c8i3vu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photon alternatives? I need something open source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhvpyv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710781181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/\"&gt;https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey guys I saw this post the other day and we have a similar set up. except I want something open source instead of a full fledge product like Photon. from what ive read, spark3 already converts data frames into columnar formatting in the catalytic optimizer. &lt;/p&gt;\n\n&lt;p&gt;so im even wondering apart from the C++ translation if I were to use photon would there be any other benefits? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;also if you know of any open source products that can be used in OP&amp;#39;s tech stack to accelerate my project lmk please&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bhvpyv", "is_robot_indexable": true, "report_reasons": null, "author": "Stunning_Wolf_4595", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhvpyv/photon_alternatives_i_need_something_open_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhvpyv/photon_alternatives_i_need_something_open_source/", "subreddit_subscribers": 169964, "created_utc": 1710781181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have delta live table which reads the CSV Files from ADLS and loads into bronze and silver. Your typical Data Pipeline.\n\nIt\u2019s not very complicated pipeline. With auto loader it instantly picks the files and loads to bronze in Unity Catalog but in between I am doing two operations -\n\n1) Usual Deduplication based on the Hash value \n\n2) Based on a configuration CSV file , I pick operations like convert a elements in a  column to Upper case , or trim the elements of the column or you replace all null/empty values with NA. Small operations which I don\u2019t think will take much time for like whole 77K records I have for just prototyping \n\nMy question is , loading the Silver table just take a lot of time , 15 minutes and that too for 77K record. \n\nI am very new to spark , being from your  traditional Software engineer background.\nI would expect that any kind of transformation can be done parallel in workers but I see only 1 worker running in Spark UI. I see DLT uses structured streaming so does it just run jobs in serial way like one after another and then gather all data set in one data frame and return ? \n\nCan\u2019t see logs in workers because of whole Unity Catalog and I don\u2019t know how to enable those. Gave some configs but it dint work.  \n\n\nIf you guys have any alternative to this or pattern which is better than this , please suggest. That will be a great help for me. I don\u2019t seem to find a lot on internet about this particular use case. \n\n\n", "author_fullname": "t2_89nifnvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help needed on Delta Live Table ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhumcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710778500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have delta live table which reads the CSV Files from ADLS and loads into bronze and silver. Your typical Data Pipeline.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s not very complicated pipeline. With auto loader it instantly picks the files and loads to bronze in Unity Catalog but in between I am doing two operations -&lt;/p&gt;\n\n&lt;p&gt;1) Usual Deduplication based on the Hash value &lt;/p&gt;\n\n&lt;p&gt;2) Based on a configuration CSV file , I pick operations like convert a elements in a  column to Upper case , or trim the elements of the column or you replace all null/empty values with NA. Small operations which I don\u2019t think will take much time for like whole 77K records I have for just prototyping &lt;/p&gt;\n\n&lt;p&gt;My question is , loading the Silver table just take a lot of time , 15 minutes and that too for 77K record. &lt;/p&gt;\n\n&lt;p&gt;I am very new to spark , being from your  traditional Software engineer background.\nI would expect that any kind of transformation can be done parallel in workers but I see only 1 worker running in Spark UI. I see DLT uses structured streaming so does it just run jobs in serial way like one after another and then gather all data set in one data frame and return ? &lt;/p&gt;\n\n&lt;p&gt;Can\u2019t see logs in workers because of whole Unity Catalog and I don\u2019t know how to enable those. Gave some configs but it dint work.  &lt;/p&gt;\n\n&lt;p&gt;If you guys have any alternative to this or pattern which is better than this , please suggest. That will be a great help for me. I don\u2019t seem to find a lot on internet about this particular use case. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhumcc", "is_robot_indexable": true, "report_reasons": null, "author": "voucherwolves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhumcc/help_needed_on_delta_live_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhumcc/help_needed_on_delta_live_table/", "subreddit_subscribers": 169964, "created_utc": 1710778500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm seeking advice on automating the transformation process for ERP data from raw (csv) to silver layer in Microsoft Fabrics. Here are some factors I'm considering for automation:\n\n-Incremental load\n\n-SCD2 handling\n\n-Automatic adjustment for column changes (Insert, Delete) in the underlying data structure while still supporting SCD2\n\n-Generating artificial surrogate keys for streamlined joins in the gold layer (any optimization suggestions, like z-order for delta tables?\n\n-Automatic adaptation of column metadata from ERP\n\n\nWhat are your thoughts on which of these factors can be effectively automated and which might require manual intervention?\n\nLooking forward to your insights and recommendations. Thanks in advance!", "author_fullname": "t2_2q1g4lkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Automating ERP Data Transformation to Silver Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhprz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710765699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking advice on automating the transformation process for ERP data from raw (csv) to silver layer in Microsoft Fabrics. Here are some factors I&amp;#39;m considering for automation:&lt;/p&gt;\n\n&lt;p&gt;-Incremental load&lt;/p&gt;\n\n&lt;p&gt;-SCD2 handling&lt;/p&gt;\n\n&lt;p&gt;-Automatic adjustment for column changes (Insert, Delete) in the underlying data structure while still supporting SCD2&lt;/p&gt;\n\n&lt;p&gt;-Generating artificial surrogate keys for streamlined joins in the gold layer (any optimization suggestions, like z-order for delta tables?&lt;/p&gt;\n\n&lt;p&gt;-Automatic adaptation of column metadata from ERP&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on which of these factors can be effectively automated and which might require manual intervention?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your insights and recommendations. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhprz9", "is_robot_indexable": true, "report_reasons": null, "author": "tomdg4", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhprz9/seeking_advice_on_automating_erp_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhprz9/seeking_advice_on_automating_erp_data/", "subreddit_subscribers": 169964, "created_utc": 1710765699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are looking into starrocks as the Open Lakehouse platform as query engine over glue(catalog) + iceberg open file format. My question is can anyone comment on the performance perpspective of this setup vs. the Starrocks with its own native storage? Will it still be levrage its outstanding Multi table Join performance as well SIMD based vectorized query engine architecture?  With its distributed MPP in memory query engine, is there any hard memory size constraints when dealing with Extremely large table tables (say billions of rows) when using the LakeHouse Iceberg table as the storage?  Thanks,  ", "author_fullname": "t2_8c6mucf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starrocks + glue + iceberg performance vs. starrocks native storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhj9jk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710739707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking into starrocks as the Open Lakehouse platform as query engine over glue(catalog) + iceberg open file format. My question is can anyone comment on the performance perpspective of this setup vs. the Starrocks with its own native storage? Will it still be levrage its outstanding Multi table Join performance as well SIMD based vectorized query engine architecture?  With its distributed MPP in memory query engine, is there any hard memory size constraints when dealing with Extremely large table tables (say billions of rows) when using the LakeHouse Iceberg table as the storage?  Thanks,  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhj9jk", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Armadillo7867", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhj9jk/starrocks_glue_iceberg_performance_vs_starrocks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhj9jk/starrocks_glue_iceberg_performance_vs_starrocks/", "subreddit_subscribers": 169964, "created_utc": 1710739707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are times when I face an unusual amount of tickets being raised for reports and dashboards. I wonder what might be a good way to give some sort of self-serve analytics.\n\nHave you guys faced a similar bottleneck? If yes, please advise on how you navigated it.", "author_fullname": "t2_1upujjf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to enable self-serve analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhdbdc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710721202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are times when I face an unusual amount of tickets being raised for reports and dashboards. I wonder what might be a good way to give some sort of self-serve analytics.&lt;/p&gt;\n\n&lt;p&gt;Have you guys faced a similar bottleneck? If yes, please advise on how you navigated it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhdbdc", "is_robot_indexable": true, "report_reasons": null, "author": "thehungryindian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhdbdc/how_to_enable_selfserve_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhdbdc/how_to_enable_selfserve_analytics/", "subreddit_subscribers": 169964, "created_utc": 1710721202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently got my first offer as a Data Engineer coming out of school and wanted to understand the possibilities of moving out of DE into SWE at some point. Has any body made this jump before? Was it hard or easy? What kinds of skills did you have to pick up either on the job or by yourself?\n\nThanks!", "author_fullname": "t2_hfl4w59z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Going from DE to SWE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhzgyi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710790126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got my first offer as a Data Engineer coming out of school and wanted to understand the possibilities of moving out of DE into SWE at some point. Has any body made this jump before? Was it hard or easy? What kinds of skills did you have to pick up either on the job or by yourself?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bhzgyi", "is_robot_indexable": true, "report_reasons": null, "author": "digging_for_memories", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhzgyi/going_from_de_to_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhzgyi/going_from_de_to_swe/", "subreddit_subscribers": 169964, "created_utc": 1710790126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am using fivetran to sync all my data from lever to snowflake. I have followed the fivetran connector guide and created an api key and provided it, however it has been 13+ hrs and still the sync is going on, I can see some of the data populated in snowflake and it is getting populated gradually as I can tell by observing the fivetran sync times and I thing the problem is with the api not allowing too much data to be extracted by the connector. I have read somewhere that if this is the case we can know by the warnings thrown by fivetran but in my case no warnings were thrown now I am just confused on what to do should I let the sync be running as this doesn\u2019t imply any charges or should I pause it until furthyer investigation with fivetran and has anyone faced anything similar with this connector ??", "author_fullname": "t2_99ac7bms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran lever connector initial sync taking too long", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhxugv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710786267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using fivetran to sync all my data from lever to snowflake. I have followed the fivetran connector guide and created an api key and provided it, however it has been 13+ hrs and still the sync is going on, I can see some of the data populated in snowflake and it is getting populated gradually as I can tell by observing the fivetran sync times and I thing the problem is with the api not allowing too much data to be extracted by the connector. I have read somewhere that if this is the case we can know by the warnings thrown by fivetran but in my case no warnings were thrown now I am just confused on what to do should I let the sync be running as this doesn\u2019t imply any charges or should I pause it until furthyer investigation with fivetran and has anyone faced anything similar with this connector ??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhxugv", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting-Fee-2836", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhxugv/fivetran_lever_connector_initial_sync_taking_too/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhxugv/fivetran_lever_connector_initial_sync_taking_too/", "subreddit_subscribers": 169964, "created_utc": 1710786267.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}