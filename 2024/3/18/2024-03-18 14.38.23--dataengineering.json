{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: Currently a self taught data engineer with about 3 years of DE experience and 3 years of data analyst experience. Graduated with a non-CS quant major from a decent university and make around $130k a year at a remote MCOL job. \n\nHas anyone made it from a mid-tier paying job to a high paying job without moving to a HCOL area?\n\nI'm wondering if the jobs themselves that are higher paying are actually more difficult in complexity? My current company has a pretty modern tech stack with good engineering practices, at least I think.\n\nIs the best way to get those higher paying jobs to get a referral and grind leetcode/learn fundamentals?", "author_fullname": "t2_fhmml14j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone made the jump from a $100-150k to +$200k position?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh6jx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 159, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 159, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710704576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: Currently a self taught data engineer with about 3 years of DE experience and 3 years of data analyst experience. Graduated with a non-CS quant major from a decent university and make around $130k a year at a remote MCOL job. &lt;/p&gt;\n\n&lt;p&gt;Has anyone made it from a mid-tier paying job to a high paying job without moving to a HCOL area?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if the jobs themselves that are higher paying are actually more difficult in complexity? My current company has a pretty modern tech stack with good engineering practices, at least I think.&lt;/p&gt;\n\n&lt;p&gt;Is the best way to get those higher paying jobs to get a referral and grind leetcode/learn fundamentals?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bh6jx2", "is_robot_indexable": true, "report_reasons": null, "author": "Capable-Jicama2155", "discussion_type": null, "num_comments": 119, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh6jx2/has_anyone_made_the_jump_from_a_100150k_to_200k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh6jx2/has_anyone_made_the_jump_from_a_100150k_to_200k/", "subreddit_subscribers": 169873, "created_utc": 1710704576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,  \nI am seeking guidance how can i become good overall engineer ? What i can learn to be better ?\n\n**Experience**: 3 years  \n**Age**: 27  \n**Current role**: I am working on low latency data pipelines using scala, flink, cassandra, redis, s3 and kafka in a product based company.  \nOther tools i am using prometheus, grafana for monitoring and kubernetes, docker for deploying apps.  \n**Leetcode rating**: My Leetcode rating is 1960 and currently practing more on it. solved around 1200 problems. Using cpp.  \n**System Design**: Completed Design data intensive applicaion book and Grokking the system design course.  \n**Low level Design**: working on it this too using cpp.\n\nRecently I read  design data intensive application book, reading it once again to get more out of it.  Next i am thinking of reading one os book (Galvin), microservices by sam richardson book. I also work on understanding how cassandra redis and kafka works internally in the free time.\n\nI didn't try to learn kuberentes and docker in detail because they are vast in terms of concepts. My current job does allow only to used them as platform for deploying apps and there are other engineers for devops work.\n\nI want your guidance on what can i do to become a good successful engineer and  join good company in future. I am not thinking of switching in few months becomes i want grind more in coming months and my current job able me to provide good free time.  **Is anything needs to be changed or added to what i am doing currently ?** Any comment would be much appreciated. Thanks in advance.\n\nSorry for my bad english.", "author_fullname": "t2_aqmxwdoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to become a good engineer ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh2wha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710697134.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710695781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI am seeking guidance how can i become good overall engineer ? What i can learn to be better ?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Experience&lt;/strong&gt;: 3 years&lt;br/&gt;\n&lt;strong&gt;Age&lt;/strong&gt;: 27&lt;br/&gt;\n&lt;strong&gt;Current role&lt;/strong&gt;: I am working on low latency data pipelines using scala, flink, cassandra, redis, s3 and kafka in a product based company.&lt;br/&gt;\nOther tools i am using prometheus, grafana for monitoring and kubernetes, docker for deploying apps.&lt;br/&gt;\n&lt;strong&gt;Leetcode rating&lt;/strong&gt;: My Leetcode rating is 1960 and currently practing more on it. solved around 1200 problems. Using cpp.&lt;br/&gt;\n&lt;strong&gt;System Design&lt;/strong&gt;: Completed Design data intensive applicaion book and Grokking the system design course.&lt;br/&gt;\n&lt;strong&gt;Low level Design&lt;/strong&gt;: working on it this too using cpp.&lt;/p&gt;\n\n&lt;p&gt;Recently I read  design data intensive application book, reading it once again to get more out of it.  Next i am thinking of reading one os book (Galvin), microservices by sam richardson book. I also work on understanding how cassandra redis and kafka works internally in the free time.&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t try to learn kuberentes and docker in detail because they are vast in terms of concepts. My current job does allow only to used them as platform for deploying apps and there are other engineers for devops work.&lt;/p&gt;\n\n&lt;p&gt;I want your guidance on what can i do to become a good successful engineer and  join good company in future. I am not thinking of switching in few months becomes i want grind more in coming months and my current job able me to provide good free time.  &lt;strong&gt;Is anything needs to be changed or added to what i am doing currently ?&lt;/strong&gt; Any comment would be much appreciated. Thanks in advance.&lt;/p&gt;\n\n&lt;p&gt;Sorry for my bad english.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bh2wha", "is_robot_indexable": true, "report_reasons": null, "author": "AggravatingParsnip89", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/", "subreddit_subscribers": 169873, "created_utc": 1710695781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently joined a new company with almost zero up to date documentation, with loads and loads of tables in different schemas.\n\nHow should I go about documenting/understanding what each table means and each field?\n\nAnd when should I start suggesting changes to some tables?\n\nI'm currently just building an ERD using dbdiagram.io with comments as much as I can but not sure if it's the right way to go.", "author_fullname": "t2_apdhbvv7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tables all the way down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh0jww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710689960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently joined a new company with almost zero up to date documentation, with loads and loads of tables in different schemas.&lt;/p&gt;\n\n&lt;p&gt;How should I go about documenting/understanding what each table means and each field?&lt;/p&gt;\n\n&lt;p&gt;And when should I start suggesting changes to some tables?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently just building an ERD using dbdiagram.io with comments as much as I can but not sure if it&amp;#39;s the right way to go.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bh0jww", "is_robot_indexable": true, "report_reasons": null, "author": "Agile-Scene-2465", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh0jww/tables_all_the_way_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh0jww/tables_all_the_way_down/", "subreddit_subscribers": 169873, "created_utc": 1710689960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey reddit, I'm a data architect with 10 years of experience and for the past few years I've been [mentoring data engineers](https://mentorcruise.com/mentor/lewisgavin/) on a site called MentorCruise.\n\nI've not long become a new dad so I can't take on as many mentees as I used to. But a lot of the advice I'm giving applies to most people looking to progress in their DE career. (I even saw a post here yesterday asking [about becoming a good engineer](https://www.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/))\n\nI get an overwhelming number of applications but can't accept as many as I'd like due to just becoming a dad for the first time. So I've decided to create a course.\n\n**This is where you come in.** I'd love to get your feedback on the course as a fellow data engineer to ensure I'm on the right track.\n\nHere is the link to the course site: [https://next-level-data.mailchimpsites.com/](https://next-level-data.mailchimpsites.com/)\n\nI'd love your feedback on:\n\n* the course contents\n* the course price\n* the website/experience in general\n\nThis may seem like I'm farming for signups but I'm genuinely just getting started with this and all I'm looking for is feedback. \n\nHowever, if you do look at the course and think it will be of interest to you then let me know. I'm happy to trade feedback for a much lower price as a thank you!\n\nThanks in advance :)", "author_fullname": "t2_9kwrl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking feedback on data engineering career course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhm35q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710751932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey reddit, I&amp;#39;m a data architect with 10 years of experience and for the past few years I&amp;#39;ve been &lt;a href=\"https://mentorcruise.com/mentor/lewisgavin/\"&gt;mentoring data engineers&lt;/a&gt; on a site called MentorCruise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve not long become a new dad so I can&amp;#39;t take on as many mentees as I used to. But a lot of the advice I&amp;#39;m giving applies to most people looking to progress in their DE career. (I even saw a post here yesterday asking &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/\"&gt;about becoming a good engineer&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;I get an overwhelming number of applications but can&amp;#39;t accept as many as I&amp;#39;d like due to just becoming a dad for the first time. So I&amp;#39;ve decided to create a course.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This is where you come in.&lt;/strong&gt; I&amp;#39;d love to get your feedback on the course as a fellow data engineer to ensure I&amp;#39;m on the right track.&lt;/p&gt;\n\n&lt;p&gt;Here is the link to the course site: &lt;a href=\"https://next-level-data.mailchimpsites.com/\"&gt;https://next-level-data.mailchimpsites.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love your feedback on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the course contents&lt;/li&gt;\n&lt;li&gt;the course price&lt;/li&gt;\n&lt;li&gt;the website/experience in general&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This may seem like I&amp;#39;m farming for signups but I&amp;#39;m genuinely just getting started with this and all I&amp;#39;m looking for is feedback. &lt;/p&gt;\n\n&lt;p&gt;However, if you do look at the course and think it will be of interest to you then let me know. I&amp;#39;m happy to trade feedback for a much lower price as a thank you!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?auto=webp&amp;s=826577a6dff9c2822defccc6abeb8a3791a4f378", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5f6652f61b1a0e56833d08f0686d30e2329eb05", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f365e7495e0645aa0c8774a5cfed8409b5674b4f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16de7d5daef6611323ac7642d9f5f287849b64a3", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f0b36b08ab5346200cd441068e98c3ac852d9ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c93bf08bf7fa6625c90d6c3f183341ce83f477e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6cb598cce81c0d8d0949be2acb444bf84ad2b82", "width": 1080, "height": 567}], "variants": {}, "id": "zCh4-DKpuT8tl8-SzPXCdRevHjz5DJZ0d8U0sLHbmXw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bhm35q", "is_robot_indexable": true, "report_reasons": null, "author": "gavlaaaaaaaa", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhm35q/seeking_feedback_on_data_engineering_career_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhm35q/seeking_feedback_on_data_engineering_career_course/", "subreddit_subscribers": 169873, "created_utc": 1710751932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "About 6 months ago, my company made the decision to purchase Ab initio. Based on my research, it, along with other licenseing ETL tools, seems to be on the decline. My company has been rolling it out to various teams and mine is up next. We will be expectsd to migrate current pipelines and new ones to Ab Initio. Currently, we use python + SQL scripts ran on Snowflake to transform our raw data ( files in AWS S3) and then just load into Snowflake tables.\n\nI'm not ready to leave this company, but I'm not so excited about developing a skill set in a dying tool. Still, is there anyway to make the most out of this situation? Is there anything to learn diving deep in ab Initio and learning all I can about the tool? Should I use the tool but focus my cv on the challenges I overcame, not the tool used? Or, should I hyper specialize in ab Initio and become an expert who drifts around to different companies to maintain their pipelines, requiring a large salary for this niche and drying skill set.\n\n", "author_fullname": "t2_jlj0h3u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting the most of out an old tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgymae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710685108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About 6 months ago, my company made the decision to purchase Ab initio. Based on my research, it, along with other licenseing ETL tools, seems to be on the decline. My company has been rolling it out to various teams and mine is up next. We will be expectsd to migrate current pipelines and new ones to Ab Initio. Currently, we use python + SQL scripts ran on Snowflake to transform our raw data ( files in AWS S3) and then just load into Snowflake tables.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not ready to leave this company, but I&amp;#39;m not so excited about developing a skill set in a dying tool. Still, is there anyway to make the most out of this situation? Is there anything to learn diving deep in ab Initio and learning all I can about the tool? Should I use the tool but focus my cv on the challenges I overcame, not the tool used? Or, should I hyper specialize in ab Initio and become an expert who drifts around to different companies to maintain their pipelines, requiring a large salary for this niche and drying skill set.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bgymae", "is_robot_indexable": true, "report_reasons": null, "author": "CriticalSouth3447", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bgymae/getting_the_most_of_out_an_old_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bgymae/getting_the_most_of_out_an_old_tool/", "subreddit_subscribers": 169873, "created_utc": 1710685108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I usually work with Databricks and I just started learning how Data Factory works. From my understanding, Data Factory can be used for data transformations, as well as for the Extract and Load parts of an ETL process. But I don\u2019t see it used for transformations by my client.\n\nMe and my colleagues use Data Factory for this client, but from what I can see (since this project started years before me arriving in the company) the pipelines 90% of the time run notebooks and send emails when the notebooks fail. Is this the norm?", "author_fullname": "t2_k97u3vqd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhnh1m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710757729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually work with Databricks and I just started learning how Data Factory works. From my understanding, Data Factory can be used for data transformations, as well as for the Extract and Load parts of an ETL process. But I don\u2019t see it used for transformations by my client.&lt;/p&gt;\n\n&lt;p&gt;Me and my colleagues use Data Factory for this client, but from what I can see (since this project started years before me arriving in the company) the pipelines 90% of the time run notebooks and send emails when the notebooks fail. Is this the norm?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhnh1m", "is_robot_indexable": true, "report_reasons": null, "author": "IlMagodelLusso", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhnh1m/azure_data_factory_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhnh1m/azure_data_factory_use/", "subreddit_subscribers": 169873, "created_utc": 1710757729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the past few months I have been learning about data strategies that employ Apache Iceberg with Snowflake DB and Apache Spark &amp; I have compiled my learnings into a short article.\n\n[https://medium.com/@pbd\\_94/skiing-with-snowflake-b196e8f7e2e6](https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6)\n\nFire away.", "author_fullname": "t2_11mmk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Supercharge your compute strategy with Apache Iceberg, Snowflake, Apache Spark, AWS Glue &amp; Project Nessie", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhhxs0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710734969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the past few months I have been learning about data strategies that employ Apache Iceberg with Snowflake DB and Apache Spark &amp;amp; I have compiled my learnings into a short article.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6\"&gt;https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Fire away.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?auto=webp&amp;s=0380b1add45a87f89967d78ac69b9471fa1758ec", "width": 1200, "height": 915}, "resolutions": [{"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df7f210d8f4b10b04cc8590372be4515d0dc2af6", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=770379812d74ee166858c6c46f32e994adb05d37", "width": 216, "height": 164}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f3dee8a276d36599de5bf5f8205968fc92ddb05", "width": 320, "height": 244}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd25d5968eb8136f3f1109b294c9cd66b8b6d3bc", "width": 640, "height": 488}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a3ba06688cc1992e79c2ea280839cd13e4b0063", "width": 960, "height": 732}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5f371a9a1991838b4c850e5adb62d389bde796a", "width": 1080, "height": 823}], "variants": {}, "id": "csQ4U8qiLyOe4t7uz2oyBKWIrTrW4Z3Y939nxE1g1Bg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhhxs0", "is_robot_indexable": true, "report_reasons": null, "author": "Pbd1194", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhhxs0/supercharge_your_compute_strategy_with_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhhxs0/supercharge_your_compute_strategy_with_apache/", "subreddit_subscribers": 169873, "created_utc": 1710734969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if anyone here moved from a SAP BW background into core Data Engineering field? Is this even possible? Looks like SAP experience is not strongly considered as a transferable skill to other tools. I have been learning data structures, streaming tools, databases etc. because its a very interesting space but unsure if I could ever switch to a DE job complementing with my SAP BW exp. ", "author_fullname": "t2_7qvd84fd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move from SAP BW to Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhfmka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710727789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if anyone here moved from a SAP BW background into core Data Engineering field? Is this even possible? Looks like SAP experience is not strongly considered as a transferable skill to other tools. I have been learning data structures, streaming tools, databases etc. because its a very interesting space but unsure if I could ever switch to a DE job complementing with my SAP BW exp. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhfmka", "is_robot_indexable": true, "report_reasons": null, "author": "SignificantOpinion92", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhfmka/how_to_move_from_sap_bw_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhfmka/how_to_move_from_sap_bw_to_data_engineering/", "subreddit_subscribers": 169873, "created_utc": 1710727789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Team,\n\nlike all data engineers, we want to have the modern data stack for a data transformation initiative. My context is that I'm having trouble getting management approval on this (mostly due to costs) and I think it is mostly due to data literacy gaps. We're doing a lot of that (teaching them) but it seems it would take more than just informing them to get them to invest. It's a large enterprise that doesn't have a lot of consumer data, but lots of other data opportunities to explore for analytics use cases in the form of supplier, vendor, partner, location data and varying industries. They currently don't have an enterprise wide implementation of anything. ALSO, they don't trust the cloud and currently is a heavy MS user.\n\nso my question to the group is, given this context, what would you advise me to do as next step? Here are my thoughts but I'm open to ideas and stories. I would recommend to at least start with building a business intelligence platform with datawarehouse first. I'm open to using ms products if it will help speed things up a long e.g. power bi and ms sql as the datawarehouse. Then I'll probably just look for an open source tool (maybe - i'm not sure what ms has in terms of on-prem integration tool) to use for ingestion then a separate server for machine learning deployments (we can use local laptops for development). my only problem as well is the encryption/ hashing tools for PII, i think. use this for a few years to prove the use cases, then hopefully gain enough approval to invest in an on-prem mds.   \n  \nI was thinking the tech debt would be worth it just to get things started and prove that having these things in place works to get more buy in later on. would really appreciate everyone's thoughts thank you!", "author_fullname": "t2_56myc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having trouble moving to MDS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhdssw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710722815.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710722568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Team,&lt;/p&gt;\n\n&lt;p&gt;like all data engineers, we want to have the modern data stack for a data transformation initiative. My context is that I&amp;#39;m having trouble getting management approval on this (mostly due to costs) and I think it is mostly due to data literacy gaps. We&amp;#39;re doing a lot of that (teaching them) but it seems it would take more than just informing them to get them to invest. It&amp;#39;s a large enterprise that doesn&amp;#39;t have a lot of consumer data, but lots of other data opportunities to explore for analytics use cases in the form of supplier, vendor, partner, location data and varying industries. They currently don&amp;#39;t have an enterprise wide implementation of anything. ALSO, they don&amp;#39;t trust the cloud and currently is a heavy MS user.&lt;/p&gt;\n\n&lt;p&gt;so my question to the group is, given this context, what would you advise me to do as next step? Here are my thoughts but I&amp;#39;m open to ideas and stories. I would recommend to at least start with building a business intelligence platform with datawarehouse first. I&amp;#39;m open to using ms products if it will help speed things up a long e.g. power bi and ms sql as the datawarehouse. Then I&amp;#39;ll probably just look for an open source tool (maybe - i&amp;#39;m not sure what ms has in terms of on-prem integration tool) to use for ingestion then a separate server for machine learning deployments (we can use local laptops for development). my only problem as well is the encryption/ hashing tools for PII, i think. use this for a few years to prove the use cases, then hopefully gain enough approval to invest in an on-prem mds.   &lt;/p&gt;\n\n&lt;p&gt;I was thinking the tech debt would be worth it just to get things started and prove that having these things in place works to get more buy in later on. would really appreciate everyone&amp;#39;s thoughts thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhdssw", "is_robot_indexable": true, "report_reasons": null, "author": "saintmichel", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhdssw/having_trouble_moving_to_mds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhdssw/having_trouble_moving_to_mds/", "subreddit_subscribers": 169873, "created_utc": 1710722568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a pipeline in GCP that involves extracting data from a PDF, modifying it, and then reconstructing the PDF with the original layout. Process-wise, this isn't terribly complicated, but I'm struggling with a tool to preserve the layout information to help with reconstructing the PDF after all is said and done. Does anyone know any tools, GCP-native or otherwise, I could use to capture/utilize the layout?", "author_fullname": "t2_o1ln6yad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recreating document layout in GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh5cf1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710701725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a pipeline in GCP that involves extracting data from a PDF, modifying it, and then reconstructing the PDF with the original layout. Process-wise, this isn&amp;#39;t terribly complicated, but I&amp;#39;m struggling with a tool to preserve the layout information to help with reconstructing the PDF after all is said and done. Does anyone know any tools, GCP-native or otherwise, I could use to capture/utilize the layout?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bh5cf1", "is_robot_indexable": true, "report_reasons": null, "author": "_tr9800a_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh5cf1/recreating_document_layout_in_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh5cf1/recreating_document_layout_in_gcp/", "subreddit_subscribers": 169873, "created_utc": 1710701725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone been using DuckDB in production? Any gotchas? ", "author_fullname": "t2_5q1b2lj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using DuckDB in Prod?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bhr3ex", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710769540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been using DuckDB in production? Any gotchas? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhr3ex", "is_robot_indexable": true, "report_reasons": null, "author": "Guilty-Commission435", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhr3ex/anyone_using_duckdb_in_prod/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhr3ex/anyone_using_duckdb_in_prod/", "subreddit_subscribers": 169873, "created_utc": 1710769540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started to enroll data enginnering course and these are the topics i am going to learn \n\nMicrosoft SQL Essentials\n\n Implementing a SQL Data Warehouse\n\n Python Programming Fundamentals\n\n Introduction to Agile Development and Scrum \n\nAzure Data Fundamentals \n\nMLOps Tools, MLflow and Hugging Face \n\n Azure Data Engineer Associate\n\n Are these topics enough to get my first data enginnering job ?", "author_fullname": "t2_9z3by6h5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hello", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bhqrrf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710768627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started to enroll data enginnering course and these are the topics i am going to learn &lt;/p&gt;\n\n&lt;p&gt;Microsoft SQL Essentials&lt;/p&gt;\n\n&lt;p&gt;Implementing a SQL Data Warehouse&lt;/p&gt;\n\n&lt;p&gt;Python Programming Fundamentals&lt;/p&gt;\n\n&lt;p&gt;Introduction to Agile Development and Scrum &lt;/p&gt;\n\n&lt;p&gt;Azure Data Fundamentals &lt;/p&gt;\n\n&lt;p&gt;MLOps Tools, MLflow and Hugging Face &lt;/p&gt;\n\n&lt;p&gt;Azure Data Engineer Associate&lt;/p&gt;\n\n&lt;p&gt;Are these topics enough to get my first data enginnering job ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhqrrf", "is_robot_indexable": true, "report_reasons": null, "author": "Primary_Student_4851", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhqrrf/hello/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhqrrf/hello/", "subreddit_subscribers": 169873, "created_utc": 1710768627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nTo be very up front: I run a small saas focused on real-time metrics. I recently announced a new feature/product I'd love some feedback on (and potentially beta testers).  [Blog Post](https://aggregations.io/blog/autodocs-coming-soon).\n\nThe idea is simple: you forward your event stream and you get a searchable schema of your events, &amp; their properties along with statistics/distributions of the field values. \n\nThe other element comes in the form of a per-version changelog, with alerting for things like type changes, cardinality fluctuations, etc. \n\nI won't go too far into the technical details, unless people are interested -- but building this is obviously has been a very intricate and complex project, I'm pretty happy with the result so far :)\n\n&amp;#x200B;\n\nI've built a system like this multiple times in the past at larger companies, so I know the value it can provide -- I'm just not sure (1) how to express it well and (2) what other scenarios/ features might be useful.\n\nFor example, post launch I know I want to add in more collaboration/annotation features (to make it more of a \"documentation hub\" for analysts, data producers, etc) -- but other things I've built before, may not be super applicable. \n\nIn the past, I enabled \"generate SQL to fetch this property in different languages\" because JSON functions can be tricky and some payloads are gnarly. I don't know if that is widely applicable? \n\n&amp;#x200B;\n\nIs this a thing that would help you? What struggles do you have with documenting your events, etc?\n\nAny thoughts or feedback would be greatly appreciated! ", "author_fullname": "t2_vpdufq3pm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics Events Documentation &amp; Monitoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bhpxx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710766234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;To be very up front: I run a small saas focused on real-time metrics. I recently announced a new feature/product I&amp;#39;d love some feedback on (and potentially beta testers).  &lt;a href=\"https://aggregations.io/blog/autodocs-coming-soon\"&gt;Blog Post&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The idea is simple: you forward your event stream and you get a searchable schema of your events, &amp;amp; their properties along with statistics/distributions of the field values. &lt;/p&gt;\n\n&lt;p&gt;The other element comes in the form of a per-version changelog, with alerting for things like type changes, cardinality fluctuations, etc. &lt;/p&gt;\n\n&lt;p&gt;I won&amp;#39;t go too far into the technical details, unless people are interested -- but building this is obviously has been a very intricate and complex project, I&amp;#39;m pretty happy with the result so far :)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a system like this multiple times in the past at larger companies, so I know the value it can provide -- I&amp;#39;m just not sure (1) how to express it well and (2) what other scenarios/ features might be useful.&lt;/p&gt;\n\n&lt;p&gt;For example, post launch I know I want to add in more collaboration/annotation features (to make it more of a &amp;quot;documentation hub&amp;quot; for analysts, data producers, etc) -- but other things I&amp;#39;ve built before, may not be super applicable. &lt;/p&gt;\n\n&lt;p&gt;In the past, I enabled &amp;quot;generate SQL to fetch this property in different languages&amp;quot; because JSON functions can be tricky and some payloads are gnarly. I don&amp;#39;t know if that is widely applicable? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this a thing that would help you? What struggles do you have with documenting your events, etc?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts or feedback would be greatly appreciated! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?auto=webp&amp;s=d82e0bdcb768bd94828b24f022cf6710f5fd134f", "width": 1024, "height": 649}, "resolutions": [{"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df99c7f6cc2a6e1b07f115e66572f9d72a9b8c92", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b006cd92fff54c42cf968e704fd21d6aaedd845d", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=03d22aa565ec345cb9aa8586fb78133fe3868106", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8739eb0b61a84385b26c321db0d6125c4c1f1835", "width": 640, "height": 405}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=216181d41f4abfddef7cbbcc1b76ad8259974862", "width": 960, "height": 608}], "variants": {}, "id": "-IguNufVGIqW9ENdLi7KpG31ptOTdRhW0AL9zjARo1g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhpxx2", "is_robot_indexable": true, "report_reasons": null, "author": "jsneedles", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhpxx2/analytics_events_documentation_monitoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhpxx2/analytics_events_documentation_monitoring/", "subreddit_subscribers": 169873, "created_utc": 1710766234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm seeking advice on automating the transformation process for ERP data from raw (csv) to silver layer in Microsoft Fabrics. Here are some factors I'm considering for automation:\n\n-Incremental load\n\n-SCD2 handling\n\n-Automatic adjustment for column changes (Insert, Delete) in the underlying data structure while still supporting SCD2\n\n-Generating artificial surrogate keys for streamlined joins in the gold layer (any optimization suggestions, like z-order for delta tables?\n\n-Automatic adaptation of column metadata from ERP\n\n\nWhat are your thoughts on which of these factors can be effectively automated and which might require manual intervention?\n\nLooking forward to your insights and recommendations. Thanks in advance!", "author_fullname": "t2_2q1g4lkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Automating ERP Data Transformation to Silver Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bhprz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710765699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking advice on automating the transformation process for ERP data from raw (csv) to silver layer in Microsoft Fabrics. Here are some factors I&amp;#39;m considering for automation:&lt;/p&gt;\n\n&lt;p&gt;-Incremental load&lt;/p&gt;\n\n&lt;p&gt;-SCD2 handling&lt;/p&gt;\n\n&lt;p&gt;-Automatic adjustment for column changes (Insert, Delete) in the underlying data structure while still supporting SCD2&lt;/p&gt;\n\n&lt;p&gt;-Generating artificial surrogate keys for streamlined joins in the gold layer (any optimization suggestions, like z-order for delta tables?&lt;/p&gt;\n\n&lt;p&gt;-Automatic adaptation of column metadata from ERP&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on which of these factors can be effectively automated and which might require manual intervention?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your insights and recommendations. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhprz9", "is_robot_indexable": true, "report_reasons": null, "author": "tomdg4", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhprz9/seeking_advice_on_automating_erp_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhprz9/seeking_advice_on_automating_erp_data/", "subreddit_subscribers": 169873, "created_utc": 1710765699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are looking into starrocks as the Open Lakehouse platform as query engine over glue(catalog) + iceberg open file format. My question is can anyone comment on the performance perpspective of this setup vs. the Starrocks with its own native storage? Will it still be levrage its outstanding Multi table Join performance as well SIMD based vectorized query engine architecture?  With its distributed MPP in memory query engine, is there any hard memory size constraints when dealing with Extremely large table tables (say billions of rows) when using the LakeHouse Iceberg table as the storage?  Thanks,  ", "author_fullname": "t2_8c6mucf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starrocks + glue + iceberg performance vs. starrocks native storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhj9jk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710739707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking into starrocks as the Open Lakehouse platform as query engine over glue(catalog) + iceberg open file format. My question is can anyone comment on the performance perpspective of this setup vs. the Starrocks with its own native storage? Will it still be levrage its outstanding Multi table Join performance as well SIMD based vectorized query engine architecture?  With its distributed MPP in memory query engine, is there any hard memory size constraints when dealing with Extremely large table tables (say billions of rows) when using the LakeHouse Iceberg table as the storage?  Thanks,  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhj9jk", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Armadillo7867", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhj9jk/starrocks_glue_iceberg_performance_vs_starrocks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhj9jk/starrocks_glue_iceberg_performance_vs_starrocks/", "subreddit_subscribers": 169873, "created_utc": 1710739707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE folks,\n\nCreated this account to gather feedback and insights from y'all. I'm fairly new to data engineering/governance. My company has been using Informatica Cloud for quite some time now.\n\n&amp;#x200B;\n\nMy organization wants to dive into the whole Generative AI segment. This is at a very nascent stage where nothing is really decided or planned, beyond the database. And the DB of choice for this initiative is Pinecone. Other than this, nothing else has been decided yet, what the architecture would be like, how and where we are planning to use Pinecone. I'm aware vector DBs are great for extending the capabilities of LLMs especially since they're infrequently trained, but again.. I have no idea if that is the use case here, or something else entirely since the company has not revealed anything around that.\n\n&amp;#x200B;\n\nThat being said, I am trying to figure out what would be the best way to catalog the metadata from Pinecone. Like I said, we're on Informatica and are using the Cloud Data Governance and Catalog tool for our operations - however this tool does not have any vector DB cataloging capabilities, nor does Informatica have any Pinecone connectors planned for the near future.\n\n&amp;#x200B;\n\nCan you share some insights, maybe articles or just your thoughts on how I should be approaching this problem given the context around it? Apologies for such a broad question.", "author_fullname": "t2_wer0lc400", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cataloging vector databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhgung", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710731530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE folks,&lt;/p&gt;\n\n&lt;p&gt;Created this account to gather feedback and insights from y&amp;#39;all. I&amp;#39;m fairly new to data engineering/governance. My company has been using Informatica Cloud for quite some time now.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My organization wants to dive into the whole Generative AI segment. This is at a very nascent stage where nothing is really decided or planned, beyond the database. And the DB of choice for this initiative is Pinecone. Other than this, nothing else has been decided yet, what the architecture would be like, how and where we are planning to use Pinecone. I&amp;#39;m aware vector DBs are great for extending the capabilities of LLMs especially since they&amp;#39;re infrequently trained, but again.. I have no idea if that is the use case here, or something else entirely since the company has not revealed anything around that.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;That being said, I am trying to figure out what would be the best way to catalog the metadata from Pinecone. Like I said, we&amp;#39;re on Informatica and are using the Cloud Data Governance and Catalog tool for our operations - however this tool does not have any vector DB cataloging capabilities, nor does Informatica have any Pinecone connectors planned for the near future.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can you share some insights, maybe articles or just your thoughts on how I should be approaching this problem given the context around it? Apologies for such a broad question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhgung", "is_robot_indexable": true, "report_reasons": null, "author": "Key-Imagination-9090", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhgung/cataloging_vector_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhgung/cataloging_vector_databases/", "subreddit_subscribers": 169873, "created_utc": 1710731530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently PoCing the new AWS Zero ETL DynamoDB/Redshift integration, and infrastructure-wise, it seems very straightforward and simple. However, the table created by the integration holds all columns of the DynamoDB source table as a single column as a large [super type](https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html) named `value`, which I have zero experience with!\n\nMy first thought here is to create a view where I extract elements of `value` into distinct, normalized columns:\n\nFor example, suppose the value of one row of `value` is:\n\n    {\n      \"id\": {\n        \"S\": \"RcVpEFPNIAMFytg=\"\n      },\n      \"timestamp\": {\n        \"N\": \"1705090208167\"\n      },\n      \"files\": {\n        \"L\": [\n          {\n            \"S\": \"s3://example/file1.raw\"\n          },\n          {\n            \"S\": \"s3://example/file2.raw\"\n          },\n          {\n            \"S\": \"s3://example/file3.raw\"\n          }\n        ]\n      }\n    }\n\nI could theoretically create a table/view to query this like:\n\n    with s1 as\n    (\n        select\n            json_extract_path_text(json_serialize(e.value), 'id', 'S')::varchar as id,\n            nullif(json_extract_path_text(json_serialize(e.value), 'timestamp', 'N'), '')::bigint as timestamp,\n            json_extract_path_text(json_serialize(e.value), 'files', 'L') as files\n        from \"example\".\"public\".\"example\" e\n    )\n    select *\n    from s1\n    order by s1.timestamp desc;\n\nHowever, I've never worked with super variables before, and I have two big questions:\n\n1. Am I right to be extracting `super` type columns into normalized columns like this, or should I be querying them as they are?\n2. This is a bit more of a detailed question, but how do I successfully extract arrays of `super` type columns? For example, in the above, the new `data` column would still be a `super` (`[{\"S\":\"s3://example/file1.raw\"},{\"S\":\"s3://example/file2.raw\"},{\"S\":\"s3://example/file3.raw\"}`), and it's not clear both how this should be extracted and 3N normalized into separate columns?\n\nAny help is appreciated, thanks!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should Redshift columns of type super be queried?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhexl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710725792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently PoCing the new AWS Zero ETL DynamoDB/Redshift integration, and infrastructure-wise, it seems very straightforward and simple. However, the table created by the integration holds all columns of the DynamoDB source table as a single column as a large &lt;a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html\"&gt;super type&lt;/a&gt; named &lt;code&gt;value&lt;/code&gt;, which I have zero experience with!&lt;/p&gt;\n\n&lt;p&gt;My first thought here is to create a view where I extract elements of &lt;code&gt;value&lt;/code&gt; into distinct, normalized columns:&lt;/p&gt;\n\n&lt;p&gt;For example, suppose the value of one row of &lt;code&gt;value&lt;/code&gt; is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;id&amp;quot;: {\n    &amp;quot;S&amp;quot;: &amp;quot;RcVpEFPNIAMFytg=&amp;quot;\n  },\n  &amp;quot;timestamp&amp;quot;: {\n    &amp;quot;N&amp;quot;: &amp;quot;1705090208167&amp;quot;\n  },\n  &amp;quot;files&amp;quot;: {\n    &amp;quot;L&amp;quot;: [\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file1.raw&amp;quot;\n      },\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file2.raw&amp;quot;\n      },\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file3.raw&amp;quot;\n      }\n    ]\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I could theoretically create a table/view to query this like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;with s1 as\n(\n    select\n        json_extract_path_text(json_serialize(e.value), &amp;#39;id&amp;#39;, &amp;#39;S&amp;#39;)::varchar as id,\n        nullif(json_extract_path_text(json_serialize(e.value), &amp;#39;timestamp&amp;#39;, &amp;#39;N&amp;#39;), &amp;#39;&amp;#39;)::bigint as timestamp,\n        json_extract_path_text(json_serialize(e.value), &amp;#39;files&amp;#39;, &amp;#39;L&amp;#39;) as files\n    from &amp;quot;example&amp;quot;.&amp;quot;public&amp;quot;.&amp;quot;example&amp;quot; e\n)\nselect *\nfrom s1\norder by s1.timestamp desc;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;However, I&amp;#39;ve never worked with super variables before, and I have two big questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Am I right to be extracting &lt;code&gt;super&lt;/code&gt; type columns into normalized columns like this, or should I be querying them as they are?&lt;/li&gt;\n&lt;li&gt;This is a bit more of a detailed question, but how do I successfully extract arrays of &lt;code&gt;super&lt;/code&gt; type columns? For example, in the above, the new &lt;code&gt;data&lt;/code&gt; column would still be a &lt;code&gt;super&lt;/code&gt; (&lt;code&gt;[{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file1.raw&amp;quot;},{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file2.raw&amp;quot;},{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file3.raw&amp;quot;}&lt;/code&gt;), and it&amp;#39;s not clear both how this should be extracted and 3N normalized into separate columns?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help is appreciated, thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhexl3", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhexl3/how_should_redshift_columns_of_type_super_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhexl3/how_should_redshift_columns_of_type_super_be/", "subreddit_subscribers": 169873, "created_utc": 1710725792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are times when I face an unusual amount of tickets being raised for reports and dashboards. I wonder what might be a good way to give some sort of self-serve analytics.\n\nHave you guys faced a similar bottleneck? If yes, please advise on how you navigated it.", "author_fullname": "t2_1upujjf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to enable self-serve analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhdbdc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710721202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are times when I face an unusual amount of tickets being raised for reports and dashboards. I wonder what might be a good way to give some sort of self-serve analytics.&lt;/p&gt;\n\n&lt;p&gt;Have you guys faced a similar bottleneck? If yes, please advise on how you navigated it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhdbdc", "is_robot_indexable": true, "report_reasons": null, "author": "thehungryindian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhdbdc/how_to_enable_selfserve_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhdbdc/how_to_enable_selfserve_analytics/", "subreddit_subscribers": 169873, "created_utc": 1710721202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are using SQL Synapse Dedicated Pool which is essentially SQL DW with ELT processes\n\nSo current process look like\n\nLoad raw incremental data from files to staging tables in vendor specific schema in DW\n\nPost that we have stored procs which load that data into main table where incremental data get appended to historic data\n\nFinal layer would be another set of stored proc which move data to dwh schema tables\n\nSo currently everything is in same DW but in different schema and dwh supports various reports (reports are import mode so each interaction does nit hit DW) and some external vendors\n\nThis system could run into issue if number of external vendors that it needs to support increases beyond certain point and if this vendors keep hitting DW with their queries throughout the day as that would give less processing power to other processed\n\nWe are thinking of either creating read only copy of just dwh schema or taking dwh schema entirely on different DW\n\nHowever if we move it out entirely we will have develop equivalent of existing stored procs in either databricks or some other ETL tool and if we just copy dwh to either parquet files or different db then copy process will have to built\n\nIdeally copy would be simple to do but daily copy might take time\n\nWhat would you do in this situation?", "author_fullname": "t2_50u0h0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Separate DB vs Separate Schema for Raw, Processed &amp; DWH?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh4lau", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710699957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are using SQL Synapse Dedicated Pool which is essentially SQL DW with ELT processes&lt;/p&gt;\n\n&lt;p&gt;So current process look like&lt;/p&gt;\n\n&lt;p&gt;Load raw incremental data from files to staging tables in vendor specific schema in DW&lt;/p&gt;\n\n&lt;p&gt;Post that we have stored procs which load that data into main table where incremental data get appended to historic data&lt;/p&gt;\n\n&lt;p&gt;Final layer would be another set of stored proc which move data to dwh schema tables&lt;/p&gt;\n\n&lt;p&gt;So currently everything is in same DW but in different schema and dwh supports various reports (reports are import mode so each interaction does nit hit DW) and some external vendors&lt;/p&gt;\n\n&lt;p&gt;This system could run into issue if number of external vendors that it needs to support increases beyond certain point and if this vendors keep hitting DW with their queries throughout the day as that would give less processing power to other processed&lt;/p&gt;\n\n&lt;p&gt;We are thinking of either creating read only copy of just dwh schema or taking dwh schema entirely on different DW&lt;/p&gt;\n\n&lt;p&gt;However if we move it out entirely we will have develop equivalent of existing stored procs in either databricks or some other ETL tool and if we just copy dwh to either parquet files or different db then copy process will have to built&lt;/p&gt;\n\n&lt;p&gt;Ideally copy would be simple to do but daily copy might take time&lt;/p&gt;\n\n&lt;p&gt;What would you do in this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bh4lau", "is_robot_indexable": true, "report_reasons": null, "author": "dilkushpatel", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh4lau/separate_db_vs_separate_schema_for_raw_processed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh4lau/separate_db_vs_separate_schema_for_raw_processed/", "subreddit_subscribers": 169873, "created_utc": 1710699957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Similar to the docker operator we have in airflow do we have something similar in mage, prefect or Dagster? ", "author_fullname": "t2_iojbk0c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker operator alternative", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh3fsi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710697135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Similar to the docker operator we have in airflow do we have something similar in mage, prefect or Dagster? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bh3fsi", "is_robot_indexable": true, "report_reasons": null, "author": "LogicalEmploy3558", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh3fsi/docker_operator_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh3fsi/docker_operator_alternative/", "subreddit_subscribers": 169873, "created_utc": 1710697135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I\u2019m the founder of Jinbaflow, a new low code workflow tool we\u2019ve been working on with the goal of turning anyone into a data analyst. We want to allow anyone to use English instructions and AI in order to easily transform, analyze, and visualize data. The idea is to keep it super straightforward with a flow-style interface so you can see how the data connects and flows together.\n\nWe want to add more AI-empowered tools beyond the code generation to allow you to focus less on coding and more on the data. Currently, we're using GPT-4 and we're super happy with the results and how good it is at generating Pandas code for us.\n\nWe would love to get some early feedback, so if you\u2019re interested please sign up here!\n\nhttps://useflow.jinba.ai/", "author_fullname": "t2_eciwmpu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jinbaflow: A New GPT-Empowered Data Analysis Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhk49v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.3, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710743151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I\u2019m the founder of Jinbaflow, a new low code workflow tool we\u2019ve been working on with the goal of turning anyone into a data analyst. We want to allow anyone to use English instructions and AI in order to easily transform, analyze, and visualize data. The idea is to keep it super straightforward with a flow-style interface so you can see how the data connects and flows together.&lt;/p&gt;\n\n&lt;p&gt;We want to add more AI-empowered tools beyond the code generation to allow you to focus less on coding and more on the data. Currently, we&amp;#39;re using GPT-4 and we&amp;#39;re super happy with the results and how good it is at generating Pandas code for us.&lt;/p&gt;\n\n&lt;p&gt;We would love to get some early feedback, so if you\u2019re interested please sign up here!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://useflow.jinba.ai/\"&gt;https://useflow.jinba.ai/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhk49v", "is_robot_indexable": true, "report_reasons": null, "author": "sho-ma", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhk49v/jinbaflow_a_new_gptempowered_data_analysis_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhk49v/jinbaflow_a_new_gptempowered_data_analysis_tool/", "subreddit_subscribers": 169873, "created_utc": 1710743151.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}