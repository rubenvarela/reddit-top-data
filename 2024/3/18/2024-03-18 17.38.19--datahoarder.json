{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nSABRENT 2TB Rocket 4 Plus NVMe 4.0 Gen4 PCIe M.2 Internal SSD Extreme Performance\n\n$130 August 10,2023\n$275 March 17, 2024\n\nThis is common for all the drives.  What is driving the prices so high?  &lt;/CYNICISM&gt;  I\u2019m told by the media inflation is over and the politicians should be re-elected because they heroically squashed it. &lt;CYNICISM/&gt;\n\nI want a lot of NVMe to run private large data for both relational database and for experimenting with AI without using equipment I don\u2019t control, i.e. cloud systems.\n\nThis increase in prices on RAM and in GPU ( separate issue ) is making this effort unaffordable.\n\nOutside of politics and monetary policies of US and financial interactions between countries ( Tariffs ), what I am not seeing that is driving up these costs?\n\n", "author_fullname": "t2_b8fjqdv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is going on with the prices is NVMe M.2 SSDs? 211% the price of 8 months ago", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhb75s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710715675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SABRENT 2TB Rocket 4 Plus NVMe 4.0 Gen4 PCIe M.2 Internal SSD Extreme Performance&lt;/p&gt;\n\n&lt;p&gt;$130 August 10,2023\n$275 March 17, 2024&lt;/p&gt;\n\n&lt;p&gt;This is common for all the drives.  What is driving the prices so high?  &amp;lt;/CYNICISM&amp;gt;  I\u2019m told by the media inflation is over and the politicians should be re-elected because they heroically squashed it. &amp;lt;CYNICISM/&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;I want a lot of NVMe to run private large data for both relational database and for experimenting with AI without using equipment I don\u2019t control, i.e. cloud systems.&lt;/p&gt;\n\n&lt;p&gt;This increase in prices on RAM and in GPU ( separate issue ) is making this effort unaffordable.&lt;/p&gt;\n\n&lt;p&gt;Outside of politics and monetary policies of US and financial interactions between countries ( Tariffs ), what I am not seeing that is driving up these costs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhb75s", "is_robot_indexable": true, "report_reasons": null, "author": "ValuePeople", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhb75s/what_is_going_on_with_the_prices_is_nvme_m2_ssds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhb75s/what_is_going_on_with_the_prices_is_nvme_m2_ssds/", "subreddit_subscribers": 739458, "created_utc": 1710715675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A while ago I had posted regarding a massive number of images I had scraped from sites that had public domain or cc0 content. Unfortunately I had to take that down due to some server costs and time.\n\n[https://www.reddit.com/r/DataHoarder/comments/112ihjk/the\\_public\\_conscious\\_centralization\\_of\\_nearly\\_2/](https://www.reddit.com/r/DataHoarder/comments/112ihjk/the_public_conscious_centralization_of_nearly_2/)\n\nWell, I am happy to announce that I am on my way to being back online:\n\nhttp://pc.lowframerate.studio/\n\nIm hoping to be back on par with the former library soon. Right now its only about 200gb.\n\nI now have a new server stack that is way more affordable to run! Ill be getting the library back to its former glory. Some of the major players are already up, including my favorite, gutenberg. This time around the sites are also less dynamic. So Im hoping this makes mirrors simple. I plan to package the sites themselves or the images as torrents. Let me know which youd prefer.   \n\n\nPlease enjoy!", "author_fullname": "t2_5gy7bcwa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Public Conscious - Reborn!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhjs9l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710742395.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710741756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while ago I had posted regarding a massive number of images I had scraped from sites that had public domain or cc0 content. Unfortunately I had to take that down due to some server costs and time.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/112ihjk/the_public_conscious_centralization_of_nearly_2/\"&gt;https://www.reddit.com/r/DataHoarder/comments/112ihjk/the_public_conscious_centralization_of_nearly_2/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Well, I am happy to announce that I am on my way to being back online:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://pc.lowframerate.studio/\"&gt;http://pc.lowframerate.studio/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Im hoping to be back on par with the former library soon. Right now its only about 200gb.&lt;/p&gt;\n\n&lt;p&gt;I now have a new server stack that is way more affordable to run! Ill be getting the library back to its former glory. Some of the major players are already up, including my favorite, gutenberg. This time around the sites are also less dynamic. So Im hoping this makes mirrors simple. I plan to package the sites themselves or the images as torrents. Let me know which youd prefer.   &lt;/p&gt;\n\n&lt;p&gt;Please enjoy!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhjs9l", "is_robot_indexable": true, "report_reasons": null, "author": "PoweredBy90sAI", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhjs9l/the_public_conscious_reborn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhjs9l/the_public_conscious_reborn/", "subreddit_subscribers": 739458, "created_utc": 1710741756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am getting paranoid too after spending some time in this reddit. Any tips are appreciated.", "author_fullname": "t2_5vku2anr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I hoard if I live in a third world country without much extra cash?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhmqih", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710754815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am getting paranoid too after spending some time in this reddit. Any tips are appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhmqih", "is_robot_indexable": true, "report_reasons": null, "author": "Midtharefaikh", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhmqih/how_do_i_hoard_if_i_live_in_a_third_world_country/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhmqih/how_do_i_hoard_if_i_live_in_a_third_world_country/", "subreddit_subscribers": 739458, "created_utc": 1710754815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a local data backup system whereby my unencrypted Windows 10 machine is manually backed up to my local Synology NAS, which then automatically syncs and local encrypts a copy of my data to the cloud via HyperBackup. My backup image is currently around 7 TB.\n\nI have intentionally kept my local files unencrypted because, on the balance, I think it is more likely that I will be able to recover files in the future if they are not on some encrypted disk array and this outweighs the probability of device theft. I may or may not be right about this and it might change in the future.\n\nI am now interested in cloning my backup image to an external drive and leaving it at my parent's house for extra redundancy. I'd like to encrypt this drive so if I forget about it or whatever its fine. What is the most future proof way to do this? VeraCrypt keeps coming up in my searches. Is this the best way?\n\nDesired features:\n\n1. Encrypt a disk with a password, not a 128 byte key that can't be remembered\n2. Runs on windows, but doesn't have deep tendrils into the OS, making it more likely to be runnable in the future (i.e. no driver/kernel stuff)\n3. Open source with large enough userbase that the software itself won't bitrot over the next 10 years\n4. Resistant to drive failures where bits might get flipped. I'm cool buying a 20TB drive and using half the disk for parity (or more).\n5. I don't want to run a mess of python code or script anything. Complexity is an enemy.\n\nFeatures I don't care about:\n\n1. The ability to update the volume (write-once is fine; I don't need incremental backups).", "author_fullname": "t2_3bjakm7z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future proof encryption for cold storage data disks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh9elh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710711416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a local data backup system whereby my unencrypted Windows 10 machine is manually backed up to my local Synology NAS, which then automatically syncs and local encrypts a copy of my data to the cloud via HyperBackup. My backup image is currently around 7 TB.&lt;/p&gt;\n\n&lt;p&gt;I have intentionally kept my local files unencrypted because, on the balance, I think it is more likely that I will be able to recover files in the future if they are not on some encrypted disk array and this outweighs the probability of device theft. I may or may not be right about this and it might change in the future.&lt;/p&gt;\n\n&lt;p&gt;I am now interested in cloning my backup image to an external drive and leaving it at my parent&amp;#39;s house for extra redundancy. I&amp;#39;d like to encrypt this drive so if I forget about it or whatever its fine. What is the most future proof way to do this? VeraCrypt keeps coming up in my searches. Is this the best way?&lt;/p&gt;\n\n&lt;p&gt;Desired features:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Encrypt a disk with a password, not a 128 byte key that can&amp;#39;t be remembered&lt;/li&gt;\n&lt;li&gt;Runs on windows, but doesn&amp;#39;t have deep tendrils into the OS, making it more likely to be runnable in the future (i.e. no driver/kernel stuff)&lt;/li&gt;\n&lt;li&gt;Open source with large enough userbase that the software itself won&amp;#39;t bitrot over the next 10 years&lt;/li&gt;\n&lt;li&gt;Resistant to drive failures where bits might get flipped. I&amp;#39;m cool buying a 20TB drive and using half the disk for parity (or more).&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t want to run a mess of python code or script anything. Complexity is an enemy.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Features I don&amp;#39;t care about:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The ability to update the volume (write-once is fine; I don&amp;#39;t need incremental backups).&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bh9elh", "is_robot_indexable": true, "report_reasons": null, "author": "Meeple-Mayor", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bh9elh/future_proof_encryption_for_cold_storage_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bh9elh/future_proof_encryption_for_cold_storage_data/", "subreddit_subscribers": 739458, "created_utc": 1710711416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know, I know, this is my fault and I accept it. I have about 22x18 HDDs in a fractal case. After a BIOS update, I didn't realise it reset my fan settings, and basically all my fans were spinning at the minimum rate. I keep it in a separate room, and don't check it much. \n\nMy HDDs started getting cooked and the average temperature was about 50 to 55 degrees. This happened for about 3 months, I only realised this after 3 of the 22 disks started failing within days of each other. The drives are about ~3.5 years old at this point; previously they were all running at around 30-40 degrees max.\n\nI reset the fan settings in my BIOS and now they're sitting tight at about 35-37 degrees under load. My question is: should I assume just about all the HDDs will soon fail, and I've burnt through 22x18TB HDDs? Ouchie.\n\nThey are WD HC550s if that helps.", "author_fullname": "t2_fmblw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accidentally ran 22x18 HDDs at ~50-55 degrees for 3 months. How screwed am I?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhr3wf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710769577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know, I know, this is my fault and I accept it. I have about 22x18 HDDs in a fractal case. After a BIOS update, I didn&amp;#39;t realise it reset my fan settings, and basically all my fans were spinning at the minimum rate. I keep it in a separate room, and don&amp;#39;t check it much. &lt;/p&gt;\n\n&lt;p&gt;My HDDs started getting cooked and the average temperature was about 50 to 55 degrees. This happened for about 3 months, I only realised this after 3 of the 22 disks started failing within days of each other. The drives are about ~3.5 years old at this point; previously they were all running at around 30-40 degrees max.&lt;/p&gt;\n\n&lt;p&gt;I reset the fan settings in my BIOS and now they&amp;#39;re sitting tight at about 35-37 degrees under load. My question is: should I assume just about all the HDDs will soon fail, and I&amp;#39;ve burnt through 22x18TB HDDs? Ouchie.&lt;/p&gt;\n\n&lt;p&gt;They are WD HC550s if that helps.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhr3wf", "is_robot_indexable": true, "report_reasons": null, "author": "goldcakes", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhr3wf/accidentally_ran_22x18_hdds_at_5055_degrees_for_3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhr3wf/accidentally_ran_22x18_hdds_at_5055_degrees_for_3/", "subreddit_subscribers": 739458, "created_utc": 1710769577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know this question may be a bit silly, but I need to verify.\n\nSo most scanner software DPI presets are 75, 150, 300, 600, 1200, etc\n\nI find that 600 isn\u2019t enough, while 1200 takes too long.\n\n**My question: Is it okay to scan at 800 DPI?** (would it create weird artifacts?) Or does the DPI need to be evenly divisible by 75 for the best \u201ctrue scan\u201d? Are scanners built to scan the most optimally (speed and quality) at their original presets of 75, 150, 300, 600, 1200?\n\nHere\u2019s my current scan plan for archiving:\n\n* 8.5x11\u201d text documents @ 300 DPI\n* 8.5x11\u201d artwork @ 800 DPI\n* 4x6\u201d photos @ 800 DPI\n\nThanks all!\n\nFor context, the scanner that I\u2019m using is the Epson Perfection V39 II", "author_fullname": "t2_4i1qb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanner DPI presets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh94wy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710712956.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710710774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this question may be a bit silly, but I need to verify.&lt;/p&gt;\n\n&lt;p&gt;So most scanner software DPI presets are 75, 150, 300, 600, 1200, etc&lt;/p&gt;\n\n&lt;p&gt;I find that 600 isn\u2019t enough, while 1200 takes too long.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My question: Is it okay to scan at 800 DPI?&lt;/strong&gt; (would it create weird artifacts?) Or does the DPI need to be evenly divisible by 75 for the best \u201ctrue scan\u201d? Are scanners built to scan the most optimally (speed and quality) at their original presets of 75, 150, 300, 600, 1200?&lt;/p&gt;\n\n&lt;p&gt;Here\u2019s my current scan plan for archiving:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;8.5x11\u201d text documents @ 300 DPI&lt;/li&gt;\n&lt;li&gt;8.5x11\u201d artwork @ 800 DPI&lt;/li&gt;\n&lt;li&gt;4x6\u201d photos @ 800 DPI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks all!&lt;/p&gt;\n\n&lt;p&gt;For context, the scanner that I\u2019m using is the Epson Perfection V39 II&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bh94wy", "is_robot_indexable": true, "report_reasons": null, "author": "DentThat", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bh94wy/scanner_dpi_presets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bh94wy/scanner_dpi_presets/", "subreddit_subscribers": 739458, "created_utc": 1710710774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have several drives, small ones that I just throw shows on and other none important stuff. But there are so many of them they are all getting in a mess. The only option I went with was writing on postit notes and sticking them to the drive.\n\nNow I'm think of just writing\n\nA\n\nB\n\nC\n\nD\n\nA1\n\nB1\n\nC1\n\nD1\n\nAnd so on, on the drive and then in a text file have what that drive contains.\n\nAnyone have any other systems?", "author_fullname": "t2_3xqpnc72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What organisation app do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhfxlt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710728683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several drives, small ones that I just throw shows on and other none important stuff. But there are so many of them they are all getting in a mess. The only option I went with was writing on postit notes and sticking them to the drive.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m think of just writing&lt;/p&gt;\n\n&lt;p&gt;A&lt;/p&gt;\n\n&lt;p&gt;B&lt;/p&gt;\n\n&lt;p&gt;C&lt;/p&gt;\n\n&lt;p&gt;D&lt;/p&gt;\n\n&lt;p&gt;A1&lt;/p&gt;\n\n&lt;p&gt;B1&lt;/p&gt;\n\n&lt;p&gt;C1&lt;/p&gt;\n\n&lt;p&gt;D1&lt;/p&gt;\n\n&lt;p&gt;And so on, on the drive and then in a text file have what that drive contains.&lt;/p&gt;\n\n&lt;p&gt;Anyone have any other systems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhfxlt", "is_robot_indexable": true, "report_reasons": null, "author": "steviefaux", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhfxlt/what_organisation_app_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhfxlt/what_organisation_app_do_you_use/", "subreddit_subscribers": 739458, "created_utc": 1710728683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, like title said, why the hell its so hard to download a complete website for offline view?\n\nI was trying to download a fandom wiki (the entire wiki about a videogame), i tried lot of tools and i always got some problems.....\n\ni tried:\n\n* Wget: got problems when downloading images....lot of them was not downloaded....\n* httrack: takes forever/ super slow, and not downloading all the images too + even with depth levels restriction keep download useless outside-domain websites\n* offline explorer: maybe the worst since everything was messed up after download + no all the images\n* Cyotek web copy: same as offline explorer\n* Wikiteam software (dumpgenerator.py): ultra messy, super hard to install and didnt worked on my windows\n\nBasically the only thing that at least download all the text + images its the chrome ctrl+s (save page), but i need to manually full load and save page by page.....and when i read in offline mode its a bit messed up, but at least i have all the thing saved......\n\n \n\n### ", "author_fullname": "t2_urrqk5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why its so hard to download a website/fandom wiki??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bhu98f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710777631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, like title said, why the hell its so hard to download a complete website for offline view?&lt;/p&gt;\n\n&lt;p&gt;I was trying to download a fandom wiki (the entire wiki about a videogame), i tried lot of tools and i always got some problems.....&lt;/p&gt;\n\n&lt;p&gt;i tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Wget: got problems when downloading images....lot of them was not downloaded....&lt;/li&gt;\n&lt;li&gt;httrack: takes forever/ super slow, and not downloading all the images too + even with depth levels restriction keep download useless outside-domain websites&lt;/li&gt;\n&lt;li&gt;offline explorer: maybe the worst since everything was messed up after download + no all the images&lt;/li&gt;\n&lt;li&gt;Cyotek web copy: same as offline explorer&lt;/li&gt;\n&lt;li&gt;Wikiteam software (dumpgenerator.py): ultra messy, super hard to install and didnt worked on my windows&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Basically the only thing that at least download all the text + images its the chrome ctrl+s (save page), but i need to manually full load and save page by page.....and when i read in offline mode its a bit messed up, but at least i have all the thing saved......&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhu98f", "is_robot_indexable": true, "report_reasons": null, "author": "Mhanz97", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhu98f/why_its_so_hard_to_download_a_websitefandom_wiki/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhu98f/why_its_so_hard_to_download_a_websitefandom_wiki/", "subreddit_subscribers": 739458, "created_utc": 1710777631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some web archiving stuff - may be useful to some.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhhhs9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_4a9ry", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "opendirectories", "selftext": "https://github.com/iipc/awesome-web-archiving\n\nfrom a bred on /g/ about saving a gardeners forum that's closing. Might xpost to datahoarders as well.", "author_fullname": "t2_4a9ry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some web archiving stuff - may be useful to some.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/opendirectories", "hidden": false, "pwls": 6, "link_flair_css_class": "psa-link", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhei51", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "PSA", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710724562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.opendirectories", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/iipc/awesome-web-archiving\"&gt;https://github.com/iipc/awesome-web-archiving&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;from a bred on /g/ about saving a gardeners forum that&amp;#39;s closing. Might xpost to datahoarders as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?auto=webp&amp;s=bcf55311cf8f00abf98f8980eef3bc43c057c736", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c48d6d5dc7b0ea9e338a0bca92e27908f8f809e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e89f720a4f5e202ad2382ba7b5b6d3c95fefc78", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d291f4f291a0c206f6fd80a77ccd89065fb22691", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=373f46dfe5ecb78b8f8ea58ccc92da53a0c702b1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2909e9cde9f010ae88906be39d17b14bbc0e908b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72022ba3134a76b21c1774ac127eb6bb56ec7543", "width": 1080, "height": 540}], "variants": {}, "id": "pfsWOjelY-55VVutvQPdcSzzkReF2GSMBW-_f_iL81I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b39d4fea-d6a6-11e1-a70b-12313b0ce1e2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r1e4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1bhei51", "is_robot_indexable": true, "report_reasons": null, "author": "ringofyre", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/opendirectories/comments/1bhei51/some_web_archiving_stuff_may_be_useful_to_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/opendirectories/comments/1bhei51/some_web_archiving_stuff_may_be_useful_to_some/", "subreddit_subscribers": 214975, "created_utc": 1710724562.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1710733554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.opendirectories", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/opendirectories/comments/1bhei51/some_web_archiving_stuff_may_be_useful_to_some/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?auto=webp&amp;s=bcf55311cf8f00abf98f8980eef3bc43c057c736", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c48d6d5dc7b0ea9e338a0bca92e27908f8f809e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e89f720a4f5e202ad2382ba7b5b6d3c95fefc78", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d291f4f291a0c206f6fd80a77ccd89065fb22691", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=373f46dfe5ecb78b8f8ea58ccc92da53a0c702b1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2909e9cde9f010ae88906be39d17b14bbc0e908b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72022ba3134a76b21c1774ac127eb6bb56ec7543", "width": 1080, "height": 540}], "variants": {}, "id": "pfsWOjelY-55VVutvQPdcSzzkReF2GSMBW-_f_iL81I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhhhs9", "is_robot_indexable": true, "report_reasons": null, "author": "ringofyre", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1bhei51", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhhhs9/some_web_archiving_stuff_may_be_useful_to_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/opendirectories/comments/1bhei51/some_web_archiving_stuff_may_be_useful_to_some/", "subreddit_subscribers": 739458, "created_utc": 1710733554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i've been using ntfs-3g for a long while now, and it never gave me any trouble. until this weekend. on two different computers and two different operating systems (Ubuntu and Manjaro) - don't know exact versions of `ntfs-3g` itself - it behaved weird and caused one data loss and one almost-data-loss.\n\n - first, on Manjaro system, i copied a bunch of files to a USB connected drive, which already contained some data. this resulted in original files disappearing from root directory and only new, copied files being visible. occupied space was the sum of old and new files' size. upon trying to see what's happening on this drive on a Windows OS, the drive started to show as RAW in DiskManager, and at that point i called quits.\n\n - second, on Ubuntu system, i tried to bulk rename a bunch of files. this resulted in half of contents of root directory disappearing (again!) and showing correct occupied/free space (again!). only this time, i was able to repair (chkdsk /f) the filesystem on WIndows, and files were brought back (recovering orhpaned blah blah...)\n\nno data was lost, because i had all this backed up, but it serously weakened my trust in FOSS NTFS implementation. one drive was using GPT schema, other was MBR with extended partition. have you heard of any serious bugs in ntfs-3g lately? or maybe ever experienced something similar? normal day-to-day operations (like data copying or files renaming) should not produce such effects. i'm seriously considering ditching NTFS on Linux and use network as a translation layer.", "author_fullname": "t2_14uv2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ntfs-3g errors?/bugs?/weird behavior and data corruption.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh1nui", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710692708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve been using ntfs-3g for a long while now, and it never gave me any trouble. until this weekend. on two different computers and two different operating systems (Ubuntu and Manjaro) - don&amp;#39;t know exact versions of &lt;code&gt;ntfs-3g&lt;/code&gt; itself - it behaved weird and caused one data loss and one almost-data-loss.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;first, on Manjaro system, i copied a bunch of files to a USB connected drive, which already contained some data. this resulted in original files disappearing from root directory and only new, copied files being visible. occupied space was the sum of old and new files&amp;#39; size. upon trying to see what&amp;#39;s happening on this drive on a Windows OS, the drive started to show as RAW in DiskManager, and at that point i called quits.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;second, on Ubuntu system, i tried to bulk rename a bunch of files. this resulted in half of contents of root directory disappearing (again!) and showing correct occupied/free space (again!). only this time, i was able to repair (chkdsk /f) the filesystem on WIndows, and files were brought back (recovering orhpaned blah blah...)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;no data was lost, because i had all this backed up, but it serously weakened my trust in FOSS NTFS implementation. one drive was using GPT schema, other was MBR with extended partition. have you heard of any serious bugs in ntfs-3g lately? or maybe ever experienced something similar? normal day-to-day operations (like data copying or files renaming) should not produce such effects. i&amp;#39;m seriously considering ditching NTFS on Linux and use network as a translation layer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bh1nui", "is_robot_indexable": true, "report_reasons": null, "author": "paprok", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bh1nui/ntfs3g_errorsbugsweird_behavior_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bh1nui/ntfs3g_errorsbugsweird_behavior_and_data/", "subreddit_subscribers": 739458, "created_utc": 1710692708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning to set up my first real NAS and have a question.\n\nWould it be better to have 3x 14GB drives or 2x 20GB. I am thinking the former, because it allows for redundancy, but I wanted to check with the experts. ", "author_fullname": "t2_aw8np", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Hoarder in Training - Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhrgde", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710770494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to set up my first real NAS and have a question.&lt;/p&gt;\n\n&lt;p&gt;Would it be better to have 3x 14GB drives or 2x 20GB. I am thinking the former, because it allows for redundancy, but I wanted to check with the experts. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhrgde", "is_robot_indexable": true, "report_reasons": null, "author": "brandontmyers", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhrgde/new_hoarder_in_training_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhrgde/new_hoarder_in_training_question/", "subreddit_subscribers": 739458, "created_utc": 1710770494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've long backed up my DVDs, and now I have a drive capable of reading BDs so I want to back those up too.\n\nI want to make an ISO, so something like AnyDVD (which can't make ISOs) sounds like it wouldn't work for me.  DVDFab is what I used to use in the past but apparently nowadays you can only make ISOs of three discs and then you must register it?\n\nBasically:  Need a program that is inexpensive or free, easy to use, and makes ISOs of blurays.", "author_fullname": "t2_yjou4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bluray backup program suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhbg5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710716296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve long backed up my DVDs, and now I have a drive capable of reading BDs so I want to back those up too.&lt;/p&gt;\n\n&lt;p&gt;I want to make an ISO, so something like AnyDVD (which can&amp;#39;t make ISOs) sounds like it wouldn&amp;#39;t work for me.  DVDFab is what I used to use in the past but apparently nowadays you can only make ISOs of three discs and then you must register it?&lt;/p&gt;\n\n&lt;p&gt;Basically:  Need a program that is inexpensive or free, easy to use, and makes ISOs of blurays.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhbg5f", "is_robot_indexable": true, "report_reasons": null, "author": "MoeDantes", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhbg5f/bluray_backup_program_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhbg5f/bluray_backup_program_suggestions/", "subreddit_subscribers": 739458, "created_utc": 1710716296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Say that you have a few random size and speed drives into a simple resiliency pool.  How does Windows handle read/write speeds?  Does Windows just get presented with whatever drive speed that the current copy job is using?  So it could fluctuate depending on what drive is being used at the moment?\n\nFor a Two drive resiliency, the files are being read/write to both drives, so whichever drive is the slowest in the pool, is what speed that you get?\n\nParity resilience I would guess may be slower than both simple and two drive resilience, because of the nature of a software raid 5 situation there?", "author_fullname": "t2_16fj1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces Speed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhcgg0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710718920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say that you have a few random size and speed drives into a simple resiliency pool.  How does Windows handle read/write speeds?  Does Windows just get presented with whatever drive speed that the current copy job is using?  So it could fluctuate depending on what drive is being used at the moment?&lt;/p&gt;\n\n&lt;p&gt;For a Two drive resiliency, the files are being read/write to both drives, so whichever drive is the slowest in the pool, is what speed that you get?&lt;/p&gt;\n\n&lt;p&gt;Parity resilience I would guess may be slower than both simple and two drive resilience, because of the nature of a software raid 5 situation there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhcgg0", "is_robot_indexable": true, "report_reasons": null, "author": "_Nismo", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhcgg0/storage_spaces_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhcgg0/storage_spaces_speed/", "subreddit_subscribers": 739458, "created_utc": 1710718920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The ronnabyte and quettabyte were officially introduced in November 2022, but I recently discovered an unofficial extension of data units which [goes back](https://phendog.livejournal.com/256972.html) a couple decades, at least up to the lumabyte; the kamabyte and beyond may be [more recent.](https://character.fandom.com/wiki/All_bytes_comparison.) (I've omitted the xonabyte and wekabyte, which the ronnabyte and quettabyte have replaced.)  One fetabyte (10\u2078\u00b9) exceeds the [Eddington number,](https://en.wikipedia.org/wiki/Eddington_number) the estimated number of protons in the observable universe (10\u2078\u2070).\n\n&amp;#x200B;\n\n&gt;Mark Liberman calculated the storage requirements for all human speech ever spoken at [42 zettabytes](https://highscalability.com/how-big-is-a-petabyte-exabyte-zettabyte-or-a-yottabyte/) if digitized as 16 kHz 16-bit audio. This was done in response to a popular expression that states \"all words ever spoken by human beings\" could be stored in approximately 5 exabytes of data. Liberman did \"freely confess that maybe the authors \\[of the exabyte estimate\\] were thinking about text.\"\n\n&amp;#x200B;\n\n&gt;The total amount of data created, captured, copied, and consumed globally is forecast to increase rapidly, reaching [64.2 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/#:~:text=The%20total%20amount%20of%20data,replicated%20reached%20a%20new%20high.) in 2020. Over the next five years up to 2025, global data creation is projected to grow to more than 180 zettabytes. In 2020, the amount of data created and replicated reached a new high.\n\n&amp;#x200B;\n\n1. 10\u00b9 -  Byte (eight bits)\n2. 10\u00b3 -  Kilobyte (one thousand bytes)\n3. 10\u2076 -  Megabyte (one million bytes)\n4. 10\u2079 -  Gigabyte (one billion bytes)\n5. 10\u00b9\u00b2 - Terabyte (one trillion bytes)\n6. 10\u00b9\u2075 - Petabyte (one quadrillion bytes)\n7. 10\u00b9\u2078 - Exabyte (one quintillion bytes)\n8. 10\u00b2\u00b9 - Zettabyte (one sextillion bytes)\n9. 10\u00b2\u2074 - Yottabyte (one septillion bytes)\n10. 10\u00b2\u2077 - Ronnabyte (one octillion bytes)\n11. 10\u00b3\u2070 - Quettabyte (one nonillion bytes)\n12. 10\u00b3\u00b3 - Vundabyte (one decillion bytes)\n13. 10\u00b3\u2076 - Udabyte (one undecillion bytes)\n14. 10\u00b3\u2079 - Tredabyte (one duodecillion bytes)\n15. 10\u2074\u00b2 - Sortabyte (one tredecillion bytes)\n16. 10\u2074\u2075 - Rintabyte (one quattuordecillion bytes)\n17. 10\u2074\u2078 - Quexabyte (one quindecillion bytes)\n18. 10\u2075\u00b9 - Peptabyte (one sexdecillion bytes)\n19. 10\u2075\u2074 - Ochabyte (one septendecillion bytes)\n20. 10\u2075\u2077 - Nenabyte (one octodecillion bytes)\n21. 10\u2076\u2070 - Mingabyte (one novemdecillion bytes)\n22. 10\u2076\u00b3 - Lumabyte (one vigintillion bytes)\n23. 10\u2076\u2076 - Kamabyte (one unvigintillion bytes)\n24. 10\u2076\u2079 - Jameabyte (one duovigintillion bytes)\n25. 10\u2077\u00b2 - Ianabyte (one trevigintillion bytes)\n26. 10\u2077\u2075 - Hevabyte (one quattuorvigintillion bytes)\n27. 10\u2077\u2078 - Gexabyte (one quinvigintillion bytes)\n28. 10\u2078\u00b9 - Fetabyte (one sesvigintillion bytes)\n29. 10\u2078\u2074 - Eottabyte (one septenvigintillion bytes)\n30. 10\u2078\u2077 - Devabyte (one octovigintillion bytes)", "author_fullname": "t2_g7ixezm01", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beyond the Quettabyte", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhqzkp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710769248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The ronnabyte and quettabyte were officially introduced in November 2022, but I recently discovered an unofficial extension of data units which &lt;a href=\"https://phendog.livejournal.com/256972.html\"&gt;goes back&lt;/a&gt; a couple decades, at least up to the lumabyte; the kamabyte and beyond may be &lt;a href=\"https://character.fandom.com/wiki/All_bytes_comparison.\"&gt;more recent.&lt;/a&gt; (I&amp;#39;ve omitted the xonabyte and wekabyte, which the ronnabyte and quettabyte have replaced.)  One fetabyte (10\u2078\u00b9) exceeds the &lt;a href=\"https://en.wikipedia.org/wiki/Eddington_number\"&gt;Eddington number,&lt;/a&gt; the estimated number of protons in the observable universe (10\u2078\u2070).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Mark Liberman calculated the storage requirements for all human speech ever spoken at &lt;a href=\"https://highscalability.com/how-big-is-a-petabyte-exabyte-zettabyte-or-a-yottabyte/\"&gt;42 zettabytes&lt;/a&gt; if digitized as 16 kHz 16-bit audio. This was done in response to a popular expression that states &amp;quot;all words ever spoken by human beings&amp;quot; could be stored in approximately 5 exabytes of data. Liberman did &amp;quot;freely confess that maybe the authors [of the exabyte estimate] were thinking about text.&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The total amount of data created, captured, copied, and consumed globally is forecast to increase rapidly, reaching &lt;a href=\"https://www.statista.com/statistics/871513/worldwide-data-created/#:%7E:text=The%20total%20amount%20of%20data,replicated%20reached%20a%20new%20high.\"&gt;64.2 zettabytes&lt;/a&gt; in 2020. Over the next five years up to 2025, global data creation is projected to grow to more than 180 zettabytes. In 2020, the amount of data created and replicated reached a new high.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;10\u00b9 -  Byte (eight bits)&lt;/li&gt;\n&lt;li&gt;10\u00b3 -  Kilobyte (one thousand bytes)&lt;/li&gt;\n&lt;li&gt;10\u2076 -  Megabyte (one million bytes)&lt;/li&gt;\n&lt;li&gt;10\u2079 -  Gigabyte (one billion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b9\u00b2 - Terabyte (one trillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b9\u2075 - Petabyte (one quadrillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b9\u2078 - Exabyte (one quintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b2\u00b9 - Zettabyte (one sextillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b2\u2074 - Yottabyte (one septillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b2\u2077 - Ronnabyte (one octillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b3\u2070 - Quettabyte (one nonillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b3\u00b3 - Vundabyte (one decillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b3\u2076 - Udabyte (one undecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u00b3\u2079 - Tredabyte (one duodecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2074\u00b2 - Sortabyte (one tredecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2074\u2075 - Rintabyte (one quattuordecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2074\u2078 - Quexabyte (one quindecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2075\u00b9 - Peptabyte (one sexdecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2075\u2074 - Ochabyte (one septendecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2075\u2077 - Nenabyte (one octodecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2076\u2070 - Mingabyte (one novemdecillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2076\u00b3 - Lumabyte (one vigintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2076\u2076 - Kamabyte (one unvigintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2076\u2079 - Jameabyte (one duovigintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2077\u00b2 - Ianabyte (one trevigintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2077\u2075 - Hevabyte (one quattuorvigintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2077\u2078 - Gexabyte (one quinvigintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2078\u00b9 - Fetabyte (one sesvigintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2078\u2074 - Eottabyte (one septenvigintillion bytes)&lt;/li&gt;\n&lt;li&gt;10\u2078\u2077 - Devabyte (one octovigintillion bytes)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d80zHIbL618mYQqL4tBZJZwfcQJSEpd8-sFe8TM1pEY.jpg?auto=webp&amp;s=ff2435b7a68db58879d7a6de73ec7938a6f78b8d", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/d80zHIbL618mYQqL4tBZJZwfcQJSEpd8-sFe8TM1pEY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a136c05ebf29858f479aff6d6db8b2dbf2d2d84c", "width": 108, "height": 108}], "variants": {}, "id": "mg0A2HAlaXaO-uJlkUtf-DhWRf5641cBtCkyXOv9CT4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Cryostasis Can Take Us to the Quettabyte Age", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1bhqzkp", "is_robot_indexable": true, "report_reasons": null, "author": "Cryogenator", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bhqzkp/beyond_the_quettabyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhqzkp/beyond_the_quettabyte/", "subreddit_subscribers": 739458, "created_utc": 1710769248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've read many contrasting opinions but seemingly long term storage seems to be unadvisable on ssds. Some insight on the matter would be greatly appreciated!\n\n&amp;#x200B;", "author_fullname": "t2_5kaeya34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi everyone! Is in 2024 a gen3 or gen4 drive a viable option as a backup drive? with backups run once every month or so", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhmm34", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.3, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710754273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read many contrasting opinions but seemingly long term storage seems to be unadvisable on ssds. Some insight on the matter would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhmm34", "is_robot_indexable": true, "report_reasons": null, "author": "Giamps94", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhmm34/hi_everyone_is_in_2024_a_gen3_or_gen4_drive_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhmm34/hi_everyone_is_in_2024_a_gen3_or_gen4_drive_a/", "subreddit_subscribers": 739458, "created_utc": 1710754273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I connected this device using my usb to sata adapter. On disk management it shows it already initialized, but also shows a capacity of 0 MB (it should show ~ 6 TB). It\u2019s assigned to drive D, but does not show up on my file explorer. I tried updating drives but it says everything is already updated. ", "author_fullname": "t2_ytho3gz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD not showing up as a Drive (Toshiba NAS N300)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhjsso", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710741815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I connected this device using my usb to sata adapter. On disk management it shows it already initialized, but also shows a capacity of 0 MB (it should show ~ 6 TB). It\u2019s assigned to drive D, but does not show up on my file explorer. I tried updating drives but it says everything is already updated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhjsso", "is_robot_indexable": true, "report_reasons": null, "author": "Atomic310", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhjsso/hdd_not_showing_up_as_a_drive_toshiba_nas_n300/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhjsso/hdd_not_showing_up_as_a_drive_toshiba_nas_n300/", "subreddit_subscribers": 739458, "created_utc": 1710741815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm embarking on a project to perform sentiment analysis on news article headlines related to key tech companies: Amazon, Facebook, and Tesla. The timeframe of interest spans from January 1st, 2019, to February 28th, 2020. Can anybody recommend an easy but comprehensive tool, that is suitable for a newbie.", "author_fullname": "t2_dpccsybh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help: Tech News Data for Sentiment Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh8mb8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710772224.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710709536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m embarking on a project to perform sentiment analysis on news article headlines related to key tech companies: Amazon, Facebook, and Tesla. The timeframe of interest spans from January 1st, 2019, to February 28th, 2020. Can anybody recommend an easy but comprehensive tool, that is suitable for a newbie.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bh8mb8", "is_robot_indexable": true, "report_reasons": null, "author": "EagleSwemz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bh8mb8/need_help_tech_news_data_for_sentiment_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bh8mb8/need_help_tech_news_data_for_sentiment_analysis/", "subreddit_subscribers": 739458, "created_utc": 1710709536.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}