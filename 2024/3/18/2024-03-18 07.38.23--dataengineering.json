{"kind": "Listing", "data": {"after": null, "dist": 19, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: Currently a self taught data engineer with about 3 years of DE experience and 3 years of data analyst experience. Graduated with a non-CS quant major from a decent university and make around $130k a year at a remote MCOL job. \n\nHas anyone made it from a mid-tier paying job to a high paying job without moving to a HCOL area?\n\nI'm wondering if the jobs themselves that are higher paying are actually more difficult in complexity? My current company has a pretty modern tech stack with good engineering practices, at least I think.\n\nIs the best way to get those higher paying jobs to get a referral and grind leetcode/learn fundamentals?", "author_fullname": "t2_fhmml14j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone made the jump from a $100-150k to +$200k position?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh6jx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 131, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 131, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710704576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: Currently a self taught data engineer with about 3 years of DE experience and 3 years of data analyst experience. Graduated with a non-CS quant major from a decent university and make around $130k a year at a remote MCOL job. &lt;/p&gt;\n\n&lt;p&gt;Has anyone made it from a mid-tier paying job to a high paying job without moving to a HCOL area?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if the jobs themselves that are higher paying are actually more difficult in complexity? My current company has a pretty modern tech stack with good engineering practices, at least I think.&lt;/p&gt;\n\n&lt;p&gt;Is the best way to get those higher paying jobs to get a referral and grind leetcode/learn fundamentals?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bh6jx2", "is_robot_indexable": true, "report_reasons": null, "author": "Capable-Jicama2155", "discussion_type": null, "num_comments": 92, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh6jx2/has_anyone_made_the_jump_from_a_100150k_to_200k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh6jx2/has_anyone_made_the_jump_from_a_100150k_to_200k/", "subreddit_subscribers": 169780, "created_utc": 1710704576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,  \nI am seeking guidance how can i become good overall engineer ? What i can learn to be better ?\n\n**Experience**: 3 years  \n**Age**: 27  \n**Current role**: I am working on low latency data pipelines using scala, flink, cassandra, redis, s3 and kafka in a product based company.  \nOther tools i am using prometheus, grafana for monitoring and kubernetes, docker for deploying apps.  \n**Leetcode rating**: My Leetcode rating is 1960 and currently practing more on it. solved around 1200 problems. Using cpp.  \n**System Design**: Completed Design data intensive applicaion book and Grokking the system design course.  \n**Low level Design**: working on it this too using cpp.\n\nRecently I read  design data intensive application book, reading it once again to get more out of it.  Next i am thinking of reading one os book (Galvin), microservices by sam richardson book. I also work on understanding how cassandra redis and kafka works internally in the free time.\n\nI didn't try to learn kuberentes and docker in detail because they are vast in terms of concepts. My current job does allow only to used them as platform for deploying apps and there are other engineers for devops work.\n\nI want your guidance on what can i do to become a good successful engineer and  join good company in future. I am not thinking of switching in few months becomes i want grind more in coming months and my current job able me to provide good free time.  **Is anything needs to be changed or added to what i am doing currently ?** Any comment would be much appreciated. Thanks in advance.\n\nSorry for my bad english.", "author_fullname": "t2_aqmxwdoy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to become a good engineer ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh2wha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710697134.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710695781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI am seeking guidance how can i become good overall engineer ? What i can learn to be better ?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Experience&lt;/strong&gt;: 3 years&lt;br/&gt;\n&lt;strong&gt;Age&lt;/strong&gt;: 27&lt;br/&gt;\n&lt;strong&gt;Current role&lt;/strong&gt;: I am working on low latency data pipelines using scala, flink, cassandra, redis, s3 and kafka in a product based company.&lt;br/&gt;\nOther tools i am using prometheus, grafana for monitoring and kubernetes, docker for deploying apps.&lt;br/&gt;\n&lt;strong&gt;Leetcode rating&lt;/strong&gt;: My Leetcode rating is 1960 and currently practing more on it. solved around 1200 problems. Using cpp.&lt;br/&gt;\n&lt;strong&gt;System Design&lt;/strong&gt;: Completed Design data intensive applicaion book and Grokking the system design course.&lt;br/&gt;\n&lt;strong&gt;Low level Design&lt;/strong&gt;: working on it this too using cpp.&lt;/p&gt;\n\n&lt;p&gt;Recently I read  design data intensive application book, reading it once again to get more out of it.  Next i am thinking of reading one os book (Galvin), microservices by sam richardson book. I also work on understanding how cassandra redis and kafka works internally in the free time.&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t try to learn kuberentes and docker in detail because they are vast in terms of concepts. My current job does allow only to used them as platform for deploying apps and there are other engineers for devops work.&lt;/p&gt;\n\n&lt;p&gt;I want your guidance on what can i do to become a good successful engineer and  join good company in future. I am not thinking of switching in few months becomes i want grind more in coming months and my current job able me to provide good free time.  &lt;strong&gt;Is anything needs to be changed or added to what i am doing currently ?&lt;/strong&gt; Any comment would be much appreciated. Thanks in advance.&lt;/p&gt;\n\n&lt;p&gt;Sorry for my bad english.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bh2wha", "is_robot_indexable": true, "report_reasons": null, "author": "AggravatingParsnip89", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/", "subreddit_subscribers": 169780, "created_utc": 1710695781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently joined a new company with almost zero up to date documentation, with loads and loads of tables in different schemas.\n\nHow should I go about documenting/understanding what each table means and each field?\n\nAnd when should I start suggesting changes to some tables?\n\nI'm currently just building an ERD using dbdiagram.io with comments as much as I can but not sure if it's the right way to go.", "author_fullname": "t2_apdhbvv7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tables all the way down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh0jww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710689960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently joined a new company with almost zero up to date documentation, with loads and loads of tables in different schemas.&lt;/p&gt;\n\n&lt;p&gt;How should I go about documenting/understanding what each table means and each field?&lt;/p&gt;\n\n&lt;p&gt;And when should I start suggesting changes to some tables?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently just building an ERD using dbdiagram.io with comments as much as I can but not sure if it&amp;#39;s the right way to go.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bh0jww", "is_robot_indexable": true, "report_reasons": null, "author": "Agile-Scene-2465", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh0jww/tables_all_the_way_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh0jww/tables_all_the_way_down/", "subreddit_subscribers": 169780, "created_utc": 1710689960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any recommendations would be really helpful. Thanks!", "author_fullname": "t2_rr6r6b8v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Important is System Design for Data Engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgth7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710667274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any recommendations would be really helpful. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bgth7l", "is_robot_indexable": true, "report_reasons": null, "author": "_areebpasha", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bgth7l/how_important_is_system_design_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bgth7l/how_important_is_system_design_for_data_engineers/", "subreddit_subscribers": 169780, "created_utc": 1710667274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, everyone! I've created this reviewer for my Data Engineering Team, and I thought I'd share it here with all of you. Feel free to use it and provide any feedback or suggestions you may have. Happy learning!\n\n**AWS Certified Data Engineer Associate DEA-C01 Practice Test**\n\n[https://www.udemy.com/course/ultimate-practice-test-aws-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST](https://www.udemy.com/course/ultimate-practice-test-aws-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST)\n\n**Databricks Certified Data Engineer Associate Practice Test 2024**\n\n[https://www.udemy.com/course/practice-test-databricks-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST](https://www.udemy.com/course/practice-test-databricks-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST)\n\nedit. P.S. It's free, so please forgive the small question bank.", "author_fullname": "t2_tgfwnw6d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reviewer for Data Engineering Certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgtc9f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710666704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everyone! I&amp;#39;ve created this reviewer for my Data Engineering Team, and I thought I&amp;#39;d share it here with all of you. Feel free to use it and provide any feedback or suggestions you may have. Happy learning!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;AWS Certified Data Engineer Associate DEA-C01 Practice Test&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/ultimate-practice-test-aws-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST\"&gt;https://www.udemy.com/course/ultimate-practice-test-aws-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Databricks Certified Data Engineer Associate Practice Test 2024&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/practice-test-databricks-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST\"&gt;https://www.udemy.com/course/practice-test-databricks-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;edit. P.S. It&amp;#39;s free, so please forgive the small question bank.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bgtc9f", "is_robot_indexable": true, "report_reasons": null, "author": "namibellmere", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bgtc9f/reviewer_for_data_engineering_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bgtc9f/reviewer_for_data_engineering_certification/", "subreddit_subscribers": 169780, "created_utc": 1710666704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Yes I know the overall goal of our work is to reduce excel reports. But everywhere I go there is some amount of \"essential\" reports.\n\nI've seen so much - S3 buckets, onedrive, SharePoint, mounted disks, email, SFTP.\n\nWhat does your company do? \n\n", "author_fullname": "t2_j3gqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your company distribute excel reports?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgwc8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710678421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yes I know the overall goal of our work is to reduce excel reports. But everywhere I go there is some amount of &amp;quot;essential&amp;quot; reports.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen so much - S3 buckets, onedrive, SharePoint, mounted disks, email, SFTP.&lt;/p&gt;\n\n&lt;p&gt;What does your company do? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bgwc8l", "is_robot_indexable": true, "report_reasons": null, "author": "exact-approximate", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bgwc8l/how_does_your_company_distribute_excel_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bgwc8l/how_does_your_company_distribute_excel_reports/", "subreddit_subscribers": 169780, "created_utc": 1710678421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "About 6 months ago, my company made the decision to purchase Ab initio. Based on my research, it, along with other licenseing ETL tools, seems to be on the decline. My company has been rolling it out to various teams and mine is up next. We will be expectsd to migrate current pipelines and new ones to Ab Initio. Currently, we use python + SQL scripts ran on Snowflake to transform our raw data ( files in AWS S3) and then just load into Snowflake tables.\n\nI'm not ready to leave this company, but I'm not so excited about developing a skill set in a dying tool. Still, is there anyway to make the most out of this situation? Is there anything to learn diving deep in ab Initio and learning all I can about the tool? Should I use the tool but focus my cv on the challenges I overcame, not the tool used? Or, should I hyper specialize in ab Initio and become an expert who drifts around to different companies to maintain their pipelines, requiring a large salary for this niche and drying skill set.\n\n", "author_fullname": "t2_jlj0h3u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting the most of out an old tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgymae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710685108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About 6 months ago, my company made the decision to purchase Ab initio. Based on my research, it, along with other licenseing ETL tools, seems to be on the decline. My company has been rolling it out to various teams and mine is up next. We will be expectsd to migrate current pipelines and new ones to Ab Initio. Currently, we use python + SQL scripts ran on Snowflake to transform our raw data ( files in AWS S3) and then just load into Snowflake tables.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not ready to leave this company, but I&amp;#39;m not so excited about developing a skill set in a dying tool. Still, is there anyway to make the most out of this situation? Is there anything to learn diving deep in ab Initio and learning all I can about the tool? Should I use the tool but focus my cv on the challenges I overcame, not the tool used? Or, should I hyper specialize in ab Initio and become an expert who drifts around to different companies to maintain their pipelines, requiring a large salary for this niche and drying skill set.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bgymae", "is_robot_indexable": true, "report_reasons": null, "author": "CriticalSouth3447", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bgymae/getting_the_most_of_out_an_old_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bgymae/getting_the_most_of_out_an_old_tool/", "subreddit_subscribers": 169780, "created_utc": 1710685108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an API wrapper, which accepts a batch of data, which has to be ingested into multiple tables. First the main facts are inserted and the ids are used to fill some additional metadata tables and junction tables for the dimensional tables.   \n\n\nHow would you efficiently handle the ingestion, when a batch of data arrives?   \n\n\nI think the following options are viable, with respective tradeoffs:\n\n1. Sequential inserts wrapper in transactions. Inserting each fact into the content table and then inserting the accompanying data into the surrounding tables. Do it in a loop for all data. Inefficient, but should give the best guarantees. \n2. Bulk insert with temporary tables. Insert the data into temporary tables (staging). Insert into fact table and get content\\_ids. Fill other temporary tables and insert into dimension tables and metadata tables. \n\nI was looking into COPY but it doesn't seem suitable for multiple tables. Am I missing an option? What setup would you go with? ", "author_fullname": "t2_uc7qtmotr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batching Ingestion (PostgreSQL): Sequential Transactions vs Temporary Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgvc2w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710674918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an API wrapper, which accepts a batch of data, which has to be ingested into multiple tables. First the main facts are inserted and the ids are used to fill some additional metadata tables and junction tables for the dimensional tables.   &lt;/p&gt;\n\n&lt;p&gt;How would you efficiently handle the ingestion, when a batch of data arrives?   &lt;/p&gt;\n\n&lt;p&gt;I think the following options are viable, with respective tradeoffs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Sequential inserts wrapper in transactions. Inserting each fact into the content table and then inserting the accompanying data into the surrounding tables. Do it in a loop for all data. Inefficient, but should give the best guarantees. &lt;/li&gt;\n&lt;li&gt;Bulk insert with temporary tables. Insert the data into temporary tables (staging). Insert into fact table and get content_ids. Fill other temporary tables and insert into dimension tables and metadata tables. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I was looking into COPY but it doesn&amp;#39;t seem suitable for multiple tables. Am I missing an option? What setup would you go with? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bgvc2w", "is_robot_indexable": true, "report_reasons": null, "author": "nicolay-ai", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bgvc2w/batching_ingestion_postgresql_sequential/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bgvc2w/batching_ingestion_postgresql_sequential/", "subreddit_subscribers": 169780, "created_utc": 1710674918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI was browsing internet and found some blogs describing a problem, and giving sample propositions on how to solve it. Do you know of any bloggers or technical content creator who does the same thing with Data engineering problems, modeling and/or Architecting solutions ?\n\nThanks for sharing", "author_fullname": "t2_948bsvs1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for technical blogs sharing solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgy3ct", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710683694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I was browsing internet and found some blogs describing a problem, and giving sample propositions on how to solve it. Do you know of any bloggers or technical content creator who does the same thing with Data engineering problems, modeling and/or Architecting solutions ?&lt;/p&gt;\n\n&lt;p&gt;Thanks for sharing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bgy3ct", "is_robot_indexable": true, "report_reasons": null, "author": "SdJbra", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bgy3ct/looking_for_technical_blogs_sharing_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bgy3ct/looking_for_technical_blogs_sharing_solutions/", "subreddit_subscribers": 169780, "created_utc": 1710683694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the past few months I have been learning about data strategies that employ Apache Iceberg with Snowflake DB and Apache Spark &amp; I have compiled my learnings into a short article.\n\n[https://medium.com/@pbd\\_94/skiing-with-snowflake-b196e8f7e2e6](https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6)\n\nFire away.", "author_fullname": "t2_11mmk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Supercharge your compute strategy with Apache Iceberg, Snowflake, Apache Spark, AWS Glue &amp; Project Nessie", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhhxs0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710734969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the past few months I have been learning about data strategies that employ Apache Iceberg with Snowflake DB and Apache Spark &amp;amp; I have compiled my learnings into a short article.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6\"&gt;https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Fire away.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?auto=webp&amp;s=0380b1add45a87f89967d78ac69b9471fa1758ec", "width": 1200, "height": 915}, "resolutions": [{"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df7f210d8f4b10b04cc8590372be4515d0dc2af6", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=770379812d74ee166858c6c46f32e994adb05d37", "width": 216, "height": 164}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f3dee8a276d36599de5bf5f8205968fc92ddb05", "width": 320, "height": 244}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd25d5968eb8136f3f1109b294c9cd66b8b6d3bc", "width": 640, "height": 488}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a3ba06688cc1992e79c2ea280839cd13e4b0063", "width": 960, "height": 732}, {"url": "https://external-preview.redd.it/eyXmikohjahmmR04HtSXJaBvx2rt_426rFTO31suIiw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5f371a9a1991838b4c850e5adb62d389bde796a", "width": 1080, "height": 823}], "variants": {}, "id": "csQ4U8qiLyOe4t7uz2oyBKWIrTrW4Z3Y939nxE1g1Bg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhhxs0", "is_robot_indexable": true, "report_reasons": null, "author": "Pbd1194", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhhxs0/supercharge_your_compute_strategy_with_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhhxs0/supercharge_your_compute_strategy_with_apache/", "subreddit_subscribers": 169780, "created_utc": 1710734969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering if anyone here moved from a SAP BW background into core Data Engineering field? Is this even possible? Looks like SAP experience is not strongly considered as a transferable skill to other tools. I have been learning data structures, streaming tools, databases etc. because its a very interesting space but unsure if I could ever switch to a DE job complementing with my SAP BW exp. ", "author_fullname": "t2_7qvd84fd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move from SAP BW to Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhfmka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710727789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if anyone here moved from a SAP BW background into core Data Engineering field? Is this even possible? Looks like SAP experience is not strongly considered as a transferable skill to other tools. I have been learning data structures, streaming tools, databases etc. because its a very interesting space but unsure if I could ever switch to a DE job complementing with my SAP BW exp. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhfmka", "is_robot_indexable": true, "report_reasons": null, "author": "SignificantOpinion92", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhfmka/how_to_move_from_sap_bw_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhfmka/how_to_move_from_sap_bw_to_data_engineering/", "subreddit_subscribers": 169780, "created_utc": 1710727789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a pipeline in GCP that involves extracting data from a PDF, modifying it, and then reconstructing the PDF with the original layout. Process-wise, this isn't terribly complicated, but I'm struggling with a tool to preserve the layout information to help with reconstructing the PDF after all is said and done. Does anyone know any tools, GCP-native or otherwise, I could use to capture/utilize the layout?", "author_fullname": "t2_o1ln6yad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recreating document layout in GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh5cf1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710701725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a pipeline in GCP that involves extracting data from a PDF, modifying it, and then reconstructing the PDF with the original layout. Process-wise, this isn&amp;#39;t terribly complicated, but I&amp;#39;m struggling with a tool to preserve the layout information to help with reconstructing the PDF after all is said and done. Does anyone know any tools, GCP-native or otherwise, I could use to capture/utilize the layout?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bh5cf1", "is_robot_indexable": true, "report_reasons": null, "author": "_tr9800a_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh5cf1/recreating_document_layout_in_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh5cf1/recreating_document_layout_in_gcp/", "subreddit_subscribers": 169780, "created_utc": 1710701725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New to DBT. \n\nI  want to run a CI/CD pipeline to perform DBT jobs against a test set in  Databricks. I want to do this in the most inexpensive way possible using  a very small amount of data so it can be triggered for every PR we  raise to the data models. Ideally this workflow would do DRY run tests so we don't need to launch any clustering. \n\n* Has anybody got any experience with this setup, can point me in the right direction, and ran into any issues? \n* Is this all possible in the  latest open source version of dbt-core?\n\nI also want to create a metadata ingest to list files from various blob store sources e.g. Azure Gen2, into a synchronised raw table of metadata of those files (for visibility/querying/and ontology). \n\n* Is this something DBT can help with? Original plan was to do this with isolated Autoloader Spark jobs, not 100% ideal. \n\nIdeally if we can shove as much as possible into DBT to provide managed CI/CD testing and deployment, and overall a structured workflow for our data tasks, that'd be great.\n\nThanks for any help! ", "author_fullname": "t2_duii2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD Databricks DBT and Github Actions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgwk0l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710679130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to DBT. &lt;/p&gt;\n\n&lt;p&gt;I  want to run a CI/CD pipeline to perform DBT jobs against a test set in  Databricks. I want to do this in the most inexpensive way possible using  a very small amount of data so it can be triggered for every PR we  raise to the data models. Ideally this workflow would do DRY run tests so we don&amp;#39;t need to launch any clustering. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Has anybody got any experience with this setup, can point me in the right direction, and ran into any issues? &lt;/li&gt;\n&lt;li&gt;Is this all possible in the  latest open source version of dbt-core?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I also want to create a metadata ingest to list files from various blob store sources e.g. Azure Gen2, into a synchronised raw table of metadata of those files (for visibility/querying/and ontology). &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is this something DBT can help with? Original plan was to do this with isolated Autoloader Spark jobs, not 100% ideal. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ideally if we can shove as much as possible into DBT to provide managed CI/CD testing and deployment, and overall a structured workflow for our data tasks, that&amp;#39;d be great.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bgwk0l", "is_robot_indexable": true, "report_reasons": null, "author": "MMACheerpuppy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bgwk0l/cicd_databricks_dbt_and_github_actions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bgwk0l/cicd_databricks_dbt_and_github_actions/", "subreddit_subscribers": 169780, "created_utc": 1710679130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I\u2019m the founder of Jinbaflow, a new low code workflow tool we\u2019ve been working on with the goal of turning anyone into a data analyst. We want to allow anyone to use English instructions and AI in order to easily transform, analyze, and visualize data. The idea is to keep it super straightforward with a flow-style interface so you can see how the data connects and flows together.\n\nWe want to add more AI-empowered tools beyond the code generation to allow you to focus less on coding and more on the data. Currently, we're using GPT-4 and we're super happy with the results and how good it is at generating Pandas code for us.\n\nWe would love to get some early feedback, so if you\u2019re interested please sign up here!\n\nhttps://useflow.jinba.ai/", "author_fullname": "t2_eciwmpu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jinbaflow: A New GPT-Empowered Data Analysis Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bhk49v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710743151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I\u2019m the founder of Jinbaflow, a new low code workflow tool we\u2019ve been working on with the goal of turning anyone into a data analyst. We want to allow anyone to use English instructions and AI in order to easily transform, analyze, and visualize data. The idea is to keep it super straightforward with a flow-style interface so you can see how the data connects and flows together.&lt;/p&gt;\n\n&lt;p&gt;We want to add more AI-empowered tools beyond the code generation to allow you to focus less on coding and more on the data. Currently, we&amp;#39;re using GPT-4 and we&amp;#39;re super happy with the results and how good it is at generating Pandas code for us.&lt;/p&gt;\n\n&lt;p&gt;We would love to get some early feedback, so if you\u2019re interested please sign up here!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://useflow.jinba.ai/\"&gt;https://useflow.jinba.ai/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhk49v", "is_robot_indexable": true, "report_reasons": null, "author": "sho-ma", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhk49v/jinbaflow_a_new_gptempowered_data_analysis_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhk49v/jinbaflow_a_new_gptempowered_data_analysis_tool/", "subreddit_subscribers": 169780, "created_utc": 1710743151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are looking into starrocks as the Open Lakehouse platform as query engine over glue(catalog) + iceberg open file format. My question is can anyone comment on the performance perpspective of this setup vs. the Starrocks with its own native storage? Will it still be levrage its outstanding Multi table Join performance as well SIMD based vectorized query engine architecture?  With its distributed MPP in memory query engine, is there any hard memory size constraints when dealing with Extremely large table tables (say billions of rows) when using the LakeHouse Iceberg table as the storage?  Thanks,  ", "author_fullname": "t2_8c6mucf3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starrocks + glue + iceberg performance vs. starrocks native storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhj9jk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710739707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking into starrocks as the Open Lakehouse platform as query engine over glue(catalog) + iceberg open file format. My question is can anyone comment on the performance perpspective of this setup vs. the Starrocks with its own native storage? Will it still be levrage its outstanding Multi table Join performance as well SIMD based vectorized query engine architecture?  With its distributed MPP in memory query engine, is there any hard memory size constraints when dealing with Extremely large table tables (say billions of rows) when using the LakeHouse Iceberg table as the storage?  Thanks,  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhj9jk", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Armadillo7867", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhj9jk/starrocks_glue_iceberg_performance_vs_starrocks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhj9jk/starrocks_glue_iceberg_performance_vs_starrocks/", "subreddit_subscribers": 169780, "created_utc": 1710739707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently PoCing the new AWS Zero ETL DynamoDB/Redshift integration, and infrastructure-wise, it seems very straightforward and simple. However, the table created by the integration holds all columns of the DynamoDB source table as a single column as a large [super type](https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html) named `value`, which I have zero experience with!\n\nMy first thought here is to create a view where I extract elements of `value` into distinct, normalized columns:\n\nFor example, suppose the value of one row of `value` is:\n\n    {\n      \"id\": {\n        \"S\": \"RcVpEFPNIAMFytg=\"\n      },\n      \"timestamp\": {\n        \"N\": \"1705090208167\"\n      },\n      \"files\": {\n        \"L\": [\n          {\n            \"S\": \"s3://example/file1.raw\"\n          },\n          {\n            \"S\": \"s3://example/file2.raw\"\n          },\n          {\n            \"S\": \"s3://example/file3.raw\"\n          }\n        ]\n      }\n    }\n\nI could theoretically create a table/view to query this like:\n\n    with s1 as\n    (\n        select\n            json_extract_path_text(json_serialize(e.value), 'id', 'S')::varchar as id,\n            nullif(json_extract_path_text(json_serialize(e.value), 'timestamp', 'N'), '')::bigint as timestamp,\n            json_extract_path_text(json_serialize(e.value), 'files', 'L') as files\n        from \"example\".\"public\".\"example\" e\n    )\n    select *\n    from s1\n    order by s1.timestamp desc;\n\nHowever, I've never worked with super variables before, and I have two big questions:\n\n1. Am I right to be extracting `super` type columns into normalized columns like this, or should I be querying them as they are?\n2. This is a bit more of a detailed question, but how do I successfully extract arrays of `super` type columns? For example, in the above, the new `data` column would still be a `super` (`[{\"S\":\"s3://example/file1.raw\"},{\"S\":\"s3://example/file2.raw\"},{\"S\":\"s3://example/file3.raw\"}`), and it's not clear both how this should be extracted and 3N normalized into separate columns?\n\nAny help is appreciated, thanks!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should Redshift columns of type super be queried?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhexl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710725792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently PoCing the new AWS Zero ETL DynamoDB/Redshift integration, and infrastructure-wise, it seems very straightforward and simple. However, the table created by the integration holds all columns of the DynamoDB source table as a single column as a large &lt;a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html\"&gt;super type&lt;/a&gt; named &lt;code&gt;value&lt;/code&gt;, which I have zero experience with!&lt;/p&gt;\n\n&lt;p&gt;My first thought here is to create a view where I extract elements of &lt;code&gt;value&lt;/code&gt; into distinct, normalized columns:&lt;/p&gt;\n\n&lt;p&gt;For example, suppose the value of one row of &lt;code&gt;value&lt;/code&gt; is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;id&amp;quot;: {\n    &amp;quot;S&amp;quot;: &amp;quot;RcVpEFPNIAMFytg=&amp;quot;\n  },\n  &amp;quot;timestamp&amp;quot;: {\n    &amp;quot;N&amp;quot;: &amp;quot;1705090208167&amp;quot;\n  },\n  &amp;quot;files&amp;quot;: {\n    &amp;quot;L&amp;quot;: [\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file1.raw&amp;quot;\n      },\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file2.raw&amp;quot;\n      },\n      {\n        &amp;quot;S&amp;quot;: &amp;quot;s3://example/file3.raw&amp;quot;\n      }\n    ]\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I could theoretically create a table/view to query this like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;with s1 as\n(\n    select\n        json_extract_path_text(json_serialize(e.value), &amp;#39;id&amp;#39;, &amp;#39;S&amp;#39;)::varchar as id,\n        nullif(json_extract_path_text(json_serialize(e.value), &amp;#39;timestamp&amp;#39;, &amp;#39;N&amp;#39;), &amp;#39;&amp;#39;)::bigint as timestamp,\n        json_extract_path_text(json_serialize(e.value), &amp;#39;files&amp;#39;, &amp;#39;L&amp;#39;) as files\n    from &amp;quot;example&amp;quot;.&amp;quot;public&amp;quot;.&amp;quot;example&amp;quot; e\n)\nselect *\nfrom s1\norder by s1.timestamp desc;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;However, I&amp;#39;ve never worked with super variables before, and I have two big questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Am I right to be extracting &lt;code&gt;super&lt;/code&gt; type columns into normalized columns like this, or should I be querying them as they are?&lt;/li&gt;\n&lt;li&gt;This is a bit more of a detailed question, but how do I successfully extract arrays of &lt;code&gt;super&lt;/code&gt; type columns? For example, in the above, the new &lt;code&gt;data&lt;/code&gt; column would still be a &lt;code&gt;super&lt;/code&gt; (&lt;code&gt;[{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file1.raw&amp;quot;},{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file2.raw&amp;quot;},{&amp;quot;S&amp;quot;:&amp;quot;s3://example/file3.raw&amp;quot;}&lt;/code&gt;), and it&amp;#39;s not clear both how this should be extracted and 3N normalized into separate columns?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help is appreciated, thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhexl3", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhexl3/how_should_redshift_columns_of_type_super_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhexl3/how_should_redshift_columns_of_type_super_be/", "subreddit_subscribers": 169780, "created_utc": 1710725792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Team,\n\nlike all data engineers, we want to have the modern data stack for a data transformation initiative. My context is that I'm having trouble getting management approval on this (mostly due to costs) and I think it is mostly due to data literacy gaps. We're doing a lot of that (teaching them) but it seems it would take more than just informing them to get them to invest. It's a large enterprise that doesn't have a lot of consumer data, but lots of other data opportunities to explore for analytics use cases in the form of supplier, vendor, partner, location data and varying industries. They currently don't have an enterprise wide implementation of anything. ALSO, they don't trust the cloud and currently is a heavy MS user.\n\nso my question to the group is, given this context, what would you advise me to do as next step? Here are my thoughts but I'm open to ideas and stories. I would recommend to at least start with building a business intelligence platform with datawarehouse first. I'm open to using ms products if it will help speed things up a long e.g. power bi and ms sql as the datawarehouse. Then I'll probably just look for an open source tool (maybe - i'm not sure what ms has in terms of on-prem integration tool) to use for ingestion then a separate server for machine learning deployments (we can use local laptops for development). my only problem as well is the encryption/ hashing tools for PII, i think. use this for a few years to prove the use cases, then hopefully gain enough approval to invest in an on-prem mds.   \n  \nI was thinking the tech debt would be worth it just to get things started and prove that having these things in place works to get more buy in later on. would really appreciate everyone's thoughts thank you!", "author_fullname": "t2_56myc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having trouble moving to MDS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhdssw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710722815.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710722568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Team,&lt;/p&gt;\n\n&lt;p&gt;like all data engineers, we want to have the modern data stack for a data transformation initiative. My context is that I&amp;#39;m having trouble getting management approval on this (mostly due to costs) and I think it is mostly due to data literacy gaps. We&amp;#39;re doing a lot of that (teaching them) but it seems it would take more than just informing them to get them to invest. It&amp;#39;s a large enterprise that doesn&amp;#39;t have a lot of consumer data, but lots of other data opportunities to explore for analytics use cases in the form of supplier, vendor, partner, location data and varying industries. They currently don&amp;#39;t have an enterprise wide implementation of anything. ALSO, they don&amp;#39;t trust the cloud and currently is a heavy MS user.&lt;/p&gt;\n\n&lt;p&gt;so my question to the group is, given this context, what would you advise me to do as next step? Here are my thoughts but I&amp;#39;m open to ideas and stories. I would recommend to at least start with building a business intelligence platform with datawarehouse first. I&amp;#39;m open to using ms products if it will help speed things up a long e.g. power bi and ms sql as the datawarehouse. Then I&amp;#39;ll probably just look for an open source tool (maybe - i&amp;#39;m not sure what ms has in terms of on-prem integration tool) to use for ingestion then a separate server for machine learning deployments (we can use local laptops for development). my only problem as well is the encryption/ hashing tools for PII, i think. use this for a few years to prove the use cases, then hopefully gain enough approval to invest in an on-prem mds.   &lt;/p&gt;\n\n&lt;p&gt;I was thinking the tech debt would be worth it just to get things started and prove that having these things in place works to get more buy in later on. would really appreciate everyone&amp;#39;s thoughts thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhdssw", "is_robot_indexable": true, "report_reasons": null, "author": "saintmichel", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhdssw/having_trouble_moving_to_mds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhdssw/having_trouble_moving_to_mds/", "subreddit_subscribers": 169780, "created_utc": 1710722568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are using SQL Synapse Dedicated Pool which is essentially SQL DW with ELT processes\n\nSo current process look like\n\nLoad raw incremental data from files to staging tables in vendor specific schema in DW\n\nPost that we have stored procs which load that data into main table where incremental data get appended to historic data\n\nFinal layer would be another set of stored proc which move data to dwh schema tables\n\nSo currently everything is in same DW but in different schema and dwh supports various reports (reports are import mode so each interaction does nit hit DW) and some external vendors\n\nThis system could run into issue if number of external vendors that it needs to support increases beyond certain point and if this vendors keep hitting DW with their queries throughout the day as that would give less processing power to other processed\n\nWe are thinking of either creating read only copy of just dwh schema or taking dwh schema entirely on different DW\n\nHowever if we move it out entirely we will have develop equivalent of existing stored procs in either databricks or some other ETL tool and if we just copy dwh to either parquet files or different db then copy process will have to built\n\nIdeally copy would be simple to do but daily copy might take time\n\nWhat would you do in this situation?", "author_fullname": "t2_50u0h0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Separate DB vs Separate Schema for Raw, Processed &amp; DWH?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh4lau", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710699957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are using SQL Synapse Dedicated Pool which is essentially SQL DW with ELT processes&lt;/p&gt;\n\n&lt;p&gt;So current process look like&lt;/p&gt;\n\n&lt;p&gt;Load raw incremental data from files to staging tables in vendor specific schema in DW&lt;/p&gt;\n\n&lt;p&gt;Post that we have stored procs which load that data into main table where incremental data get appended to historic data&lt;/p&gt;\n\n&lt;p&gt;Final layer would be another set of stored proc which move data to dwh schema tables&lt;/p&gt;\n\n&lt;p&gt;So currently everything is in same DW but in different schema and dwh supports various reports (reports are import mode so each interaction does nit hit DW) and some external vendors&lt;/p&gt;\n\n&lt;p&gt;This system could run into issue if number of external vendors that it needs to support increases beyond certain point and if this vendors keep hitting DW with their queries throughout the day as that would give less processing power to other processed&lt;/p&gt;\n\n&lt;p&gt;We are thinking of either creating read only copy of just dwh schema or taking dwh schema entirely on different DW&lt;/p&gt;\n\n&lt;p&gt;However if we move it out entirely we will have develop equivalent of existing stored procs in either databricks or some other ETL tool and if we just copy dwh to either parquet files or different db then copy process will have to built&lt;/p&gt;\n\n&lt;p&gt;Ideally copy would be simple to do but daily copy might take time&lt;/p&gt;\n\n&lt;p&gt;What would you do in this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bh4lau", "is_robot_indexable": true, "report_reasons": null, "author": "dilkushpatel", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh4lau/separate_db_vs_separate_schema_for_raw_processed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh4lau/separate_db_vs_separate_schema_for_raw_processed/", "subreddit_subscribers": 169780, "created_utc": 1710699957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Similar to the docker operator we have in airflow do we have something similar in mage, prefect or Dagster? ", "author_fullname": "t2_iojbk0c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker operator alternative", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh3fsi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710697135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Similar to the docker operator we have in airflow do we have something similar in mage, prefect or Dagster? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bh3fsi", "is_robot_indexable": true, "report_reasons": null, "author": "LogicalEmploy3558", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bh3fsi/docker_operator_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bh3fsi/docker_operator_alternative/", "subreddit_subscribers": 169780, "created_utc": 1710697135.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}