{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nMy hope this year was to rebuild my server rack to hold a couple massive JBODs, but some financial events prevented me from realizing that dream.\n\nI'm currently living many of your nightmares - about 20TB stored across 4TB drives, hitting its maximum, with not a single parity bit in sight. The drives are well past their MTTF. I don't have a lot I can spend on this project, but some.\n\nMy initial thought was to simply buy 3x 16TB drives and replace them in-situ in the HTPC chassis I've been using for years now, creating a small ZFS pod, which would let me extend it later. Since the chassis is maxed out, I can remove the individual drives and use a SATA USB adapter I have to reload the data back to the new drives. That said - it's been years since I've bought drives to expand this thing (it's full of 4TB, when that was about the max you could buy). I'm well aware of avoiding SMR, but are there any other considerations I should take, since I'll be doing this \"stop gap\" project on a budget? Back in the day, I would simply buy some WD Reds and be done with it (but I'm under the impression that even that's not a great experience, these days).\n\nAny help would be appreciated, and any tips on what to look for, avoid, or how to create a good ZFS storage pod on a budget (as someone getting into ZFS for the first time, as well) would be very much appreciated! Noise is not an issue as I have my HTPC chassis mounted in my server rack in my basement.", "author_fullname": "t2_43498", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best, Current Way to Rebuild Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgxank", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710681597.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710681410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;My hope this year was to rebuild my server rack to hold a couple massive JBODs, but some financial events prevented me from realizing that dream.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently living many of your nightmares - about 20TB stored across 4TB drives, hitting its maximum, with not a single parity bit in sight. The drives are well past their MTTF. I don&amp;#39;t have a lot I can spend on this project, but some.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was to simply buy 3x 16TB drives and replace them in-situ in the HTPC chassis I&amp;#39;ve been using for years now, creating a small ZFS pod, which would let me extend it later. Since the chassis is maxed out, I can remove the individual drives and use a SATA USB adapter I have to reload the data back to the new drives. That said - it&amp;#39;s been years since I&amp;#39;ve bought drives to expand this thing (it&amp;#39;s full of 4TB, when that was about the max you could buy). I&amp;#39;m well aware of avoiding SMR, but are there any other considerations I should take, since I&amp;#39;ll be doing this &amp;quot;stop gap&amp;quot; project on a budget? Back in the day, I would simply buy some WD Reds and be done with it (but I&amp;#39;m under the impression that even that&amp;#39;s not a great experience, these days).&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated, and any tips on what to look for, avoid, or how to create a good ZFS storage pod on a budget (as someone getting into ZFS for the first time, as well) would be very much appreciated! Noise is not an issue as I have my HTPC chassis mounted in my server rack in my basement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bgxank", "is_robot_indexable": true, "report_reasons": null, "author": "wspnut", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bgxank/best_current_way_to_rebuild_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bgxank/best_current_way_to_rebuild_storage/", "subreddit_subscribers": 739402, "created_utc": 1710681410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A while ago I had posted regarding a massive number of images I had scraped from sites that had public domain or cc0 content. Unfortunately I had to take that down due to some server costs and time.\n\n[https://www.reddit.com/r/DataHoarder/comments/112ihjk/the\\_public\\_conscious\\_centralization\\_of\\_nearly\\_2/](https://www.reddit.com/r/DataHoarder/comments/112ihjk/the_public_conscious_centralization_of_nearly_2/)\n\nWell, I am happy to announce that I am on my way to being back online:\n\nhttp://pc.lowframerate.studio/\n\nIm hoping to be back on par with the former library soon. Right now its only about 200gb.\n\nI now have a new server stack that is way more affordable to run! Ill be getting the library back to its former glory. Some of the major players are already up, including my favorite, gutenberg. This time around the sites are also less dynamic. So Im hoping this makes mirrors simple. I plan to package the sites themselves or the images as torrents. Let me know which youd prefer.   \n\n\nPlease enjoy!", "author_fullname": "t2_5gy7bcwa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Public Conscious - Reborn!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhjs9l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710742395.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710741756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while ago I had posted regarding a massive number of images I had scraped from sites that had public domain or cc0 content. Unfortunately I had to take that down due to some server costs and time.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/112ihjk/the_public_conscious_centralization_of_nearly_2/\"&gt;https://www.reddit.com/r/DataHoarder/comments/112ihjk/the_public_conscious_centralization_of_nearly_2/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Well, I am happy to announce that I am on my way to being back online:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://pc.lowframerate.studio/\"&gt;http://pc.lowframerate.studio/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Im hoping to be back on par with the former library soon. Right now its only about 200gb.&lt;/p&gt;\n\n&lt;p&gt;I now have a new server stack that is way more affordable to run! Ill be getting the library back to its former glory. Some of the major players are already up, including my favorite, gutenberg. This time around the sites are also less dynamic. So Im hoping this makes mirrors simple. I plan to package the sites themselves or the images as torrents. Let me know which youd prefer.   &lt;/p&gt;\n\n&lt;p&gt;Please enjoy!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhjs9l", "is_robot_indexable": true, "report_reasons": null, "author": "PoweredBy90sAI", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhjs9l/the_public_conscious_reborn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhjs9l/the_public_conscious_reborn/", "subreddit_subscribers": 739402, "created_utc": 1710741756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a local data backup system whereby my unencrypted Windows 10 machine is manually backed up to my local Synology NAS, which then automatically syncs and local encrypts a copy of my data to the cloud via HyperBackup. My backup image is currently around 7 TB.\n\nI have intentionally kept my local files unencrypted because, on the balance, I think it is more likely that I will be able to recover files in the future if they are not on some encrypted disk array and this outweighs the probability of device theft. I may or may not be right about this and it might change in the future.\n\nI am now interested in cloning my backup image to an external drive and leaving it at my parent's house for extra redundancy. I'd like to encrypt this drive so if I forget about it or whatever its fine. What is the most future proof way to do this? VeraCrypt keeps coming up in my searches. Is this the best way?\n\nDesired features:\n\n1. Encrypt a disk with a password, not a 128 byte key that can't be remembered\n2. Runs on windows, but doesn't have deep tendrils into the OS, making it more likely to be runnable in the future (i.e. no driver/kernel stuff)\n3. Open source with large enough userbase that the software itself won't bitrot over the next 10 years\n4. Resistant to drive failures where bits might get flipped. I'm cool buying a 20TB drive and using half the disk for parity (or more).\n5. I don't want to run a mess of python code or script anything. Complexity is an enemy.\n\nFeatures I don't care about:\n\n1. The ability to update the volume (write-once is fine; I don't need incremental backups).", "author_fullname": "t2_3bjakm7z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future proof encryption for cold storage data disks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh9elh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710711416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a local data backup system whereby my unencrypted Windows 10 machine is manually backed up to my local Synology NAS, which then automatically syncs and local encrypts a copy of my data to the cloud via HyperBackup. My backup image is currently around 7 TB.&lt;/p&gt;\n\n&lt;p&gt;I have intentionally kept my local files unencrypted because, on the balance, I think it is more likely that I will be able to recover files in the future if they are not on some encrypted disk array and this outweighs the probability of device theft. I may or may not be right about this and it might change in the future.&lt;/p&gt;\n\n&lt;p&gt;I am now interested in cloning my backup image to an external drive and leaving it at my parent&amp;#39;s house for extra redundancy. I&amp;#39;d like to encrypt this drive so if I forget about it or whatever its fine. What is the most future proof way to do this? VeraCrypt keeps coming up in my searches. Is this the best way?&lt;/p&gt;\n\n&lt;p&gt;Desired features:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Encrypt a disk with a password, not a 128 byte key that can&amp;#39;t be remembered&lt;/li&gt;\n&lt;li&gt;Runs on windows, but doesn&amp;#39;t have deep tendrils into the OS, making it more likely to be runnable in the future (i.e. no driver/kernel stuff)&lt;/li&gt;\n&lt;li&gt;Open source with large enough userbase that the software itself won&amp;#39;t bitrot over the next 10 years&lt;/li&gt;\n&lt;li&gt;Resistant to drive failures where bits might get flipped. I&amp;#39;m cool buying a 20TB drive and using half the disk for parity (or more).&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t want to run a mess of python code or script anything. Complexity is an enemy.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Features I don&amp;#39;t care about:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The ability to update the volume (write-once is fine; I don&amp;#39;t need incremental backups).&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bh9elh", "is_robot_indexable": true, "report_reasons": null, "author": "Meeple-Mayor", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bh9elh/future_proof_encryption_for_cold_storage_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bh9elh/future_proof_encryption_for_cold_storage_data/", "subreddit_subscribers": 739402, "created_utc": 1710711416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nSABRENT 2TB Rocket 4 Plus NVMe 4.0 Gen4 PCIe M.2 Internal SSD Extreme Performance\n\n$130 August 10,2023\n$275 March 17, 2024\n\nThis is common for all the drives.  What is driving the prices so high?  &lt;/CYNICISM&gt;  I\u2019m told by the media inflation is over and the politicians should be re-elected because they heroically squashed it. &lt;CYNICISM/&gt;\n\nI want a lot of NVMe to run private large data for both relational database and for experimenting with AI without using equipment I don\u2019t control, i.e. cloud systems.\n\nThis increase in prices on RAM and in GPU ( separate issue ) is making this effort unaffordable.\n\nOutside of politics and monetary policies of US and financial interactions between countries ( Tariffs ), what I am not seeing that is driving up these costs?\n\n", "author_fullname": "t2_b8fjqdv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is going on with the prices is NVMe M.2 SSDs? 211% the price of 8 months ago", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhb75s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710715675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SABRENT 2TB Rocket 4 Plus NVMe 4.0 Gen4 PCIe M.2 Internal SSD Extreme Performance&lt;/p&gt;\n\n&lt;p&gt;$130 August 10,2023\n$275 March 17, 2024&lt;/p&gt;\n\n&lt;p&gt;This is common for all the drives.  What is driving the prices so high?  &amp;lt;/CYNICISM&amp;gt;  I\u2019m told by the media inflation is over and the politicians should be re-elected because they heroically squashed it. &amp;lt;CYNICISM/&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;I want a lot of NVMe to run private large data for both relational database and for experimenting with AI without using equipment I don\u2019t control, i.e. cloud systems.&lt;/p&gt;\n\n&lt;p&gt;This increase in prices on RAM and in GPU ( separate issue ) is making this effort unaffordable.&lt;/p&gt;\n\n&lt;p&gt;Outside of politics and monetary policies of US and financial interactions between countries ( Tariffs ), what I am not seeing that is driving up these costs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhb75s", "is_robot_indexable": true, "report_reasons": null, "author": "ValuePeople", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhb75s/what_is_going_on_with_the_prices_is_nvme_m2_ssds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhb75s/what_is_going_on_with_the_prices_is_nvme_m2_ssds/", "subreddit_subscribers": 739402, "created_utc": 1710715675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Usually it seems HBAs have 2 SAS connectors for up to 8 drives. If you want to add more drives you add another HBA or connect up an expander. Seems straightforward enough\u2026\n\nBut then I also see 16i cards, 16i4e cards, 24i and even 24i4e cards. What\u2019s the deal with these? Are they HBA and expander in one? Or do their chips just allow for more drives like a more powerful version of an 8i card?", "author_fullname": "t2_lgb3co8e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-connect HBAs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgvyoz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710677139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Usually it seems HBAs have 2 SAS connectors for up to 8 drives. If you want to add more drives you add another HBA or connect up an expander. Seems straightforward enough\u2026&lt;/p&gt;\n\n&lt;p&gt;But then I also see 16i cards, 16i4e cards, 24i and even 24i4e cards. What\u2019s the deal with these? Are they HBA and expander in one? Or do their chips just allow for more drives like a more powerful version of an 8i card?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bgvyoz", "is_robot_indexable": true, "report_reasons": null, "author": "Acceptable-Rise8783", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bgvyoz/multiconnect_hbas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bgvyoz/multiconnect_hbas/", "subreddit_subscribers": 739402, "created_utc": 1710677139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know this question may be a bit silly, but I need to verify.\n\nSo most scanner software DPI presets are 75, 150, 300, 600, 1200, etc\n\nI find that 600 isn\u2019t enough, while 1200 takes too long.\n\n**My question: Is it okay to scan at 800 DPI?** (would it create weird artifacts?) Or does the DPI need to be evenly divisible by 75 for the best \u201ctrue scan\u201d? Are scanners built to scan the most optimally (speed and quality) at their original presets of 75, 150, 300, 600, 1200?\n\nHere\u2019s my current scan plan for archiving:\n\n* 8.5x11\u201d text documents @ 300 DPI\n* 8.5x11\u201d artwork @ 800 DPI\n* 4x6\u201d photos @ 800 DPI\n\nThanks all!\n\nFor context, the scanner that I\u2019m using is the Epson Perfection V39 II", "author_fullname": "t2_4i1qb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanner DPI presets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh94wy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710712956.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710710774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this question may be a bit silly, but I need to verify.&lt;/p&gt;\n\n&lt;p&gt;So most scanner software DPI presets are 75, 150, 300, 600, 1200, etc&lt;/p&gt;\n\n&lt;p&gt;I find that 600 isn\u2019t enough, while 1200 takes too long.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My question: Is it okay to scan at 800 DPI?&lt;/strong&gt; (would it create weird artifacts?) Or does the DPI need to be evenly divisible by 75 for the best \u201ctrue scan\u201d? Are scanners built to scan the most optimally (speed and quality) at their original presets of 75, 150, 300, 600, 1200?&lt;/p&gt;\n\n&lt;p&gt;Here\u2019s my current scan plan for archiving:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;8.5x11\u201d text documents @ 300 DPI&lt;/li&gt;\n&lt;li&gt;8.5x11\u201d artwork @ 800 DPI&lt;/li&gt;\n&lt;li&gt;4x6\u201d photos @ 800 DPI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks all!&lt;/p&gt;\n\n&lt;p&gt;For context, the scanner that I\u2019m using is the Epson Perfection V39 II&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bh94wy", "is_robot_indexable": true, "report_reasons": null, "author": "DentThat", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bh94wy/scanner_dpi_presets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bh94wy/scanner_dpi_presets/", "subreddit_subscribers": 739402, "created_utc": 1710710774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have several drives, small ones that I just throw shows on and other none important stuff. But there are so many of them they are all getting in a mess. The only option I went with was writing on postit notes and sticking them to the drive.\n\nNow I'm think of just writing\n\nA\n\nB\n\nC\n\nD\n\nA1\n\nB1\n\nC1\n\nD1\n\nAnd so on, on the drive and then in a text file have what that drive contains.\n\nAnyone have any other systems?", "author_fullname": "t2_3xqpnc72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What organisation app do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhfxlt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710728683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several drives, small ones that I just throw shows on and other none important stuff. But there are so many of them they are all getting in a mess. The only option I went with was writing on postit notes and sticking them to the drive.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m think of just writing&lt;/p&gt;\n\n&lt;p&gt;A&lt;/p&gt;\n\n&lt;p&gt;B&lt;/p&gt;\n\n&lt;p&gt;C&lt;/p&gt;\n\n&lt;p&gt;D&lt;/p&gt;\n\n&lt;p&gt;A1&lt;/p&gt;\n\n&lt;p&gt;B1&lt;/p&gt;\n\n&lt;p&gt;C1&lt;/p&gt;\n\n&lt;p&gt;D1&lt;/p&gt;\n\n&lt;p&gt;And so on, on the drive and then in a text file have what that drive contains.&lt;/p&gt;\n\n&lt;p&gt;Anyone have any other systems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhfxlt", "is_robot_indexable": true, "report_reasons": null, "author": "steviefaux", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhfxlt/what_organisation_app_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhfxlt/what_organisation_app_do_you_use/", "subreddit_subscribers": 739402, "created_utc": 1710728683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Say that you have a few random size and speed drives into a simple resiliency pool.  How does Windows handle read/write speeds?  Does Windows just get presented with whatever drive speed that the current copy job is using?  So it could fluctuate depending on what drive is being used at the moment?\n\nFor a Two drive resiliency, the files are being read/write to both drives, so whichever drive is the slowest in the pool, is what speed that you get?\n\nParity resilience I would guess may be slower than both simple and two drive resilience, because of the nature of a software raid 5 situation there?", "author_fullname": "t2_16fj1s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces Speed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhcgg0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710718920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say that you have a few random size and speed drives into a simple resiliency pool.  How does Windows handle read/write speeds?  Does Windows just get presented with whatever drive speed that the current copy job is using?  So it could fluctuate depending on what drive is being used at the moment?&lt;/p&gt;\n\n&lt;p&gt;For a Two drive resiliency, the files are being read/write to both drives, so whichever drive is the slowest in the pool, is what speed that you get?&lt;/p&gt;\n\n&lt;p&gt;Parity resilience I would guess may be slower than both simple and two drive resilience, because of the nature of a software raid 5 situation there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhcgg0", "is_robot_indexable": true, "report_reasons": null, "author": "_Nismo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhcgg0/storage_spaces_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhcgg0/storage_spaces_speed/", "subreddit_subscribers": 739402, "created_utc": 1710718920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've long backed up my DVDs, and now I have a drive capable of reading BDs so I want to back those up too.\n\nI want to make an ISO, so something like AnyDVD (which can't make ISOs) sounds like it wouldn't work for me.  DVDFab is what I used to use in the past but apparently nowadays you can only make ISOs of three discs and then you must register it?\n\nBasically:  Need a program that is inexpensive or free, easy to use, and makes ISOs of blurays.", "author_fullname": "t2_yjou4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bluray backup program suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhbg5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710716296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve long backed up my DVDs, and now I have a drive capable of reading BDs so I want to back those up too.&lt;/p&gt;\n\n&lt;p&gt;I want to make an ISO, so something like AnyDVD (which can&amp;#39;t make ISOs) sounds like it wouldn&amp;#39;t work for me.  DVDFab is what I used to use in the past but apparently nowadays you can only make ISOs of three discs and then you must register it?&lt;/p&gt;\n\n&lt;p&gt;Basically:  Need a program that is inexpensive or free, easy to use, and makes ISOs of blurays.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhbg5f", "is_robot_indexable": true, "report_reasons": null, "author": "MoeDantes", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhbg5f/bluray_backup_program_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhbg5f/bluray_backup_program_suggestions/", "subreddit_subscribers": 739402, "created_utc": 1710716296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i've been using ntfs-3g for a long while now, and it never gave me any trouble. until this weekend. on two different computers and two different operating systems (Ubuntu and Manjaro) - don't know exact versions of `ntfs-3g` itself - it behaved weird and caused one data loss and one almost-data-loss.\n\n - first, on Manjaro system, i copied a bunch of files to a USB connected drive, which already contained some data. this resulted in original files disappearing from root directory and only new, copied files being visible. occupied space was the sum of old and new files' size. upon trying to see what's happening on this drive on a Windows OS, the drive started to show as RAW in DiskManager, and at that point i called quits.\n\n - second, on Ubuntu system, i tried to bulk rename a bunch of files. this resulted in half of contents of root directory disappearing (again!) and showing correct occupied/free space (again!). only this time, i was able to repair (chkdsk /f) the filesystem on WIndows, and files were brought back (recovering orhpaned blah blah...)\n\nno data was lost, because i had all this backed up, but it serously weakened my trust in FOSS NTFS implementation. one drive was using GPT schema, other was MBR with extended partition. have you heard of any serious bugs in ntfs-3g lately? or maybe ever experienced something similar? normal day-to-day operations (like data copying or files renaming) should not produce such effects. i'm seriously considering ditching NTFS on Linux and use network as a translation layer.", "author_fullname": "t2_14uv2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ntfs-3g errors?/bugs?/weird behavior and data corruption.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh1nui", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710692708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve been using ntfs-3g for a long while now, and it never gave me any trouble. until this weekend. on two different computers and two different operating systems (Ubuntu and Manjaro) - don&amp;#39;t know exact versions of &lt;code&gt;ntfs-3g&lt;/code&gt; itself - it behaved weird and caused one data loss and one almost-data-loss.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;first, on Manjaro system, i copied a bunch of files to a USB connected drive, which already contained some data. this resulted in original files disappearing from root directory and only new, copied files being visible. occupied space was the sum of old and new files&amp;#39; size. upon trying to see what&amp;#39;s happening on this drive on a Windows OS, the drive started to show as RAW in DiskManager, and at that point i called quits.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;second, on Ubuntu system, i tried to bulk rename a bunch of files. this resulted in half of contents of root directory disappearing (again!) and showing correct occupied/free space (again!). only this time, i was able to repair (chkdsk /f) the filesystem on WIndows, and files were brought back (recovering orhpaned blah blah...)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;no data was lost, because i had all this backed up, but it serously weakened my trust in FOSS NTFS implementation. one drive was using GPT schema, other was MBR with extended partition. have you heard of any serious bugs in ntfs-3g lately? or maybe ever experienced something similar? normal day-to-day operations (like data copying or files renaming) should not produce such effects. i&amp;#39;m seriously considering ditching NTFS on Linux and use network as a translation layer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bh1nui", "is_robot_indexable": true, "report_reasons": null, "author": "paprok", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bh1nui/ntfs3g_errorsbugsweird_behavior_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bh1nui/ntfs3g_errorsbugsweird_behavior_and_data/", "subreddit_subscribers": 739402, "created_utc": 1710692708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got a 4-bay NAS with 4x6tb HDD that have hit capacity (with 1 showing a SMART failure), setup in RAID10.\n\nHave just ordered 4x 18tb HDD and I'm thinking despite a huge storage increase, I'd love to future proof by maximising capacity (but with redundancy) and get a lot of Remux 4k content on it so toying with RAID5.\n\nEssentially my NAS operates as a backup for important data (documents/family photos) which are stored on my main PC (and also in the Cloud) and as the sole storage for movies/TV shows (which I stream solely within my network on Plex).\n\nGiven the time to set this up I want to make sure I get it right first time. With my usage, is there any reason why I shouldn't go down the RAID5 route? (I sort of feel the main negative ie risk another drives fails during rebuild is negated by the fact that the data on the Nas is either not life important or is just a back up of stuff that is).", "author_fullname": "t2_5sbnoumi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expanding storage + RAID downgrade?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgu6di", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710670268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got a 4-bay NAS with 4x6tb HDD that have hit capacity (with 1 showing a SMART failure), setup in RAID10.&lt;/p&gt;\n\n&lt;p&gt;Have just ordered 4x 18tb HDD and I&amp;#39;m thinking despite a huge storage increase, I&amp;#39;d love to future proof by maximising capacity (but with redundancy) and get a lot of Remux 4k content on it so toying with RAID5.&lt;/p&gt;\n\n&lt;p&gt;Essentially my NAS operates as a backup for important data (documents/family photos) which are stored on my main PC (and also in the Cloud) and as the sole storage for movies/TV shows (which I stream solely within my network on Plex).&lt;/p&gt;\n\n&lt;p&gt;Given the time to set this up I want to make sure I get it right first time. With my usage, is there any reason why I shouldn&amp;#39;t go down the RAID5 route? (I sort of feel the main negative ie risk another drives fails during rebuild is negated by the fact that the data on the Nas is either not life important or is just a back up of stuff that is).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bgu6di", "is_robot_indexable": true, "report_reasons": null, "author": "reviewwworld", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bgu6di/expanding_storage_raid_downgrade/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bgu6di/expanding_storage_raid_downgrade/", "subreddit_subscribers": 739402, "created_utc": 1710670268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some web archiving stuff - may be useful to some.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhhhs9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_4a9ry", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "opendirectories", "selftext": "https://github.com/iipc/awesome-web-archiving\n\nfrom a bred on /g/ about saving a gardeners forum that's closing. Might xpost to datahoarders as well.", "author_fullname": "t2_4a9ry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some web archiving stuff - may be useful to some.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/opendirectories", "hidden": false, "pwls": 6, "link_flair_css_class": "psa-link", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhei51", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "PSA", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710724562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.opendirectories", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/iipc/awesome-web-archiving\"&gt;https://github.com/iipc/awesome-web-archiving&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;from a bred on /g/ about saving a gardeners forum that&amp;#39;s closing. Might xpost to datahoarders as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?auto=webp&amp;s=bcf55311cf8f00abf98f8980eef3bc43c057c736", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c48d6d5dc7b0ea9e338a0bca92e27908f8f809e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e89f720a4f5e202ad2382ba7b5b6d3c95fefc78", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d291f4f291a0c206f6fd80a77ccd89065fb22691", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=373f46dfe5ecb78b8f8ea58ccc92da53a0c702b1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2909e9cde9f010ae88906be39d17b14bbc0e908b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72022ba3134a76b21c1774ac127eb6bb56ec7543", "width": 1080, "height": 540}], "variants": {}, "id": "pfsWOjelY-55VVutvQPdcSzzkReF2GSMBW-_f_iL81I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b39d4fea-d6a6-11e1-a70b-12313b0ce1e2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r1e4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1bhei51", "is_robot_indexable": true, "report_reasons": null, "author": "ringofyre", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/opendirectories/comments/1bhei51/some_web_archiving_stuff_may_be_useful_to_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/opendirectories/comments/1bhei51/some_web_archiving_stuff_may_be_useful_to_some/", "subreddit_subscribers": 214970, "created_utc": 1710724562.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1710733554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.opendirectories", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/opendirectories/comments/1bhei51/some_web_archiving_stuff_may_be_useful_to_some/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?auto=webp&amp;s=bcf55311cf8f00abf98f8980eef3bc43c057c736", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c48d6d5dc7b0ea9e338a0bca92e27908f8f809e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e89f720a4f5e202ad2382ba7b5b6d3c95fefc78", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d291f4f291a0c206f6fd80a77ccd89065fb22691", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=373f46dfe5ecb78b8f8ea58ccc92da53a0c702b1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2909e9cde9f010ae88906be39d17b14bbc0e908b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/A7hTgmO86qzjefO00HMehSk0XGq7JG_ny2DN0nt3kbM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72022ba3134a76b21c1774ac127eb6bb56ec7543", "width": 1080, "height": 540}], "variants": {}, "id": "pfsWOjelY-55VVutvQPdcSzzkReF2GSMBW-_f_iL81I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhhhs9", "is_robot_indexable": true, "report_reasons": null, "author": "ringofyre", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1bhei51", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhhhs9/some_web_archiving_stuff_may_be_useful_to_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/opendirectories/comments/1bhei51/some_web_archiving_stuff_may_be_useful_to_some/", "subreddit_subscribers": 739402, "created_utc": 1710733554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking to replace 12 drives in Lacie 12Big. Compatibility list suggests that it is only Seagate Ironwolf Pro NE HDD\u2019s (while Lacie 6Big shown as compatible with both NT and NE series) https://www.lacie.com/gb/en/support/kb/lacie-big-products-drive-compatibility-list-007780en/\n\nI wonder if anyone can help me to figure out if Ironwolf NT\u2019s or Exos will work with that enclosure? Is the 14tb real top limit? (Can\u2019t find anything meaningful online).", "author_fullname": "t2_oeqb2xfs0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use of Seagate Exos in Lacie 12Big enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bgtjyo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710667595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to replace 12 drives in Lacie 12Big. Compatibility list suggests that it is only Seagate Ironwolf Pro NE HDD\u2019s (while Lacie 6Big shown as compatible with both NT and NE series) &lt;a href=\"https://www.lacie.com/gb/en/support/kb/lacie-big-products-drive-compatibility-list-007780en/\"&gt;https://www.lacie.com/gb/en/support/kb/lacie-big-products-drive-compatibility-list-007780en/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wonder if anyone can help me to figure out if Ironwolf NT\u2019s or Exos will work with that enclosure? Is the 14tb real top limit? (Can\u2019t find anything meaningful online).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bgtjyo", "is_robot_indexable": true, "report_reasons": null, "author": "ChallengeSeparate441", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bgtjyo/use_of_seagate_exos_in_lacie_12big_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bgtjyo/use_of_seagate_exos_in_lacie_12big_enclosure/", "subreddit_subscribers": 739402, "created_utc": 1710667595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I connected this device using my usb to sata adapter. On disk management it shows it already initialized, but also shows a capacity of 0 MB (it should show ~ 6 TB). It\u2019s assigned to drive D, but does not show up on my file explorer. I tried updating drives but it says everything is already updated. ", "author_fullname": "t2_ytho3gz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD not showing up as a Drive (Toshiba NAS N300)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhjsso", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710741815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I connected this device using my usb to sata adapter. On disk management it shows it already initialized, but also shows a capacity of 0 MB (it should show ~ 6 TB). It\u2019s assigned to drive D, but does not show up on my file explorer. I tried updating drives but it says everything is already updated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bhjsso", "is_robot_indexable": true, "report_reasons": null, "author": "Atomic310", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bhjsso/hdd_not_showing_up_as_a_drive_toshiba_nas_n300/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bhjsso/hdd_not_showing_up_as_a_drive_toshiba_nas_n300/", "subreddit_subscribers": 739402, "created_utc": 1710741815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm embarking on a project to perform sentiment analysis on news article headlines related to key tech companies: Amazon, Facebook, and Tesla. The timeframe of interest spans from January 1st, 2019, to February 28th, 2020. Can anybody recommend an easy but comprehensive tool, that is suitable for a newbie. (I need it for an academic project so I should be able to explain how I got the info without commiting 20 copyright crimes)", "author_fullname": "t2_dpccsybh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help: Tech News Data for Sentiment Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bh8mb8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710709536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m embarking on a project to perform sentiment analysis on news article headlines related to key tech companies: Amazon, Facebook, and Tesla. The timeframe of interest spans from January 1st, 2019, to February 28th, 2020. Can anybody recommend an easy but comprehensive tool, that is suitable for a newbie. (I need it for an academic project so I should be able to explain how I got the info without commiting 20 copyright crimes)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bh8mb8", "is_robot_indexable": true, "report_reasons": null, "author": "EagleSwemz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bh8mb8/need_help_tech_news_data_for_sentiment_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bh8mb8/need_help_tech_news_data_for_sentiment_analysis/", "subreddit_subscribers": 739402, "created_utc": 1710709536.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}