{"kind": "Listing", "data": {"after": "t3_1berdf5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF's of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.\n\nI work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.", "author_fullname": "t2_t1bnfnc4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the hardest you have ever seen someone work manually?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beltlg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 93, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 93, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710425040.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710424759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF&amp;#39;s of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.&lt;/p&gt;\n\n&lt;p&gt;I work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1beltlg", "is_robot_indexable": true, "report_reasons": null, "author": "bjogc42069", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/", "subreddit_subscribers": 168830, "created_utc": 1710424759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the  project.\n\nThe dataset had over 5000 tables that were used across the board. \n\nMost of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don\u2019t help.\n\nIs there a way to restore it because we might lose the client at this rate? \n\nSample id of a table: \u2018project.Master.table1\u2019", "author_fullname": "t2_5hvalgp8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to recover a deleted dataset from BigQuery? (Urgent)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bekue2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710421925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the  project.&lt;/p&gt;\n\n&lt;p&gt;The dataset had over 5000 tables that were used across the board. &lt;/p&gt;\n\n&lt;p&gt;Most of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don\u2019t help.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to restore it because we might lose the client at this rate? &lt;/p&gt;\n\n&lt;p&gt;Sample id of a table: \u2018project.Master.table1\u2019&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bekue2", "is_robot_indexable": true, "report_reasons": null, "author": "honpra", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/", "subreddit_subscribers": 168830, "created_utc": 1710421925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Early stage startups can't fund a DE team** obviously - not enough data, time, or money! But startups have a ton of data (stripe, customer, app analytics, finance/payments etc..) generally a better understanding of data. \n\n**So... for you guys:** how can early stage startups make the most of their data in your experience? Is there a way to stitch together all the key data without a large engineering effort? What specific examples have you seen work in your experience? Is this even a data engineering problem or is ad-hoc the way to go? ", "author_fullname": "t2_tszztjadn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should early stage startups approach data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be7kij", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710375587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Early stage startups can&amp;#39;t fund a DE team&lt;/strong&gt; obviously - not enough data, time, or money! But startups have a ton of data (stripe, customer, app analytics, finance/payments etc..) generally a better understanding of data. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So... for you guys:&lt;/strong&gt; how can early stage startups make the most of their data in your experience? Is there a way to stitch together all the key data without a large engineering effort? What specific examples have you seen work in your experience? Is this even a data engineering problem or is ad-hoc the way to go? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1be7kij", "is_robot_indexable": true, "report_reasons": null, "author": "jmack_startups", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be7kij/how_should_early_stage_startups_approach_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be7kij/how_should_early_stage_startups_approach_data/", "subreddit_subscribers": 168830, "created_utc": 1710375587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4ymkgdql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python requests best practices for data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1begg9k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ugbrL_EtU52ai4ErXvB2sZD09kEziLG1QVKocyJn93c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710405308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/python-requests-best-practices-for-data-engineers", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?auto=webp&amp;s=fdae695fd5fe2cee64e1fb4a7ec4483c6443e584", "width": 2912, "height": 1632}, "resolutions": [{"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f31260549e9d62ab9efa9beedc3fb6de9b637b1", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=930ed37cb47a4999c9f98a4e2bdb99166c32c43e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0669238db69cae8dcc36a03fc19af9df9df8b54b", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db93ea8d2e717d1b325f1c01272dad51892ecf90", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b7dcf25cde15dce4e116d09646fd78936820785", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77bd0feb4c744910ad476681f1a10765dff24509", "width": 1080, "height": 605}], "variants": {}, "id": "dW0cXVdRG0AiMhGU9NGc_dayd_CcrOfYkH0KamIBEUQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1begg9k", "is_robot_indexable": true, "report_reasons": null, "author": "SnooBeans3890", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1begg9k/python_requests_best_practices_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/python-requests-best-practices-for-data-engineers", "subreddit_subscribers": 168830, "created_utc": 1710405308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, founder at Latitude here.\n\nWe spent the last 2 years building software for data teams. After many iterations, we've decided to rebuild everything from scratch and open-source it for the entire community.\n\nLatitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.\n\nYou can check out the repo here: [https://github.com/latitude-dev/latitude](https://github.com/latitude-dev/latitude)\n\nWe're actively looking for feedback and contributors. Let me know your thoughts!", "author_fullname": "t2_o4qnw2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Latitude: an open-source web framework to build data apps using SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bej704", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710416485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, founder at Latitude here.&lt;/p&gt;\n\n&lt;p&gt;We spent the last 2 years building software for data teams. After many iterations, we&amp;#39;ve decided to rebuild everything from scratch and open-source it for the entire community.&lt;/p&gt;\n\n&lt;p&gt;Latitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.&lt;/p&gt;\n\n&lt;p&gt;You can check out the repo here: &lt;a href=\"https://github.com/latitude-dev/latitude\"&gt;https://github.com/latitude-dev/latitude&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re actively looking for feedback and contributors. Let me know your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?auto=webp&amp;s=6eebca997ea4d4b55aeb0f760233c0415a0bdf63", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d827756cdde837af4ec07d99725da141bd435f68", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5612dbc3c955b0133b686b2df347ee7160426a8d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d57471adc017d17799740c2e311be154ac893f0c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbdd7b25a13222445010fda16e55f3b9df18a71b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ff1193415b2a1522056645b8cd2f1bfb564810b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba65964d68ee85a55125b4e94d704ac87a36d2e3", "width": 1080, "height": 540}], "variants": {}, "id": "zRu4yaIX9sS06soWCMlaUqVBLjk_mkvd6UlhzEJIqNY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bej704", "is_robot_indexable": true, "report_reasons": null, "author": "EloquentPickle", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bej704/latitude_an_opensource_web_framework_to_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bej704/latitude_an_opensource_web_framework_to_build/", "subreddit_subscribers": 168830, "created_utc": 1710416485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing research on open source data quality tools, and I've found these so far:\n\n1. dbt core\n2. Apache Griffin\n3. Soda Core\n4. Deequ\n5. Tensorflow Data Validation\n6. Moby DQ\n7. Great Expectatons\n\nI've been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I'm missing here? \n\n(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).", "author_fullname": "t2_mc935wf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-Source Data Quality Tools Abound", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bemv7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710427511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing research on open source data quality tools, and I&amp;#39;ve found these so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;dbt core&lt;/li&gt;\n&lt;li&gt;Apache Griffin&lt;/li&gt;\n&lt;li&gt;Soda Core&lt;/li&gt;\n&lt;li&gt;Deequ&lt;/li&gt;\n&lt;li&gt;Tensorflow Data Validation&lt;/li&gt;\n&lt;li&gt;Moby DQ&lt;/li&gt;\n&lt;li&gt;Great Expectatons&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I&amp;#39;m missing here? &lt;/p&gt;\n\n&lt;p&gt;(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bemv7l", "is_robot_indexable": true, "report_reasons": null, "author": "ValidInternetCitizen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bemv7l/opensource_data_quality_tools_abound/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bemv7l/opensource_data_quality_tools_abound/", "subreddit_subscribers": 168830, "created_utc": 1710427511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Link to blog post here](https://www.y42.com/blog/gitops-for-data-2) \\- feedback welcome!  \n\n\nDo you test all your changes in prod? \ud83e\udd26\u200d\u2642\ufe0f Let's borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.\n\nJust released a blog post explaining it and showing how to make sure you're:\n\n* Always working on production data in an isolated environment (dev/staging/prod environments)\n* Collaborating securely with custom approval flows (GitOps)\n* Preventing faulty builds from going into production (CI/CD)\n\nCheck it out and share your thoughts :)", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitOps for Data - the Write-Audit-Publish (WAP) pattern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ben94r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710428522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.y42.com/blog/gitops-for-data-2\"&gt;Link to blog post here&lt;/a&gt; - feedback welcome!  &lt;/p&gt;\n\n&lt;p&gt;Do you test all your changes in prod? \ud83e\udd26\u200d\u2642\ufe0f Let&amp;#39;s borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.&lt;/p&gt;\n\n&lt;p&gt;Just released a blog post explaining it and showing how to make sure you&amp;#39;re:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Always working on production data in an isolated environment (dev/staging/prod environments)&lt;/li&gt;\n&lt;li&gt;Collaborating securely with custom approval flows (GitOps)&lt;/li&gt;\n&lt;li&gt;Preventing faulty builds from going into production (CI/CD)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Check it out and share your thoughts :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?auto=webp&amp;s=856e48bb6457d70b7d36615e87154df2ab80dd27", "width": 1456, "height": 816}, "resolutions": [{"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97a8117a8e7bd9d6e64bdfd71298da59725bee8a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c843ee7f0815e03799e7130789dd887ba7e04c3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71d797e7294077f0703e7de512fcdddfdbe6c3a4", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7947c9ce6d187c11827e95b5a17ee667fa4ea581", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d844f43d666e13ad989844094ff4c0fadb5f220", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e0246833a8cb6ab3a92145d268ba71b2d94d98d", "width": 1080, "height": 605}], "variants": {}, "id": "VPfZqKH060z4qwGQaqgebyoFVaPD4Rohnv811YzZ5WM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ben94r", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ben94r/gitops_for_data_the_writeauditpublish_wap_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ben94r/gitops_for_data_the_writeauditpublish_wap_pattern/", "subreddit_subscribers": 168830, "created_utc": 1710428522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you guys are using DBT, I would like to know if my approach is good or not.\n\nI am using dbt with bigquery. In same gcp project, I have dataset\\_staging and dataset\\_production. In production target, I use dataset\\_production as source. And for local testing or staging, im using dataset\\_staging. The tables in dataset\\_staging is just the copy of tables from dataset\\_production with limited data from [table sampling](https://cloud.google.com/bigquery/docs/table-sampling).\n\nTo use them as source for a dbt model, this is what I am doing in my source yml:\n\n    sources: \n      - name: some_dataset\n        schema:  \"{{ 'dataset_production' if target.name == \"production\" else 'dataset_staging' }}\"\n        database: gcp_project_id\n        tables:\n         - name: table1\n         - name: table2\n\nI am not sure if this is the standard way. Or am I supposed to use dataset\\_production as source even for local testing and staging purpose? The actual goal is not to scan whole table partition during testing, as each partition is more than 5TB.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_dda8zr28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT source for production and testing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bedblt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710392440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you guys are using DBT, I would like to know if my approach is good or not.&lt;/p&gt;\n\n&lt;p&gt;I am using dbt with bigquery. In same gcp project, I have dataset_staging and dataset_production. In production target, I use dataset_production as source. And for local testing or staging, im using dataset_staging. The tables in dataset_staging is just the copy of tables from dataset_production with limited data from &lt;a href=\"https://cloud.google.com/bigquery/docs/table-sampling\"&gt;table sampling&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;To use them as source for a dbt model, this is what I am doing in my source yml:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sources: \n  - name: some_dataset\n    schema:  &amp;quot;{{ &amp;#39;dataset_production&amp;#39; if target.name == &amp;quot;production&amp;quot; else &amp;#39;dataset_staging&amp;#39; }}&amp;quot;\n    database: gcp_project_id\n    tables:\n     - name: table1\n     - name: table2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I am not sure if this is the standard way. Or am I supposed to use dataset_production as source even for local testing and staging purpose? The actual goal is not to scan whole table partition during testing, as each partition is more than 5TB.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?auto=webp&amp;s=b3c1793ddfb0595cba1bbb23fba79360953beb8d", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0329d4207ada0345185e70a97a0ef1f27aec034", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8722bf8052baa4647e96ebeb0d22f50bf529b6ac", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6562c4a330763746058f2250630ec6d3854b2e3d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fff0deae054d2476ac870508887dbbee06d9387c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6a32be275833b4d47802b79f3345f568bd43a4d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97e8d6d94b95697d64482f5fcda32d11814df7b8", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bedblt", "is_robot_indexable": true, "report_reasons": null, "author": "seeker114", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bedblt/dbt_source_for_production_and_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bedblt/dbt_source_for_production_and_testing/", "subreddit_subscribers": 168830, "created_utc": 1710392440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What services do you recommend to make this migration as fast and smooth as possible, just once? First though is glue (used before but different purpose), or DMS service (have not never used it).ed daily, storing only 90 days. This is one of the problems of the migration, that querying from it is impossible my first approach was with a simple lambda, MySQL connector, and python script and chunk it, but would take me about 2 days if I do that. Also, the idea is to have this data somewhere else before thinking of a Lakehouse solution. My questions are:  \n\n\n* What ETL do you propose to make this process daily (1.5M records) comes to glue again if i am succesfull with the first bullet pointe but for different purposes), or DMS service (have not never used it).\n* What ETL do you propose to make this process daily (1.5M records) come to glue again if I am succesfull with the first bullet point\n* Lastly, this data is desired to be used for analytics, initially will be in S3 to make queries using Athena while the team gains an idea about the KPIs they want to track, in the future the idea is to have it somewhere else (I though of a lakehouse) that makes it fast to query and build sql models with it. The whole company env. is in AWS so my first thought is RedShift but I like the efficiency and how GBQ handles this amount of data\n\nThank you so much for reading!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDS to S3 migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be7oi7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710375876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What services do you recommend to make this migration as fast and smooth as possible, just once? First though is glue (used before but different purpose), or DMS service (have not never used it).ed daily, storing only 90 days. This is one of the problems of the migration, that querying from it is impossible my first approach was with a simple lambda, MySQL connector, and python script and chunk it, but would take me about 2 days if I do that. Also, the idea is to have this data somewhere else before thinking of a Lakehouse solution. My questions are:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What ETL do you propose to make this process daily (1.5M records) comes to glue again if i am succesfull with the first bullet pointe but for different purposes), or DMS service (have not never used it).&lt;/li&gt;\n&lt;li&gt;What ETL do you propose to make this process daily (1.5M records) come to glue again if I am succesfull with the first bullet point&lt;/li&gt;\n&lt;li&gt;Lastly, this data is desired to be used for analytics, initially will be in S3 to make queries using Athena while the team gains an idea about the KPIs they want to track, in the future the idea is to have it somewhere else (I though of a lakehouse) that makes it fast to query and build sql models with it. The whole company env. is in AWS so my first thought is RedShift but I like the efficiency and how GBQ handles this amount of data&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you so much for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1be7oi7", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be7oi7/rds_to_s3_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be7oi7/rds_to_s3_migration/", "subreddit_subscribers": 168830, "created_utc": 1710375876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you showing visibility if you are part of a non tech management and org in general\nCurrent team structure is of an analyst and data scientist.\nDA and DS roles are front facing/stakeholder facing so they naturally get visibility. In a team structure like this, how do you make sure your work\u2019s visible and adds value", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solo DEs : How are you making sure your work is visible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes4bx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you showing visibility if you are part of a non tech management and org in general\nCurrent team structure is of an analyst and data scientist.\nDA and DS roles are front facing/stakeholder facing so they naturally get visibility. In a team structure like this, how do you make sure your work\u2019s visible and adds value&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes4bx", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes4bx/solo_des_how_are_you_making_sure_your_work_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes4bx/solo_des_how_are_you_making_sure_your_work_is/", "subreddit_subscribers": 168830, "created_utc": 1710440720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI'm organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using OSS products: Bytewax, Pinot, and Streamlit.\nFor more details and to RSVP (attendance is free), please visit:\nhttps://bytewax.io/events/real-time-pizza-analytics\n\nI believe this workshop is especially noteworthy for those interested, as I've personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it \ud83d\ude04 I believe many data engineers would prefer Python solutions over JVM based.\n\nI am still in the process of updating the repository and will share it later, but here is the previous version:\nhttps://dev.startree.ai/docs/pinot/demo-apps/pizza-shop", "author_fullname": "t2_zus64vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Virtual Workshop: Real time data streaming + Analytics + Visualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2xai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710364229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using OSS products: Bytewax, Pinot, and Streamlit.\nFor more details and to RSVP (attendance is free), please visit:\n&lt;a href=\"https://bytewax.io/events/real-time-pizza-analytics\"&gt;https://bytewax.io/events/real-time-pizza-analytics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I believe this workshop is especially noteworthy for those interested, as I&amp;#39;ve personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it \ud83d\ude04 I believe many data engineers would prefer Python solutions over JVM based.&lt;/p&gt;\n\n&lt;p&gt;I am still in the process of updating the repository and will share it later, but here is the previous version:\n&lt;a href=\"https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop\"&gt;https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?auto=webp&amp;s=9710ac602cc3d55cdc3483c30610f0d6cec287c7", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=be1e5d4935c553e543ed40f9063b0890186f9b65", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc522633caa9ca91f4222971361ed6867641bdfa", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a14311bc251f6667bef9851c488608dafcfda35f", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa1a7d640b445b93da6892292643b670a9e21ecd", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55556a2666f8268a495d077ff92ee5135aa6ee4b", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c3598d6efc61818791e553aabc53f37918820c8", "width": 1080, "height": 607}], "variants": {}, "id": "kpggDHbug4GLpb_LymyREbDxwQCTGOuosNWiRAKAgVg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1be2xai", "is_robot_indexable": true, "report_reasons": null, "author": "oli_k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be2xai/virtual_workshop_real_time_data_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be2xai/virtual_workshop_real_time_data_streaming/", "subreddit_subscribers": 168830, "created_utc": 1710364229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nAs a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here's the concept:\n\nWe need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:\n\n* Static (meaning they update or add their rows every *n* minutes or hours).\n* Temporally ordered (i.e., Time Series).\n\nIt is highly likely that the observations, which are to be fetched every *n* seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by 'Real Time', I mean that it gets updated every *n* seconds).\n\nAlso, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.\n\nI am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.\n\nThank you in advance", "author_fullname": "t2_c0ghewdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What architecture would you suggest for a Real Time Streaming project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beo2yq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710430878.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710430666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;As a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here&amp;#39;s the concept:&lt;/p&gt;\n\n&lt;p&gt;We need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Static (meaning they update or add their rows every &lt;em&gt;n&lt;/em&gt; minutes or hours).&lt;/li&gt;\n&lt;li&gt;Temporally ordered (i.e., Time Series).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It is highly likely that the observations, which are to be fetched every &lt;em&gt;n&lt;/em&gt; seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by &amp;#39;Real Time&amp;#39;, I mean that it gets updated every &lt;em&gt;n&lt;/em&gt; seconds).&lt;/p&gt;\n\n&lt;p&gt;Also, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.&lt;/p&gt;\n\n&lt;p&gt;I am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1beo2yq", "is_robot_indexable": true, "report_reasons": null, "author": "hasty-beaver", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beo2yq/what_architecture_would_you_suggest_for_a_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beo2yq/what_architecture_would_you_suggest_for_a_real/", "subreddit_subscribers": 168830, "created_utc": 1710430666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nPlease give me any suggestions what will be the best way to redesign this data pipeline, to ingest data from the BI tool back to the source table in Snowflake.  \nCurrent setup:\n\nhttps://preview.redd.it/94pdir9f16oc1.png?width=1096&amp;format=png&amp;auto=webp&amp;s=b0f3c45506cf2a3c288d780fead9f33d979e3267\n\n I have a Google spreadsheet that is transformed daily with DBT and stored in Snowflake DB as a mart-ready table for analysis. This table undergoes a full refresh daily. Later, this table is connected to the BI tool where Data Analysts analyze the data. However, they now want to manually input data in Power BI so that this data will be saved in the initial source table in Snowflake.\n\nHow should I approach this, considering that the initial table is refreshed daily? I'm thinking of changing DBT materialization to incremental or creating a new extra table in Snowflake that will be a combination of raw source data and input from BI. Maybe there is a better way yo do it?  \n", "author_fullname": "t2_2odyjk2e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redesign ETL process, DBT table materialization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 40, "top_awarded_type": null, "hide_score": false, "media_metadata": {"94pdir9f16oc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 31, "x": 108, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3cfbf74ade92410d88497db8af9b8ca27c372c2d"}, {"y": 62, "x": 216, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=193da8daaf9a56362353b507bc96cbfb62728b22"}, {"y": 92, "x": 320, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0755624e597482769b696c4eca8f753b2b873a4f"}, {"y": 184, "x": 640, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=acf0061a8c4917dd169a02a9982c1ba70cf35117"}, {"y": 276, "x": 960, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee2b256c89aa02600584f6bec6d9bda5d9b096aa"}, {"y": 311, "x": 1080, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4ad491af8a3239eb5ef8e96e103cfaa068ff6ea"}], "s": {"y": 316, "x": 1096, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=1096&amp;format=png&amp;auto=webp&amp;s=b0f3c45506cf2a3c288d780fead9f33d979e3267"}, "id": "94pdir9f16oc1"}}, "name": "t3_1be395d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/63jK-_OvQUavcQrCcrDL58R0CJs6FkXm08oDXfbz9Ac.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710364982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Please give me any suggestions what will be the best way to redesign this data pipeline, to ingest data from the BI tool back to the source table in Snowflake.&lt;br/&gt;\nCurrent setup:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/94pdir9f16oc1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0f3c45506cf2a3c288d780fead9f33d979e3267\"&gt;https://preview.redd.it/94pdir9f16oc1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0f3c45506cf2a3c288d780fead9f33d979e3267&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a Google spreadsheet that is transformed daily with DBT and stored in Snowflake DB as a mart-ready table for analysis. This table undergoes a full refresh daily. Later, this table is connected to the BI tool where Data Analysts analyze the data. However, they now want to manually input data in Power BI so that this data will be saved in the initial source table in Snowflake.&lt;/p&gt;\n\n&lt;p&gt;How should I approach this, considering that the initial table is refreshed daily? I&amp;#39;m thinking of changing DBT materialization to incremental or creating a new extra table in Snowflake that will be a combination of raw source data and input from BI. Maybe there is a better way yo do it?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1be395d", "is_robot_indexable": true, "report_reasons": null, "author": "Krukach", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be395d/redesign_etl_process_dbt_table_materialization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be395d/redesign_etl_process_dbt_table_materialization/", "subreddit_subscribers": 168830, "created_utc": 1710364982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n&amp;#x200B;\n\n# How to integrate Great Expecation Data Quality tests in Airflow?\n\n\ud83d\udcf7[**Blog**](https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&amp;restrict_sr=1) Orchestrate Modern Data Stack\n\nVlog on how to how to integrate Great Expectation Data Quality tests in the Apache Airflow.  We use the Great Expectation (GE) provider for Airlow and run the Great Expectations suite. The target data asset is a PostgreSQL table.  We use Airflow as the orchestrator for the ETL (ELT) Data piepline and GE as the testing framework\n\n[https://www.youtube.com/watch?v=WAgbFrHUk50](https://www.youtube.com/watch?v=WAgbFrHUk50)\n\nTopics covered:\n\n* Data Orchestartion\n* Airflow &amp; DAG\n* Airflow GE Provider\n* Run Modern Data Stack with Airflow\n\nTech Stack: **Airflow, Great Expecations, Data Quality, Python**", "author_fullname": "t2_vj0466m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to integrate Great Expecation Data Quality tests in Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2kwd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710363424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;How to integrate Great Expecation Data Quality tests in Airflow?&lt;/h1&gt;\n\n&lt;p&gt;\ud83d\udcf7&lt;a href=\"https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&amp;amp;restrict_sr=1\"&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; Orchestrate Modern Data Stack&lt;/p&gt;\n\n&lt;p&gt;Vlog on how to how to integrate Great Expectation Data Quality tests in the Apache Airflow.  We use the Great Expectation (GE) provider for Airlow and run the Great Expectations suite. The target data asset is a PostgreSQL table.  We use Airflow as the orchestrator for the ETL (ELT) Data piepline and GE as the testing framework&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=WAgbFrHUk50\"&gt;https://www.youtube.com/watch?v=WAgbFrHUk50&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Topics covered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Orchestartion&lt;/li&gt;\n&lt;li&gt;Airflow &amp;amp; DAG&lt;/li&gt;\n&lt;li&gt;Airflow GE Provider&lt;/li&gt;\n&lt;li&gt;Run Modern Data Stack with Airflow&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tech Stack: &lt;strong&gt;Airflow, Great Expecations, Data Quality, Python&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b8HcC71hUZBtsg4TiNqi6bKNvIzjnyg76TIZCCp-XyA.jpg?auto=webp&amp;s=1f9fac17204c854b5b4d082002059c44d5ae03ed", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/b8HcC71hUZBtsg4TiNqi6bKNvIzjnyg76TIZCCp-XyA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8be4ceb1eb56baade4c73afe89f795ba729582c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/b8HcC71hUZBtsg4TiNqi6bKNvIzjnyg76TIZCCp-XyA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a24799e256cfd2e1a27df43a0bac89db7a456ad0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/b8HcC71hUZBtsg4TiNqi6bKNvIzjnyg76TIZCCp-XyA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb45dfe184a212d81531e5227e21e5f5f95a26b2", "width": 320, "height": 240}], "variants": {}, "id": "TSidoV9hmIfyC3VVXAqXTE2OWfjCP7yrcSm4ForS6lE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1be2kwd", "is_robot_indexable": true, "report_reasons": null, "author": "Either-Adeptness6638", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be2kwd/how_to_integrate_great_expecation_data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be2kwd/how_to_integrate_great_expecation_data_quality/", "subreddit_subscribers": 168830, "created_utc": 1710363424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In [this demo](https://github.com/airbytehq/quickstarts/blob/main/pyairbyte_notebooks/PyAirbyte_GA4_Demo.ipynb), we use the Airbyte\u2019s Python library to ingest data from Google Analytics 4, followed by a series of transformations and analysis using pandas.\n\nThis demonstrates how you can do fast data ingestion prototyping of a complex data source, without having to start from scratch.\n\nhttps://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;format=png&amp;auto=webp&amp;s=8756a27a3ca8e790281300ea550e28a04406558e", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting data from GA4 with a few Python statements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": true, "media_metadata": {"z9kyphw6hcoc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8ad7b80115aacdb1951caf26d714fd4d2ef58cc"}, {"y": 133, "x": 216, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40221bbc247a82140698c248eae9832b1753dd82"}, {"y": 198, "x": 320, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bd2331fbe54b67d31cdd95f6a77efe8e6b650ba"}, {"y": 396, "x": 640, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=03a915953cee724d87b1049572d1fa9d805ab48e"}, {"y": 594, "x": 960, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f77889758fc6a82e48015e7aa77519f8f198ff2"}], "s": {"y": 652, "x": 1052, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;format=png&amp;auto=webp&amp;s=8756a27a3ca8e790281300ea550e28a04406558e"}, "id": "z9kyphw6hcoc1"}}, "name": "t3_1besj42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zfbIWE_-nFwh8BNnpCmbiQSB3u8wW6btjPhDlYIJhRg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710441695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In &lt;a href=\"https://github.com/airbytehq/quickstarts/blob/main/pyairbyte_notebooks/PyAirbyte_GA4_Demo.ipynb\"&gt;this demo&lt;/a&gt;, we use the Airbyte\u2019s Python library to ingest data from Google Analytics 4, followed by a series of transformations and analysis using pandas.&lt;/p&gt;\n\n&lt;p&gt;This demonstrates how you can do fast data ingestion prototyping of a complex data source, without having to start from scratch.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8756a27a3ca8e790281300ea550e28a04406558e\"&gt;https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8756a27a3ca8e790281300ea550e28a04406558e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1besj42", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1besj42/ingesting_data_from_ga4_with_a_few_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1besj42/ingesting_data_from_ga4_with_a_few_python/", "subreddit_subscribers": 168830, "created_utc": 1710441695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently joined a company that stores all Primary Keys for all tables as UUIDs instead of integers.\n\nAnd for some reason all these columns' dtypes are `VARCHAR(64)`. I know that storing them as `CHAR(36)` would be better.\n\nI've tried to run some benchmarking to backup my decision, but found almost no storage benefit, and minimal query performance improvement (for simple selects, and also for joins).\n\nWould it really make a difference or should I disregard this initiative?\n\nPS: we're using mariadb, and I've benchmarked by creating three identical 1M rows table with varchar64, varchar36, and char36.", "author_fullname": "t2_apdhbvv7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Var vs varchar for uuids", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bernd3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710439544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently joined a company that stores all Primary Keys for all tables as UUIDs instead of integers.&lt;/p&gt;\n\n&lt;p&gt;And for some reason all these columns&amp;#39; dtypes are &lt;code&gt;VARCHAR(64)&lt;/code&gt;. I know that storing them as &lt;code&gt;CHAR(36)&lt;/code&gt; would be better.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to run some benchmarking to backup my decision, but found almost no storage benefit, and minimal query performance improvement (for simple selects, and also for joins).&lt;/p&gt;\n\n&lt;p&gt;Would it really make a difference or should I disregard this initiative?&lt;/p&gt;\n\n&lt;p&gt;PS: we&amp;#39;re using mariadb, and I&amp;#39;ve benchmarked by creating three identical 1M rows table with varchar64, varchar36, and char36.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bernd3", "is_robot_indexable": true, "report_reasons": null, "author": "Agile-Scene-2465", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bernd3/var_vs_varchar_for_uuids/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bernd3/var_vs_varchar_for_uuids/", "subreddit_subscribers": 168830, "created_utc": 1710439544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://blog.twingdata.com/p/identify-unused-columns-in-snowflake](https://blog.twingdata.com/p/identify-unused-columns-in-snowflake)\n\n  \nTLDR: If you're on enterprise Snowflake you can do it via a query. Otherwise there's a simple Python script that uses the sqlglot library to extract the physical columns and compares them against the info schema.  \n", "author_fullname": "t2_bp7tn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Query + Script to identify unused columns in Snowflake and other data warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1belgv1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710423768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://blog.twingdata.com/p/identify-unused-columns-in-snowflake\"&gt;https://blog.twingdata.com/p/identify-unused-columns-in-snowflake&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TLDR: If you&amp;#39;re on enterprise Snowflake you can do it via a query. Otherwise there&amp;#39;s a simple Python script that uses the sqlglot library to extract the physical columns and compares them against the info schema.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?auto=webp&amp;s=7a9fd4cbac369bdf44c0932caf089f3acf4f0183", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95df4917027026cfacaa016b5e3dba6ac427e5a1", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=798883132e928a3b65e3ac750e786fe2200c8ec5", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3e3d4248e553427fdc1d7972039b469fc247f84", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cfb65a334b8b1c97d00db9a89200396f1cad03f", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d219b01314c5718db4f8d8186a6272a0846572c", "width": 960, "height": 562}], "variants": {}, "id": "3CvJ-CSBlahwziE_SAyoY8Vwk75zbSq4MidCUoIDcOI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1belgv1", "is_robot_indexable": true, "report_reasons": null, "author": "dangoldin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1belgv1/query_script_to_identify_unused_columns_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1belgv1/query_script_to_identify_unused_columns_in/", "subreddit_subscribers": 168830, "created_utc": 1710423768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I am working as an Informatica developer with a  total of 7 yr of experience but now I think my career is not progressing as much. Please guide me what should i do next...TIA. ", "author_fullname": "t2_dwbp6nww0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informatica : career help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beg9e1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710404461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I am working as an Informatica developer with a  total of 7 yr of experience but now I think my career is not progressing as much. Please guide me what should i do next...TIA. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1beg9e1", "is_robot_indexable": true, "report_reasons": null, "author": "amorcita_fishy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beg9e1/informatica_career_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beg9e1/informatica_career_help/", "subreddit_subscribers": 168830, "created_utc": 1710404461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI've been thinking a lot about how we, as data professionals, are tackling data adoption hurdles, especially in areas like drug development and personalized medicine (Life Sciences); so I wrote this article. \n\n[https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare](https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare)\n\nAre you facing similar challenges? How are you managing them, and what strategies or tools have made a difference in your work?\n\nReally curious to hear if your experiences align with mine and how you're navigating this complex landscape.", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data in Life Sciences: Are We on the Same Page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be1kpo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710361010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been thinking a lot about how we, as data professionals, are tackling data adoption hurdles, especially in areas like drug development and personalized medicine (Life Sciences); so I wrote this article. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare\"&gt;https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Are you facing similar challenges? How are you managing them, and what strategies or tools have made a difference in your work?&lt;/p&gt;\n\n&lt;p&gt;Really curious to hear if your experiences align with mine and how you&amp;#39;re navigating this complex landscape.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?auto=webp&amp;s=91c90fdf2875134891d193f444905b830d0864da", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e5046c0a3229b2e9e21ed3054b87f1903055596", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86c79bee66d97e1e8dcb050394863d568359d343", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b69b996d84839071da5ae484eb249679d4b969d", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2237495e1c0a2c721481a18987c1e1e4940a2a4", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ce5da23d057f738322d8874cda1f5d036e44aef", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd322b26a9ac0c1761783fa0722662cb6b7dd90e", "width": 1080, "height": 564}], "variants": {}, "id": "9AKl43XPJbE9__vNApnXsZxefA6nyXZTbapru4_2mhE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1be1kpo", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be1kpo/data_in_life_sciences_are_we_on_the_same_page/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be1kpo/data_in_life_sciences_are_we_on_the_same_page/", "subreddit_subscribers": 168830, "created_utc": 1710361010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been encountering a problem lately while refreshing a PowerBI Desktop report from a SharePoint Online list. The refresh gets stuck indefinitely, even after leaving it running for an hour with a stable connection. This issue arose as my SharePoint list grew substantially (around 2k items), although the online report continued to refresh successfully through the service.\n\n&amp;#x200B;\n\nAfter investigation, I discovered that the problem lies with attachments in some of my SharePoint list items. When expanding the AttachmentFiles column in the SP online list connector to retrieve the URL of the first attachment, the refresh becomes sluggish. Removing this column resolves the issue, but I need to access the attachment URLs for my analysis.\n\n&amp;#x200B;\n\nHas anyone encountered a similar issue or have advice on how to pull attachment URLs from a SharePoint list? The conventional SP connector doesn't seem to provide a solution. Appreciate any insights or suggestions!", "author_fullname": "t2_9r5qrevk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with SharePoint Online list attachment refresh issue in PowerBI Desktop!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1betqlh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710444645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been encountering a problem lately while refreshing a PowerBI Desktop report from a SharePoint Online list. The refresh gets stuck indefinitely, even after leaving it running for an hour with a stable connection. This issue arose as my SharePoint list grew substantially (around 2k items), although the online report continued to refresh successfully through the service.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;After investigation, I discovered that the problem lies with attachments in some of my SharePoint list items. When expanding the AttachmentFiles column in the SP online list connector to retrieve the URL of the first attachment, the refresh becomes sluggish. Removing this column resolves the issue, but I need to access the attachment URLs for my analysis.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone encountered a similar issue or have advice on how to pull attachment URLs from a SharePoint list? The conventional SP connector doesn&amp;#39;t seem to provide a solution. Appreciate any insights or suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1betqlh", "is_robot_indexable": true, "report_reasons": null, "author": "Fabro_vaz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1betqlh/need_help_with_sharepoint_online_list_attachment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1betqlh/need_help_with_sharepoint_online_list_attachment/", "subreddit_subscribers": 168830, "created_utc": 1710444645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe are transitioning from executing Bigquery stored procedure to DBT (or maybe Dataform).\n\nThere are many advantages but one things that bothers me is how to easily debug a model.\n\nFor example if some data are not shown properly in the end table, backtracking to find the faulty model is quite cumbersome.\n\nHaving CTEs everywhere makes it annoying because unless I manually change the CTEs to a CREATE TABLE, which is annoying, I need to wait for all the CTEs to execute and it can take some time depending on the models.\n\nHaving real tables instead of CTEs is not helping either because I still need to write the CREATE TEMP TABLE etc..\n\nI am probably nitpicking but since it was easier before, some coworkers do not like it.\n\nPreviously I just had to copy paste the CREATE TEMP TABLE statements from the stored procedure to BigQuery and I was good to go.\n\nDo you have any tips or tools to improve that ?\n\nI have just found about Count which seems great but is also expensive.", "author_fullname": "t2_muek5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you debug DBT models ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bese1h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710441358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We are transitioning from executing Bigquery stored procedure to DBT (or maybe Dataform).&lt;/p&gt;\n\n&lt;p&gt;There are many advantages but one things that bothers me is how to easily debug a model.&lt;/p&gt;\n\n&lt;p&gt;For example if some data are not shown properly in the end table, backtracking to find the faulty model is quite cumbersome.&lt;/p&gt;\n\n&lt;p&gt;Having CTEs everywhere makes it annoying because unless I manually change the CTEs to a CREATE TABLE, which is annoying, I need to wait for all the CTEs to execute and it can take some time depending on the models.&lt;/p&gt;\n\n&lt;p&gt;Having real tables instead of CTEs is not helping either because I still need to write the CREATE TEMP TABLE etc..&lt;/p&gt;\n\n&lt;p&gt;I am probably nitpicking but since it was easier before, some coworkers do not like it.&lt;/p&gt;\n\n&lt;p&gt;Previously I just had to copy paste the CREATE TEMP TABLE statements from the stored procedure to BigQuery and I was good to go.&lt;/p&gt;\n\n&lt;p&gt;Do you have any tips or tools to improve that ?&lt;/p&gt;\n\n&lt;p&gt;I have just found about Count which seems great but is also expensive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bese1h", "is_robot_indexable": true, "report_reasons": null, "author": "Phalcon22", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bese1h/how_do_you_debug_dbt_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bese1h/how_do_you_debug_dbt_models/", "subreddit_subscribers": 168830, "created_utc": 1710441358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for nothing more than anecdotal data here. I\u2019m looking at these two options and am wondering what people end up paying at scale. If you could provide a bit about your configuration and use-cases, even better \ud83d\ude00", "author_fullname": "t2_5bf8p6qg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much do you pay for your Clickhouse or Apache Pinot implementations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes9nf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710441071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for nothing more than anecdotal data here. I\u2019m looking at these two options and am wondering what people end up paying at scale. If you could provide a bit about your configuration and use-cases, even better \ud83d\ude00&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes9nf", "is_robot_indexable": true, "report_reasons": null, "author": "gorba004", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes9nf/how_much_do_you_pay_for_your_clickhouse_or_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes9nf/how_much_do_you_pay_for_your_clickhouse_or_apache/", "subreddit_subscribers": 168830, "created_utc": 1710441071.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have pyspark structured streaming code which writes to hive using in micro batch mode. I am using foreachbatch. I want to know if that particular batch failed then check point is commited or not. When exactly checkpoint is commited? Is it when function passed to foreachbatch runs sucessfully or just after passing df to that function .", "author_fullname": "t2_63ryq4q7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you help me understand checkpoints in structured streaming ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes500", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have pyspark structured streaming code which writes to hive using in micro batch mode. I am using foreachbatch. I want to know if that particular batch failed then check point is commited or not. When exactly checkpoint is commited? Is it when function passed to foreachbatch runs sucessfully or just after passing df to that function .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes500", "is_robot_indexable": true, "report_reasons": null, "author": "stuart_little_03", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes500/can_you_help_me_understand_checkpoints_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes500/can_you_help_me_understand_checkpoints_in/", "subreddit_subscribers": 168830, "created_utc": 1710440764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a project that involves database schema migration,  and I'm looking for some guidance and suggestions from the community.  \nThe  goal of the project is to create a tool that can migrate the schema  from one database to another. Initially, we're targeting support for  MySQL, PostgreSQL, and MongoDB. The tool should be able to retrieve the  schema information from the source database, generate the necessary SQL  statements or equivalent commands to recreate the schema in the target  database, execute the schema migration, and perform validation to ensure  the migration was successful.  \nI've started implementing a package  structure for the schema migration process, which includes a retriever  package for retrieving schema information from different databases, a  generator package for generating the migration statements, an executor  package for executing the migration, and a validator package for  validating the migrated schema.  \nHere are my questions:  \nAre there  any existing open-source libraries in Go that already provide schema  migration functionality similar to what I'm trying to achieve? If so,  could you recommend some libraries worth checking out?     \n\n\nIf there aren't any suitable libraries available, I would appreciate  any suggestions or ideas on how to approach the implementation of the  schema migration tool. Are there any best practices, common pitfalls, or  important considerations I should keep in mind?     \n\n\nFor those who have worked on similar projects or have experience  with database schema migration, could you share any insights or lessons  learned that could help guide my implementation?  \nI'm open to any feedback, suggestions, or recommendations that could help me move forward with this project.   \n\nThanks   ", "author_fullname": "t2_s9sv2dxdt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for open-source libraries for database schema migration and suggestions on implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1berurq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project that involves database schema migration,  and I&amp;#39;m looking for some guidance and suggestions from the community.&lt;br/&gt;\nThe  goal of the project is to create a tool that can migrate the schema  from one database to another. Initially, we&amp;#39;re targeting support for  MySQL, PostgreSQL, and MongoDB. The tool should be able to retrieve the  schema information from the source database, generate the necessary SQL  statements or equivalent commands to recreate the schema in the target  database, execute the schema migration, and perform validation to ensure  the migration was successful.&lt;br/&gt;\nI&amp;#39;ve started implementing a package  structure for the schema migration process, which includes a retriever  package for retrieving schema information from different databases, a  generator package for generating the migration statements, an executor  package for executing the migration, and a validator package for  validating the migrated schema.&lt;br/&gt;\nHere are my questions:&lt;br/&gt;\nAre there  any existing open-source libraries in Go that already provide schema  migration functionality similar to what I&amp;#39;m trying to achieve? If so,  could you recommend some libraries worth checking out?     &lt;/p&gt;\n\n&lt;p&gt;If there aren&amp;#39;t any suitable libraries available, I would appreciate  any suggestions or ideas on how to approach the implementation of the  schema migration tool. Are there any best practices, common pitfalls, or  important considerations I should keep in mind?     &lt;/p&gt;\n\n&lt;p&gt;For those who have worked on similar projects or have experience  with database schema migration, could you share any insights or lessons  learned that could help guide my implementation?&lt;br/&gt;\nI&amp;#39;m open to any feedback, suggestions, or recommendations that could help me move forward with this project.   &lt;/p&gt;\n\n&lt;p&gt;Thanks   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1berurq", "is_robot_indexable": true, "report_reasons": null, "author": "Helpful-Natural6628", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1berurq/looking_for_opensource_libraries_for_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1berurq/looking_for_opensource_libraries_for_database/", "subreddit_subscribers": 168830, "created_utc": 1710440063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You would think this would fall under data governance (internal and external). How ever it\u2019s really not, because obviously I wouldn\u2019t be asking this if there was proper governance, in the ethical form not business logic. More a question for  senior members. I\u2019ve been through this before at banks and parted ways. \nWhat would be the best route, if the data team is manipulating data (intentionally), albeit their objective is stockholders perception but I see this could have repercussions beyond the company.  How would u recommended absolving my involvement and not losing that contract because I don\u2019t know how far it climbs to the e-team. Sorry but I can\u2019t give details of the data. I was thinking just a registered letter to my lawyer (he agrees). I\u2019m also not into this whole \u201c I was being told what to do\u201d deal. The last time this happened it was pretty bad, I parted ways with the bank and their team tried to use me as a scape goat and the bank was considering pursuing charges against me (no joke) but the fact I left and gave reasons of \u201cteam dynamics\u201d and \u201cunethical data manipulation\u201d, which I guess they didn\u2019t read because it was on the second page of the voluntary contract termination I drafted up, my lawyer basically told them to take a hike but this back and forth a  few weeks costed me a few grand.\n", "author_fullname": "t2_6lfdg0it", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data policy / ethics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1berdf5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710438871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You would think this would fall under data governance (internal and external). How ever it\u2019s really not, because obviously I wouldn\u2019t be asking this if there was proper governance, in the ethical form not business logic. More a question for  senior members. I\u2019ve been through this before at banks and parted ways. \nWhat would be the best route, if the data team is manipulating data (intentionally), albeit their objective is stockholders perception but I see this could have repercussions beyond the company.  How would u recommended absolving my involvement and not losing that contract because I don\u2019t know how far it climbs to the e-team. Sorry but I can\u2019t give details of the data. I was thinking just a registered letter to my lawyer (he agrees). I\u2019m also not into this whole \u201c I was being told what to do\u201d deal. The last time this happened it was pretty bad, I parted ways with the bank and their team tried to use me as a scape goat and the bank was considering pursuing charges against me (no joke) but the fact I left and gave reasons of \u201cteam dynamics\u201d and \u201cunethical data manipulation\u201d, which I guess they didn\u2019t read because it was on the second page of the voluntary contract termination I drafted up, my lawyer basically told them to take a hike but this back and forth a  few weeks costed me a few grand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1berdf5", "is_robot_indexable": true, "report_reasons": null, "author": "soundboyselecta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1berdf5/data_policy_ethics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1berdf5/data_policy_ethics/", "subreddit_subscribers": 168830, "created_utc": 1710438871.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}