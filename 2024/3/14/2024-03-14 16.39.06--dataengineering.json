{"kind": "Listing", "data": {"after": "t3_1be9w0z", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which profession would earn you most money in the long run? I think data analyst salaries usually don\u2019t surpass $200k while DE can make $300k and more. What has been your experience or what have you seen salary wise for DE and DA?", "author_fullname": "t2_kjeptd4cq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer vs Data Analyst Salary", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzrkh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 91, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 91, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710356680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which profession would earn you most money in the long run? I think data analyst salaries usually don\u2019t surpass $200k while DE can make $300k and more. What has been your experience or what have you seen salary wise for DE and DA?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bdzrkh", "is_robot_indexable": true, "report_reasons": null, "author": "Mergirl610", "discussion_type": null, "num_comments": 96, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdzrkh/data_engineer_vs_data_analyst_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdzrkh/data_engineer_vs_data_analyst_salary/", "subreddit_subscribers": 168768, "created_utc": 1710356680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm fairly new so I'm trying to gather a consensus on how often your pipelines break and why?\n\nIs it expected that they will eventually break and there's nothing you can do about it? I've heard people say be as defensive in your pipelines as possible?\n\nHow do you get alerted about the breakages?", "author_fullname": "t2_tnf3rfrjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often do your pipelines break?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be165f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "287cf772-ac9d-11eb-aa84-0ead36cb44af", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710360042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fairly new so I&amp;#39;m trying to gather a consensus on how often your pipelines break and why?&lt;/p&gt;\n\n&lt;p&gt;Is it expected that they will eventually break and there&amp;#39;s nothing you can do about it? I&amp;#39;ve heard people say be as defensive in your pipelines as possible?&lt;/p&gt;\n\n&lt;p&gt;How do you get alerted about the breakages?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Software Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1be165f", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Coat5856", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1be165f/how_often_do_your_pipelines_break/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be165f/how_often_do_your_pipelines_break/", "subreddit_subscribers": 168768, "created_utc": 1710360042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF's of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.\n\nI work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.", "author_fullname": "t2_t1bnfnc4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the hardest you have ever seen someone work manually?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beltlg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710425040.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710424759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF&amp;#39;s of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.&lt;/p&gt;\n\n&lt;p&gt;I work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1beltlg", "is_robot_indexable": true, "report_reasons": null, "author": "bjogc42069", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/", "subreddit_subscribers": 168768, "created_utc": 1710424759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Early stage startups can't fund a DE team** obviously - not enough data, time, or money! But startups have a ton of data (stripe, customer, app analytics, finance/payments etc..) generally a better understanding of data. \n\n**So... for you guys:** how can early stage startups make the most of their data in your experience? Is there a way to stitch together all the key data without a large engineering effort? What specific examples have you seen work in your experience? Is this even a data engineering problem or is ad-hoc the way to go? ", "author_fullname": "t2_tszztjadn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should early stage startups approach data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be7kij", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710375587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Early stage startups can&amp;#39;t fund a DE team&lt;/strong&gt; obviously - not enough data, time, or money! But startups have a ton of data (stripe, customer, app analytics, finance/payments etc..) generally a better understanding of data. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So... for you guys:&lt;/strong&gt; how can early stage startups make the most of their data in your experience? Is there a way to stitch together all the key data without a large engineering effort? What specific examples have you seen work in your experience? Is this even a data engineering problem or is ad-hoc the way to go? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1be7kij", "is_robot_indexable": true, "report_reasons": null, "author": "jmack_startups", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be7kij/how_should_early_stage_startups_approach_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be7kij/how_should_early_stage_startups_approach_data/", "subreddit_subscribers": 168768, "created_utc": 1710375587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just for all the people out there that are applying for jobs and having difficulty landing anything, keep pushing! \n\nWhen the market picks back up you will be happy that you kept upskilling yourself and you didn\u2019t give up ", "author_fullname": "t2_5q1b2lj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Positive Job Market Outlook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be05tn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710357640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just for all the people out there that are applying for jobs and having difficulty landing anything, keep pushing! &lt;/p&gt;\n\n&lt;p&gt;When the market picks back up you will be happy that you kept upskilling yourself and you didn\u2019t give up &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1be05tn", "is_robot_indexable": true, "report_reasons": null, "author": "Guilty-Commission435", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be05tn/positive_job_market_outlook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be05tn/positive_job_market_outlook/", "subreddit_subscribers": 168768, "created_utc": 1710357640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the  project.\n\nThe dataset had over 5000 tables that were used across the board. \n\nMost of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don\u2019t help.\n\nIs there a way to restore it because we might lose the client at this rate? \n\nSample id of a table: \u2018project.Master.table1\u2019", "author_fullname": "t2_5hvalgp8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to recover a deleted dataset from BigQuery? (Urgent)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bekue2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710421925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the  project.&lt;/p&gt;\n\n&lt;p&gt;The dataset had over 5000 tables that were used across the board. &lt;/p&gt;\n\n&lt;p&gt;Most of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don\u2019t help.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to restore it because we might lose the client at this rate? &lt;/p&gt;\n\n&lt;p&gt;Sample id of a table: \u2018project.Master.table1\u2019&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bekue2", "is_robot_indexable": true, "report_reasons": null, "author": "honpra", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/", "subreddit_subscribers": 168768, "created_utc": 1710421925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, founder at Latitude here.\n\nWe spent the last 2 years building software for data teams. After many iterations, we've decided to rebuild everything from scratch and open-source it for the entire community.\n\nLatitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.\n\nYou can check out the repo here: [https://github.com/latitude-dev/latitude](https://github.com/latitude-dev/latitude)\n\nWe're actively looking for feedback and contributors. Let me know your thoughts!", "author_fullname": "t2_o4qnw2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Latitude: an open-source web framework to build data apps using SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bej704", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710416485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, founder at Latitude here.&lt;/p&gt;\n\n&lt;p&gt;We spent the last 2 years building software for data teams. After many iterations, we&amp;#39;ve decided to rebuild everything from scratch and open-source it for the entire community.&lt;/p&gt;\n\n&lt;p&gt;Latitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.&lt;/p&gt;\n\n&lt;p&gt;You can check out the repo here: &lt;a href=\"https://github.com/latitude-dev/latitude\"&gt;https://github.com/latitude-dev/latitude&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re actively looking for feedback and contributors. Let me know your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?auto=webp&amp;s=6eebca997ea4d4b55aeb0f760233c0415a0bdf63", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d827756cdde837af4ec07d99725da141bd435f68", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5612dbc3c955b0133b686b2df347ee7160426a8d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d57471adc017d17799740c2e311be154ac893f0c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbdd7b25a13222445010fda16e55f3b9df18a71b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ff1193415b2a1522056645b8cd2f1bfb564810b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba65964d68ee85a55125b4e94d704ac87a36d2e3", "width": 1080, "height": 540}], "variants": {}, "id": "zRu4yaIX9sS06soWCMlaUqVBLjk_mkvd6UlhzEJIqNY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bej704", "is_robot_indexable": true, "report_reasons": null, "author": "EloquentPickle", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bej704/latitude_an_opensource_web_framework_to_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bej704/latitude_an_opensource_web_framework_to_build/", "subreddit_subscribers": 168768, "created_utc": 1710416485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing research on open source data quality tools, and I've found these so far:\n\n1. dbt core\n2. Apache Griffin\n3. Soda Core\n4. Deequ\n5. Tensorflow Data Validation\n6. Moby DQ\n7. Great Expectatons\n\nI've been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I'm missing here? \n\n(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).", "author_fullname": "t2_mc935wf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-Source Data Quality Tools Abound", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bemv7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710427511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing research on open source data quality tools, and I&amp;#39;ve found these so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;dbt core&lt;/li&gt;\n&lt;li&gt;Apache Griffin&lt;/li&gt;\n&lt;li&gt;Soda Core&lt;/li&gt;\n&lt;li&gt;Deequ&lt;/li&gt;\n&lt;li&gt;Tensorflow Data Validation&lt;/li&gt;\n&lt;li&gt;Moby DQ&lt;/li&gt;\n&lt;li&gt;Great Expectatons&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I&amp;#39;m missing here? &lt;/p&gt;\n\n&lt;p&gt;(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bemv7l", "is_robot_indexable": true, "report_reasons": null, "author": "ValidInternetCitizen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bemv7l/opensource_data_quality_tools_abound/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bemv7l/opensource_data_quality_tools_abound/", "subreddit_subscribers": 168768, "created_utc": 1710427511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4ymkgdql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python requests best practices for data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1begg9k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ugbrL_EtU52ai4ErXvB2sZD09kEziLG1QVKocyJn93c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710405308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/python-requests-best-practices-for-data-engineers", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?auto=webp&amp;s=fdae695fd5fe2cee64e1fb4a7ec4483c6443e584", "width": 2912, "height": 1632}, "resolutions": [{"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f31260549e9d62ab9efa9beedc3fb6de9b637b1", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=930ed37cb47a4999c9f98a4e2bdb99166c32c43e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0669238db69cae8dcc36a03fc19af9df9df8b54b", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db93ea8d2e717d1b325f1c01272dad51892ecf90", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b7dcf25cde15dce4e116d09646fd78936820785", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77bd0feb4c744910ad476681f1a10765dff24509", "width": 1080, "height": 605}], "variants": {}, "id": "dW0cXVdRG0AiMhGU9NGc_dayd_CcrOfYkH0KamIBEUQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1begg9k", "is_robot_indexable": true, "report_reasons": null, "author": "SnooBeans3890", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1begg9k/python_requests_best_practices_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/python-requests-best-practices-for-data-engineers", "subreddit_subscribers": 168768, "created_utc": 1710405308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been data engineer (mostly working on ETLs in cloud environments) for 5 years. I started a new position yesterday in an industry that requires to work on premises. Nothing exists yet, the project starts with me, which is very enthusiastic. \n\nHowever I am a bit lost after 2 days struggling on the stack. Right now I am trying to identify which tools I should use. The load of data isn't that big yet but it's required to make something a bit scalable; that could welcome new sources of data (structured and unstructured)\n\nMy idea is the following : \n\nAirflow as an orchestrator (already have an experience on it), HDFS for a datalake, PySpark scripts for ETL (comfortable on it) , Hive or Presto for datawarehousing, Metabase for viz \n\nTherefore right now I have created docker containers running Spark, HDFS and Airflow. However the more I go further, the more I am doubting. There are so many tools existing for everything and I feel like this stack is a bit oldish ... I've also checked open source versions of Dremio, Cloudera, Iceberg/minIO but I am not sure how suitable it is for a production environment where all the data weighs probably less than 100GB. Also, I'll stay at my position for approx 6 months, I need to make something that would be easily maintained by people who don't work in data engineering field, I feel like Iceberg would be overkill. I sometimes also think about doing easier stuff with pandas/postgres but this is limiting for unstructured data. \n\nI would be glad to receive your opinions about it !\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_6d257z60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On premise stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzyr0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710357167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been data engineer (mostly working on ETLs in cloud environments) for 5 years. I started a new position yesterday in an industry that requires to work on premises. Nothing exists yet, the project starts with me, which is very enthusiastic. &lt;/p&gt;\n\n&lt;p&gt;However I am a bit lost after 2 days struggling on the stack. Right now I am trying to identify which tools I should use. The load of data isn&amp;#39;t that big yet but it&amp;#39;s required to make something a bit scalable; that could welcome new sources of data (structured and unstructured)&lt;/p&gt;\n\n&lt;p&gt;My idea is the following : &lt;/p&gt;\n\n&lt;p&gt;Airflow as an orchestrator (already have an experience on it), HDFS for a datalake, PySpark scripts for ETL (comfortable on it) , Hive or Presto for datawarehousing, Metabase for viz &lt;/p&gt;\n\n&lt;p&gt;Therefore right now I have created docker containers running Spark, HDFS and Airflow. However the more I go further, the more I am doubting. There are so many tools existing for everything and I feel like this stack is a bit oldish ... I&amp;#39;ve also checked open source versions of Dremio, Cloudera, Iceberg/minIO but I am not sure how suitable it is for a production environment where all the data weighs probably less than 100GB. Also, I&amp;#39;ll stay at my position for approx 6 months, I need to make something that would be easily maintained by people who don&amp;#39;t work in data engineering field, I feel like Iceberg would be overkill. I sometimes also think about doing easier stuff with pandas/postgres but this is limiting for unstructured data. &lt;/p&gt;\n\n&lt;p&gt;I would be glad to receive your opinions about it !&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bdzyr0", "is_robot_indexable": true, "report_reasons": null, "author": "Ruyia31", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdzyr0/on_premise_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdzyr0/on_premise_stack/", "subreddit_subscribers": 168768, "created_utc": 1710357167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Link to blog post here](https://www.y42.com/blog/gitops-for-data-2) \\- feedback welcome!  \n\n\nDo you test all your changes in prod? \ud83e\udd26\u200d\u2642\ufe0f Let's borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.\n\nJust released a blog post explaining it and showing how to make sure you're:\n\n* Always working on production data in an isolated environment (dev/staging/prod environments)\n* Collaborating securely with custom approval flows (GitOps)\n* Preventing faulty builds from going into production (CI/CD)\n\nCheck it out and share your thoughts :)", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitOps for Data - the Write-Audit-Publish (WAP) pattern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ben94r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710428522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.y42.com/blog/gitops-for-data-2\"&gt;Link to blog post here&lt;/a&gt; - feedback welcome!  &lt;/p&gt;\n\n&lt;p&gt;Do you test all your changes in prod? \ud83e\udd26\u200d\u2642\ufe0f Let&amp;#39;s borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.&lt;/p&gt;\n\n&lt;p&gt;Just released a blog post explaining it and showing how to make sure you&amp;#39;re:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Always working on production data in an isolated environment (dev/staging/prod environments)&lt;/li&gt;\n&lt;li&gt;Collaborating securely with custom approval flows (GitOps)&lt;/li&gt;\n&lt;li&gt;Preventing faulty builds from going into production (CI/CD)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Check it out and share your thoughts :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?auto=webp&amp;s=856e48bb6457d70b7d36615e87154df2ab80dd27", "width": 1456, "height": 816}, "resolutions": [{"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97a8117a8e7bd9d6e64bdfd71298da59725bee8a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c843ee7f0815e03799e7130789dd887ba7e04c3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71d797e7294077f0703e7de512fcdddfdbe6c3a4", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7947c9ce6d187c11827e95b5a17ee667fa4ea581", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d844f43d666e13ad989844094ff4c0fadb5f220", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e0246833a8cb6ab3a92145d268ba71b2d94d98d", "width": 1080, "height": 605}], "variants": {}, "id": "VPfZqKH060z4qwGQaqgebyoFVaPD4Rohnv811YzZ5WM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ben94r", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ben94r/gitops_for_data_the_writeauditpublish_wap_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ben94r/gitops_for_data_the_writeauditpublish_wap_pattern/", "subreddit_subscribers": 168768, "created_utc": 1710428522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you guys are using DBT, I would like to know if my approach is good or not.\n\nI am using dbt with bigquery. In same gcp project, I have dataset\\_staging and dataset\\_production. In production target, I use dataset\\_production as source. And for local testing or staging, im using dataset\\_staging. The tables in dataset\\_staging is just the copy of tables from dataset\\_production with limited data from [table sampling](https://cloud.google.com/bigquery/docs/table-sampling).\n\nTo use them as source for a dbt model, this is what I am doing in my source yml:\n\n    sources: \n      - name: some_dataset\n        schema:  \"{{ 'dataset_production' if target.name == \"production\" else 'dataset_staging' }}\"\n        database: gcp_project_id\n        tables:\n         - name: table1\n         - name: table2\n\nI am not sure if this is the standard way. Or am I supposed to use dataset\\_production as source even for local testing and staging purpose? The actual goal is not to scan whole table partition during testing, as each partition is more than 5TB.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_dda8zr28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT source for production and testing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bedblt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710392440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you guys are using DBT, I would like to know if my approach is good or not.&lt;/p&gt;\n\n&lt;p&gt;I am using dbt with bigquery. In same gcp project, I have dataset_staging and dataset_production. In production target, I use dataset_production as source. And for local testing or staging, im using dataset_staging. The tables in dataset_staging is just the copy of tables from dataset_production with limited data from &lt;a href=\"https://cloud.google.com/bigquery/docs/table-sampling\"&gt;table sampling&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;To use them as source for a dbt model, this is what I am doing in my source yml:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sources: \n  - name: some_dataset\n    schema:  &amp;quot;{{ &amp;#39;dataset_production&amp;#39; if target.name == &amp;quot;production&amp;quot; else &amp;#39;dataset_staging&amp;#39; }}&amp;quot;\n    database: gcp_project_id\n    tables:\n     - name: table1\n     - name: table2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I am not sure if this is the standard way. Or am I supposed to use dataset_production as source even for local testing and staging purpose? The actual goal is not to scan whole table partition during testing, as each partition is more than 5TB.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?auto=webp&amp;s=b3c1793ddfb0595cba1bbb23fba79360953beb8d", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0329d4207ada0345185e70a97a0ef1f27aec034", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8722bf8052baa4647e96ebeb0d22f50bf529b6ac", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6562c4a330763746058f2250630ec6d3854b2e3d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fff0deae054d2476ac870508887dbbee06d9387c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6a32be275833b4d47802b79f3345f568bd43a4d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97e8d6d94b95697d64482f5fcda32d11814df7b8", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bedblt", "is_robot_indexable": true, "report_reasons": null, "author": "seeker114", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bedblt/dbt_source_for_production_and_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bedblt/dbt_source_for_production_and_testing/", "subreddit_subscribers": 168768, "created_utc": 1710392440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What services do you recommend to make this migration as fast and smooth as possible, just once? First though is glue (used before but different purpose), or DMS service (have not never used it).ed daily, storing only 90 days. This is one of the problems of the migration, that querying from it is impossible my first approach was with a simple lambda, MySQL connector, and python script and chunk it, but would take me about 2 days if I do that. Also, the idea is to have this data somewhere else before thinking of a Lakehouse solution. My questions are:  \n\n\n* What ETL do you propose to make this process daily (1.5M records) comes to glue again if i am succesfull with the first bullet pointe but for different purposes), or DMS service (have not never used it).\n* What ETL do you propose to make this process daily (1.5M records) come to glue again if I am succesfull with the first bullet point\n* Lastly, this data is desired to be used for analytics, initially will be in S3 to make queries using Athena while the team gains an idea about the KPIs they want to track, in the future the idea is to have it somewhere else (I though of a lakehouse) that makes it fast to query and build sql models with it. The whole company env. is in AWS so my first thought is RedShift but I like the efficiency and how GBQ handles this amount of data\n\nThank you so much for reading!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDS to S3 migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be7oi7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710375876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What services do you recommend to make this migration as fast and smooth as possible, just once? First though is glue (used before but different purpose), or DMS service (have not never used it).ed daily, storing only 90 days. This is one of the problems of the migration, that querying from it is impossible my first approach was with a simple lambda, MySQL connector, and python script and chunk it, but would take me about 2 days if I do that. Also, the idea is to have this data somewhere else before thinking of a Lakehouse solution. My questions are:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What ETL do you propose to make this process daily (1.5M records) comes to glue again if i am succesfull with the first bullet pointe but for different purposes), or DMS service (have not never used it).&lt;/li&gt;\n&lt;li&gt;What ETL do you propose to make this process daily (1.5M records) come to glue again if I am succesfull with the first bullet point&lt;/li&gt;\n&lt;li&gt;Lastly, this data is desired to be used for analytics, initially will be in S3 to make queries using Athena while the team gains an idea about the KPIs they want to track, in the future the idea is to have it somewhere else (I though of a lakehouse) that makes it fast to query and build sql models with it. The whole company env. is in AWS so my first thought is RedShift but I like the efficiency and how GBQ handles this amount of data&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you so much for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1be7oi7", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be7oi7/rds_to_s3_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be7oi7/rds_to_s3_migration/", "subreddit_subscribers": 168768, "created_utc": 1710375876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI'm organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using OSS products: Bytewax, Pinot, and Streamlit.\nFor more details and to RSVP (attendance is free), please visit:\nhttps://bytewax.io/events/real-time-pizza-analytics\n\nI believe this workshop is especially noteworthy for those interested, as I've personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it \ud83d\ude04 I believe many data engineers would prefer Python solutions over JVM based.\n\nI am still in the process of updating the repository and will share it later, but here is the previous version:\nhttps://dev.startree.ai/docs/pinot/demo-apps/pizza-shop", "author_fullname": "t2_zus64vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Virtual Workshop: Real time data streaming + Analytics + Visualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2xai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710364229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using OSS products: Bytewax, Pinot, and Streamlit.\nFor more details and to RSVP (attendance is free), please visit:\n&lt;a href=\"https://bytewax.io/events/real-time-pizza-analytics\"&gt;https://bytewax.io/events/real-time-pizza-analytics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I believe this workshop is especially noteworthy for those interested, as I&amp;#39;ve personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it \ud83d\ude04 I believe many data engineers would prefer Python solutions over JVM based.&lt;/p&gt;\n\n&lt;p&gt;I am still in the process of updating the repository and will share it later, but here is the previous version:\n&lt;a href=\"https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop\"&gt;https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?auto=webp&amp;s=9710ac602cc3d55cdc3483c30610f0d6cec287c7", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=be1e5d4935c553e543ed40f9063b0890186f9b65", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc522633caa9ca91f4222971361ed6867641bdfa", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a14311bc251f6667bef9851c488608dafcfda35f", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa1a7d640b445b93da6892292643b670a9e21ecd", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55556a2666f8268a495d077ff92ee5135aa6ee4b", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/zuVRsJ0ECO_zQVLsE5M031bHRfIX625iDDAqcBQ9y-0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c3598d6efc61818791e553aabc53f37918820c8", "width": 1080, "height": 607}], "variants": {}, "id": "kpggDHbug4GLpb_LymyREbDxwQCTGOuosNWiRAKAgVg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1be2xai", "is_robot_indexable": true, "report_reasons": null, "author": "oli_k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be2xai/virtual_workshop_real_time_data_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be2xai/virtual_workshop_real_time_data_streaming/", "subreddit_subscribers": 168768, "created_utc": 1710364229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi data fellas, with version 2.1.0 available, we take this opportunity to re-introduce Apache Doris as an open-source data warehouse that provides:\n\n* **Higher out-of-the-box query performance**: 100% faster speed proven by TPC-DS 1TB benchmark tests.\n* **Improved data lake analytics capabilities**: 4\\~6 times faster than Trino and Spark, compatibility with various SQL dialects for smooth migration, read/write interface based on Arrow Flight for 100 times faster data transfer.\n* **Solid support for semi-structured data analysis**: a newly-added Variant data type, support for more IP types, and a more comprehensive suite of analytic functions.\n* **Materialized view with multiple tables**: a new feature to accelerate multi-table joins, allowing transparent rewriting, auto refresh, materialized views of external tables, and direct query.\n* **Enhanced real-time writing efficiency**: faster data writing at scale powered by AUTO\\_INCREMENT column, AUTO PARTITION, forward placement of MemTable, and Group Commit.\n* **Better workload management**: optimizations of the Workload Group mechanism for higher performance stability and the display of SQL resource consumption in the runtime.\n\nThis very informative release note provides details of the performance tests and the implementation of the new features and mechanisms that make the difference. Take a look and tell us what you think!  \n[https://doris.apache.org/blog/release-note-2.1.0](https://doris.apache.org/blog/release-note-2.1.0)", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Doris 2.1.0 is released, with doubled out-of-the-box performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdvhll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710346656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi data fellas, with version 2.1.0 available, we take this opportunity to re-introduce Apache Doris as an open-source data warehouse that provides:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Higher out-of-the-box query performance&lt;/strong&gt;: 100% faster speed proven by TPC-DS 1TB benchmark tests.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Improved data lake analytics capabilities&lt;/strong&gt;: 4~6 times faster than Trino and Spark, compatibility with various SQL dialects for smooth migration, read/write interface based on Arrow Flight for 100 times faster data transfer.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Solid support for semi-structured data analysis&lt;/strong&gt;: a newly-added Variant data type, support for more IP types, and a more comprehensive suite of analytic functions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Materialized view with multiple tables&lt;/strong&gt;: a new feature to accelerate multi-table joins, allowing transparent rewriting, auto refresh, materialized views of external tables, and direct query.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced real-time writing efficiency&lt;/strong&gt;: faster data writing at scale powered by AUTO_INCREMENT column, AUTO PARTITION, forward placement of MemTable, and Group Commit.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Better workload management&lt;/strong&gt;: optimizations of the Workload Group mechanism for higher performance stability and the display of SQL resource consumption in the runtime.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This very informative release note provides details of the performance tests and the implementation of the new features and mechanisms that make the difference. Take a look and tell us what you think!&lt;br/&gt;\n&lt;a href=\"https://doris.apache.org/blog/release-note-2.1.0\"&gt;https://doris.apache.org/blog/release-note-2.1.0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/txa-yKDhLL1PUJDzydQrqFFgkFwOucMpxhL-PhabCZs.jpg?auto=webp&amp;s=11ea49814dea9685e2e1f544cfcee4bdcdc37feb", "width": 1800, "height": 768}, "resolutions": [{"url": "https://external-preview.redd.it/txa-yKDhLL1PUJDzydQrqFFgkFwOucMpxhL-PhabCZs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5658ef6d8410cc1d396be16a9ecca937def2281", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/txa-yKDhLL1PUJDzydQrqFFgkFwOucMpxhL-PhabCZs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dbbb6f0c25a70f68275b84b06eab33ddd1a92980", "width": 216, "height": 92}, {"url": "https://external-preview.redd.it/txa-yKDhLL1PUJDzydQrqFFgkFwOucMpxhL-PhabCZs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4dd9d38c646f94128d78a2f56086d51d43d0d88", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/txa-yKDhLL1PUJDzydQrqFFgkFwOucMpxhL-PhabCZs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1594732efa7e36dc0d8386e059464c620558dbe", "width": 640, "height": 273}, {"url": "https://external-preview.redd.it/txa-yKDhLL1PUJDzydQrqFFgkFwOucMpxhL-PhabCZs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=04d781acd91cbcd252e32b196156d5f4276ea2ca", "width": 960, "height": 409}, {"url": "https://external-preview.redd.it/txa-yKDhLL1PUJDzydQrqFFgkFwOucMpxhL-PhabCZs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ac9e97f87dbb83201c28edcdab037a4859ef9f5", "width": 1080, "height": 460}], "variants": {}, "id": "lO_-l9zrS9XOVWUk5oc40xL8UJr9pZxafcbBIwujrOc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bdvhll", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bdvhll/apache_doris_210_is_released_with_doubled/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bdvhll/apache_doris_210_is_released_with_doubled/", "subreddit_subscribers": 168768, "created_utc": 1710346656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nPlease give me any suggestions what will be the best way to redesign this data pipeline, to ingest data from the BI tool back to the source table in Snowflake.  \nCurrent setup:\n\nhttps://preview.redd.it/94pdir9f16oc1.png?width=1096&amp;format=png&amp;auto=webp&amp;s=b0f3c45506cf2a3c288d780fead9f33d979e3267\n\n I have a Google spreadsheet that is transformed daily with DBT and stored in Snowflake DB as a mart-ready table for analysis. This table undergoes a full refresh daily. Later, this table is connected to the BI tool where Data Analysts analyze the data. However, they now want to manually input data in Power BI so that this data will be saved in the initial source table in Snowflake.\n\nHow should I approach this, considering that the initial table is refreshed daily? I'm thinking of changing DBT materialization to incremental or creating a new extra table in Snowflake that will be a combination of raw source data and input from BI. Maybe there is a better way yo do it?  \n", "author_fullname": "t2_2odyjk2e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redesign ETL process, DBT table materialization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 40, "top_awarded_type": null, "hide_score": false, "media_metadata": {"94pdir9f16oc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 31, "x": 108, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3cfbf74ade92410d88497db8af9b8ca27c372c2d"}, {"y": 62, "x": 216, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=193da8daaf9a56362353b507bc96cbfb62728b22"}, {"y": 92, "x": 320, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0755624e597482769b696c4eca8f753b2b873a4f"}, {"y": 184, "x": 640, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=acf0061a8c4917dd169a02a9982c1ba70cf35117"}, {"y": 276, "x": 960, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee2b256c89aa02600584f6bec6d9bda5d9b096aa"}, {"y": 311, "x": 1080, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4ad491af8a3239eb5ef8e96e103cfaa068ff6ea"}], "s": {"y": 316, "x": 1096, "u": "https://preview.redd.it/94pdir9f16oc1.png?width=1096&amp;format=png&amp;auto=webp&amp;s=b0f3c45506cf2a3c288d780fead9f33d979e3267"}, "id": "94pdir9f16oc1"}}, "name": "t3_1be395d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/63jK-_OvQUavcQrCcrDL58R0CJs6FkXm08oDXfbz9Ac.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710364982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Please give me any suggestions what will be the best way to redesign this data pipeline, to ingest data from the BI tool back to the source table in Snowflake.&lt;br/&gt;\nCurrent setup:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/94pdir9f16oc1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0f3c45506cf2a3c288d780fead9f33d979e3267\"&gt;https://preview.redd.it/94pdir9f16oc1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0f3c45506cf2a3c288d780fead9f33d979e3267&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a Google spreadsheet that is transformed daily with DBT and stored in Snowflake DB as a mart-ready table for analysis. This table undergoes a full refresh daily. Later, this table is connected to the BI tool where Data Analysts analyze the data. However, they now want to manually input data in Power BI so that this data will be saved in the initial source table in Snowflake.&lt;/p&gt;\n\n&lt;p&gt;How should I approach this, considering that the initial table is refreshed daily? I&amp;#39;m thinking of changing DBT materialization to incremental or creating a new extra table in Snowflake that will be a combination of raw source data and input from BI. Maybe there is a better way yo do it?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1be395d", "is_robot_indexable": true, "report_reasons": null, "author": "Krukach", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be395d/redesign_etl_process_dbt_table_materialization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be395d/redesign_etl_process_dbt_table_materialization/", "subreddit_subscribers": 168768, "created_utc": 1710364982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, a junior data engineer here.\n\nI am working on a mobile application that's similar to true caller but designed for the gulf countries (this kind of apps is famous there). The app allows users to search by phone number and name (full text search).\n\nI am using postgres as a database and I am planning to have this design :\n\n* a table for phone numbers : the table will have phone numbers, name associated with the phone number, country code, and some meta data. The table will be partitioned by the country code.\n\n* a table for names: the same schema of the phones table, but I am planning to add 4 more columns for the first letter of each word in the name. Like that, I can filter out some rows before applying full text search.\n\nFor example, if I have \"Ahmed El Said\" I would store the name in the name column then store \"A\", \"E\", and \"S\" in the the 4 columns (with the last one null).\n\nFor indexes, I will add an index on the phone number for the first table and another one for each column of the 4 columns in the names table + an index on the name column.\n\nWhat do you think of this design? Do you think I can use one table for both searchs?\n\nThanks in advance for your help.", "author_fullname": "t2_dcz09dzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me design this database ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1beocg8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710431363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, a junior data engineer here.&lt;/p&gt;\n\n&lt;p&gt;I am working on a mobile application that&amp;#39;s similar to true caller but designed for the gulf countries (this kind of apps is famous there). The app allows users to search by phone number and name (full text search).&lt;/p&gt;\n\n&lt;p&gt;I am using postgres as a database and I am planning to have this design :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;a table for phone numbers : the table will have phone numbers, name associated with the phone number, country code, and some meta data. The table will be partitioned by the country code.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;a table for names: the same schema of the phones table, but I am planning to add 4 more columns for the first letter of each word in the name. Like that, I can filter out some rows before applying full text search.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For example, if I have &amp;quot;Ahmed El Said&amp;quot; I would store the name in the name column then store &amp;quot;A&amp;quot;, &amp;quot;E&amp;quot;, and &amp;quot;S&amp;quot; in the the 4 columns (with the last one null).&lt;/p&gt;\n\n&lt;p&gt;For indexes, I will add an index on the phone number for the first table and another one for each column of the 4 columns in the names table + an index on the name column.&lt;/p&gt;\n\n&lt;p&gt;What do you think of this design? Do you think I can use one table for both searchs?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1beocg8", "is_robot_indexable": true, "report_reasons": null, "author": "reda10dk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beocg8/help_me_design_this_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beocg8/help_me_design_this_database/", "subreddit_subscribers": 168768, "created_utc": 1710431363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I am working as an Informatica developer with a  total of 7 yr of experience but now I think my career is not progressing as much. Please guide me what should i do next...TIA. ", "author_fullname": "t2_dwbp6nww0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informatica : career help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beg9e1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710404461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I am working as an Informatica developer with a  total of 7 yr of experience but now I think my career is not progressing as much. Please guide me what should i do next...TIA. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1beg9e1", "is_robot_indexable": true, "report_reasons": null, "author": "amorcita_fishy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beg9e1/informatica_career_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beg9e1/informatica_career_help/", "subreddit_subscribers": 168768, "created_utc": 1710404461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n&amp;#x200B;\n\n# How to integrate Great Expecation Data Quality tests in Airflow?\n\n\ud83d\udcf7[**Blog**](https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&amp;restrict_sr=1) Orchestrate Modern Data Stack\n\nVlog on how to how to integrate Great Expectation Data Quality tests in the Apache Airflow.  We use the Great Expectation (GE) provider for Airlow and run the Great Expectations suite. The target data asset is a PostgreSQL table.  We use Airflow as the orchestrator for the ETL (ELT) Data piepline and GE as the testing framework\n\n[https://www.youtube.com/watch?v=WAgbFrHUk50](https://www.youtube.com/watch?v=WAgbFrHUk50)\n\nTopics covered:\n\n* Data Orchestartion\n* Airflow &amp; DAG\n* Airflow GE Provider\n* Run Modern Data Stack with Airflow\n\nTech Stack: **Airflow, Great Expecations, Data Quality, Python**", "author_fullname": "t2_vj0466m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to integrate Great Expecation Data Quality tests in Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2kwd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710363424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;How to integrate Great Expecation Data Quality tests in Airflow?&lt;/h1&gt;\n\n&lt;p&gt;\ud83d\udcf7&lt;a href=\"https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&amp;amp;restrict_sr=1\"&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; Orchestrate Modern Data Stack&lt;/p&gt;\n\n&lt;p&gt;Vlog on how to how to integrate Great Expectation Data Quality tests in the Apache Airflow.  We use the Great Expectation (GE) provider for Airlow and run the Great Expectations suite. The target data asset is a PostgreSQL table.  We use Airflow as the orchestrator for the ETL (ELT) Data piepline and GE as the testing framework&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=WAgbFrHUk50\"&gt;https://www.youtube.com/watch?v=WAgbFrHUk50&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Topics covered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Orchestartion&lt;/li&gt;\n&lt;li&gt;Airflow &amp;amp; DAG&lt;/li&gt;\n&lt;li&gt;Airflow GE Provider&lt;/li&gt;\n&lt;li&gt;Run Modern Data Stack with Airflow&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tech Stack: &lt;strong&gt;Airflow, Great Expecations, Data Quality, Python&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b8HcC71hUZBtsg4TiNqi6bKNvIzjnyg76TIZCCp-XyA.jpg?auto=webp&amp;s=1f9fac17204c854b5b4d082002059c44d5ae03ed", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/b8HcC71hUZBtsg4TiNqi6bKNvIzjnyg76TIZCCp-XyA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8be4ceb1eb56baade4c73afe89f795ba729582c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/b8HcC71hUZBtsg4TiNqi6bKNvIzjnyg76TIZCCp-XyA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a24799e256cfd2e1a27df43a0bac89db7a456ad0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/b8HcC71hUZBtsg4TiNqi6bKNvIzjnyg76TIZCCp-XyA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb45dfe184a212d81531e5227e21e5f5f95a26b2", "width": 320, "height": 240}], "variants": {}, "id": "TSidoV9hmIfyC3VVXAqXTE2OWfjCP7yrcSm4ForS6lE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1be2kwd", "is_robot_indexable": true, "report_reasons": null, "author": "Either-Adeptness6638", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be2kwd/how_to_integrate_great_expecation_data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be2kwd/how_to_integrate_great_expecation_data_quality/", "subreddit_subscribers": 168768, "created_utc": 1710363424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI've been thinking a lot about how we, as data professionals, are tackling data adoption hurdles, especially in areas like drug development and personalized medicine (Life Sciences); so I wrote this article. \n\n[https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare](https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare)\n\nAre you facing similar challenges? How are you managing them, and what strategies or tools have made a difference in your work?\n\nReally curious to hear if your experiences align with mine and how you're navigating this complex landscape.", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data in Life Sciences: Are We on the Same Page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be1kpo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710361010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been thinking a lot about how we, as data professionals, are tackling data adoption hurdles, especially in areas like drug development and personalized medicine (Life Sciences); so I wrote this article. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare\"&gt;https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Are you facing similar challenges? How are you managing them, and what strategies or tools have made a difference in your work?&lt;/p&gt;\n\n&lt;p&gt;Really curious to hear if your experiences align with mine and how you&amp;#39;re navigating this complex landscape.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?auto=webp&amp;s=91c90fdf2875134891d193f444905b830d0864da", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e5046c0a3229b2e9e21ed3054b87f1903055596", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86c79bee66d97e1e8dcb050394863d568359d343", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b69b996d84839071da5ae484eb249679d4b969d", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2237495e1c0a2c721481a18987c1e1e4940a2a4", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ce5da23d057f738322d8874cda1f5d036e44aef", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/iIxtwAsUPqXntMCgJDyGLsViZ9LLJ_2mJ5bVdjpBJAc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd322b26a9ac0c1761783fa0722662cb6b7dd90e", "width": 1080, "height": 564}], "variants": {}, "id": "9AKl43XPJbE9__vNApnXsZxefA6nyXZTbapru4_2mhE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1be1kpo", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be1kpo/data_in_life_sciences_are_we_on_the_same_page/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be1kpo/data_in_life_sciences_are_we_on_the_same_page/", "subreddit_subscribers": 168768, "created_utc": 1710361010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nAs a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here's the concept:\n\nWe need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:\n\n* Static (meaning they update or add their rows every *n* minutes or hours).\n* Temporally ordered (i.e., Time Series).\n\nIt is highly likely that the observations, which are to be fetched every *n* seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by 'Real Time', I mean that it gets updated every *n* seconds).\n\nAlso, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.\n\nI am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.\n\nThank you in advance", "author_fullname": "t2_c0ghewdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What architecture would you suggest for a Real Time Streaming project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1beo2yq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710430878.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710430666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;As a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here&amp;#39;s the concept:&lt;/p&gt;\n\n&lt;p&gt;We need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Static (meaning they update or add their rows every &lt;em&gt;n&lt;/em&gt; minutes or hours).&lt;/li&gt;\n&lt;li&gt;Temporally ordered (i.e., Time Series).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It is highly likely that the observations, which are to be fetched every &lt;em&gt;n&lt;/em&gt; seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by &amp;#39;Real Time&amp;#39;, I mean that it gets updated every &lt;em&gt;n&lt;/em&gt; seconds).&lt;/p&gt;\n\n&lt;p&gt;Also, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.&lt;/p&gt;\n\n&lt;p&gt;I am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1beo2yq", "is_robot_indexable": true, "report_reasons": null, "author": "hasty-beaver", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beo2yq/what_architecture_would_you_suggest_for_a_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beo2yq/what_architecture_would_you_suggest_for_a_real/", "subreddit_subscribers": 168768, "created_utc": 1710430666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello community. I like my job so much that I would like to use some new tools to build complete architecture for my project. input: world crime data &gt; output: streamlit app on internet. So far I'm using complete MS environment (SQL, ADF, PBI) or for simpler projects (KNIME, PBI). Have \"weak NUC\" as server like 8GB ram 4 threads and so on. My picture is something like this, only open source: Local PC (64GB RAM, RX 6900XT): extract data via API with dagster and load to DuckDB, do modeling using polars and then prediction with pytorch. Pipeline between local PC DuckDB and server streamlit app maybe via jekins? I woul like to use most modern tools even if they are overkill, but willing to learn. Do you have any recommendations or comments? I am not 100% sure for usage of Jenkins as CI/CD between duckdb and streamlit. Thank you for every comment and recommendation gyus!", "author_fullname": "t2_61skm8bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data architecture for home project - crime preduction in streamlit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bem5cq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710425620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello community. I like my job so much that I would like to use some new tools to build complete architecture for my project. input: world crime data &amp;gt; output: streamlit app on internet. So far I&amp;#39;m using complete MS environment (SQL, ADF, PBI) or for simpler projects (KNIME, PBI). Have &amp;quot;weak NUC&amp;quot; as server like 8GB ram 4 threads and so on. My picture is something like this, only open source: Local PC (64GB RAM, RX 6900XT): extract data via API with dagster and load to DuckDB, do modeling using polars and then prediction with pytorch. Pipeline between local PC DuckDB and server streamlit app maybe via jekins? I woul like to use most modern tools even if they are overkill, but willing to learn. Do you have any recommendations or comments? I am not 100% sure for usage of Jenkins as CI/CD between duckdb and streamlit. Thank you for every comment and recommendation gyus!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bem5cq", "is_robot_indexable": true, "report_reasons": null, "author": "betonaren", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bem5cq/data_architecture_for_home_project_crime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bem5cq/data_architecture_for_home_project_crime/", "subreddit_subscribers": 168768, "created_utc": 1710425620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa5dw92do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No More Surprises in Snowflake with Resource Monitors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 112, "top_awarded_type": null, "hide_score": false, "name": "t3_1beluak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QUxQS5-rJxsw4DNxTxU7vEO2v0bGx0prGYgZ550Vmv4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710424812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "baselit.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://baselit.ai/blog/no-more-surprises-in-snowflake-with-resource-monitors", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DDjJfmOLWdeegYWitTOLo-mfj1aGXABtsoRN8i9Ucbc.jpg?auto=webp&amp;s=9d70cb747b97f736c5749073351847050f654940", "width": 1828, "height": 1466}, "resolutions": [{"url": "https://external-preview.redd.it/DDjJfmOLWdeegYWitTOLo-mfj1aGXABtsoRN8i9Ucbc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4bf59d16f8fc9b692ac26079b45b88b4a21e130", "width": 108, "height": 86}, {"url": "https://external-preview.redd.it/DDjJfmOLWdeegYWitTOLo-mfj1aGXABtsoRN8i9Ucbc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a84b5b366df305c86f82e82d723c6354b9a4c3f", "width": 216, "height": 173}, {"url": "https://external-preview.redd.it/DDjJfmOLWdeegYWitTOLo-mfj1aGXABtsoRN8i9Ucbc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=759df7bf918a8c2ad35d5c3c8937c5d1ae47350e", "width": 320, "height": 256}, {"url": "https://external-preview.redd.it/DDjJfmOLWdeegYWitTOLo-mfj1aGXABtsoRN8i9Ucbc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4cbfd111553670f4f1d525ce062ab29ec1e80a4", "width": 640, "height": 513}, {"url": "https://external-preview.redd.it/DDjJfmOLWdeegYWitTOLo-mfj1aGXABtsoRN8i9Ucbc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d28a7e1aad9a673cd5ed34737de34b61f6f3cc8e", "width": 960, "height": 769}, {"url": "https://external-preview.redd.it/DDjJfmOLWdeegYWitTOLo-mfj1aGXABtsoRN8i9Ucbc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=154ce76085e2b80e2cb67a1befa6280b9f92bd4b", "width": 1080, "height": 866}], "variants": {}, "id": "WzHO6PPFvUncmnSdaLnK2OQyIu_HZYxmeHYRP8-VEDY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1beluak", "is_robot_indexable": true, "report_reasons": null, "author": "sahil_singla", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beluak/no_more_surprises_in_snowflake_with_resource/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://baselit.ai/blog/no-more-surprises-in-snowflake-with-resource-monitors", "subreddit_subscribers": 168768, "created_utc": 1710424812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://blog.twingdata.com/p/identify-unused-columns-in-snowflake](https://blog.twingdata.com/p/identify-unused-columns-in-snowflake)\n\n  \nTLDR: If you're on enterprise Snowflake you can do it via a query. Otherwise there's a simple Python script that uses the sqlglot library to extract the physical columns and compares them against the info schema.  \n", "author_fullname": "t2_bp7tn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Query + Script to identify unused columns in Snowflake and other data warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1belgv1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710423768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://blog.twingdata.com/p/identify-unused-columns-in-snowflake\"&gt;https://blog.twingdata.com/p/identify-unused-columns-in-snowflake&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TLDR: If you&amp;#39;re on enterprise Snowflake you can do it via a query. Otherwise there&amp;#39;s a simple Python script that uses the sqlglot library to extract the physical columns and compares them against the info schema.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?auto=webp&amp;s=7a9fd4cbac369bdf44c0932caf089f3acf4f0183", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95df4917027026cfacaa016b5e3dba6ac427e5a1", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=798883132e928a3b65e3ac750e786fe2200c8ec5", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3e3d4248e553427fdc1d7972039b469fc247f84", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cfb65a334b8b1c97d00db9a89200396f1cad03f", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d219b01314c5718db4f8d8186a6272a0846572c", "width": 960, "height": 562}], "variants": {}, "id": "3CvJ-CSBlahwziE_SAyoY8Vwk75zbSq4MidCUoIDcOI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1belgv1", "is_robot_indexable": true, "report_reasons": null, "author": "dangoldin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1belgv1/query_script_to_identify_unused_columns_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1belgv1/query_script_to_identify_unused_columns_in/", "subreddit_subscribers": 168768, "created_utc": 1710423768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like title says.\n\nWhen do you feel that it is a time to move next company? Here is a list that I can think of\n\n- want to play with different type of data\n- hard to get promotion\n- not learning much\n- your function is less appreciated in the org\n- stayed one place too long\n- just want to try something new\n- want to relax and not working crazy\n\n\nI have been with one startup about 5 years. Working with decent size of data with Python, airflow, snowflake, spark, aws, k8s, docker, etc.\n\nI have been putting a lot of effort in this company although I'm at lower level. Several rounds of layoffs happened over the last 2 years and my team lost more than half.\n\nAlso a few of good team members left while company's revenue is still growing decently. Salary is lower side of median band.\n\nI still don't know which sector I want to try, but want to hear from other DE's when they decide to move. Do you have your own indicator? Or mostly goes with your feelings?", "author_fullname": "t2_ub3cyaaa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When do you feel to move next?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be9w0z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710381987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like title says.&lt;/p&gt;\n\n&lt;p&gt;When do you feel that it is a time to move next company? Here is a list that I can think of&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;want to play with different type of data&lt;/li&gt;\n&lt;li&gt;hard to get promotion&lt;/li&gt;\n&lt;li&gt;not learning much&lt;/li&gt;\n&lt;li&gt;your function is less appreciated in the org&lt;/li&gt;\n&lt;li&gt;stayed one place too long&lt;/li&gt;\n&lt;li&gt;just want to try something new&lt;/li&gt;\n&lt;li&gt;want to relax and not working crazy&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have been with one startup about 5 years. Working with decent size of data with Python, airflow, snowflake, spark, aws, k8s, docker, etc.&lt;/p&gt;\n\n&lt;p&gt;I have been putting a lot of effort in this company although I&amp;#39;m at lower level. Several rounds of layoffs happened over the last 2 years and my team lost more than half.&lt;/p&gt;\n\n&lt;p&gt;Also a few of good team members left while company&amp;#39;s revenue is still growing decently. Salary is lower side of median band.&lt;/p&gt;\n\n&lt;p&gt;I still don&amp;#39;t know which sector I want to try, but want to hear from other DE&amp;#39;s when they decide to move. Do you have your own indicator? Or mostly goes with your feelings?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1be9w0z", "is_robot_indexable": true, "report_reasons": null, "author": "winderous", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1be9w0z/when_do_you_feel_to_move_next/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1be9w0z/when_do_you_feel_to_move_next/", "subreddit_subscribers": 168768, "created_utc": 1710381987.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}