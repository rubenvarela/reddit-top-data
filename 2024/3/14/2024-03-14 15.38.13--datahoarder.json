{"kind": "Listing", "data": {"after": "t3_1beblhc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I came across some info graphic depicting common storage media and their size:\n\n* various generations of magnetic tape = 10TB to 100GB\n* BluRay = 25GB\n* DVD = 4.5GB\n* CD = 700MB\n* 3.5in floppy disk = 1.5MB\n\nwas there really such a huge jump from 3.5inch floppies to CDs? It almost skipped two orders of magnitude, 10MB and 100MB.  \nI did some research and found some special floppy disks that could hold 10MB to 100MB, but they seem rather rare.\n\n**Did i miss something or was there no popular physical media in that size range?**\n\nIs that just cherry picking the numbers? Worst floppies vs. best CDs\n\nGaming Consoles had a period of cartridges, was there something similar for PCs?\n\nWas swapping hard drives \"a thing\" in that time?\n\nWas there no need for a intermediate medium because floppies were just so cheap? So just using 3 to 40 floppies was cheaper than getting a new medium.\n\nWere CDs just so innovative in their design? Optical instead of magnetic, funding from the music industry", "author_fullname": "t2_u8ypxau6l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Retro] Was the jump from 3.5in floppy to CD really that big? Were there no 10MB to 100MB storage media?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdwc2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 252, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 252, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710348671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across some info graphic depicting common storage media and their size:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;various generations of magnetic tape = 10TB to 100GB&lt;/li&gt;\n&lt;li&gt;BluRay = 25GB&lt;/li&gt;\n&lt;li&gt;DVD = 4.5GB&lt;/li&gt;\n&lt;li&gt;CD = 700MB&lt;/li&gt;\n&lt;li&gt;3.5in floppy disk = 1.5MB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;was there really such a huge jump from 3.5inch floppies to CDs? It almost skipped two orders of magnitude, 10MB and 100MB.&lt;br/&gt;\nI did some research and found some special floppy disks that could hold 10MB to 100MB, but they seem rather rare.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Did i miss something or was there no popular physical media in that size range?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is that just cherry picking the numbers? Worst floppies vs. best CDs&lt;/p&gt;\n\n&lt;p&gt;Gaming Consoles had a period of cartridges, was there something similar for PCs?&lt;/p&gt;\n\n&lt;p&gt;Was swapping hard drives &amp;quot;a thing&amp;quot; in that time?&lt;/p&gt;\n\n&lt;p&gt;Was there no need for a intermediate medium because floppies were just so cheap? So just using 3 to 40 floppies was cheaper than getting a new medium.&lt;/p&gt;\n\n&lt;p&gt;Were CDs just so innovative in their design? Optical instead of magnetic, funding from the music industry&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1bdwc2k", "is_robot_indexable": true, "report_reasons": null, "author": "Robert_A2D0FF", "discussion_type": null, "num_comments": 405, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdwc2k/retro_was_the_jump_from_35in_floppy_to_cd_really/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdwc2k/retro_was_the_jump_from_35in_floppy_to_cd_really/", "subreddit_subscribers": 738575, "created_utc": 1710348671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ck2fr5mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warner Bros. Discovery Disappears Games People Already Purchased", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2o3l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 221, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 221, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RhtuabX2AZtqmze1q9DPpMCkkIn4ceU3JAAZEoWeI4M.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710363635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techdirt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techdirt.com/2024/03/12/warner-bros-discovery-disappears-games-people-already-purchased/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?auto=webp&amp;s=03a5fde066455710ac1676ca5e55fdf4cff5f177", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd544711a911105c2c84e8778e42492e6627f7ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=581443ef43c416964d464ddd6b5b28eec7b2b77c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=98eca8135ccdbe0daad2aad9f2f21c250d1564ab", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60f612dc25b6693520e6d4a342645939506f3afd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30ce1c1c245620a8b70a4d0a298b39657711ef0e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b24b6b5a4ae88248a5d6b951b789da794832c41", "width": 1080, "height": 567}], "variants": {}, "id": "86PGtE2qmX3coS9Htmb8TUfXMSg2HaYO4Rk8A0YbGow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be2o3l", "is_robot_indexable": true, "report_reasons": null, "author": "AbolishDisney", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1be2o3l/warner_bros_discovery_disappears_games_people/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techdirt.com/2024/03/12/warner-bros-discovery-disappears-games-people-already-purchased/", "subreddit_subscribers": 738575, "created_utc": 1710363635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\nthis is my first post here, so I hope I'm not breaking any rules here. I'd like to point the community's attention towards the following issue.\n\nIn an unprecedented act of digital vandalism, Nintendo decided to rm all levels from Super Mario Maker 1, because they hate people who like their games. That's over 100 million creations from regular users like you and I. This is obviously a huge loss to gaming history and to gamers in general.\n\nSome software exists to back this up, but not fully, there's no easy way to load the backups to check them, and there is no recent dump.\n\nThere's [this tool by PretendoNetwork](https://github.com/PretendoNetwork/archival-tools/tree/master/super-mario-maker) but that's ways off from actually having the levels playable in an emulator or on a console. Worst of all there's no way to tell if it currently works because there's no tool for *loading* levels dumped this way.\n\nThere's also [this tool by HerobrineTV](https://github.com/HerobrineTV/SMM1-Level-Downloader) and a post [here](https://archive.org/details/super_mario_maker_courses_202105) that explains what's involved in getting dumps done with their tool to register inside a WiiU. That post is on a dump that also contains a bunch of courses, as I understand.\n\nI believe HerobrineTV's and PretendoNetwork's tools both capture different kinds of data *and* different kinds of metadata.\n\nSomeone actually has to run those tools and get all that stuff. It requires a working nintendo login - I don't have that right now, my Wii U is in storage in one of a million boxes. I'd have started myself already.\n\nThere's a partial dump of some sort that's [on archive](https://archive.org/download/smm_levels), but it's from early 2022 - so a lot of levels are going to be missing. The author of that dump stopped at close to 70 million levels, but that's not all. Note on this dump: the first 10 000 levels are dumped in some other format that does not actually include the level data; further levels seem to contain that level data, but bear in mind that the .torrent file available on that archive page does not include those dumps, so you'll have to download all those 6-12 GB files via http(s).\n\nThat dump also doesn't seem to include level screenshots, and I believe pretendo's tool doesn't get them. Also, that dump was made using an older version of the tool, which exports less metadata.\n\nGiven the size of that partial dump (around 600-800 GB), my guess is a full dump would be on the order of 1-2TB. It's not a LOT lot, but it's quite a lot, so work has to be done in parallel by multiple people to ensure this goal is met before end of March. \n\nHowever it bears keeping in mind that the older dumps will contain levels that have since been deleted. So they are still worthwhile and worth incorporating.\n\nRuTr has a release of SMM2 Switch with a loader tool, but I don't know how different the loader tool would have to be made to make it load levels into SMM1. It looks like everyone just focused on SMM2 sideloading, so SMM1 needs help with software dev to even ensure the backups work at all.\n\nA library tool of some sort would be useful, too. As would transforming all that json output from PretendoNetwork's tool to an sqlite database with a fixed schema, so it can be queried (eg to find all levels by one creator).\n\nAs of right now, there is no fully useable dump available anywhere I looked, and no loader tool seems to be present.\n\nThe current state is:\n\n- two dumper tools, one by HerobrineTV, one by PretendoNetwork, which seem to capture different data and different metadata\n\n- two partial dumps, which may contain already deleted courses and associated metadata\n\n- a lot of missing levels (latest dump is from mid 2022, so the levels might have grown by 2x since then).\n\n- an [unpacking tool](http://wiibrew.org/wiki/ASH_Extractor) for the binary files to turn them into files that have to be on the WiiU's system to make them loadable\n\n- a manual method of loading the courses onto a WiiU (or into an emulator)\n\n- HerobrineTV seems to be working on a GUI of some sort, but it doesn't seem to be public so far\n\n- no way of uploading all those files and related metadata onto archive\n\n- no convenient torrent with all the dumped data\n\nIt is an understatement that anyone who helps save this data is an absolute hero of a human being, so I hope to spur some attention to this here.", "author_fullname": "t2_5wpjh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Super Mario Maker servers are going away very soon, and the 1-2TB of user submitted levels need archiving", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bejk7e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 175, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 175, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710417767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nthis is my first post here, so I hope I&amp;#39;m not breaking any rules here. I&amp;#39;d like to point the community&amp;#39;s attention towards the following issue.&lt;/p&gt;\n\n&lt;p&gt;In an unprecedented act of digital vandalism, Nintendo decided to rm all levels from Super Mario Maker 1, because they hate people who like their games. That&amp;#39;s over 100 million creations from regular users like you and I. This is obviously a huge loss to gaming history and to gamers in general.&lt;/p&gt;\n\n&lt;p&gt;Some software exists to back this up, but not fully, there&amp;#39;s no easy way to load the backups to check them, and there is no recent dump.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s &lt;a href=\"https://github.com/PretendoNetwork/archival-tools/tree/master/super-mario-maker\"&gt;this tool by PretendoNetwork&lt;/a&gt; but that&amp;#39;s ways off from actually having the levels playable in an emulator or on a console. Worst of all there&amp;#39;s no way to tell if it currently works because there&amp;#39;s no tool for &lt;em&gt;loading&lt;/em&gt; levels dumped this way.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also &lt;a href=\"https://github.com/HerobrineTV/SMM1-Level-Downloader\"&gt;this tool by HerobrineTV&lt;/a&gt; and a post &lt;a href=\"https://archive.org/details/super_mario_maker_courses_202105\"&gt;here&lt;/a&gt; that explains what&amp;#39;s involved in getting dumps done with their tool to register inside a WiiU. That post is on a dump that also contains a bunch of courses, as I understand.&lt;/p&gt;\n\n&lt;p&gt;I believe HerobrineTV&amp;#39;s and PretendoNetwork&amp;#39;s tools both capture different kinds of data &lt;em&gt;and&lt;/em&gt; different kinds of metadata.&lt;/p&gt;\n\n&lt;p&gt;Someone actually has to run those tools and get all that stuff. It requires a working nintendo login - I don&amp;#39;t have that right now, my Wii U is in storage in one of a million boxes. I&amp;#39;d have started myself already.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a partial dump of some sort that&amp;#39;s &lt;a href=\"https://archive.org/download/smm_levels\"&gt;on archive&lt;/a&gt;, but it&amp;#39;s from early 2022 - so a lot of levels are going to be missing. The author of that dump stopped at close to 70 million levels, but that&amp;#39;s not all. Note on this dump: the first 10 000 levels are dumped in some other format that does not actually include the level data; further levels seem to contain that level data, but bear in mind that the .torrent file available on that archive page does not include those dumps, so you&amp;#39;ll have to download all those 6-12 GB files via http(s).&lt;/p&gt;\n\n&lt;p&gt;That dump also doesn&amp;#39;t seem to include level screenshots, and I believe pretendo&amp;#39;s tool doesn&amp;#39;t get them. Also, that dump was made using an older version of the tool, which exports less metadata.&lt;/p&gt;\n\n&lt;p&gt;Given the size of that partial dump (around 600-800 GB), my guess is a full dump would be on the order of 1-2TB. It&amp;#39;s not a LOT lot, but it&amp;#39;s quite a lot, so work has to be done in parallel by multiple people to ensure this goal is met before end of March. &lt;/p&gt;\n\n&lt;p&gt;However it bears keeping in mind that the older dumps will contain levels that have since been deleted. So they are still worthwhile and worth incorporating.&lt;/p&gt;\n\n&lt;p&gt;RuTr has a release of SMM2 Switch with a loader tool, but I don&amp;#39;t know how different the loader tool would have to be made to make it load levels into SMM1. It looks like everyone just focused on SMM2 sideloading, so SMM1 needs help with software dev to even ensure the backups work at all.&lt;/p&gt;\n\n&lt;p&gt;A library tool of some sort would be useful, too. As would transforming all that json output from PretendoNetwork&amp;#39;s tool to an sqlite database with a fixed schema, so it can be queried (eg to find all levels by one creator).&lt;/p&gt;\n\n&lt;p&gt;As of right now, there is no fully useable dump available anywhere I looked, and no loader tool seems to be present.&lt;/p&gt;\n\n&lt;p&gt;The current state is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;two dumper tools, one by HerobrineTV, one by PretendoNetwork, which seem to capture different data and different metadata&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;two partial dumps, which may contain already deleted courses and associated metadata&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;a lot of missing levels (latest dump is from mid 2022, so the levels might have grown by 2x since then).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;an &lt;a href=\"http://wiibrew.org/wiki/ASH_Extractor\"&gt;unpacking tool&lt;/a&gt; for the binary files to turn them into files that have to be on the WiiU&amp;#39;s system to make them loadable&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;a manual method of loading the courses onto a WiiU (or into an emulator)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;HerobrineTV seems to be working on a GUI of some sort, but it doesn&amp;#39;t seem to be public so far&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;no way of uploading all those files and related metadata onto archive&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;no convenient torrent with all the dumped data&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It is an understatement that anyone who helps save this data is an absolute hero of a human being, so I hope to spur some attention to this here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?auto=webp&amp;s=3b84cca0e694973eaf37fe4fb7f21548f34776e5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efcf1a7390ca781365977201e28ef45c237fe60d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3715372d9328807e32326748014da19db5b18d7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=42685486e182e51f9f38c6117ae18f1a910d14ae", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea3202c2906374ec5bae6ba983a976ee0bbce498", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea1ffa0c883610a04b2ec80cb7b7d0e1d872b504", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=901c52e165135fdad9500210f847521b9150cdbf", "width": 1080, "height": 540}], "variants": {}, "id": "iJx7WgxqsvWXnDymvJda-NvEEKUFva8UFLJWX_2Jcc0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1bejk7e", "is_robot_indexable": true, "report_reasons": null, "author": "cheater00", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bejk7e/super_mario_maker_servers_are_going_away_very/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bejk7e/super_mario_maker_servers_are_going_away_very/", "subreddit_subscribers": 738575, "created_utc": 1710417767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ci9q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Publishing historic data from around year 2000?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 120, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdu64z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3XFGMGAH2-GlT7rIcF9v4jta1434UFAZkbdT4T7RwIw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710343426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ne0teadzc4oc1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ne0teadzc4oc1.png?auto=webp&amp;s=f9f44a92c9a33863fefd90d5a06e01c593f46cad", "width": 1031, "height": 890}, "resolutions": [{"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=11af8a72bb534f22b7c44c2fedb19c69be0e2a97", "width": 108, "height": 93}, {"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c380d7c02e7ab686d84b7702b49c15b684348cc", "width": 216, "height": 186}, {"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2f5c9dfd97523abeceb2c6192a7950ba730c18f", "width": 320, "height": 276}, {"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a35e4d168d3820fe7e710c939bc67833fe86c899", "width": 640, "height": 552}, {"url": "https://preview.redd.it/ne0teadzc4oc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c2ddcb74149d818e55a9fe2e8030c3200fb4678", "width": 960, "height": 828}], "variants": {}, "id": "KsNaxLvy3LpavfH2h2KoYzMTDNVFrp2E549N_RergbI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdu64z", "is_robot_indexable": true, "report_reasons": null, "author": "NagateTanikaze", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdu64z/publishing_historic_data_from_around_year_2000/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ne0teadzc4oc1.png", "subreddit_subscribers": 738575, "created_utc": 1710343426.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As part of my desire for a \"better Datahoarder\" \\[no criticism of the moderators by the way\\] I'd also like to suggest the following rule for **any questions** that seek a recommendation for a product or service for eventual hire/purchase/engagement **that is not already going to be reasonably covered** by rule 1 and rule 2.\n\n1. State your budget\n2. State the country where you expect the item to be delivered to\n3. If you are jabbering on about the product, get the product right, so not a \"Seagate Exus A349 18tb\" if you actually mean something else, like a Seagate Exos ....\"\n4. Proof-read your damn question to see if makes vague sense. One \"question\" earlier was more of a life story and several members here couldn't be 100% if it was a question, and if it was rule 1 exists.\n5. and still, use search first based on what you would have written and see if magic happens.\n\nAnnoyed slightly that someone asked for a recommendation today in an area. Got a recommendation. Was unhappy \"not that area\".  They didn't say that area=country name or that area except XYZ.  \n\n\nIt's not bloody difficult with a microsecond of thought and brain engagement. And if it really is a subject that you know zero about and honestly can't Google it, then say what you've tried. I mean, I'm not a car person, so I might have to describe \"My car is making a noise somewhere from behind the plastic panel on the right hand side, a sort of knocking, but before the engine compartment and it only happens when I start it up. A car person might know \\[made up now\\] it's your alternator probably, then I could start searching for repair \\[model name\\] alternator or something.  I think these sort of exclusions are rare though. I'd not write \"my car makes a knocking sound, where to fix it?\" and leave it at that...\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_z85hu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better questions...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzaqm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710355592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As part of my desire for a &amp;quot;better Datahoarder&amp;quot; [no criticism of the moderators by the way] I&amp;#39;d also like to suggest the following rule for &lt;strong&gt;any questions&lt;/strong&gt; that seek a recommendation for a product or service for eventual hire/purchase/engagement &lt;strong&gt;that is not already going to be reasonably covered&lt;/strong&gt; by rule 1 and rule 2.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;State your budget&lt;/li&gt;\n&lt;li&gt;State the country where you expect the item to be delivered to&lt;/li&gt;\n&lt;li&gt;If you are jabbering on about the product, get the product right, so not a &amp;quot;Seagate Exus A349 18tb&amp;quot; if you actually mean something else, like a Seagate Exos ....&amp;quot;&lt;/li&gt;\n&lt;li&gt;Proof-read your damn question to see if makes vague sense. One &amp;quot;question&amp;quot; earlier was more of a life story and several members here couldn&amp;#39;t be 100% if it was a question, and if it was rule 1 exists.&lt;/li&gt;\n&lt;li&gt;and still, use search first based on what you would have written and see if magic happens.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Annoyed slightly that someone asked for a recommendation today in an area. Got a recommendation. Was unhappy &amp;quot;not that area&amp;quot;.  They didn&amp;#39;t say that area=country name or that area except XYZ.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not bloody difficult with a microsecond of thought and brain engagement. And if it really is a subject that you know zero about and honestly can&amp;#39;t Google it, then say what you&amp;#39;ve tried. I mean, I&amp;#39;m not a car person, so I might have to describe &amp;quot;My car is making a noise somewhere from behind the plastic panel on the right hand side, a sort of knocking, but before the engine compartment and it only happens when I start it up. A car person might know [made up now] it&amp;#39;s your alternator probably, then I could start searching for repair [model name] alternator or something.  I think these sort of exclusions are rare though. I&amp;#39;d not write &amp;quot;my car makes a knocking sound, where to fix it?&amp;quot; and leave it at that...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "c170Tb local, c. 300Tb on other servers &amp; a lot in the cloud. ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1bdzaqm", "is_robot_indexable": true, "report_reasons": null, "author": "GodSaveUsFromPettyMo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bdzaqm/better_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdzaqm/better_questions/", "subreddit_subscribers": 738575, "created_utc": 1710355592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m trying to get some  Warner Brothers DVDs (old Joan Crawford films) into my Plex server but they are stubbornly refusing to rip all the way. I\u2019ve tried Leawo, MakeMKV and Dumbofab on both Mac and PC. If the apps don\u2019t fail then they only rip about 2/3 of the movie. They won\u2019t even PLAY past that point in VLC. I\u2019ve never seen anything like this before; it\u2019s either some insane copy protection or these discs are just messed up somehow. I think they play fine in a DVD player (I\u2019ll check again tomorrow). Currently trying to rip to .iso just to see if I can get the whole film at least. ", "author_fullname": "t2_rorvibp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone try to rip a DVD, only to have it consistently cut off the last third of the film?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1behbie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710409131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to get some  Warner Brothers DVDs (old Joan Crawford films) into my Plex server but they are stubbornly refusing to rip all the way. I\u2019ve tried Leawo, MakeMKV and Dumbofab on both Mac and PC. If the apps don\u2019t fail then they only rip about 2/3 of the movie. They won\u2019t even PLAY past that point in VLC. I\u2019ve never seen anything like this before; it\u2019s either some insane copy protection or these discs are just messed up somehow. I think they play fine in a DVD player (I\u2019ll check again tomorrow). Currently trying to rip to .iso just to see if I can get the whole film at least. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1behbie", "is_robot_indexable": true, "report_reasons": null, "author": "darwinDMG08", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1behbie/anyone_try_to_rip_a_dvd_only_to_have_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1behbie/anyone_try_to_rip_a_dvd_only_to_have_it/", "subreddit_subscribers": 738575, "created_utc": 1710409131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m sure many of you are aware of Taringa, the social media platform that\u2019s been a part of Latin American internet culture for years. Sadly, it\u2019s been announced that Taringa will be shutting down soon.\n\nThis site holds a treasure trove of memories and content that reflects a significant era of digital history. Before it goes offline, I wanted to ask if anyone here is planning to archive Taringa, or if there\u2019s already an ongoing effort to do so.", "author_fullname": "t2_364dp83u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring taringa.net", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2q6y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710363763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m sure many of you are aware of Taringa, the social media platform that\u2019s been a part of Latin American internet culture for years. Sadly, it\u2019s been announced that Taringa will be shutting down soon.&lt;/p&gt;\n\n&lt;p&gt;This site holds a treasure trove of memories and content that reflects a significant era of digital history. Before it goes offline, I wanted to ask if anyone here is planning to archive Taringa, or if there\u2019s already an ongoing effort to do so.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1be2q6y", "is_robot_indexable": true, "report_reasons": null, "author": "2muchnet42day", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1be2q6y/mirroring_taringanet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be2q6y/mirroring_taringanet/", "subreddit_subscribers": 738575, "created_utc": 1710363763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks,\n\nI'm trying to build a proper NAS for myself with a couple of disks and I want to consolidate all my data from my previous disks (and other family members' HDDs). The problem is that a lot of data is duplicated (because sometime in the past it was migrated to a new disk, then added some more data, then migrated again, etc.)...\n\nDo you have any tools or tricks to easily manage duplicated data? I would prefer Linux cli apps instead of anything GUI related. Also I want to solve this problem at the application level, even if some magic filesystems could deal with this problem by themselves (e.g. ZFS, or I don't know)\n\nThanks in advance!", "author_fullname": "t2_gb9xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you keep your random personal data deduplicated?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bel8mo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710423096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to build a proper NAS for myself with a couple of disks and I want to consolidate all my data from my previous disks (and other family members&amp;#39; HDDs). The problem is that a lot of data is duplicated (because sometime in the past it was migrated to a new disk, then added some more data, then migrated again, etc.)...&lt;/p&gt;\n\n&lt;p&gt;Do you have any tools or tricks to easily manage duplicated data? I would prefer Linux cli apps instead of anything GUI related. Also I want to solve this problem at the application level, even if some magic filesystems could deal with this problem by themselves (e.g. ZFS, or I don&amp;#39;t know)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bel8mo", "is_robot_indexable": true, "report_reasons": null, "author": "semmu", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bel8mo/how_do_you_keep_your_random_personal_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bel8mo/how_do_you_keep_your_random_personal_data/", "subreddit_subscribers": 738575, "created_utc": 1710423096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\nHopefully a easy question with a simple answer. I have two 4TB LaCie rugged hard drives (USB-C version), which I use for back-ups. I hook them up to a 2020 MacBook Pro with thunderbolt ports. One hard drive mirrors the other and I'm storing them in different places. I noticed that the transfer speeds between drives is slow (between 8 and 25mb/s). Why is this? They reach up to 140 mb/s when testing them using Blackmagic Disk Speed Test. The MacBook is not doing something else on the meantime. Is this fixable? Thanks! ", "author_fullname": "t2_594xfrq9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slow transfer between hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1behyoo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710411714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!\nHopefully a easy question with a simple answer. I have two 4TB LaCie rugged hard drives (USB-C version), which I use for back-ups. I hook them up to a 2020 MacBook Pro with thunderbolt ports. One hard drive mirrors the other and I&amp;#39;m storing them in different places. I noticed that the transfer speeds between drives is slow (between 8 and 25mb/s). Why is this? They reach up to 140 mb/s when testing them using Blackmagic Disk Speed Test. The MacBook is not doing something else on the meantime. Is this fixable? Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1behyoo", "is_robot_indexable": true, "report_reasons": null, "author": "Bromska", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1behyoo/slow_transfer_between_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1behyoo/slow_transfer_between_hard_drives/", "subreddit_subscribers": 738575, "created_utc": 1710411714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to purpose a rPi for offline content and not sure which approach would be superior. Is anyone familiar with IIAB as a host image? What benefits does it confer over having a barebones installation with Kiwix on top of it?\n\nI have IIAB currently on my 3b+ but it is slow AF and am trying to find more efficient alternatives that I can run with Docker.", "author_fullname": "t2_1hrlp7qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "R-Pi with basic install plus Kiwix VS I-I-A-B and Kiwix - Which is better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2wrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710364197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to purpose a rPi for offline content and not sure which approach would be superior. Is anyone familiar with IIAB as a host image? What benefits does it confer over having a barebones installation with Kiwix on top of it?&lt;/p&gt;\n\n&lt;p&gt;I have IIAB currently on my 3b+ but it is slow AF and am trying to find more efficient alternatives that I can run with Docker.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1be2wrj", "is_robot_indexable": true, "report_reasons": null, "author": "Bruh-Nanaz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be2wrj/rpi_with_basic_install_plus_kiwix_vs_iiab_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be2wrj/rpi_with_basic_install_plus_kiwix_vs_iiab_and/", "subreddit_subscribers": 738575, "created_utc": 1710364197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hard cover, glued, I also need to scan the front, the side and the back of the hard cover then proceed to scan each page.\n\n&amp;nbsp;\n\nAnyone has a nice video tutorial on Youtube or something? I don't want to ruin the cover pages since it has nice art, I also don't want to ruin the inside edges of each page since some art works are divided into 2 pages.\n\n&amp;nbsp;\n\nTrying to preserve each page with pristine conditions.", "author_fullname": "t2_u3tr5lo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I disassemble a bound book (art book) safely and with minimal damage? To scan each page.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beeljx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710398008.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710397307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hard cover, glued, I also need to scan the front, the side and the back of the hard cover then proceed to scan each page.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Anyone has a nice video tutorial on Youtube or something? I don&amp;#39;t want to ruin the cover pages since it has nice art, I also don&amp;#39;t want to ruin the inside edges of each page since some art works are divided into 2 pages.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Trying to preserve each page with pristine conditions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1beeljx", "is_robot_indexable": true, "report_reasons": null, "author": "Jdpnobs", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beeljx/how_do_i_disassemble_a_bound_book_art_book_safely/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beeljx/how_do_i_disassemble_a_bound_book_art_book_safely/", "subreddit_subscribers": 738575, "created_utc": 1710397307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking to see if there's a case that can handle 24 drives and a regular PSU that doesn't sound like a jet engine.  Are there any?  I realize the title says CPU. I brain farted as I was typing fast. ", "author_fullname": "t2_3o3j6oa3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quiet 24 bay chassis with regular CPU and backplane?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beclev", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710394600.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710390044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to see if there&amp;#39;s a case that can handle 24 drives and a regular PSU that doesn&amp;#39;t sound like a jet engine.  Are there any?  I realize the title says CPU. I brain farted as I was typing fast. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1beclev", "is_robot_indexable": true, "report_reasons": null, "author": "MartiniCommander", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beclev/quiet_24_bay_chassis_with_regular_cpu_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beclev/quiet_24_bay_chassis_with_regular_cpu_and/", "subreddit_subscribers": 738575, "created_utc": 1710390044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have multi-TB HDD I use for Emby/Plex. \n\nEvery month I copy the new movies to an external HDD. \n\nI don\u2019t need to backup any images, I just need software that will add the newest folders on the server to the external HDD automatically. This only needs done every 2 weeks. \n\nI\u2019ve seen rclone, duplicati, macrium etc\u2026  but is there something simple that will do this for windows/ubuntu?\n\nContext: it\u2019s not always easy to manually backup new folders because if I change the server to a different OS, the file structure of \u201ccreated\u201d and \u201cmodified\u201d changes, which makes it difficult to find the actual latest folders. \n\nThanks in advance and I hope I made this clear. ", "author_fullname": "t2_x1giw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manual backup help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bebild", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710386741.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have multi-TB HDD I use for Emby/Plex. &lt;/p&gt;\n\n&lt;p&gt;Every month I copy the new movies to an external HDD. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t need to backup any images, I just need software that will add the newest folders on the server to the external HDD automatically. This only needs done every 2 weeks. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen rclone, duplicati, macrium etc\u2026  but is there something simple that will do this for windows/ubuntu?&lt;/p&gt;\n\n&lt;p&gt;Context: it\u2019s not always easy to manually backup new folders because if I change the server to a different OS, the file structure of \u201ccreated\u201d and \u201cmodified\u201d changes, which makes it difficult to find the actual latest folders. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance and I hope I made this clear. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bebild", "is_robot_indexable": true, "report_reasons": null, "author": "santovalentino", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bebild/manual_backup_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bebild/manual_backup_help/", "subreddit_subscribers": 738575, "created_utc": 1710386741.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am wondering what people here prefer in terms of the type of speed the drives use. \n\nLet\u2018s assume 5400rpm drives cost the same per TB as the 7200rpm drives (what I found to be true with a short comparison online). I just don\u2019t want to include \u201echeapest\u201c as an option\u2026\n\n[View Poll](https://www.reddit.com/poll/1be913f)", "author_fullname": "t2_9rwdy0id", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5400rpm vs 7200 vs enterprise drives/SSD\u2018s", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be913f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710379583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering what people here prefer in terms of the type of speed the drives use. &lt;/p&gt;\n\n&lt;p&gt;Let\u2018s assume 5400rpm drives cost the same per TB as the 7200rpm drives (what I found to be true with a short comparison online). I just don\u2019t want to include \u201echeapest\u201c as an option\u2026&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1be913f\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be913f", "is_robot_indexable": true, "report_reasons": null, "author": "hamdiboi37", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1710984383311, "options": [{"text": "7200rpm hdd", "id": "27405924"}, {"text": "5400rpm hdd", "id": "27405925"}, {"text": "10k+ hdd", "id": "27405926"}, {"text": "SSD/NVMe", "id": "27405927"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 108, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be913f/5400rpm_vs_7200_vs_enterprise_drivesssds/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/1be913f/5400rpm_vs_7200_vs_enterprise_drivesssds/", "subreddit_subscribers": 738575, "created_utc": 1710379583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good day reddit, \n\nI am moving overseas with a shipping container, I have a small server build in a Fractal Design Define 7 XL. 10 Drives in it. My question is how do I best pack and ship it?   \n\n\nDo I ship it built or take out the drives?  \n\n&amp;#x200B;", "author_fullname": "t2_9mfyf0jv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving server overseas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ben7jn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710428426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day reddit, &lt;/p&gt;\n\n&lt;p&gt;I am moving overseas with a shipping container, I have a small server build in a Fractal Design Define 7 XL. 10 Drives in it. My question is how do I best pack and ship it?   &lt;/p&gt;\n\n&lt;p&gt;Do I ship it built or take out the drives?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1ben7jn", "is_robot_indexable": true, "report_reasons": null, "author": "MishaTiTvog", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1ben7jn/moving_server_overseas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1ben7jn/moving_server_overseas/", "subreddit_subscribers": 738575, "created_utc": 1710428426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I only recently found out that this feature of Instaloader doesn't work anymore, and I'm looking for some help to fix it/ an alternative tool.\n\nI need to download Instagram Posts filtered by date and hashtag. The problem with instaloader is the following:\n\nI'm attempting to download all posts associated with a particular hashtag that were posted within a specified timeframe. I've followed the instructions provided, and after troubleshooting, it appears that Instaloader is unable to locate any posts associated with the hashtag or doesn't have access to them. I've experimented with multiple hashtags and timeframes, including popular hashtags like #gender and broader time periods, yet the issue persists.\n\nUpon running the code, the first line displays: 'JSON Query to explore/tags/#gendern/: 404 Not Found.'\n\nI found 2 github posts on the subject, one is[ mine](https://github.com/instaloader/instaloader/issues/2206), the other is from [December 2023 and is more detailed](https://github.com/instaloader/instaloader/issues/2144), but about the same issue.\n\nAny ideas are greatly appreciated!", "author_fullname": "t2_115ydi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Instaloader fix/ alternative way to scrape instagram posts filtered by hashtag", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beh7yo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710408696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I only recently found out that this feature of Instaloader doesn&amp;#39;t work anymore, and I&amp;#39;m looking for some help to fix it/ an alternative tool.&lt;/p&gt;\n\n&lt;p&gt;I need to download Instagram Posts filtered by date and hashtag. The problem with instaloader is the following:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m attempting to download all posts associated with a particular hashtag that were posted within a specified timeframe. I&amp;#39;ve followed the instructions provided, and after troubleshooting, it appears that Instaloader is unable to locate any posts associated with the hashtag or doesn&amp;#39;t have access to them. I&amp;#39;ve experimented with multiple hashtags and timeframes, including popular hashtags like #gender and broader time periods, yet the issue persists.&lt;/p&gt;\n\n&lt;p&gt;Upon running the code, the first line displays: &amp;#39;JSON Query to explore/tags/#gendern/: 404 Not Found.&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;I found 2 github posts on the subject, one is&lt;a href=\"https://github.com/instaloader/instaloader/issues/2206\"&gt; mine&lt;/a&gt;, the other is from &lt;a href=\"https://github.com/instaloader/instaloader/issues/2144\"&gt;December 2023 and is more detailed&lt;/a&gt;, but about the same issue.&lt;/p&gt;\n\n&lt;p&gt;Any ideas are greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?auto=webp&amp;s=33723d7782ae0377d417d6a441e009bb8821b486", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=abd6c3bfd8a00ac8880540f4e36ef33e373520f5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4455a3d7b30c695e68a4e8738e90cdb87e35b5c0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=914aa6cdda38f8630ccf6e3e7537dc57d46ee8ec", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c0ae0ce01a6b3fa69bfa3e97a188905ec4d5f7b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0206ccffdee2cc6baffe41e0d263ac4dd858aa6e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e065dec59a9e7c90004066f1e2f8caf63f514c4", "width": 1080, "height": 540}], "variants": {}, "id": "H_2BagLwxdXZAI1VYTu2eTjuucPzNqKizgD1e6kDigw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1beh7yo", "is_robot_indexable": true, "report_reasons": null, "author": "xchernyy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beh7yo/looking_for_instaloader_fix_alternative_way_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beh7yo/looking_for_instaloader_fix_alternative_way_to/", "subreddit_subscribers": 738575, "created_utc": 1710408696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This might be a stupid question and if so I apologize. I've tried googling it but can't seem to find the answer. My understanding is that in Windows 10 a long format identifies bad sectors. Isn't that essentially what a SMART long test tests for as well? What is the difference, any why is a SMART test preferable?", "author_fullname": "t2_7g06zcjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not sure of the difference between a long SMART test and a long format.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be5ylr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710371506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This might be a stupid question and if so I apologize. I&amp;#39;ve tried googling it but can&amp;#39;t seem to find the answer. My understanding is that in Windows 10 a long format identifies bad sectors. Isn&amp;#39;t that essentially what a SMART long test tests for as well? What is the difference, any why is a SMART test preferable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be5ylr", "is_robot_indexable": true, "report_reasons": null, "author": "ZealousidealAd9428", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be5ylr/not_sure_of_the_difference_between_a_long_smart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be5ylr/not_sure_of_the_difference_between_a_long_smart/", "subreddit_subscribers": 738575, "created_utc": 1710371506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello everyone, I need help with Synology Photos ? Why it's so slow to view a video with Quickconnect or OpenVPN ? I don't understand, I have 5G for my cellphone and 1 Gbps Fiber symetric (1 Gbps down/1 Gbps up). Synology Photos is fine on my LAN, but when I'm away, it's so slow, it's not working at all with videos only. Any ideas to fix it ? Thanks for your suggestions !!!! ", "author_fullname": "t2_p67lkkz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Synology photos expert here to find a fix (slow videos when outside of my LAN)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be5ig8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710370407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I need help with Synology Photos ? Why it&amp;#39;s so slow to view a video with Quickconnect or OpenVPN ? I don&amp;#39;t understand, I have 5G for my cellphone and 1 Gbps Fiber symetric (1 Gbps down/1 Gbps up). Synology Photos is fine on my LAN, but when I&amp;#39;m away, it&amp;#39;s so slow, it&amp;#39;s not working at all with videos only. Any ideas to fix it ? Thanks for your suggestions !!!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be5ig8", "is_robot_indexable": true, "report_reasons": null, "author": "d2racing911", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be5ig8/any_synology_photos_expert_here_to_find_a_fix/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be5ig8/any_synology_photos_expert_here_to_find_a_fix/", "subreddit_subscribers": 738575, "created_utc": 1710370407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " The Spotify Charts site (charts.spotify.com)  has the option to download the currently viewed chart as a CSV, which  contains a bunch of useful information (total days/weeks on chart,  position last week, etc)/ Does anyone know if there's a way to  scrape/bulk-download the CSV's for different charts?    ", "author_fullname": "t2_ewdcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping Spotify Charts CSV s", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzth9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710356811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Spotify Charts site (charts.spotify.com)  has the option to download the currently viewed chart as a CSV, which  contains a bunch of useful information (total days/weeks on chart,  position last week, etc)/ Does anyone know if there&amp;#39;s a way to  scrape/bulk-download the CSV&amp;#39;s for different charts?    &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdzth9", "is_robot_indexable": true, "report_reasons": null, "author": "Penguintim", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdzth9/scraping_spotify_charts_csv_s/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdzth9/scraping_spotify_charts_csv_s/", "subreddit_subscribers": 738575, "created_utc": 1710356811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Especially if you are apart of one, do you find value in being a member?\n\nOr are they resume boosters, with occasional sideline benefits?", "author_fullname": "t2_aqgp2v9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is joining a professional archive association worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdxbhj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710351000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Especially if you are apart of one, do you find value in being a member?&lt;/p&gt;\n\n&lt;p&gt;Or are they resume boosters, with occasional sideline benefits?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdxbhj", "is_robot_indexable": true, "report_reasons": null, "author": "violet-doggo-2019", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdxbhj/is_joining_a_professional_archive_association/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdxbhj/is_joining_a_professional_archive_association/", "subreddit_subscribers": 738575, "created_utc": 1710351000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "EDIT: Upgraded CPU to a used i5-9500 with iGPU, can remove GPU and save power+faster speeds\n\nI want to add more drives to my unraid server but encountered a problem, i cant add a LSI SAS card because the large PCle slot is being used by the GPU for my Plex transcoding and offloading from the CPU.  Setup:\n\n* Fractal Desing Node 304 (Mini ITX or M-ATX)\n* Asrock M-ATX H310 DVS\n* CPU   i3-9100F\n* GPU NVIDIA Quadro p400\n* x1 PCle Slot SATA expansion card for +4 SATA ports\n\nWhat should i change to add an LSI SAS  x8 PCle Card?\n\nChange my motherboard for two x16 PCle slots?\n\nUse my CPU for transcoding as it has intel quick sync and change the GPU slot for the new LSI  SAS card?\n\ni did not anticipate this hardware limitation as i already have x2 Cache drives, x1 Parity and x5 Disks in the array but planning on adding a second parity and 3 more drives to the array.", "author_fullname": "t2_66ale9u9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hardware Dilemma", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdtr3p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710429947.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710342401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT: Upgraded CPU to a used i5-9500 with iGPU, can remove GPU and save power+faster speeds&lt;/p&gt;\n\n&lt;p&gt;I want to add more drives to my unraid server but encountered a problem, i cant add a LSI SAS card because the large PCle slot is being used by the GPU for my Plex transcoding and offloading from the CPU.  Setup:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fractal Desing Node 304 (Mini ITX or M-ATX)&lt;/li&gt;\n&lt;li&gt;Asrock M-ATX H310 DVS&lt;/li&gt;\n&lt;li&gt;CPU   i3-9100F&lt;/li&gt;\n&lt;li&gt;GPU NVIDIA Quadro p400&lt;/li&gt;\n&lt;li&gt;x1 PCle Slot SATA expansion card for +4 SATA ports&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What should i change to add an LSI SAS  x8 PCle Card?&lt;/p&gt;\n\n&lt;p&gt;Change my motherboard for two x16 PCle slots?&lt;/p&gt;\n\n&lt;p&gt;Use my CPU for transcoding as it has intel quick sync and change the GPU slot for the new LSI  SAS card?&lt;/p&gt;\n\n&lt;p&gt;i did not anticipate this hardware limitation as i already have x2 Cache drives, x1 Parity and x5 Disks in the array but planning on adding a second parity and 3 more drives to the array.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdtr3p", "is_robot_indexable": true, "report_reasons": null, "author": "iweputo", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdtr3p/hardware_dilemma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdtr3p/hardware_dilemma/", "subreddit_subscribers": 738575, "created_utc": 1710342401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone get the email where PolarBackup is reneging on their lifetime 5TB offer?  ", "author_fullname": "t2_32rhr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polar Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1belz0b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710425145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone get the email where PolarBackup is reneging on their lifetime 5TB offer?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1belz0b", "is_robot_indexable": true, "report_reasons": null, "author": "tudorwhiteley", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1belz0b/polar_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1belz0b/polar_backup/", "subreddit_subscribers": 738575, "created_utc": 1710425145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As a teenager I was super careless and put a ton of (now) important and memorable photos and videos on a ZFS RAID-0 array. Naturally, it failed and I'm only just now getting around to looking at it (4 years later).\n\nI still have all 3 drives. I ran photorec on the remaining working 2 drives which proved that the data I was after was still there. It took about 3 days total but I've managed to recover some small photos, screenshots, mp3s and flacs just from those two drives. However the failed drive refuses to even be recognized by the operating system. Some motherboards I've tried it on refuse to complete the POST as it knows that the drive has failed and will display an error saying to remove the drive and reboot. It makes a horrible clicking sound when the computer is powered on.\n\nHas anyone had success with a data recovery firm to recover data from a drive that's failed in such a way? (I'm not going to try recover the data myself since I don't trust myself with that). I've recovered a significant amount of data, more than I could ever hope for free. But obviously large files (which are the important ones) are partially or entirely corrupt at the moment.", "author_fullname": "t2_dfl078nkf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Likelihood of data recovery from a clicking hard drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bej22p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710415958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a teenager I was super careless and put a ton of (now) important and memorable photos and videos on a ZFS RAID-0 array. Naturally, it failed and I&amp;#39;m only just now getting around to looking at it (4 years later).&lt;/p&gt;\n\n&lt;p&gt;I still have all 3 drives. I ran photorec on the remaining working 2 drives which proved that the data I was after was still there. It took about 3 days total but I&amp;#39;ve managed to recover some small photos, screenshots, mp3s and flacs just from those two drives. However the failed drive refuses to even be recognized by the operating system. Some motherboards I&amp;#39;ve tried it on refuse to complete the POST as it knows that the drive has failed and will display an error saying to remove the drive and reboot. It makes a horrible clicking sound when the computer is powered on.&lt;/p&gt;\n\n&lt;p&gt;Has anyone had success with a data recovery firm to recover data from a drive that&amp;#39;s failed in such a way? (I&amp;#39;m not going to try recover the data myself since I don&amp;#39;t trust myself with that). I&amp;#39;ve recovered a significant amount of data, more than I could ever hope for free. But obviously large files (which are the important ones) are partially or entirely corrupt at the moment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "4TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bej22p", "is_robot_indexable": true, "report_reasons": null, "author": "nbtm_sh", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bej22p/likelihood_of_data_recovery_from_a_clicking_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bej22p/likelihood_of_data_recovery_from_a_clicking_hard/", "subreddit_subscribers": 738575, "created_utc": 1710415958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry that this is long, I wanted to cover my bases and give/get the clearest picture possible, while getting unanswered questions brought to light and suggestions in one fell swoop.\n\n**TL;DR: What should I buy guys, WD or Seagate?**\n\n**I'm kidding. I want to buy more storage but I feel like I need to pivot mediums because I'm inundated with external hard drives. What medium or structure I should go for based on the nuance of my hardware situation** ***while also looking to the future based on my current storage conditions and usage/data trends. i.e. maybe I wait on getting into LTO tape because I'm in the early stages of \"archiving\" my work, and most of my files need to be accessed. Maybe I don't build a NAS system because I don't really use network capabilities, maybe I don't do any of this and keep buying external hard drives because I hate 3rd party software, etc.***\n\n**I also want to know where my thought processes about the mediums are incorrect because frankly, I\u2019m very overwhelmed and there\u2019s lots of information, so this write-up is helping me parse it as well. Answer whatever you know, I\u2019m happy to just get suggestions or anecdotal tidbits on any of the subjects I\u2019m describing. Thank you in advance. Sorry again this is so fucking long.**\n\n# Preamble:\n\nAt the moment, I have \\~40TB of external drives not including copies/backups and they're at capacity and growing rapidly. A majority are full of video files that I access but as I start to accumulate stuff that I don't regularly need to access, I'm thinking about more long term storage -- **if it's not a hassle**. The rest is media, less important and redownloadable, but needs space.\n\nI have *24 TB* in External Desktop Drives &amp; 15TB in External Portable Drives:\n\n***2x 10TB WD element desktop drives***\n\n***1x 4TB WD element desktop drive***: my **oldest** Desktop drive, I bought it in 2014 for like $100.\n\n***~~1x 5TB WD Element portable drive~~*** \u2014 It\u2019s actually emptying as we speak because it\u2019s slooooow (&lt;1Mbps write speed &amp; has 200 bad sectors) \u2014I\u2019m transferring 4TB of data from it and it\u2019s taken me 5 days so far &amp; \u201cover a day\u201d to go and that\u2019s after long formatting it back to NTFS. So I think it\u2019s *on it\u2019s way out* and needs to be replaced LOL\n\n***1x 5TB LaCie***\n\n***1x 4TB Seagate drive***\n\n***1x 1TB WD passport:*** This may make you NAS and Plex users laugh or cringe: This drive is just media gathered from my desktop drives that I plug into my TV's USB manually.\n\nSide note/transition: I\u2019ve read about \u201cNAS\u201d systems and \u201cPlex\u201d but I don\u2019t know if it\u2019s for me. As far as I can tell, NAS just gives you access to the files from any device on your home network. That\u2019s cool, but one of my PCs offers to do that and I never really use it. My PC does pop up on my SMART TV Hub\u2026..I don't really know why, but is that like the same thing that would happen if I built a NAS? That leads to my first potential option.\n\n# Option 1: Repurpose my PC and buy NAS HDDs\n\nWriting this out made me think, my PC is basically a mutt of PC parts (which I\u2019m looking to upgrade anyway, and if I wanted to do that I\u2019d need to replace my Gpu, Cpu, Ram, etc because NOTHING today is compatible with my PC but I didn't want to upgrade and waste the parts). The wiki says you need a GPU, RAM, etc. to build a NAS. So it sounds like a PC but one that\u2019s not specd for \u201cgaming\u201d like mine was.\n\nIf I were to upgrade, I\u2019d have extra parts for another whole PC. Would repurposing these PC parts be kosher if I were to just buy NAS HDDs? Or am I oversimplifying it and need other hardware? \n\nHere are the part specs:\n\n    CPU: Intel Core i7-8700K\n    GPU: Nvidia GTX 960\n    RAM: Corsair Vengeance LPX DDR4 3000 C15 2x8GB\n    MBD: Asus ROG STRIX Z370-E\n    Boot Media would be SATA\n\nThe wiki says you need parts that match and mine definitely do but I\u2019m kind of lost on hardware requirements or if certain hardware prevents bottlenecking.\n\nNAS doesn\u2019t *really* seem like it\u2019s right for me because I don\u2019t use the network feature, mainly because I don\u2019t watch 90% of my media on a weekly basis, so transferring it onto a hard drive from another hard drive when I want to is easy. Plus not all my media is playable on my TV so I have to be selective and not all my TV\u2019s have network capabilities, but they all have USB ports. **Mind you,** this isn't even a huge inconvenience for me, but maybe there's more to it so I'm just trying to make sure I'm on the right track before ruling NAS out of my current needs, especially if it turns out to be the more robust than I'm imagining.\n\n# Option 2: Buy JBOD Enclosure and HDDs\n\nWhat I really like about NAS is the adjustable capacity because you can pretty much just swap the drives out, so I\u2019ve read about 4 bay (or more) JBOD enclosures. I like this idea, because it seems straightforward, and the capacity is adjustable based on the disks you put in, but I feel like I\u2019m missing something.\n\nIf I were to take this route, I could just shuck my WD desktop drives, drop the HDDs into the JBOD, and it\u2019d basically be the same thing? As in, my windows file manager would still see them all as the separate drives they are, but they\u2019d be pop up in file manager through one cable versus the three ports I have occupied now. Right?\n\nI like this idea and I lean towards it more because it checks my boxes for adjustable capacity and ease of use, but I just feel like I\u2019m missing something, like some sort of dreaded software that'll be needed to access my drives....\n\n# Option 3: Buy 22 TB External Hard Drive for media backup &amp; LTO Tape (gen 5 or 6) for important stuff\n\nVery unfamiliar with LTO but I think I want to learn and adopt it in the future for my archived work. All you fear mongers here make me nervous that my HDDs are going to all keel over any minute LOL. Especially since most of my HDDs are reaching the 10 year mark.\n\nI see a lot of info about it and realistically learned about it like a month ago, but I still am not understanding what to expect with this medium so I\u2019d appreciate clarity. \n\nWhat I gather you need is:\n\n1. a tape drive \u2014 which I see is cheaper for LTO 5/6 vs the holy-price-gouging of LTO-7+\n\n2. Tapes \u2014 1.5/3TB capacity doesn\u2019t excite me, but I love the idea of splitting and organizing by physical tapes.\n\n3. but now Software? \u2014 I don\u2019t get this part at all. I hear that you need to pay for a software, which turns me off of this. Is there no way to just plug the tape drive into your computer and be able to access your files like an external drive?\n\nI don't like the idea that I can\u2019t access my files unless I have a software from some specific company. I\u2019m not pro-cloud for similar reasons, so if that's a barrier for LTO tape then I'll probably pass on that for Option 3.\n\nMaybe it\u2019s irrational because technically you're almost always at the whim of some company's software. Finder and File Manager are like \u201cMac/windows software\u201d *I guess* but I\u2019ve been using those systems my whole life AND you can force stuff with command line so it feels less \"gated.\"\n\nThe 22TB would just be for all the visual media I have to be in one place because its split between drives. None of that media is stuff I'd want \"protected\" long term because it's easily retrievable.\n\n&amp;#x200B;\n\nSo\u2026. Again sorry that this is long, I seriously appreciate you taking time to read and possibly give input. I saw the recent post about low-effort posts and I was thinking my question was too dumb to ask here but I've just been absolutely STUMPED for probably weeks on what may be an easy, yet cost effective, and somewhat future-proof, scenario. Even though I know all this stuff can all be a gamble sometimes LOL\n\n&amp;#x200B;", "author_fullname": "t2_42mqql5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me understand my options for consolidating my harem of external hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzpa9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710356526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry that this is long, I wanted to cover my bases and give/get the clearest picture possible, while getting unanswered questions brought to light and suggestions in one fell swoop.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: What should I buy guys, WD or Seagate?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;m kidding. I want to buy more storage but I feel like I need to pivot mediums because I&amp;#39;m inundated with external hard drives. What medium or structure I should go for based on the nuance of my hardware situation&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;while also looking to the future based on my current storage conditions and usage/data trends. i.e. maybe I wait on getting into LTO tape because I&amp;#39;m in the early stages of &amp;quot;archiving&amp;quot; my work, and most of my files need to be accessed. Maybe I don&amp;#39;t build a NAS system because I don&amp;#39;t really use network capabilities, maybe I don&amp;#39;t do any of this and keep buying external hard drives because I hate 3rd party software, etc.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I also want to know where my thought processes about the mediums are incorrect because frankly, I\u2019m very overwhelmed and there\u2019s lots of information, so this write-up is helping me parse it as well. Answer whatever you know, I\u2019m happy to just get suggestions or anecdotal tidbits on any of the subjects I\u2019m describing. Thank you in advance. Sorry again this is so fucking long.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Preamble:&lt;/h1&gt;\n\n&lt;p&gt;At the moment, I have ~40TB of external drives not including copies/backups and they&amp;#39;re at capacity and growing rapidly. A majority are full of video files that I access but as I start to accumulate stuff that I don&amp;#39;t regularly need to access, I&amp;#39;m thinking about more long term storage -- &lt;strong&gt;if it&amp;#39;s not a hassle&lt;/strong&gt;. The rest is media, less important and redownloadable, but needs space.&lt;/p&gt;\n\n&lt;p&gt;I have &lt;em&gt;24 TB&lt;/em&gt; in External Desktop Drives &amp;amp; 15TB in External Portable Drives:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;2x 10TB WD element desktop drives&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1x 4TB WD element desktop drive&lt;/em&gt;&lt;/strong&gt;: my &lt;strong&gt;oldest&lt;/strong&gt; Desktop drive, I bought it in 2014 for like $100.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;del&gt;1x 5TB WD Element portable drive&lt;/del&gt;&lt;/em&gt;&lt;/strong&gt; \u2014 It\u2019s actually emptying as we speak because it\u2019s slooooow (&amp;lt;1Mbps write speed &amp;amp; has 200 bad sectors) \u2014I\u2019m transferring 4TB of data from it and it\u2019s taken me 5 days so far &amp;amp; \u201cover a day\u201d to go and that\u2019s after long formatting it back to NTFS. So I think it\u2019s &lt;em&gt;on it\u2019s way out&lt;/em&gt; and needs to be replaced LOL&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1x 5TB LaCie&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1x 4TB Seagate drive&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1x 1TB WD passport:&lt;/em&gt;&lt;/strong&gt; This may make you NAS and Plex users laugh or cringe: This drive is just media gathered from my desktop drives that I plug into my TV&amp;#39;s USB manually.&lt;/p&gt;\n\n&lt;p&gt;Side note/transition: I\u2019ve read about \u201cNAS\u201d systems and \u201cPlex\u201d but I don\u2019t know if it\u2019s for me. As far as I can tell, NAS just gives you access to the files from any device on your home network. That\u2019s cool, but one of my PCs offers to do that and I never really use it. My PC does pop up on my SMART TV Hub\u2026..I don&amp;#39;t really know why, but is that like the same thing that would happen if I built a NAS? That leads to my first potential option.&lt;/p&gt;\n\n&lt;h1&gt;Option 1: Repurpose my PC and buy NAS HDDs&lt;/h1&gt;\n\n&lt;p&gt;Writing this out made me think, my PC is basically a mutt of PC parts (which I\u2019m looking to upgrade anyway, and if I wanted to do that I\u2019d need to replace my Gpu, Cpu, Ram, etc because NOTHING today is compatible with my PC but I didn&amp;#39;t want to upgrade and waste the parts). The wiki says you need a GPU, RAM, etc. to build a NAS. So it sounds like a PC but one that\u2019s not specd for \u201cgaming\u201d like mine was.&lt;/p&gt;\n\n&lt;p&gt;If I were to upgrade, I\u2019d have extra parts for another whole PC. Would repurposing these PC parts be kosher if I were to just buy NAS HDDs? Or am I oversimplifying it and need other hardware? &lt;/p&gt;\n\n&lt;p&gt;Here are the part specs:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CPU: Intel Core i7-8700K\nGPU: Nvidia GTX 960\nRAM: Corsair Vengeance LPX DDR4 3000 C15 2x8GB\nMBD: Asus ROG STRIX Z370-E\nBoot Media would be SATA\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The wiki says you need parts that match and mine definitely do but I\u2019m kind of lost on hardware requirements or if certain hardware prevents bottlenecking.&lt;/p&gt;\n\n&lt;p&gt;NAS doesn\u2019t &lt;em&gt;really&lt;/em&gt; seem like it\u2019s right for me because I don\u2019t use the network feature, mainly because I don\u2019t watch 90% of my media on a weekly basis, so transferring it onto a hard drive from another hard drive when I want to is easy. Plus not all my media is playable on my TV so I have to be selective and not all my TV\u2019s have network capabilities, but they all have USB ports. &lt;strong&gt;Mind you,&lt;/strong&gt; this isn&amp;#39;t even a huge inconvenience for me, but maybe there&amp;#39;s more to it so I&amp;#39;m just trying to make sure I&amp;#39;m on the right track before ruling NAS out of my current needs, especially if it turns out to be the more robust than I&amp;#39;m imagining.&lt;/p&gt;\n\n&lt;h1&gt;Option 2: Buy JBOD Enclosure and HDDs&lt;/h1&gt;\n\n&lt;p&gt;What I really like about NAS is the adjustable capacity because you can pretty much just swap the drives out, so I\u2019ve read about 4 bay (or more) JBOD enclosures. I like this idea, because it seems straightforward, and the capacity is adjustable based on the disks you put in, but I feel like I\u2019m missing something.&lt;/p&gt;\n\n&lt;p&gt;If I were to take this route, I could just shuck my WD desktop drives, drop the HDDs into the JBOD, and it\u2019d basically be the same thing? As in, my windows file manager would still see them all as the separate drives they are, but they\u2019d be pop up in file manager through one cable versus the three ports I have occupied now. Right?&lt;/p&gt;\n\n&lt;p&gt;I like this idea and I lean towards it more because it checks my boxes for adjustable capacity and ease of use, but I just feel like I\u2019m missing something, like some sort of dreaded software that&amp;#39;ll be needed to access my drives....&lt;/p&gt;\n\n&lt;h1&gt;Option 3: Buy 22 TB External Hard Drive for media backup &amp;amp; LTO Tape (gen 5 or 6) for important stuff&lt;/h1&gt;\n\n&lt;p&gt;Very unfamiliar with LTO but I think I want to learn and adopt it in the future for my archived work. All you fear mongers here make me nervous that my HDDs are going to all keel over any minute LOL. Especially since most of my HDDs are reaching the 10 year mark.&lt;/p&gt;\n\n&lt;p&gt;I see a lot of info about it and realistically learned about it like a month ago, but I still am not understanding what to expect with this medium so I\u2019d appreciate clarity. &lt;/p&gt;\n\n&lt;p&gt;What I gather you need is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;a tape drive \u2014 which I see is cheaper for LTO 5/6 vs the holy-price-gouging of LTO-7+&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Tapes \u2014 1.5/3TB capacity doesn\u2019t excite me, but I love the idea of splitting and organizing by physical tapes.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;but now Software? \u2014 I don\u2019t get this part at all. I hear that you need to pay for a software, which turns me off of this. Is there no way to just plug the tape drive into your computer and be able to access your files like an external drive?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I don&amp;#39;t like the idea that I can\u2019t access my files unless I have a software from some specific company. I\u2019m not pro-cloud for similar reasons, so if that&amp;#39;s a barrier for LTO tape then I&amp;#39;ll probably pass on that for Option 3.&lt;/p&gt;\n\n&lt;p&gt;Maybe it\u2019s irrational because technically you&amp;#39;re almost always at the whim of some company&amp;#39;s software. Finder and File Manager are like \u201cMac/windows software\u201d &lt;em&gt;I guess&lt;/em&gt; but I\u2019ve been using those systems my whole life AND you can force stuff with command line so it feels less &amp;quot;gated.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The 22TB would just be for all the visual media I have to be in one place because its split between drives. None of that media is stuff I&amp;#39;d want &amp;quot;protected&amp;quot; long term because it&amp;#39;s easily retrievable.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So\u2026. Again sorry that this is long, I seriously appreciate you taking time to read and possibly give input. I saw the recent post about low-effort posts and I was thinking my question was too dumb to ask here but I&amp;#39;ve just been absolutely STUMPED for probably weeks on what may be an easy, yet cost effective, and somewhat future-proof, scenario. Even though I know all this stuff can all be a gamble sometimes LOL&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdzpa9", "is_robot_indexable": true, "report_reasons": null, "author": "long-ryde", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdzpa9/help_me_understand_my_options_for_consolidating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdzpa9/help_me_understand_my_options_for_consolidating/", "subreddit_subscribers": 738575, "created_utc": 1710356526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Thank you for your input.", "author_fullname": "t2_3luaynwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "As Data Hoarders which heatsinks do you guys use/recommend for double sided M.2 NVMe SSDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beblhc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710386995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for your input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1beblhc", "is_robot_indexable": true, "report_reasons": null, "author": "LightDarkCloud", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beblhc/as_data_hoarders_which_heatsinks_do_you_guys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beblhc/as_data_hoarders_which_heatsinks_do_you_guys/", "subreddit_subscribers": 738575, "created_utc": 1710386995.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}