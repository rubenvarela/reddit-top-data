{"kind": "Listing", "data": {"after": "t3_1be5ylr", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\nthis is my first post here, so I hope I'm not breaking any rules here. I'd like to point the community's attention towards the following issue.\n\nIn an unprecedented act of digital vandalism, Nintendo decided to rm all levels from Super Mario Maker 1, because they hate people who like their games. That's over 100 million creations from regular users like you and I. This is obviously a huge loss to gaming history and to gamers in general.\n\nSome software exists to back this up, but not fully, there's no easy way to load the backups to check them, and there is no recent dump.\n\nThere's [this tool by PretendoNetwork](https://github.com/PretendoNetwork/archival-tools/tree/master/super-mario-maker) but that's ways off from actually having the levels playable in an emulator or on a console. Worst of all there's no way to tell if it currently works because there's no tool for *loading* levels dumped this way.\n\nThere's also [this tool by HerobrineTV](https://github.com/HerobrineTV/SMM1-Level-Downloader) and a post [here](https://archive.org/details/super_mario_maker_courses_202105) that explains what's involved in getting dumps done with their tool to register inside a WiiU. That post is on a dump that also contains a bunch of courses, as I understand.\n\nI believe HerobrineTV's and PretendoNetwork's tools both capture different kinds of data *and* different kinds of metadata.\n\nSomeone actually has to run those tools and get all that stuff. It requires a working nintendo login - I don't have that right now, my Wii U is in storage in one of a million boxes. I'd have started myself already.\n\nThere's a partial dump of some sort that's [on archive](https://archive.org/download/smm_levels), but it's from early 2022 - so a lot of levels are going to be missing. The author of that dump stopped at close to 70 million levels, but that's not all. Note on this dump: the first 10 000 levels are dumped in some other format that does not actually include the level data; further levels seem to contain that level data, but bear in mind that the .torrent file available on that archive page does not include those dumps, so you'll have to download all those 6-12 GB files via http(s).\n\nThat dump also doesn't seem to include level screenshots, and I believe pretendo's tool doesn't get them. Also, that dump was made using an older version of the tool, which exports less metadata.\n\nGiven the size of that partial dump (around 600-800 GB), my guess is a full dump would be on the order of 1-2TB. It's not a LOT lot, but it's quite a lot, so work has to be done in parallel by multiple people to ensure this goal is met before end of March. \n\nHowever it bears keeping in mind that the older dumps will contain levels that have since been deleted. So they are still worthwhile and worth incorporating.\n\nRuTr has a release of SMM2 Switch with a loader tool, but I don't know how different the loader tool would have to be made to make it load levels into SMM1. It looks like everyone just focused on SMM2 sideloading, so SMM1 needs help with software dev to even ensure the backups work at all.\n\nA library tool of some sort would be useful, too. As would transforming all that json output from PretendoNetwork's tool to an sqlite database with a fixed schema, so it can be queried (eg to find all levels by one creator).\n\nAs of right now, there is no fully useable dump available anywhere I looked, and no loader tool seems to be present.\n\nThe current state is:\n\n- two dumper tools, one by HerobrineTV, one by PretendoNetwork, which seem to capture different data and different metadata\n\n- two partial dumps, which may contain already deleted courses and associated metadata\n\n- a lot of missing levels (latest dump is from mid 2022, so the levels might have grown by 2x since then).\n\n- an [unpacking tool](http://wiibrew.org/wiki/ASH_Extractor) for the binary files to turn them into files that have to be on the WiiU's system to make them loadable\n\n- a manual method of loading the courses onto a WiiU (or into an emulator)\n\n- HerobrineTV seems to be working on a GUI of some sort, but it doesn't seem to be public so far\n\n- no way of uploading all those files and related metadata onto archive\n\n- no convenient torrent with all the dumped data\n\nIt is an understatement that anyone who helps save this data is an absolute hero of a human being, so I hope to spur some attention to this here.", "author_fullname": "t2_5wpjh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Super Mario Maker servers are going away very soon, and the 1-2TB of user submitted levels need archiving", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bejk7e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 320, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 320, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710417767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nthis is my first post here, so I hope I&amp;#39;m not breaking any rules here. I&amp;#39;d like to point the community&amp;#39;s attention towards the following issue.&lt;/p&gt;\n\n&lt;p&gt;In an unprecedented act of digital vandalism, Nintendo decided to rm all levels from Super Mario Maker 1, because they hate people who like their games. That&amp;#39;s over 100 million creations from regular users like you and I. This is obviously a huge loss to gaming history and to gamers in general.&lt;/p&gt;\n\n&lt;p&gt;Some software exists to back this up, but not fully, there&amp;#39;s no easy way to load the backups to check them, and there is no recent dump.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s &lt;a href=\"https://github.com/PretendoNetwork/archival-tools/tree/master/super-mario-maker\"&gt;this tool by PretendoNetwork&lt;/a&gt; but that&amp;#39;s ways off from actually having the levels playable in an emulator or on a console. Worst of all there&amp;#39;s no way to tell if it currently works because there&amp;#39;s no tool for &lt;em&gt;loading&lt;/em&gt; levels dumped this way.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also &lt;a href=\"https://github.com/HerobrineTV/SMM1-Level-Downloader\"&gt;this tool by HerobrineTV&lt;/a&gt; and a post &lt;a href=\"https://archive.org/details/super_mario_maker_courses_202105\"&gt;here&lt;/a&gt; that explains what&amp;#39;s involved in getting dumps done with their tool to register inside a WiiU. That post is on a dump that also contains a bunch of courses, as I understand.&lt;/p&gt;\n\n&lt;p&gt;I believe HerobrineTV&amp;#39;s and PretendoNetwork&amp;#39;s tools both capture different kinds of data &lt;em&gt;and&lt;/em&gt; different kinds of metadata.&lt;/p&gt;\n\n&lt;p&gt;Someone actually has to run those tools and get all that stuff. It requires a working nintendo login - I don&amp;#39;t have that right now, my Wii U is in storage in one of a million boxes. I&amp;#39;d have started myself already.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a partial dump of some sort that&amp;#39;s &lt;a href=\"https://archive.org/download/smm_levels\"&gt;on archive&lt;/a&gt;, but it&amp;#39;s from early 2022 - so a lot of levels are going to be missing. The author of that dump stopped at close to 70 million levels, but that&amp;#39;s not all. Note on this dump: the first 10 000 levels are dumped in some other format that does not actually include the level data; further levels seem to contain that level data, but bear in mind that the .torrent file available on that archive page does not include those dumps, so you&amp;#39;ll have to download all those 6-12 GB files via http(s).&lt;/p&gt;\n\n&lt;p&gt;That dump also doesn&amp;#39;t seem to include level screenshots, and I believe pretendo&amp;#39;s tool doesn&amp;#39;t get them. Also, that dump was made using an older version of the tool, which exports less metadata.&lt;/p&gt;\n\n&lt;p&gt;Given the size of that partial dump (around 600-800 GB), my guess is a full dump would be on the order of 1-2TB. It&amp;#39;s not a LOT lot, but it&amp;#39;s quite a lot, so work has to be done in parallel by multiple people to ensure this goal is met before end of March. &lt;/p&gt;\n\n&lt;p&gt;However it bears keeping in mind that the older dumps will contain levels that have since been deleted. So they are still worthwhile and worth incorporating.&lt;/p&gt;\n\n&lt;p&gt;RuTr has a release of SMM2 Switch with a loader tool, but I don&amp;#39;t know how different the loader tool would have to be made to make it load levels into SMM1. It looks like everyone just focused on SMM2 sideloading, so SMM1 needs help with software dev to even ensure the backups work at all.&lt;/p&gt;\n\n&lt;p&gt;A library tool of some sort would be useful, too. As would transforming all that json output from PretendoNetwork&amp;#39;s tool to an sqlite database with a fixed schema, so it can be queried (eg to find all levels by one creator).&lt;/p&gt;\n\n&lt;p&gt;As of right now, there is no fully useable dump available anywhere I looked, and no loader tool seems to be present.&lt;/p&gt;\n\n&lt;p&gt;The current state is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;two dumper tools, one by HerobrineTV, one by PretendoNetwork, which seem to capture different data and different metadata&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;two partial dumps, which may contain already deleted courses and associated metadata&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;a lot of missing levels (latest dump is from mid 2022, so the levels might have grown by 2x since then).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;an &lt;a href=\"http://wiibrew.org/wiki/ASH_Extractor\"&gt;unpacking tool&lt;/a&gt; for the binary files to turn them into files that have to be on the WiiU&amp;#39;s system to make them loadable&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;a manual method of loading the courses onto a WiiU (or into an emulator)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;HerobrineTV seems to be working on a GUI of some sort, but it doesn&amp;#39;t seem to be public so far&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;no way of uploading all those files and related metadata onto archive&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;no convenient torrent with all the dumped data&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It is an understatement that anyone who helps save this data is an absolute hero of a human being, so I hope to spur some attention to this here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?auto=webp&amp;s=3b84cca0e694973eaf37fe4fb7f21548f34776e5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efcf1a7390ca781365977201e28ef45c237fe60d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3715372d9328807e32326748014da19db5b18d7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=42685486e182e51f9f38c6117ae18f1a910d14ae", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea3202c2906374ec5bae6ba983a976ee0bbce498", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea1ffa0c883610a04b2ec80cb7b7d0e1d872b504", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0HEmHYwY-8WvDOL-ZyYphrf2ZsoUqO51m_tFZQ6RgYI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=901c52e165135fdad9500210f847521b9150cdbf", "width": 1080, "height": 540}], "variants": {}, "id": "iJx7WgxqsvWXnDymvJda-NvEEKUFva8UFLJWX_2Jcc0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1bejk7e", "is_robot_indexable": true, "report_reasons": null, "author": "cheater00", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bejk7e/super_mario_maker_servers_are_going_away_very/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bejk7e/super_mario_maker_servers_are_going_away_very/", "subreddit_subscribers": 738612, "created_utc": 1710417767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ck2fr5mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warner Bros. Discovery Disappears Games People Already Purchased", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2o3l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 244, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 244, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RhtuabX2AZtqmze1q9DPpMCkkIn4ceU3JAAZEoWeI4M.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710363635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techdirt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techdirt.com/2024/03/12/warner-bros-discovery-disappears-games-people-already-purchased/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?auto=webp&amp;s=03a5fde066455710ac1676ca5e55fdf4cff5f177", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd544711a911105c2c84e8778e42492e6627f7ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=581443ef43c416964d464ddd6b5b28eec7b2b77c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=98eca8135ccdbe0daad2aad9f2f21c250d1564ab", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60f612dc25b6693520e6d4a342645939506f3afd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30ce1c1c245620a8b70a4d0a298b39657711ef0e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b24b6b5a4ae88248a5d6b951b789da794832c41", "width": 1080, "height": 567}], "variants": {}, "id": "86PGtE2qmX3coS9Htmb8TUfXMSg2HaYO4Rk8A0YbGow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be2o3l", "is_robot_indexable": true, "report_reasons": null, "author": "AbolishDisney", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1be2o3l/warner_bros_discovery_disappears_games_people/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techdirt.com/2024/03/12/warner-bros-discovery-disappears-games-people-already-purchased/", "subreddit_subscribers": 738612, "created_utc": 1710363635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m trying to get some  Warner Brothers DVDs (old Joan Crawford films) into my Plex server but they are stubbornly refusing to rip all the way. I\u2019ve tried Leawo, MakeMKV and Dumbofab on both Mac and PC. If the apps don\u2019t fail then they only rip about 2/3 of the movie. They won\u2019t even PLAY past that point in VLC. I\u2019ve never seen anything like this before; it\u2019s either some insane copy protection or these discs are just messed up somehow. I think they play fine in a DVD player (I\u2019ll check again tomorrow). Currently trying to rip to .iso just to see if I can get the whole film at least. ", "author_fullname": "t2_rorvibp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone try to rip a DVD, only to have it consistently cut off the last third of the film?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1behbie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710409131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to get some  Warner Brothers DVDs (old Joan Crawford films) into my Plex server but they are stubbornly refusing to rip all the way. I\u2019ve tried Leawo, MakeMKV and Dumbofab on both Mac and PC. If the apps don\u2019t fail then they only rip about 2/3 of the movie. They won\u2019t even PLAY past that point in VLC. I\u2019ve never seen anything like this before; it\u2019s either some insane copy protection or these discs are just messed up somehow. I think they play fine in a DVD player (I\u2019ll check again tomorrow). Currently trying to rip to .iso just to see if I can get the whole film at least. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1behbie", "is_robot_indexable": true, "report_reasons": null, "author": "darwinDMG08", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1behbie/anyone_try_to_rip_a_dvd_only_to_have_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1behbie/anyone_try_to_rip_a_dvd_only_to_have_it/", "subreddit_subscribers": 738612, "created_utc": 1710409131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As part of my desire for a \"better Datahoarder\" \\[no criticism of the moderators by the way\\] I'd also like to suggest the following rule for **any questions** that seek a recommendation for a product or service for eventual hire/purchase/engagement **that is not already going to be reasonably covered** by rule 1 and rule 2.\n\n1. State your budget\n2. State the country where you expect the item to be delivered to\n3. If you are jabbering on about the product, get the product right, so not a \"Seagate Exus A349 18tb\" if you actually mean something else, like a Seagate Exos ....\"\n4. Proof-read your damn question to see if makes vague sense. One \"question\" earlier was more of a life story and several members here couldn't be 100% if it was a question, and if it was rule 1 exists.\n5. and still, use search first based on what you would have written and see if magic happens.\n\nAnnoyed slightly that someone asked for a recommendation today in an area. Got a recommendation. Was unhappy \"not that area\".  They didn't say that area=country name or that area except XYZ.  \n\n\nIt's not bloody difficult with a microsecond of thought and brain engagement. And if it really is a subject that you know zero about and honestly can't Google it, then say what you've tried. I mean, I'm not a car person, so I might have to describe \"My car is making a noise somewhere from behind the plastic panel on the right hand side, a sort of knocking, but before the engine compartment and it only happens when I start it up. A car person might know \\[made up now\\] it's your alternator probably, then I could start searching for repair \\[model name\\] alternator or something.  I think these sort of exclusions are rare though. I'd not write \"my car makes a knocking sound, where to fix it?\" and leave it at that...\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_z85hu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better questions...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzaqm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710355592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As part of my desire for a &amp;quot;better Datahoarder&amp;quot; [no criticism of the moderators by the way] I&amp;#39;d also like to suggest the following rule for &lt;strong&gt;any questions&lt;/strong&gt; that seek a recommendation for a product or service for eventual hire/purchase/engagement &lt;strong&gt;that is not already going to be reasonably covered&lt;/strong&gt; by rule 1 and rule 2.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;State your budget&lt;/li&gt;\n&lt;li&gt;State the country where you expect the item to be delivered to&lt;/li&gt;\n&lt;li&gt;If you are jabbering on about the product, get the product right, so not a &amp;quot;Seagate Exus A349 18tb&amp;quot; if you actually mean something else, like a Seagate Exos ....&amp;quot;&lt;/li&gt;\n&lt;li&gt;Proof-read your damn question to see if makes vague sense. One &amp;quot;question&amp;quot; earlier was more of a life story and several members here couldn&amp;#39;t be 100% if it was a question, and if it was rule 1 exists.&lt;/li&gt;\n&lt;li&gt;and still, use search first based on what you would have written and see if magic happens.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Annoyed slightly that someone asked for a recommendation today in an area. Got a recommendation. Was unhappy &amp;quot;not that area&amp;quot;.  They didn&amp;#39;t say that area=country name or that area except XYZ.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not bloody difficult with a microsecond of thought and brain engagement. And if it really is a subject that you know zero about and honestly can&amp;#39;t Google it, then say what you&amp;#39;ve tried. I mean, I&amp;#39;m not a car person, so I might have to describe &amp;quot;My car is making a noise somewhere from behind the plastic panel on the right hand side, a sort of knocking, but before the engine compartment and it only happens when I start it up. A car person might know [made up now] it&amp;#39;s your alternator probably, then I could start searching for repair [model name] alternator or something.  I think these sort of exclusions are rare though. I&amp;#39;d not write &amp;quot;my car makes a knocking sound, where to fix it?&amp;quot; and leave it at that...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "c170Tb local, c. 300Tb on other servers &amp; a lot in the cloud. ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1bdzaqm", "is_robot_indexable": true, "report_reasons": null, "author": "GodSaveUsFromPettyMo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bdzaqm/better_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdzaqm/better_questions/", "subreddit_subscribers": 738612, "created_utc": 1710355592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m sure many of you are aware of Taringa, the social media platform that\u2019s been a part of Latin American internet culture for years. Sadly, it\u2019s been announced that Taringa will be shutting down soon.\n\nThis site holds a treasure trove of memories and content that reflects a significant era of digital history. Before it goes offline, I wanted to ask if anyone here is planning to archive Taringa, or if there\u2019s already an ongoing effort to do so.", "author_fullname": "t2_364dp83u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring taringa.net", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2q6y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710363763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m sure many of you are aware of Taringa, the social media platform that\u2019s been a part of Latin American internet culture for years. Sadly, it\u2019s been announced that Taringa will be shutting down soon.&lt;/p&gt;\n\n&lt;p&gt;This site holds a treasure trove of memories and content that reflects a significant era of digital history. Before it goes offline, I wanted to ask if anyone here is planning to archive Taringa, or if there\u2019s already an ongoing effort to do so.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1be2q6y", "is_robot_indexable": true, "report_reasons": null, "author": "2muchnet42day", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1be2q6y/mirroring_taringanet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be2q6y/mirroring_taringanet/", "subreddit_subscribers": 738612, "created_utc": 1710363763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I reported several posts (yes, I'm THAT guy!) for Rule #1 and they're quickly gone. Hope this continues as IMO it makes this sub a better experience. ", "author_fullname": "t2_7aj1lgdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mods are quickly removing reported posts. Thank you!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1beqjg6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Editable Flair", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710436807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I reported several posts (yes, I&amp;#39;m THAT guy!) for Rule #1 and they&amp;#39;re quickly gone. Hope this continues as IMO it makes this sub a better experience. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1beqjg6", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Marsupial6303", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beqjg6/mods_are_quickly_removing_reported_posts_thank_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beqjg6/mods_are_quickly_removing_reported_posts_thank_you/", "subreddit_subscribers": 738612, "created_utc": 1710436807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks,\n\nI'm trying to build a proper NAS for myself with a couple of disks and I want to consolidate all my data from my previous disks (and other family members' HDDs). The problem is that a lot of data is duplicated (because sometime in the past it was migrated to a new disk, then added some more data, then migrated again, etc.)...\n\nDo you have any tools or tricks to easily manage duplicated data? I would prefer Linux cli apps instead of anything GUI related. Also I want to solve this problem at the application level, even if some magic filesystems could deal with this problem by themselves (e.g. ZFS, or I don't know)\n\nThanks in advance!", "author_fullname": "t2_gb9xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you keep your random personal data deduplicated?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bel8mo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710423096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to build a proper NAS for myself with a couple of disks and I want to consolidate all my data from my previous disks (and other family members&amp;#39; HDDs). The problem is that a lot of data is duplicated (because sometime in the past it was migrated to a new disk, then added some more data, then migrated again, etc.)...&lt;/p&gt;\n\n&lt;p&gt;Do you have any tools or tricks to easily manage duplicated data? I would prefer Linux cli apps instead of anything GUI related. Also I want to solve this problem at the application level, even if some magic filesystems could deal with this problem by themselves (e.g. ZFS, or I don&amp;#39;t know)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bel8mo", "is_robot_indexable": true, "report_reasons": null, "author": "semmu", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bel8mo/how_do_you_keep_your_random_personal_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bel8mo/how_do_you_keep_your_random_personal_data/", "subreddit_subscribers": 738612, "created_utc": 1710423096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to purpose a rPi for offline content and not sure which approach would be superior. Is anyone familiar with IIAB as a host image? What benefits does it confer over having a barebones installation with Kiwix on top of it?\n\nI have IIAB currently on my 3b+ but it is slow AF and am trying to find more efficient alternatives that I can run with Docker.", "author_fullname": "t2_1hrlp7qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "R-Pi with basic install plus Kiwix VS I-I-A-B and Kiwix - Which is better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be2wrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710364197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to purpose a rPi for offline content and not sure which approach would be superior. Is anyone familiar with IIAB as a host image? What benefits does it confer over having a barebones installation with Kiwix on top of it?&lt;/p&gt;\n\n&lt;p&gt;I have IIAB currently on my 3b+ but it is slow AF and am trying to find more efficient alternatives that I can run with Docker.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1be2wrj", "is_robot_indexable": true, "report_reasons": null, "author": "Bruh-Nanaz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be2wrj/rpi_with_basic_install_plus_kiwix_vs_iiab_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be2wrj/rpi_with_basic_install_plus_kiwix_vs_iiab_and/", "subreddit_subscribers": 738612, "created_utc": 1710364197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\nHopefully a easy question with a simple answer. I have two 4TB LaCie rugged hard drives (USB-C version), which I use for back-ups. I hook them up to a 2020 MacBook Pro with thunderbolt ports. One hard drive mirrors the other and I'm storing them in different places. I noticed that the transfer speeds between drives is slow (between 8 and 25mb/s). Why is this? They reach up to 140 mb/s when testing them using Blackmagic Disk Speed Test. The MacBook is not doing something else on the meantime. Is this fixable? Thanks! ", "author_fullname": "t2_594xfrq9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slow transfer between hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1behyoo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710411714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!\nHopefully a easy question with a simple answer. I have two 4TB LaCie rugged hard drives (USB-C version), which I use for back-ups. I hook them up to a 2020 MacBook Pro with thunderbolt ports. One hard drive mirrors the other and I&amp;#39;m storing them in different places. I noticed that the transfer speeds between drives is slow (between 8 and 25mb/s). Why is this? They reach up to 140 mb/s when testing them using Blackmagic Disk Speed Test. The MacBook is not doing something else on the meantime. Is this fixable? Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1behyoo", "is_robot_indexable": true, "report_reasons": null, "author": "Bromska", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1behyoo/slow_transfer_between_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1behyoo/slow_transfer_between_hard_drives/", "subreddit_subscribers": 738612, "created_utc": 1710411714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have multi-TB HDD I use for Emby/Plex. \n\nEvery month I copy the new movies to an external HDD. \n\nI don\u2019t need to backup any images, I just need software that will add the newest folders on the server to the external HDD automatically. This only needs done every 2 weeks. \n\nI\u2019ve seen rclone, duplicati, macrium etc\u2026  but is there something simple that will do this for windows/ubuntu?\n\nContext: it\u2019s not always easy to manually backup new folders because if I change the server to a different OS, the file structure of \u201ccreated\u201d and \u201cmodified\u201d changes, which makes it difficult to find the actual latest folders. \n\nThanks in advance and I hope I made this clear. ", "author_fullname": "t2_x1giw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manual backup help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bebild", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710386741.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have multi-TB HDD I use for Emby/Plex. &lt;/p&gt;\n\n&lt;p&gt;Every month I copy the new movies to an external HDD. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t need to backup any images, I just need software that will add the newest folders on the server to the external HDD automatically. This only needs done every 2 weeks. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen rclone, duplicati, macrium etc\u2026  but is there something simple that will do this for windows/ubuntu?&lt;/p&gt;\n\n&lt;p&gt;Context: it\u2019s not always easy to manually backup new folders because if I change the server to a different OS, the file structure of \u201ccreated\u201d and \u201cmodified\u201d changes, which makes it difficult to find the actual latest folders. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance and I hope I made this clear. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bebild", "is_robot_indexable": true, "report_reasons": null, "author": "santovalentino", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bebild/manual_backup_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bebild/manual_backup_help/", "subreddit_subscribers": 738612, "created_utc": 1710386741.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am wondering what people here prefer in terms of the type of speed the drives use. \n\nLet\u2018s assume 5400rpm drives cost the same per TB as the 7200rpm drives (what I found to be true with a short comparison online). I just don\u2019t want to include \u201echeapest\u201c as an option\u2026\n\n[View Poll](https://www.reddit.com/poll/1be913f)", "author_fullname": "t2_9rwdy0id", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5400rpm vs 7200 vs enterprise drives/SSD\u2018s", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be913f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710379583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering what people here prefer in terms of the type of speed the drives use. &lt;/p&gt;\n\n&lt;p&gt;Let\u2018s assume 5400rpm drives cost the same per TB as the 7200rpm drives (what I found to be true with a short comparison online). I just don\u2019t want to include \u201echeapest\u201c as an option\u2026&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1be913f\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be913f", "is_robot_indexable": true, "report_reasons": null, "author": "hamdiboi37", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1710984383311, "options": [{"text": "7200rpm hdd", "id": "27405924"}, {"text": "5400rpm hdd", "id": "27405925"}, {"text": "10k+ hdd", "id": "27405926"}, {"text": "SSD/NVMe", "id": "27405927"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 122, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be913f/5400rpm_vs_7200_vs_enterprise_drivesssds/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/1be913f/5400rpm_vs_7200_vs_enterprise_drivesssds/", "subreddit_subscribers": 738612, "created_utc": 1710379583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a truenas core server and this week one of the drives started to rapidly develope bad sectors, and its looking like its on its way out. Its under warranty and im sending it in for rma. I ordered a new drive today and it will be here in two days. Is there any reason why i shouldnt keep using my server with the known bad drive until the new drive arrives?\n\nAs i understand it, if i keep using it, the worst case scenario is that the drive fails all together and i cant access that storage pool until i replace the drive. Is that correct?", "author_fullname": "t2_52e8mg6l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reason not to keep using bad drive in zfs pool while i wait for replacement?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1ber7n5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710438468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a truenas core server and this week one of the drives started to rapidly develope bad sectors, and its looking like its on its way out. Its under warranty and im sending it in for rma. I ordered a new drive today and it will be here in two days. Is there any reason why i shouldnt keep using my server with the known bad drive until the new drive arrives?&lt;/p&gt;\n\n&lt;p&gt;As i understand it, if i keep using it, the worst case scenario is that the drive fails all together and i cant access that storage pool until i replace the drive. Is that correct?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1ber7n5", "is_robot_indexable": true, "report_reasons": null, "author": "grinder323", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1ber7n5/any_reason_not_to_keep_using_bad_drive_in_zfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1ber7n5/any_reason_not_to_keep_using_bad_drive_in_zfs/", "subreddit_subscribers": 738612, "created_utc": 1710438468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm in need/search of a reddit subreddit/user profile scraper for archival purposes and I was curious what everyone else has been using lately? Thanks!", "author_fullname": "t2_rgkwxfpc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ISO of Reddit Scraper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1beps5i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710434949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in need/search of a reddit subreddit/user profile scraper for archival purposes and I was curious what everyone else has been using lately? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1beps5i", "is_robot_indexable": true, "report_reasons": null, "author": "cherylsc0ttsbull", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beps5i/iso_of_reddit_scraper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beps5i/iso_of_reddit_scraper/", "subreddit_subscribers": 738612, "created_utc": 1710434949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is probably going to be a 2-3 part question, I\u2019m not new to ripping and storing music but I am to blu rays. Hopefully I can ask this here. I worked the overnight last night so I\u2019ve been up a while, I\u2019m not sure how exactly to ask this but thank you if you read it. \n\nSo first, have you ripped your discs onto a storage device? What\u2019s the best options that are relatively cheap and work good? \n\nSecond, I have a seagate drive, I started to put music on it. I believe in always backing up your files, but is there a better storage option? Mine is the 4 tb one, I have about 60 movies and probably 20 tv shows I\u2019d want to put on the storage device, what size tb do you think I\u2019d need? \n\nThird, if I get the first two what\u2019s a good ripping device, I bought one for my cds but the drive stopped reading a bunch of them after awhile so I\u2019ll probably need to buy a new one. What do you recommend? ", "author_fullname": "t2_5aems8rd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping blu rays and storage devices. ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bepmc3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710434532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is probably going to be a 2-3 part question, I\u2019m not new to ripping and storing music but I am to blu rays. Hopefully I can ask this here. I worked the overnight last night so I\u2019ve been up a while, I\u2019m not sure how exactly to ask this but thank you if you read it. &lt;/p&gt;\n\n&lt;p&gt;So first, have you ripped your discs onto a storage device? What\u2019s the best options that are relatively cheap and work good? &lt;/p&gt;\n\n&lt;p&gt;Second, I have a seagate drive, I started to put music on it. I believe in always backing up your files, but is there a better storage option? Mine is the 4 tb one, I have about 60 movies and probably 20 tv shows I\u2019d want to put on the storage device, what size tb do you think I\u2019d need? &lt;/p&gt;\n\n&lt;p&gt;Third, if I get the first two what\u2019s a good ripping device, I bought one for my cds but the drive stopped reading a bunch of them after awhile so I\u2019ll probably need to buy a new one. What do you recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bepmc3", "is_robot_indexable": true, "report_reasons": null, "author": "tsunamitom1-", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bepmc3/ripping_blu_rays_and_storage_devices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bepmc3/ripping_blu_rays_and_storage_devices/", "subreddit_subscribers": 738612, "created_utc": 1710434532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hard cover, glued, I also need to scan the front, the side and the back of the hard cover then proceed to scan each page.\n\n&amp;nbsp;\n\nAnyone has a nice video tutorial on Youtube or something? I don't want to ruin the cover pages since it has nice art, I also don't want to ruin the inside edges of each page since some art works are divided into 2 pages.\n\n&amp;nbsp;\n\nTrying to preserve each page with pristine conditions.", "author_fullname": "t2_u3tr5lo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I disassemble a bound book (art book) safely and with minimal damage? To scan each page.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beeljx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710398008.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710397307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hard cover, glued, I also need to scan the front, the side and the back of the hard cover then proceed to scan each page.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Anyone has a nice video tutorial on Youtube or something? I don&amp;#39;t want to ruin the cover pages since it has nice art, I also don&amp;#39;t want to ruin the inside edges of each page since some art works are divided into 2 pages.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Trying to preserve each page with pristine conditions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1beeljx", "is_robot_indexable": true, "report_reasons": null, "author": "Jdpnobs", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beeljx/how_do_i_disassemble_a_bound_book_art_book_safely/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beeljx/how_do_i_disassemble_a_bound_book_art_book_safely/", "subreddit_subscribers": 738612, "created_utc": 1710397307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking to see if there's a case that can handle 24 drives and a regular PSU that doesn't sound like a jet engine.  Are there any?  I realize the title says CPU. I brain farted as I was typing fast. ", "author_fullname": "t2_3o3j6oa3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quiet 24 bay chassis with regular CPU and backplane?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beclev", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710394600.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710390044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to see if there&amp;#39;s a case that can handle 24 drives and a regular PSU that doesn&amp;#39;t sound like a jet engine.  Are there any?  I realize the title says CPU. I brain farted as I was typing fast. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1beclev", "is_robot_indexable": true, "report_reasons": null, "author": "MartiniCommander", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beclev/quiet_24_bay_chassis_with_regular_cpu_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beclev/quiet_24_bay_chassis_with_regular_cpu_and/", "subreddit_subscribers": 738612, "created_utc": 1710390044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello everyone, I need help with Synology Photos ? Why it's so slow to view a video with Quickconnect or OpenVPN ? I don't understand, I have 5G for my cellphone and 1 Gbps Fiber symetric (1 Gbps down/1 Gbps up). Synology Photos is fine on my LAN, but when I'm away, it's so slow, it's not working at all with videos only. Any ideas to fix it ? Thanks for your suggestions !!!! ", "author_fullname": "t2_p67lkkz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Synology photos expert here to find a fix (slow videos when outside of my LAN)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be5ig8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710370407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I need help with Synology Photos ? Why it&amp;#39;s so slow to view a video with Quickconnect or OpenVPN ? I don&amp;#39;t understand, I have 5G for my cellphone and 1 Gbps Fiber symetric (1 Gbps down/1 Gbps up). Synology Photos is fine on my LAN, but when I&amp;#39;m away, it&amp;#39;s so slow, it&amp;#39;s not working at all with videos only. Any ideas to fix it ? Thanks for your suggestions !!!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be5ig8", "is_robot_indexable": true, "report_reasons": null, "author": "d2racing911", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be5ig8/any_synology_photos_expert_here_to_find_a_fix/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be5ig8/any_synology_photos_expert_here_to_find_a_fix/", "subreddit_subscribers": 738612, "created_utc": 1710370407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " The Spotify Charts site (charts.spotify.com)  has the option to download the currently viewed chart as a CSV, which  contains a bunch of useful information (total days/weeks on chart,  position last week, etc)/ Does anyone know if there's a way to  scrape/bulk-download the CSV's for different charts?    ", "author_fullname": "t2_ewdcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping Spotify Charts CSV s", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzth9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710356811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Spotify Charts site (charts.spotify.com)  has the option to download the currently viewed chart as a CSV, which  contains a bunch of useful information (total days/weeks on chart,  position last week, etc)/ Does anyone know if there&amp;#39;s a way to  scrape/bulk-download the CSV&amp;#39;s for different charts?    &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdzth9", "is_robot_indexable": true, "report_reasons": null, "author": "Penguintim", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdzth9/scraping_spotify_charts_csv_s/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdzth9/scraping_spotify_charts_csv_s/", "subreddit_subscribers": 738612, "created_utc": 1710356811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry that this is long, I wanted to cover my bases and give/get the clearest picture possible, while getting unanswered questions brought to light and suggestions in one fell swoop.\n\n**TL;DR: What should I buy guys, WD or Seagate?**\n\n**I'm kidding. I want to buy more storage but I feel like I need to pivot mediums because I'm inundated with external hard drives. What medium or structure I should go for based on the nuance of my hardware situation** ***while also looking to the future based on my current storage conditions and usage/data trends. i.e. maybe I wait on getting into LTO tape because I'm in the early stages of \"archiving\" my work, and most of my files need to be accessed. Maybe I don't build a NAS system because I don't really use network capabilities, maybe I don't do any of this and keep buying external hard drives because I hate 3rd party software, etc.***\n\n**I also want to know where my thought processes about the mediums are incorrect because frankly, I\u2019m very overwhelmed and there\u2019s lots of information, so this write-up is helping me parse it as well. Answer whatever you know, I\u2019m happy to just get suggestions or anecdotal tidbits on any of the subjects I\u2019m describing. Thank you in advance. Sorry again this is so fucking long.**\n\n# Preamble:\n\nAt the moment, I have \\~40TB of external drives not including copies/backups and they're at capacity and growing rapidly. A majority are full of video files that I access but as I start to accumulate stuff that I don't regularly need to access, I'm thinking about more long term storage -- **if it's not a hassle**. The rest is media, less important and redownloadable, but needs space.\n\nI have *24 TB* in External Desktop Drives &amp; 15TB in External Portable Drives:\n\n***2x 10TB WD element desktop drives***\n\n***1x 4TB WD element desktop drive***: my **oldest** Desktop drive, I bought it in 2014 for like $100.\n\n***~~1x 5TB WD Element portable drive~~*** \u2014 It\u2019s actually emptying as we speak because it\u2019s slooooow (&lt;1Mbps write speed &amp; has 200 bad sectors) \u2014I\u2019m transferring 4TB of data from it and it\u2019s taken me 5 days so far &amp; \u201cover a day\u201d to go and that\u2019s after long formatting it back to NTFS. So I think it\u2019s *on it\u2019s way out* and needs to be replaced LOL\n\n***1x 5TB LaCie***\n\n***1x 4TB Seagate drive***\n\n***1x 1TB WD passport:*** This may make you NAS and Plex users laugh or cringe: This drive is just media gathered from my desktop drives that I plug into my TV's USB manually.\n\nSide note/transition: I\u2019ve read about \u201cNAS\u201d systems and \u201cPlex\u201d but I don\u2019t know if it\u2019s for me. As far as I can tell, NAS just gives you access to the files from any device on your home network. That\u2019s cool, but one of my PCs offers to do that and I never really use it. My PC does pop up on my SMART TV Hub\u2026..I don't really know why, but is that like the same thing that would happen if I built a NAS? That leads to my first potential option.\n\n# Option 1: Repurpose my PC and buy NAS HDDs\n\nWriting this out made me think, my PC is basically a mutt of PC parts (which I\u2019m looking to upgrade anyway, and if I wanted to do that I\u2019d need to replace my Gpu, Cpu, Ram, etc because NOTHING today is compatible with my PC but I didn't want to upgrade and waste the parts). The wiki says you need a GPU, RAM, etc. to build a NAS. So it sounds like a PC but one that\u2019s not specd for \u201cgaming\u201d like mine was.\n\nIf I were to upgrade, I\u2019d have extra parts for another whole PC. Would repurposing these PC parts be kosher if I were to just buy NAS HDDs? Or am I oversimplifying it and need other hardware? \n\nHere are the part specs:\n\n    CPU: Intel Core i7-8700K\n    GPU: Nvidia GTX 960\n    RAM: Corsair Vengeance LPX DDR4 3000 C15 2x8GB\n    MBD: Asus ROG STRIX Z370-E\n    Boot Media would be SATA\n\nThe wiki says you need parts that match and mine definitely do but I\u2019m kind of lost on hardware requirements or if certain hardware prevents bottlenecking.\n\nNAS doesn\u2019t *really* seem like it\u2019s right for me because I don\u2019t use the network feature, mainly because I don\u2019t watch 90% of my media on a weekly basis, so transferring it onto a hard drive from another hard drive when I want to is easy. Plus not all my media is playable on my TV so I have to be selective and not all my TV\u2019s have network capabilities, but they all have USB ports. **Mind you,** this isn't even a huge inconvenience for me, but maybe there's more to it so I'm just trying to make sure I'm on the right track before ruling NAS out of my current needs, especially if it turns out to be the more robust than I'm imagining.\n\n# Option 2: Buy JBOD Enclosure and HDDs\n\nWhat I really like about NAS is the adjustable capacity because you can pretty much just swap the drives out, so I\u2019ve read about 4 bay (or more) JBOD enclosures. I like this idea, because it seems straightforward, and the capacity is adjustable based on the disks you put in, but I feel like I\u2019m missing something.\n\nIf I were to take this route, I could just shuck my WD desktop drives, drop the HDDs into the JBOD, and it\u2019d basically be the same thing? As in, my windows file manager would still see them all as the separate drives they are, but they\u2019d be pop up in file manager through one cable versus the three ports I have occupied now. Right?\n\nI like this idea and I lean towards it more because it checks my boxes for adjustable capacity and ease of use, but I just feel like I\u2019m missing something, like some sort of dreaded software that'll be needed to access my drives....\n\n# Option 3: Buy 22 TB External Hard Drive for media backup &amp; LTO Tape (gen 5 or 6) for important stuff\n\nVery unfamiliar with LTO but I think I want to learn and adopt it in the future for my archived work. All you fear mongers here make me nervous that my HDDs are going to all keel over any minute LOL. Especially since most of my HDDs are reaching the 10 year mark.\n\nI see a lot of info about it and realistically learned about it like a month ago, but I still am not understanding what to expect with this medium so I\u2019d appreciate clarity. \n\nWhat I gather you need is:\n\n1. a tape drive \u2014 which I see is cheaper for LTO 5/6 vs the holy-price-gouging of LTO-7+\n\n2. Tapes \u2014 1.5/3TB capacity doesn\u2019t excite me, but I love the idea of splitting and organizing by physical tapes.\n\n3. but now Software? \u2014 I don\u2019t get this part at all. I hear that you need to pay for a software, which turns me off of this. Is there no way to just plug the tape drive into your computer and be able to access your files like an external drive?\n\nI don't like the idea that I can\u2019t access my files unless I have a software from some specific company. I\u2019m not pro-cloud for similar reasons, so if that's a barrier for LTO tape then I'll probably pass on that for Option 3.\n\nMaybe it\u2019s irrational because technically you're almost always at the whim of some company's software. Finder and File Manager are like \u201cMac/windows software\u201d *I guess* but I\u2019ve been using those systems my whole life AND you can force stuff with command line so it feels less \"gated.\"\n\nThe 22TB would just be for all the visual media I have to be in one place because its split between drives. None of that media is stuff I'd want \"protected\" long term because it's easily retrievable.\n\n&amp;#x200B;\n\nSo\u2026. Again sorry that this is long, I seriously appreciate you taking time to read and possibly give input. I saw the recent post about low-effort posts and I was thinking my question was too dumb to ask here but I've just been absolutely STUMPED for probably weeks on what may be an easy, yet cost effective, and somewhat future-proof, scenario. Even though I know all this stuff can all be a gamble sometimes LOL\n\n&amp;#x200B;", "author_fullname": "t2_42mqql5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me understand my options for consolidating my harem of external hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bdzpa9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710356526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry that this is long, I wanted to cover my bases and give/get the clearest picture possible, while getting unanswered questions brought to light and suggestions in one fell swoop.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: What should I buy guys, WD or Seagate?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;m kidding. I want to buy more storage but I feel like I need to pivot mediums because I&amp;#39;m inundated with external hard drives. What medium or structure I should go for based on the nuance of my hardware situation&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;while also looking to the future based on my current storage conditions and usage/data trends. i.e. maybe I wait on getting into LTO tape because I&amp;#39;m in the early stages of &amp;quot;archiving&amp;quot; my work, and most of my files need to be accessed. Maybe I don&amp;#39;t build a NAS system because I don&amp;#39;t really use network capabilities, maybe I don&amp;#39;t do any of this and keep buying external hard drives because I hate 3rd party software, etc.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I also want to know where my thought processes about the mediums are incorrect because frankly, I\u2019m very overwhelmed and there\u2019s lots of information, so this write-up is helping me parse it as well. Answer whatever you know, I\u2019m happy to just get suggestions or anecdotal tidbits on any of the subjects I\u2019m describing. Thank you in advance. Sorry again this is so fucking long.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Preamble:&lt;/h1&gt;\n\n&lt;p&gt;At the moment, I have ~40TB of external drives not including copies/backups and they&amp;#39;re at capacity and growing rapidly. A majority are full of video files that I access but as I start to accumulate stuff that I don&amp;#39;t regularly need to access, I&amp;#39;m thinking about more long term storage -- &lt;strong&gt;if it&amp;#39;s not a hassle&lt;/strong&gt;. The rest is media, less important and redownloadable, but needs space.&lt;/p&gt;\n\n&lt;p&gt;I have &lt;em&gt;24 TB&lt;/em&gt; in External Desktop Drives &amp;amp; 15TB in External Portable Drives:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;2x 10TB WD element desktop drives&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1x 4TB WD element desktop drive&lt;/em&gt;&lt;/strong&gt;: my &lt;strong&gt;oldest&lt;/strong&gt; Desktop drive, I bought it in 2014 for like $100.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;del&gt;1x 5TB WD Element portable drive&lt;/del&gt;&lt;/em&gt;&lt;/strong&gt; \u2014 It\u2019s actually emptying as we speak because it\u2019s slooooow (&amp;lt;1Mbps write speed &amp;amp; has 200 bad sectors) \u2014I\u2019m transferring 4TB of data from it and it\u2019s taken me 5 days so far &amp;amp; \u201cover a day\u201d to go and that\u2019s after long formatting it back to NTFS. So I think it\u2019s &lt;em&gt;on it\u2019s way out&lt;/em&gt; and needs to be replaced LOL&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1x 5TB LaCie&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1x 4TB Seagate drive&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;1x 1TB WD passport:&lt;/em&gt;&lt;/strong&gt; This may make you NAS and Plex users laugh or cringe: This drive is just media gathered from my desktop drives that I plug into my TV&amp;#39;s USB manually.&lt;/p&gt;\n\n&lt;p&gt;Side note/transition: I\u2019ve read about \u201cNAS\u201d systems and \u201cPlex\u201d but I don\u2019t know if it\u2019s for me. As far as I can tell, NAS just gives you access to the files from any device on your home network. That\u2019s cool, but one of my PCs offers to do that and I never really use it. My PC does pop up on my SMART TV Hub\u2026..I don&amp;#39;t really know why, but is that like the same thing that would happen if I built a NAS? That leads to my first potential option.&lt;/p&gt;\n\n&lt;h1&gt;Option 1: Repurpose my PC and buy NAS HDDs&lt;/h1&gt;\n\n&lt;p&gt;Writing this out made me think, my PC is basically a mutt of PC parts (which I\u2019m looking to upgrade anyway, and if I wanted to do that I\u2019d need to replace my Gpu, Cpu, Ram, etc because NOTHING today is compatible with my PC but I didn&amp;#39;t want to upgrade and waste the parts). The wiki says you need a GPU, RAM, etc. to build a NAS. So it sounds like a PC but one that\u2019s not specd for \u201cgaming\u201d like mine was.&lt;/p&gt;\n\n&lt;p&gt;If I were to upgrade, I\u2019d have extra parts for another whole PC. Would repurposing these PC parts be kosher if I were to just buy NAS HDDs? Or am I oversimplifying it and need other hardware? &lt;/p&gt;\n\n&lt;p&gt;Here are the part specs:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CPU: Intel Core i7-8700K\nGPU: Nvidia GTX 960\nRAM: Corsair Vengeance LPX DDR4 3000 C15 2x8GB\nMBD: Asus ROG STRIX Z370-E\nBoot Media would be SATA\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The wiki says you need parts that match and mine definitely do but I\u2019m kind of lost on hardware requirements or if certain hardware prevents bottlenecking.&lt;/p&gt;\n\n&lt;p&gt;NAS doesn\u2019t &lt;em&gt;really&lt;/em&gt; seem like it\u2019s right for me because I don\u2019t use the network feature, mainly because I don\u2019t watch 90% of my media on a weekly basis, so transferring it onto a hard drive from another hard drive when I want to is easy. Plus not all my media is playable on my TV so I have to be selective and not all my TV\u2019s have network capabilities, but they all have USB ports. &lt;strong&gt;Mind you,&lt;/strong&gt; this isn&amp;#39;t even a huge inconvenience for me, but maybe there&amp;#39;s more to it so I&amp;#39;m just trying to make sure I&amp;#39;m on the right track before ruling NAS out of my current needs, especially if it turns out to be the more robust than I&amp;#39;m imagining.&lt;/p&gt;\n\n&lt;h1&gt;Option 2: Buy JBOD Enclosure and HDDs&lt;/h1&gt;\n\n&lt;p&gt;What I really like about NAS is the adjustable capacity because you can pretty much just swap the drives out, so I\u2019ve read about 4 bay (or more) JBOD enclosures. I like this idea, because it seems straightforward, and the capacity is adjustable based on the disks you put in, but I feel like I\u2019m missing something.&lt;/p&gt;\n\n&lt;p&gt;If I were to take this route, I could just shuck my WD desktop drives, drop the HDDs into the JBOD, and it\u2019d basically be the same thing? As in, my windows file manager would still see them all as the separate drives they are, but they\u2019d be pop up in file manager through one cable versus the three ports I have occupied now. Right?&lt;/p&gt;\n\n&lt;p&gt;I like this idea and I lean towards it more because it checks my boxes for adjustable capacity and ease of use, but I just feel like I\u2019m missing something, like some sort of dreaded software that&amp;#39;ll be needed to access my drives....&lt;/p&gt;\n\n&lt;h1&gt;Option 3: Buy 22 TB External Hard Drive for media backup &amp;amp; LTO Tape (gen 5 or 6) for important stuff&lt;/h1&gt;\n\n&lt;p&gt;Very unfamiliar with LTO but I think I want to learn and adopt it in the future for my archived work. All you fear mongers here make me nervous that my HDDs are going to all keel over any minute LOL. Especially since most of my HDDs are reaching the 10 year mark.&lt;/p&gt;\n\n&lt;p&gt;I see a lot of info about it and realistically learned about it like a month ago, but I still am not understanding what to expect with this medium so I\u2019d appreciate clarity. &lt;/p&gt;\n\n&lt;p&gt;What I gather you need is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;a tape drive \u2014 which I see is cheaper for LTO 5/6 vs the holy-price-gouging of LTO-7+&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Tapes \u2014 1.5/3TB capacity doesn\u2019t excite me, but I love the idea of splitting and organizing by physical tapes.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;but now Software? \u2014 I don\u2019t get this part at all. I hear that you need to pay for a software, which turns me off of this. Is there no way to just plug the tape drive into your computer and be able to access your files like an external drive?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I don&amp;#39;t like the idea that I can\u2019t access my files unless I have a software from some specific company. I\u2019m not pro-cloud for similar reasons, so if that&amp;#39;s a barrier for LTO tape then I&amp;#39;ll probably pass on that for Option 3.&lt;/p&gt;\n\n&lt;p&gt;Maybe it\u2019s irrational because technically you&amp;#39;re almost always at the whim of some company&amp;#39;s software. Finder and File Manager are like \u201cMac/windows software\u201d &lt;em&gt;I guess&lt;/em&gt; but I\u2019ve been using those systems my whole life AND you can force stuff with command line so it feels less &amp;quot;gated.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The 22TB would just be for all the visual media I have to be in one place because its split between drives. None of that media is stuff I&amp;#39;d want &amp;quot;protected&amp;quot; long term because it&amp;#39;s easily retrievable.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So\u2026. Again sorry that this is long, I seriously appreciate you taking time to read and possibly give input. I saw the recent post about low-effort posts and I was thinking my question was too dumb to ask here but I&amp;#39;ve just been absolutely STUMPED for probably weeks on what may be an easy, yet cost effective, and somewhat future-proof, scenario. Even though I know all this stuff can all be a gamble sometimes LOL&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bdzpa9", "is_robot_indexable": true, "report_reasons": null, "author": "long-ryde", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bdzpa9/help_me_understand_my_options_for_consolidating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bdzpa9/help_me_understand_my_options_for_consolidating/", "subreddit_subscribers": 738612, "created_utc": 1710356526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I back up my Mac, iphone and ipad to an external ssd inside veracrypt container (the container is about 800gb and the SSD is about 2TB) and I do these backups once a month and also store this same copy on a secondary SSD(or sd card) + 1 backup online just to be extra safe, what is the likelihood of data corruption or me being unable to retrieve my data? Assuming I have(for example) 1 SSD like Samsung t5 shield and one micro SD card SanDisk ultra and I would replace both of them after 5 years of usage(or immediately if they fail).", "author_fullname": "t2_53irb6so", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Encrypted backups on ssd", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1beqe4t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710436445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I back up my Mac, iphone and ipad to an external ssd inside veracrypt container (the container is about 800gb and the SSD is about 2TB) and I do these backups once a month and also store this same copy on a secondary SSD(or sd card) + 1 backup online just to be extra safe, what is the likelihood of data corruption or me being unable to retrieve my data? Assuming I have(for example) 1 SSD like Samsung t5 shield and one micro SD card SanDisk ultra and I would replace both of them after 5 years of usage(or immediately if they fail).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1beqe4t", "is_robot_indexable": true, "report_reasons": null, "author": "Smartboy_r_n", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beqe4t/encrypted_backups_on_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beqe4t/encrypted_backups_on_ssd/", "subreddit_subscribers": 738612, "created_utc": 1710436445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "System info:\n\n* Windows 11 Pro\n* Ryzen 3900x\n* MSI B550-A PRO MOBO\n* 64 GB DDR4 RAM \n\nI have had a QNAP TR-004 for about 3 years now. I originally set up the enclosure in a RAID5 configuration with 3 drives (all WD140EDGZ).\n\nI use the storage for a variety of things, ranging from media to document storage, but the largest allocation is for my Plex media, by far.\n\nNow, I want to add a 4th HDD to the array. I need to backup all my data and recreate the array to do so. In trying to do this, I am finding that it is going to take anywhere from 4 days to a full *week* to get this data onto another drive.\n\nI've had some problems with Plex media playback from the array stuttering before if I were doing heavy writes to the drive (like adding new media to the library at the same time as playback), but no real transfer speed concerns since I rarely moved very large sets of data in/out of the array. I have no idea if this is the speed it has been operating at since it was set up.\n\nRunning CrystalDiskMark with a few 1GiB passes looks like this for MB/s throughput:\n\nhttps://imgur.com/khrumvF\n\nAnd in IOPS:\n\nhttps://imgur.com/WhbBxfz\n\nThese are consistent with the transfer speeds I have been getting when trying to backup my data.\n\nThings I have tried:\n\n* Various different USB cables \n* Different USB ports (all direct to MOBO)\n* Cleaning out any dust in the fans\n* Ensuring enclosure firmware is updated\n* Checking SMART for any issues\n\n\nIs there some config I may have done incorrectly that I can check? Is this just a faulty enclosure? Looking for anything I can try to troubleshoot!", "author_fullname": "t2_n4zd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Abysmal speeds with QNAP TR-004 in RAID", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bepnre", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710434636.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;System info:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Windows 11 Pro&lt;/li&gt;\n&lt;li&gt;Ryzen 3900x&lt;/li&gt;\n&lt;li&gt;MSI B550-A PRO MOBO&lt;/li&gt;\n&lt;li&gt;64 GB DDR4 RAM &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have had a QNAP TR-004 for about 3 years now. I originally set up the enclosure in a RAID5 configuration with 3 drives (all WD140EDGZ).&lt;/p&gt;\n\n&lt;p&gt;I use the storage for a variety of things, ranging from media to document storage, but the largest allocation is for my Plex media, by far.&lt;/p&gt;\n\n&lt;p&gt;Now, I want to add a 4th HDD to the array. I need to backup all my data and recreate the array to do so. In trying to do this, I am finding that it is going to take anywhere from 4 days to a full &lt;em&gt;week&lt;/em&gt; to get this data onto another drive.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had some problems with Plex media playback from the array stuttering before if I were doing heavy writes to the drive (like adding new media to the library at the same time as playback), but no real transfer speed concerns since I rarely moved very large sets of data in/out of the array. I have no idea if this is the speed it has been operating at since it was set up.&lt;/p&gt;\n\n&lt;p&gt;Running CrystalDiskMark with a few 1GiB passes looks like this for MB/s throughput:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/khrumvF\"&gt;https://imgur.com/khrumvF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And in IOPS:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/WhbBxfz\"&gt;https://imgur.com/WhbBxfz&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;These are consistent with the transfer speeds I have been getting when trying to backup my data.&lt;/p&gt;\n\n&lt;p&gt;Things I have tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Various different USB cables &lt;/li&gt;\n&lt;li&gt;Different USB ports (all direct to MOBO)&lt;/li&gt;\n&lt;li&gt;Cleaning out any dust in the fans&lt;/li&gt;\n&lt;li&gt;Ensuring enclosure firmware is updated&lt;/li&gt;\n&lt;li&gt;Checking SMART for any issues&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is there some config I may have done incorrectly that I can check? Is this just a faulty enclosure? Looking for anything I can try to troubleshoot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wLfby4xI8YPiVcHWQy9jpHI_iwayW60gIdK79kG8d1s.jpg?auto=webp&amp;s=7a3d580dfa9e4cb3bc757693ac38153bdb550383", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/wLfby4xI8YPiVcHWQy9jpHI_iwayW60gIdK79kG8d1s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d25164063c17b603e48b84f82afb8b72f6babe94", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wLfby4xI8YPiVcHWQy9jpHI_iwayW60gIdK79kG8d1s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=725f6edf233db13f81f2e6c29cf90a1b16ff56a1", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wLfby4xI8YPiVcHWQy9jpHI_iwayW60gIdK79kG8d1s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf67080cadd7905726aac35145b4dc9fabcdc436", "width": 320, "height": 168}], "variants": {}, "id": "TScH_Vq-cWLjyxWZwweN7ALJF8T3S2E19SuOp5EpPLQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bepnre", "is_robot_indexable": true, "report_reasons": null, "author": "stovebison", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bepnre/abysmal_speeds_with_qnap_tr004_in_raid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bepnre/abysmal_speeds_with_qnap_tr004_in_raid/", "subreddit_subscribers": 738612, "created_utc": 1710434636.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What do you guys use? \n\nFor example. 500GB Windows drive to a new 1TB SSD. Just need to clone and resize partitions so they use the full space.\n\nBoth drives will be connected to a separate PC using USB adapters so no need to boot into pre-os or anything like that. \n\nOr should I use diskcopy or whatever it's called that's free and resize using disk management inside windows afterwords?", "author_fullname": "t2_3tsomxow", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Clone SSD to another SSD (externally using SATA to USB) while resizing partitions for larger drive for FREE. ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beovky", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710432677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys use? &lt;/p&gt;\n\n&lt;p&gt;For example. 500GB Windows drive to a new 1TB SSD. Just need to clone and resize partitions so they use the full space.&lt;/p&gt;\n\n&lt;p&gt;Both drives will be connected to a separate PC using USB adapters so no need to boot into pre-os or anything like that. &lt;/p&gt;\n\n&lt;p&gt;Or should I use diskcopy or whatever it&amp;#39;s called that&amp;#39;s free and resize using disk management inside windows afterwords?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1beovky", "is_robot_indexable": true, "report_reasons": null, "author": "Not_So_Typical_Gamer", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beovky/clone_ssd_to_another_ssd_externally_using_sata_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beovky/clone_ssd_to_another_ssd_externally_using_sata_to/", "subreddit_subscribers": 738612, "created_utc": 1710432677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I only recently found out that this feature of Instaloader doesn't work anymore, and I'm looking for some help to fix it/ an alternative tool.\n\nI need to download Instagram Posts filtered by date and hashtag. The problem with instaloader is the following:\n\nI'm attempting to download all posts associated with a particular hashtag that were posted within a specified timeframe. I've followed the instructions provided, and after troubleshooting, it appears that Instaloader is unable to locate any posts associated with the hashtag or doesn't have access to them. I've experimented with multiple hashtags and timeframes, including popular hashtags like #gender and broader time periods, yet the issue persists.\n\nUpon running the code, the first line displays: 'JSON Query to explore/tags/#gendern/: 404 Not Found.'\n\nI found 2 github posts on the subject, one is[ mine](https://github.com/instaloader/instaloader/issues/2206), the other is from [December 2023 and is more detailed](https://github.com/instaloader/instaloader/issues/2144), but about the same issue.\n\nAny ideas are greatly appreciated!", "author_fullname": "t2_115ydi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Instaloader fix/ alternative way to scrape instagram posts filtered by hashtag", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beh7yo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710408696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I only recently found out that this feature of Instaloader doesn&amp;#39;t work anymore, and I&amp;#39;m looking for some help to fix it/ an alternative tool.&lt;/p&gt;\n\n&lt;p&gt;I need to download Instagram Posts filtered by date and hashtag. The problem with instaloader is the following:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m attempting to download all posts associated with a particular hashtag that were posted within a specified timeframe. I&amp;#39;ve followed the instructions provided, and after troubleshooting, it appears that Instaloader is unable to locate any posts associated with the hashtag or doesn&amp;#39;t have access to them. I&amp;#39;ve experimented with multiple hashtags and timeframes, including popular hashtags like #gender and broader time periods, yet the issue persists.&lt;/p&gt;\n\n&lt;p&gt;Upon running the code, the first line displays: &amp;#39;JSON Query to explore/tags/#gendern/: 404 Not Found.&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;I found 2 github posts on the subject, one is&lt;a href=\"https://github.com/instaloader/instaloader/issues/2206\"&gt; mine&lt;/a&gt;, the other is from &lt;a href=\"https://github.com/instaloader/instaloader/issues/2144\"&gt;December 2023 and is more detailed&lt;/a&gt;, but about the same issue.&lt;/p&gt;\n\n&lt;p&gt;Any ideas are greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?auto=webp&amp;s=33723d7782ae0377d417d6a441e009bb8821b486", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=abd6c3bfd8a00ac8880540f4e36ef33e373520f5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4455a3d7b30c695e68a4e8738e90cdb87e35b5c0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=914aa6cdda38f8630ccf6e3e7537dc57d46ee8ec", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c0ae0ce01a6b3fa69bfa3e97a188905ec4d5f7b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0206ccffdee2cc6baffe41e0d263ac4dd858aa6e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/wfY_SHzMMAJ3NApwwW6LkMPX8swo3PWBST5gM6YD-_w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e065dec59a9e7c90004066f1e2f8caf63f514c4", "width": 1080, "height": 540}], "variants": {}, "id": "H_2BagLwxdXZAI1VYTu2eTjuucPzNqKizgD1e6kDigw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1beh7yo", "is_robot_indexable": true, "report_reasons": null, "author": "xchernyy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1beh7yo/looking_for_instaloader_fix_alternative_way_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1beh7yo/looking_for_instaloader_fix_alternative_way_to/", "subreddit_subscribers": 738612, "created_utc": 1710408696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Many users prefer to have a compact NAS unit, which usually means if you're building your own, the use of a Mini ITX motherboard.\n\nThis can typically limit expansion options, unless you're willing to pay a significant fee for a higher end motherboard like the CWWK which is pretty full featured but also costs about $450USD: https://cwwk.net/products/cwwk-amd-7735hs-7840hs-8845hs-7940hs-8-bay-9-bay-nas-usb4-40g-rate-8k-display-4-network-2-5g-9-sata-pcie-x16-itx-motherboard\n\nWhile looking at AliExpress, I came across some options that included an N100 and N5105 CPU and included six SATA ports and two M.2 slots and four 2.5GbE ports. I ended up picking up both versions, N5105 from AliExpress, N100 from Amazon.\n\nThe two units I purchased, both ~ $125USD:\n\nN5105: https://www.aliexpress.us/item/3256805947799076.html\n\nN100: https://www.amazon.com/dp/B0CQZH8X2P\n\nFull disclosure, after I had started some testing on the N100 board, it started showing issues, An ethernet controller would disappear, then I'd get phantom lockups. I also noticed that while the N5105 SATA chip had a heatsink on it, the N100 did not even though it has holes to mount one. Thankfully this is the one I bought from Amazon so issued an RMA and they promptly shipped me a new board which seemed to work perfectly fine throughout the testing.\n\nI posted a review video if you're interested, but most of the pertinent info is below: https://youtu.be/PO8Kfi4qpY8?si=9AuYTaGZmmMfM5NG\n\n**COMPONENTS**\n\nThey both offer:\n\n* two 10Gbps USB3 Type A ports\n* two M.2 SATA ports\n* four 2.5GbE ports managed by the Intel I226-V chip\n* six onboard SATA ports with the JMicron JMB585 controller\n\nUnique to each:\n\n* one DDR5 So-DIMM (N100)\n* two DDR4 So-DIMM (N5105)\n* one PCIe 1x slot (N100)\n\nThat SATA controller supports up to 5 SATA III ports so I can only imagine the other one is provided by the CPU. The N5105 spec indicates that it can support two SATA ports, but the N100 specs weren't clear.\n\nThe N100 has a single DDR5 So-DIMM slot that supports up to 16GB, that limit is apparently enforced by the CPU design. I don't have a 32GB DDR5 So-DIMM otherwise I'd see if it actually can support it. The N5105 has two DDR4 slots, and like the N100 is limited to 16GB total RAM. I did insert a single 16GB chip in one slot, but it wouldn't boot. But two 8GB or single 8GB worked just fine.\n\nOne unique thing about the N100 board is it offers a PCIe 1x slot. The N100 supports 9 PCIe 3.0 lanes, whereas the N5105 only 8, which is likely the reason it's not on the N5105 version. That slot is open ended so you can add longer cards. The only caveat is that the card has to slot between the two rows of SATA ports. It fits fine, as I plugged in an RX6400 and GTX 1050 Ti video card, but you can't use clipped SATA connectors because the clip will overlap into the area where the PCIe card wants to fit. Plus you will need 90 degree right angle connectors on the one side to avoid hitting any protruding part off the PCIe card.\n\n**OS INSTALLATION**\n\nI installed five operating systems on each motherboard:\n\n* Windows 11\n* Ubuntu\n* OpenMediaVault\n* TrueNAS Scale\n* UnRAID\n\nInstallation of the Linux based OS's went perfectly fine. Windows 11, on the other hand was lacking many devices, but most importantly were the Intel I226-V 2.5GbE drivers, so you couldn't even connect to the internet without them. This can be problematic because Windows likes to force you on the internet during install. But a nice little workaround I found was to use `SHIFT-F10` which will bring up a console window and then type `oobe\\bypassnro` reboot, and then you will get an option to install without internet, all the while trying to make you feel bad about yourself for not commiting your email and soul to Microsoft.\n\nOnce I got up and running, I loaded drivers from a USB (https://intel.com/content/www/us/en/download/15084/intel-ethernet-adapter-complete-driver-pack.html), and then performed the Windows update marathon. The N5105 was missing several drivers still, but I did find drivers on Gigabytes Website. I needed the chipset drivers from here: https://gigabyte.com/Motherboard/N5105I-H-rev-10/support#support-dl-driver-chipset\n\nFor the N100, I used the same I226-V drivers from the USB, and after updates there was just some Audio driver missing which was not so easy to track down. I did manage to get it from here: https://catalog.update.microsoft.com/Search.aspx?q=10.29.0.9677+media\n\nBut then after installing that, another audio or SM Bus Driver was still missing which I managed to get from the tenforums website, which linked to a Google Drive download. Sure, a bit shady, but this motherboard was already from AliExpress out of China, so I've probably already compromised my identity at this point. But seriously, I scanned it for viruses and it came up clean. You can grab it here: https://www.tenforums.com/sound-audio/182081-latest-realtek-hd-audio-driver-version-3-a-103.html\n\nSo with everything up and running I ran a multitude of tests on the different components.\n\n**WINDOWS SYSTEM BENCHMARKS**\n\nFor general system tests, I ran Cinebench R23 in Windows and tracked the CPU usage, temps, power, etc. Nothing out of the ordinary. If you're interested results were:\n\n    N5105 Single CPU: 577\n    N100 Single CPU:  886\n    N5105 Multi Core: 1990\n    N100 Multi Core:  2504\n\nBoth CPU temps hovered in the upper 70's, but after the re-paste, the N100 dropped by about 20C and the N5105 by about 10C.\n\nI also ran Handbrake encoding test of a 4k60 10 minute video using the Handbrake \"1080p Fast\" default setting which encodes to 1080p/30. The results were as follows:\n\n    N5105 QSV:  32.4 minutes\n    N5105 CPU:  39.7 minutes\n    N100 QSV:   21.2 minutes\n    N100 CPU:   28.6 minutes\n\nSo anywhere from 20-40 minutes for a 10 minute video. Not too impressive.\n\nI also fired up a Plex media server on each motherboard, and it served up to four 4k videos just fine as long as they were native resolution and format. I mean, that's just a bandwidth thing.\n\nBut when it came to transcoding, forget it. I tried to transcode a single 4k/60 video to 1080p/30 and it would take up to a minute to encode about 15-20 seconds of video. So it would constantly buffer with the CPU running at full tilt 100% utilization.\n\n**2.5GbE INTEL I226-V ETHERNET PORTS**\n\nFor the 2.5GbE Ethernet ports, I did a basic 10x 1GB file copy test and measured the resultant performance. They all performed about 270-280 MB/sec read and write. For some reason the N5105 in Windows write test was only about 240 MB/sec, but up to about 275MB/sec with read. Other OS's it performed as expected. So not sure what to make of that other than Windows being Windows.\n\n**M.2 and USB**\n\nFor M.2 and USB ports I ran CrystalDiskMark (Windows), KDiskMark (Ubuntu), hdparm -t read test (Linux OS's), and a 10x 1GB file copy.\n\nBottom line, The M.2 and PCIe slot are definitely PCIe 1x (3.0). CrystalDiskMark, KDiskMark, and `hdparm -t` tests resulted in about 850-900 MB/sec sequential read/write. During the actual 10x 1GB file transfer tests, the N5105 faltered a bit running at only about 650 MB/sec in OMV, TrueNAS, and UnRAID.\n\nThe USB ports actually performed better than the M.2 slots running over 1000 MB/sec with the artificial CrystalDiskMark/KDiskMark sequential and `hdparm -t` tests. However, real world file transfers were all over the place. But that seems par for the course for USB.\n\n**SATA PORTS**\n\nNow when it comes to the SATA ports, both motherboards use the JMicron JMB585 controller. This chip provides support for up to 5 SATA III (600 MB/sec) ports. Considering there are six SATA ports, I believe on comes from the CPU.\n\nOddly enough, The N100 SATA ports seemed to be limiting overall performance. Connecting a single Samsung 870 Evo 2.5\" SATA SSD to each port, it only resulted in about 430 MB/sec on five of the six ports. The sixth port managed about 550 MB/sec which is about max performance of this SSD when connected to a traditional desktop SATA port (where it hits 560MB/sec). The N5105 on the other hand performed at about 550 MB/sec.\n\nI also used an Orico M.2 Six SATA port adapter that uses the ASMedia ASM 1166 controller as kind of a control sample, because I know it performs at expected speeds. The Orico M.2 in both the N100 and N5105 performed as well as in a traditional desktop. So there is some limitation there.\n\nWhile this may not seem concerning if you're using hard drives because they only tend to run at about 250 MB/sec or slower, SSD's could be problematic. But worse is the RAID performance.\n\n**OPENMEDIAVAULT**\n\nI set up a few scenarios, but I'll only discuss the 6x RAID 0 and 12x RAID 60 (OMV) / Two 6x RAID Z2 VDevs (TrueNAS). I used ST500DM002 500GB SATA hard drives which performs at about 200 MB/sec sequential speeds when empty. A 6x RAID 0 should offer over 1000 MB/sec with these.\n\nWith the 6x RAID 0, the N100 only offered up about 500 MB/sec. On the N5105 it hit over 1000 MB/sec.\n\nI also set up a 6x RAID 6 and 6x RAID 60. I built one RAID 6 at a time, then went back and built two RAID 6's at a time to check if the system could handle it, then I merged them into an mdadm striped array for RAID 60.\n\nResults from the RAID 6 build times:\n\n    Single RAID 6 Build Onboard SATA:\n    - N100:  127 Minutes\n    - N5105: 106 Minutes\n    Dual RAID 6 onboard SATA:\n    - N100:  145 Minutes\n    - N5105: 106 Minutes\n    Dual RAID 6 Orico M.2 Adapter:\n    - N100:  114 Minutes\n    - N5105: 106 Minutes\n\nSo you can see that the N5105 handled the RAID 6 single build and when building two RAID 6 arrays simultaneously, without a hitch. The N100 took quite a bit longer.\n\nRegarding CPU usage during the builds, both hit about 50% CPU utilization throughout with the 15 minute load average peaking at about 4, although the N5105 jumped up to about 70% utilization and 15 minute load average of about 4.5 for a brief period. Either way, it seemed the system could handle it just fine.\n\n**UnRAID**\n\nFor UnRAID I set up a 4x Data Disk + 2x Parity Disk scenario and measured the performance of a build, as well as a parity check. Results as follows:\n\n    Initial Sync Onboard SATA:\n    - N100:  77 Minutes\n    - N5105: 53 Minutes\n    Initial Sync Orico M.2 Adapter:\n    - N100:  53 Minutes\n    - N5105: 53 Minutes\n    Parity Check Onboard SATA:\n    - N100:  93 Minutes\n    - N5105: 54 Minutes\n    Parity Check Orico M.2 Adapter:\n    - N100:  60 Minutes\n    - N5105: 54 Minutes\n\nSo it appears the N100 SATA ports are causing slower performance here as well.\n\n**TRUENAS SCALE**\n\nFor TrueNAS Scale I created a six disk RAIDZ2 pool and did a 1TB file transfer over 2.5GbE as well as removing a disk and then performing a resilver after that 1TB of data was written.\n\n    File Transfer 1TB over 2.5GbE:\n    - N100:       80 Minutes\n    - N100 Orico: 78 Minutes\n    - N5105:      83 Minutes\n    Resilver 1TB Data:\n    - N100:       47 Minutes\n    - N100 Orico: 38 Minutes\n    - N5105:      38 Minutes\n\nHere again, it seems the onboard SATA port resulted in reduced performance compared with the N5105 and Orico M.2 adapter.\n\n**POWER DRAW**\n\nPower draw with a basic configuration of 1x M.2 PCIe SSD, 16GB RAM, 1x Ethernet cable connected, using a 500W EVGA Gold PSU resulted in about 20W while idle, and N100 would peak at about 40W under load, while the N5105 peaked at 30W power draw from the wall.\n\n**FINAL THOUGHTS**\n\nIf you're on a budget and looking for a NAS motherboard to support over the traditional 2 or 4 SATA ports that are usually offered on most ITX motherboards, these offer a good option. The reduced SATA performance of the N100 is a bit of a head scratcher considering both the N100 and N5105 use the same JMicron JMB585 controller chip. But the N100 does offer the 1x PCIe slot and general performance was slightly faster. So I guess it depends on what you're looking for.\n\nWhile I thought it might be just this specific board, the one I had to RMA also exhibited a similar result. Not sure if other vendor boards have the same issue or not.\n\nSo, I help this info was useful. You'll probably find more details in the video, but I wouldn't want to make anyone listen to my mumblings if they don't have to.", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "N100 and N5105 ITX NAS Motherboard Review (six onboard SATA ports, two M.2 slots)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bedpcy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Review", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1710434180.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710393813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many users prefer to have a compact NAS unit, which usually means if you&amp;#39;re building your own, the use of a Mini ITX motherboard.&lt;/p&gt;\n\n&lt;p&gt;This can typically limit expansion options, unless you&amp;#39;re willing to pay a significant fee for a higher end motherboard like the CWWK which is pretty full featured but also costs about $450USD: &lt;a href=\"https://cwwk.net/products/cwwk-amd-7735hs-7840hs-8845hs-7940hs-8-bay-9-bay-nas-usb4-40g-rate-8k-display-4-network-2-5g-9-sata-pcie-x16-itx-motherboard\"&gt;https://cwwk.net/products/cwwk-amd-7735hs-7840hs-8845hs-7940hs-8-bay-9-bay-nas-usb4-40g-rate-8k-display-4-network-2-5g-9-sata-pcie-x16-itx-motherboard&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While looking at AliExpress, I came across some options that included an N100 and N5105 CPU and included six SATA ports and two M.2 slots and four 2.5GbE ports. I ended up picking up both versions, N5105 from AliExpress, N100 from Amazon.&lt;/p&gt;\n\n&lt;p&gt;The two units I purchased, both ~ $125USD:&lt;/p&gt;\n\n&lt;p&gt;N5105: &lt;a href=\"https://www.aliexpress.us/item/3256805947799076.html\"&gt;https://www.aliexpress.us/item/3256805947799076.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;N100: &lt;a href=\"https://www.amazon.com/dp/B0CQZH8X2P\"&gt;https://www.amazon.com/dp/B0CQZH8X2P&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Full disclosure, after I had started some testing on the N100 board, it started showing issues, An ethernet controller would disappear, then I&amp;#39;d get phantom lockups. I also noticed that while the N5105 SATA chip had a heatsink on it, the N100 did not even though it has holes to mount one. Thankfully this is the one I bought from Amazon so issued an RMA and they promptly shipped me a new board which seemed to work perfectly fine throughout the testing.&lt;/p&gt;\n\n&lt;p&gt;I posted a review video if you&amp;#39;re interested, but most of the pertinent info is below: &lt;a href=\"https://youtu.be/PO8Kfi4qpY8?si=9AuYTaGZmmMfM5NG\"&gt;https://youtu.be/PO8Kfi4qpY8?si=9AuYTaGZmmMfM5NG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;COMPONENTS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;They both offer:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;two 10Gbps USB3 Type A ports&lt;/li&gt;\n&lt;li&gt;two M.2 SATA ports&lt;/li&gt;\n&lt;li&gt;four 2.5GbE ports managed by the Intel I226-V chip&lt;/li&gt;\n&lt;li&gt;six onboard SATA ports with the JMicron JMB585 controller&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Unique to each:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;one DDR5 So-DIMM (N100)&lt;/li&gt;\n&lt;li&gt;two DDR4 So-DIMM (N5105)&lt;/li&gt;\n&lt;li&gt;one PCIe 1x slot (N100)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That SATA controller supports up to 5 SATA III ports so I can only imagine the other one is provided by the CPU. The N5105 spec indicates that it can support two SATA ports, but the N100 specs weren&amp;#39;t clear.&lt;/p&gt;\n\n&lt;p&gt;The N100 has a single DDR5 So-DIMM slot that supports up to 16GB, that limit is apparently enforced by the CPU design. I don&amp;#39;t have a 32GB DDR5 So-DIMM otherwise I&amp;#39;d see if it actually can support it. The N5105 has two DDR4 slots, and like the N100 is limited to 16GB total RAM. I did insert a single 16GB chip in one slot, but it wouldn&amp;#39;t boot. But two 8GB or single 8GB worked just fine.&lt;/p&gt;\n\n&lt;p&gt;One unique thing about the N100 board is it offers a PCIe 1x slot. The N100 supports 9 PCIe 3.0 lanes, whereas the N5105 only 8, which is likely the reason it&amp;#39;s not on the N5105 version. That slot is open ended so you can add longer cards. The only caveat is that the card has to slot between the two rows of SATA ports. It fits fine, as I plugged in an RX6400 and GTX 1050 Ti video card, but you can&amp;#39;t use clipped SATA connectors because the clip will overlap into the area where the PCIe card wants to fit. Plus you will need 90 degree right angle connectors on the one side to avoid hitting any protruding part off the PCIe card.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OS INSTALLATION&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I installed five operating systems on each motherboard:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Windows 11&lt;/li&gt;\n&lt;li&gt;Ubuntu&lt;/li&gt;\n&lt;li&gt;OpenMediaVault&lt;/li&gt;\n&lt;li&gt;TrueNAS Scale&lt;/li&gt;\n&lt;li&gt;UnRAID&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Installation of the Linux based OS&amp;#39;s went perfectly fine. Windows 11, on the other hand was lacking many devices, but most importantly were the Intel I226-V 2.5GbE drivers, so you couldn&amp;#39;t even connect to the internet without them. This can be problematic because Windows likes to force you on the internet during install. But a nice little workaround I found was to use &lt;code&gt;SHIFT-F10&lt;/code&gt; which will bring up a console window and then type &lt;code&gt;oobe\\bypassnro&lt;/code&gt; reboot, and then you will get an option to install without internet, all the while trying to make you feel bad about yourself for not commiting your email and soul to Microsoft.&lt;/p&gt;\n\n&lt;p&gt;Once I got up and running, I loaded drivers from a USB (&lt;a href=\"https://intel.com/content/www/us/en/download/15084/intel-ethernet-adapter-complete-driver-pack.html\"&gt;https://intel.com/content/www/us/en/download/15084/intel-ethernet-adapter-complete-driver-pack.html&lt;/a&gt;), and then performed the Windows update marathon. The N5105 was missing several drivers still, but I did find drivers on Gigabytes Website. I needed the chipset drivers from here: &lt;a href=\"https://gigabyte.com/Motherboard/N5105I-H-rev-10/support#support-dl-driver-chipset\"&gt;https://gigabyte.com/Motherboard/N5105I-H-rev-10/support#support-dl-driver-chipset&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For the N100, I used the same I226-V drivers from the USB, and after updates there was just some Audio driver missing which was not so easy to track down. I did manage to get it from here: &lt;a href=\"https://catalog.update.microsoft.com/Search.aspx?q=10.29.0.9677+media\"&gt;https://catalog.update.microsoft.com/Search.aspx?q=10.29.0.9677+media&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But then after installing that, another audio or SM Bus Driver was still missing which I managed to get from the tenforums website, which linked to a Google Drive download. Sure, a bit shady, but this motherboard was already from AliExpress out of China, so I&amp;#39;ve probably already compromised my identity at this point. But seriously, I scanned it for viruses and it came up clean. You can grab it here: &lt;a href=\"https://www.tenforums.com/sound-audio/182081-latest-realtek-hd-audio-driver-version-3-a-103.html\"&gt;https://www.tenforums.com/sound-audio/182081-latest-realtek-hd-audio-driver-version-3-a-103.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So with everything up and running I ran a multitude of tests on the different components.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;WINDOWS SYSTEM BENCHMARKS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For general system tests, I ran Cinebench R23 in Windows and tracked the CPU usage, temps, power, etc. Nothing out of the ordinary. If you&amp;#39;re interested results were:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;N5105 Single CPU: 577\nN100 Single CPU:  886\nN5105 Multi Core: 1990\nN100 Multi Core:  2504\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Both CPU temps hovered in the upper 70&amp;#39;s, but after the re-paste, the N100 dropped by about 20C and the N5105 by about 10C.&lt;/p&gt;\n\n&lt;p&gt;I also ran Handbrake encoding test of a 4k60 10 minute video using the Handbrake &amp;quot;1080p Fast&amp;quot; default setting which encodes to 1080p/30. The results were as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;N5105 QSV:  32.4 minutes\nN5105 CPU:  39.7 minutes\nN100 QSV:   21.2 minutes\nN100 CPU:   28.6 minutes\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So anywhere from 20-40 minutes for a 10 minute video. Not too impressive.&lt;/p&gt;\n\n&lt;p&gt;I also fired up a Plex media server on each motherboard, and it served up to four 4k videos just fine as long as they were native resolution and format. I mean, that&amp;#39;s just a bandwidth thing.&lt;/p&gt;\n\n&lt;p&gt;But when it came to transcoding, forget it. I tried to transcode a single 4k/60 video to 1080p/30 and it would take up to a minute to encode about 15-20 seconds of video. So it would constantly buffer with the CPU running at full tilt 100% utilization.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2.5GbE INTEL I226-V ETHERNET PORTS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For the 2.5GbE Ethernet ports, I did a basic 10x 1GB file copy test and measured the resultant performance. They all performed about 270-280 MB/sec read and write. For some reason the N5105 in Windows write test was only about 240 MB/sec, but up to about 275MB/sec with read. Other OS&amp;#39;s it performed as expected. So not sure what to make of that other than Windows being Windows.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;M.2 and USB&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For M.2 and USB ports I ran CrystalDiskMark (Windows), KDiskMark (Ubuntu), hdparm -t read test (Linux OS&amp;#39;s), and a 10x 1GB file copy.&lt;/p&gt;\n\n&lt;p&gt;Bottom line, The M.2 and PCIe slot are definitely PCIe 1x (3.0). CrystalDiskMark, KDiskMark, and &lt;code&gt;hdparm -t&lt;/code&gt; tests resulted in about 850-900 MB/sec sequential read/write. During the actual 10x 1GB file transfer tests, the N5105 faltered a bit running at only about 650 MB/sec in OMV, TrueNAS, and UnRAID.&lt;/p&gt;\n\n&lt;p&gt;The USB ports actually performed better than the M.2 slots running over 1000 MB/sec with the artificial CrystalDiskMark/KDiskMark sequential and &lt;code&gt;hdparm -t&lt;/code&gt; tests. However, real world file transfers were all over the place. But that seems par for the course for USB.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SATA PORTS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Now when it comes to the SATA ports, both motherboards use the JMicron JMB585 controller. This chip provides support for up to 5 SATA III (600 MB/sec) ports. Considering there are six SATA ports, I believe on comes from the CPU.&lt;/p&gt;\n\n&lt;p&gt;Oddly enough, The N100 SATA ports seemed to be limiting overall performance. Connecting a single Samsung 870 Evo 2.5&amp;quot; SATA SSD to each port, it only resulted in about 430 MB/sec on five of the six ports. The sixth port managed about 550 MB/sec which is about max performance of this SSD when connected to a traditional desktop SATA port (where it hits 560MB/sec). The N5105 on the other hand performed at about 550 MB/sec.&lt;/p&gt;\n\n&lt;p&gt;I also used an Orico M.2 Six SATA port adapter that uses the ASMedia ASM 1166 controller as kind of a control sample, because I know it performs at expected speeds. The Orico M.2 in both the N100 and N5105 performed as well as in a traditional desktop. So there is some limitation there.&lt;/p&gt;\n\n&lt;p&gt;While this may not seem concerning if you&amp;#39;re using hard drives because they only tend to run at about 250 MB/sec or slower, SSD&amp;#39;s could be problematic. But worse is the RAID performance.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OPENMEDIAVAULT&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I set up a few scenarios, but I&amp;#39;ll only discuss the 6x RAID 0 and 12x RAID 60 (OMV) / Two 6x RAID Z2 VDevs (TrueNAS). I used ST500DM002 500GB SATA hard drives which performs at about 200 MB/sec sequential speeds when empty. A 6x RAID 0 should offer over 1000 MB/sec with these.&lt;/p&gt;\n\n&lt;p&gt;With the 6x RAID 0, the N100 only offered up about 500 MB/sec. On the N5105 it hit over 1000 MB/sec.&lt;/p&gt;\n\n&lt;p&gt;I also set up a 6x RAID 6 and 6x RAID 60. I built one RAID 6 at a time, then went back and built two RAID 6&amp;#39;s at a time to check if the system could handle it, then I merged them into an mdadm striped array for RAID 60.&lt;/p&gt;\n\n&lt;p&gt;Results from the RAID 6 build times:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Single RAID 6 Build Onboard SATA:\n- N100:  127 Minutes\n- N5105: 106 Minutes\nDual RAID 6 onboard SATA:\n- N100:  145 Minutes\n- N5105: 106 Minutes\nDual RAID 6 Orico M.2 Adapter:\n- N100:  114 Minutes\n- N5105: 106 Minutes\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So you can see that the N5105 handled the RAID 6 single build and when building two RAID 6 arrays simultaneously, without a hitch. The N100 took quite a bit longer.&lt;/p&gt;\n\n&lt;p&gt;Regarding CPU usage during the builds, both hit about 50% CPU utilization throughout with the 15 minute load average peaking at about 4, although the N5105 jumped up to about 70% utilization and 15 minute load average of about 4.5 for a brief period. Either way, it seemed the system could handle it just fine.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;UnRAID&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For UnRAID I set up a 4x Data Disk + 2x Parity Disk scenario and measured the performance of a build, as well as a parity check. Results as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Initial Sync Onboard SATA:\n- N100:  77 Minutes\n- N5105: 53 Minutes\nInitial Sync Orico M.2 Adapter:\n- N100:  53 Minutes\n- N5105: 53 Minutes\nParity Check Onboard SATA:\n- N100:  93 Minutes\n- N5105: 54 Minutes\nParity Check Orico M.2 Adapter:\n- N100:  60 Minutes\n- N5105: 54 Minutes\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So it appears the N100 SATA ports are causing slower performance here as well.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TRUENAS SCALE&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For TrueNAS Scale I created a six disk RAIDZ2 pool and did a 1TB file transfer over 2.5GbE as well as removing a disk and then performing a resilver after that 1TB of data was written.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;File Transfer 1TB over 2.5GbE:\n- N100:       80 Minutes\n- N100 Orico: 78 Minutes\n- N5105:      83 Minutes\nResilver 1TB Data:\n- N100:       47 Minutes\n- N100 Orico: 38 Minutes\n- N5105:      38 Minutes\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Here again, it seems the onboard SATA port resulted in reduced performance compared with the N5105 and Orico M.2 adapter.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;POWER DRAW&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Power draw with a basic configuration of 1x M.2 PCIe SSD, 16GB RAM, 1x Ethernet cable connected, using a 500W EVGA Gold PSU resulted in about 20W while idle, and N100 would peak at about 40W under load, while the N5105 peaked at 30W power draw from the wall.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;FINAL THOUGHTS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re on a budget and looking for a NAS motherboard to support over the traditional 2 or 4 SATA ports that are usually offered on most ITX motherboards, these offer a good option. The reduced SATA performance of the N100 is a bit of a head scratcher considering both the N100 and N5105 use the same JMicron JMB585 controller chip. But the N100 does offer the 1x PCIe slot and general performance was slightly faster. So I guess it depends on what you&amp;#39;re looking for.&lt;/p&gt;\n\n&lt;p&gt;While I thought it might be just this specific board, the one I had to RMA also exhibited a similar result. Not sure if other vendor boards have the same issue or not.&lt;/p&gt;\n\n&lt;p&gt;So, I help this info was useful. You&amp;#39;ll probably find more details in the video, but I wouldn&amp;#39;t want to make anyone listen to my mumblings if they don&amp;#39;t have to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Gu9c7wUGU3T0JiWZeSvNazcxGVYHLELNLzBDwDrGmpQ.jpg?auto=webp&amp;s=b1719e364960d1209d01298365411903e417f4af", "width": 630, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Gu9c7wUGU3T0JiWZeSvNazcxGVYHLELNLzBDwDrGmpQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f01ad4d90d44de19b0001f889fdd2bd66c6c0a6", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Gu9c7wUGU3T0JiWZeSvNazcxGVYHLELNLzBDwDrGmpQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4df06dd68f6ad3bf403acea5a1e2e2e1a94cc2f3", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Gu9c7wUGU3T0JiWZeSvNazcxGVYHLELNLzBDwDrGmpQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=450786013f0151cdb98675ab2348999df371f7c3", "width": 320, "height": 320}], "variants": {}, "id": "zCN__0HTx6e3xf9TrbNjCiswyNmCoW98tZM3-MTLtoo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1bedpcy", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bedpcy/n100_and_n5105_itx_nas_motherboard_review_six/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bedpcy/n100_and_n5105_itx_nas_motherboard_review_six/", "subreddit_subscribers": 738612, "created_utc": 1710393813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This might be a stupid question and if so I apologize. I've tried googling it but can't seem to find the answer. My understanding is that in Windows 10 a long format identifies bad sectors. Isn't that essentially what a SMART long test tests for as well? What is the difference, any why is a SMART test preferable?", "author_fullname": "t2_7g06zcjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not sure of the difference between a long SMART test and a long format.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1be5ylr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710371506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This might be a stupid question and if so I apologize. I&amp;#39;ve tried googling it but can&amp;#39;t seem to find the answer. My understanding is that in Windows 10 a long format identifies bad sectors. Isn&amp;#39;t that essentially what a SMART long test tests for as well? What is the difference, any why is a SMART test preferable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1be5ylr", "is_robot_indexable": true, "report_reasons": null, "author": "ZealousidealAd9428", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1be5ylr/not_sure_of_the_difference_between_a_long_smart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1be5ylr/not_sure_of_the_difference_between_a_long_smart/", "subreddit_subscribers": 738612, "created_utc": 1710371506.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}