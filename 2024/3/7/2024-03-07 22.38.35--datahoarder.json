{"kind": "Listing", "data": {"after": "t3_1b8jngj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Small Radios Big Televisions* is being delisted in a few weeks.\n\nThere is also a comment from the game dev of *Fist Puncher:* \" I'm one of the creators and developers of Fist Puncher which was also published by Adult Swim on Steam. We received the same notice from Warner Bros. that Fist Puncher would be retired.\"\n\n[https://arstechnica.com/gaming/2024/03/its-kind-of-depressing-wb-discovery-pulls-indie-game-for-business-changes/?comments=1&amp;comments-page=2](https://arstechnica.com/gaming/2024/03/its-kind-of-depressing-wb-discovery-pulls-indie-game-for-business-changes/?comments=1&amp;comments-page=2)", "author_fullname": "t2_ropw64r3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This is gonna be overshadowed by Rooster Teeth, but Adult Swim Games are being delisted....", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8hvg4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 160, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 160, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709775737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Small Radios Big Televisions&lt;/em&gt; is being delisted in a few weeks.&lt;/p&gt;\n\n&lt;p&gt;There is also a comment from the game dev of &lt;em&gt;Fist Puncher:&lt;/em&gt; &amp;quot; I&amp;#39;m one of the creators and developers of Fist Puncher which was also published by Adult Swim on Steam. We received the same notice from Warner Bros. that Fist Puncher would be retired.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/gaming/2024/03/its-kind-of-depressing-wb-discovery-pulls-indie-game-for-business-changes/?comments=1&amp;amp;comments-page=2\"&gt;https://arstechnica.com/gaming/2024/03/its-kind-of-depressing-wb-discovery-pulls-indie-game-for-business-changes/?comments=1&amp;amp;comments-page=2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?auto=webp&amp;s=f8949da775fab798e377b9d4bda0eacdede93ab3", "width": 760, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c6fce87c2d1623d509d6631bd26e1e0a18f78d8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ca244099ee3e5eaa99da45934bbe88d66517244b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfba3aab374ae635218d146cf5c47c9298c2a6ea", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c1185bf8619f97099896dd10f5afc211b661cf4", "width": 640, "height": 320}], "variants": {}, "id": "y0PD7u2rWaG0Pe2POVjasiROH_byp5EUih62OTHGey8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8hvg4", "is_robot_indexable": true, "report_reasons": null, "author": "MiopicDream", "discussion_type": null, "num_comments": 15, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8hvg4/this_is_gonna_be_overshadowed_by_rooster_teeth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8hvg4/this_is_gonna_be_overshadowed_by_rooster_teeth/", "subreddit_subscribers": 737071, "created_utc": 1709775737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have several WD Reds, WD Red Pros, and HGST Ultrastar He8 8TB hard drives running 24/7 for almost 8 years. Is it time to replace them?\n\nDo we know if there is going to be a WD Red Pro 24TB released this year?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/e509mgvzpsmc1.png?width=1274&amp;format=png&amp;auto=webp&amp;s=be051d31d85cc6b00cd7a152e616aa34ede04a76", "author_fullname": "t2_44voo4fk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When do I replace my drives? (7+ years)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"e509mgvzpsmc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f4b0c3e1a38bed632b16490750176ee7168d626"}, {"y": 77, "x": 216, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f0d81366bb5c11a3c7a6ee4566482399578c6747"}, {"y": 115, "x": 320, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=809f75a9a3c3684763cf124886d85c48454dfe76"}, {"y": 231, "x": 640, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c5fcadcbb6c17a7b3c8f73a22bd4454bf9ea2c9"}, {"y": 346, "x": 960, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4aeb830499067c0af98015d3798877825229a8da"}, {"y": 389, "x": 1080, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac7f92bc7250a4d0520e610f4fb4648f7e7bc480"}], "s": {"y": 460, "x": 1274, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=1274&amp;format=png&amp;auto=webp&amp;s=be051d31d85cc6b00cd7a152e616aa34ede04a76"}, "id": "e509mgvzpsmc1"}}, "name": "t3_1b8ee7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3PxXpllm0CgcNPzqAo2BbFtI84F4N6xSG66AMfo27Xg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709766650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several WD Reds, WD Red Pros, and HGST Ultrastar He8 8TB hard drives running 24/7 for almost 8 years. Is it time to replace them?&lt;/p&gt;\n\n&lt;p&gt;Do we know if there is going to be a WD Red Pro 24TB released this year?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e509mgvzpsmc1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be051d31d85cc6b00cd7a152e616aa34ede04a76\"&gt;https://preview.redd.it/e509mgvzpsmc1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be051d31d85cc6b00cd7a152e616aa34ede04a76&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8ee7w", "is_robot_indexable": true, "report_reasons": null, "author": "tastethebean", "discussion_type": null, "num_comments": 37, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8ee7w/when_do_i_replace_my_drives_7_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8ee7w/when_do_i_replace_my_drives_7_years/", "subreddit_subscribers": 737071, "created_utc": 1709766650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have nearly 5 terabytes of games, websites, music, documents and software on my Linux RAID. What is a good way to efficiently search through this mess? Simple file search (say via \\`find\\`) takes forever and isn't very fast when I enable regexes. Another type of query I want to perform is about the content of the files, I use \\`silversearcher-ag\\` for this and its reasonably fast but still a pain to use and sometimes very slow.\n\nIs there any tool that can index this properly and be better at searching both files with certain name and/or files with certain data. Bonus points if it has some web ui so that I can make it available over the network.", "author_fullname": "t2_bhm4t9j3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you efficiently search through your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8wqyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709823646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have nearly 5 terabytes of games, websites, music, documents and software on my Linux RAID. What is a good way to efficiently search through this mess? Simple file search (say via `find`) takes forever and isn&amp;#39;t very fast when I enable regexes. Another type of query I want to perform is about the content of the files, I use `silversearcher-ag` for this and its reasonably fast but still a pain to use and sometimes very slow.&lt;/p&gt;\n\n&lt;p&gt;Is there any tool that can index this properly and be better at searching both files with certain name and/or files with certain data. Bonus points if it has some web ui so that I can make it available over the network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8wqyd", "is_robot_indexable": true, "report_reasons": null, "author": "Emergency_Apricot_77", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8wqyd/how_do_you_efficiently_search_through_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8wqyd/how_do_you_efficiently_search_through_your_data/", "subreddit_subscribers": 737071, "created_utc": 1709823646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently been backing up some of my favourite videos/channels and wondered how much you can download in a day without getting blocked by Google/YouTube (or throttled I guess?)\n\nHas anyone ever ran into this issue? I'm not talking about using multiple instances of Youtube-dl, YT-DLP either, just using one command to download an entire channel/playlist. \n\nI guess my question(s) are/is:\n\n1. What's the most amount of videos/GB you've downloaded in a single day / few hours?\n2. Has anyone ever had any issues due to downloading too much via YT-DLP/youtube-dl?", "author_fullname": "t2_hinnm58s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the most you've downloaded with YT-DLP before?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8cq5v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709762729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently been backing up some of my favourite videos/channels and wondered how much you can download in a day without getting blocked by Google/YouTube (or throttled I guess?)&lt;/p&gt;\n\n&lt;p&gt;Has anyone ever ran into this issue? I&amp;#39;m not talking about using multiple instances of Youtube-dl, YT-DLP either, just using one command to download an entire channel/playlist. &lt;/p&gt;\n\n&lt;p&gt;I guess my question(s) are/is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What&amp;#39;s the most amount of videos/GB you&amp;#39;ve downloaded in a single day / few hours?&lt;/li&gt;\n&lt;li&gt;Has anyone ever had any issues due to downloading too much via YT-DLP/youtube-dl?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8cq5v", "is_robot_indexable": true, "report_reasons": null, "author": "WonderingWhenSayHi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8cq5v/whats_the_most_youve_downloaded_with_ytdlp_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8cq5v/whats_the_most_youve_downloaded_with_ytdlp_before/", "subreddit_subscribers": 737071, "created_utc": 1709762729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "You may recall around Black Friday these drives went on sale briefly for around $335 or so... Newegg did, I believe Amazon matched and a few other stores.  I ordered one from Newegg and many were delayed.   I got the one from Newegg about 2 weeks later.   Not too bad, IMO.   But for my NAS I needed two and I scrambled and they had gone out of stock literally just as I had ordered from Newegg.   So I ordered from another store that had lowered their prices to match.   Well, they kept delaying and delaying, but sending emails.  Well, two weeks ago they sent an email saying it was discontinued and cancelled the order.   But it's not discontinued.  It's this drive:  WD221KFGX     Or more specifically, this one:  WD221KFGX-SPB9KN0  \n\n\nSo clue me in... am I wrong?  Is this discontinued?    It's in stock on their site,  WD seems to think it's alive and well on their product page.  But when I pointed that out, the store pointed to their SKU, not the one above.   But the email they sent points to this SKU as well as their own (both are on it).   The only place on the entire internet with the SKU they are using is their own store.    So I asked for them to explain that.    Hard for me to believe they are getting a sweet special SKU from WD when even Amazon doesn't.   ...and what exactly could be different.  It's almost certainly just their own item number.  \n\n If you are curious to know, it's Adorama.     Now I know Adorama is a mixed blessing at best, but still, dragging me for two months then cancelling the order is actually worse than just cancelling outright.  So I want them to honor their discount.   Or clarify their position.   I mean, it's about $80, give or take.  At this point I'm just pissed at them.   \n\nFeel free to offer any helpful thoughts.      It's late, I have work, I'm going to bed, so I'll check back in the morning so my apologies if I don't comment immediately.   I'll update though if I get feedback from their CS dept and people are interested.   ", "author_fullname": "t2_5jxdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not sure if anyone else had this experience, but a certain store is not wanting to honor their sale price for a 22TB internal WD Red Pro NAS drive.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8l5iw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709784937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You may recall around Black Friday these drives went on sale briefly for around $335 or so... Newegg did, I believe Amazon matched and a few other stores.  I ordered one from Newegg and many were delayed.   I got the one from Newegg about 2 weeks later.   Not too bad, IMO.   But for my NAS I needed two and I scrambled and they had gone out of stock literally just as I had ordered from Newegg.   So I ordered from another store that had lowered their prices to match.   Well, they kept delaying and delaying, but sending emails.  Well, two weeks ago they sent an email saying it was discontinued and cancelled the order.   But it&amp;#39;s not discontinued.  It&amp;#39;s this drive:  WD221KFGX     Or more specifically, this one:  WD221KFGX-SPB9KN0  &lt;/p&gt;\n\n&lt;p&gt;So clue me in... am I wrong?  Is this discontinued?    It&amp;#39;s in stock on their site,  WD seems to think it&amp;#39;s alive and well on their product page.  But when I pointed that out, the store pointed to their SKU, not the one above.   But the email they sent points to this SKU as well as their own (both are on it).   The only place on the entire internet with the SKU they are using is their own store.    So I asked for them to explain that.    Hard for me to believe they are getting a sweet special SKU from WD when even Amazon doesn&amp;#39;t.   ...and what exactly could be different.  It&amp;#39;s almost certainly just their own item number.  &lt;/p&gt;\n\n&lt;p&gt;If you are curious to know, it&amp;#39;s Adorama.     Now I know Adorama is a mixed blessing at best, but still, dragging me for two months then cancelling the order is actually worse than just cancelling outright.  So I want them to honor their discount.   Or clarify their position.   I mean, it&amp;#39;s about $80, give or take.  At this point I&amp;#39;m just pissed at them.   &lt;/p&gt;\n\n&lt;p&gt;Feel free to offer any helpful thoughts.      It&amp;#39;s late, I have work, I&amp;#39;m going to bed, so I&amp;#39;ll check back in the morning so my apologies if I don&amp;#39;t comment immediately.   I&amp;#39;ll update though if I get feedback from their CS dept and people are interested.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b8l5iw", "is_robot_indexable": true, "report_reasons": null, "author": "0ttr", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8l5iw/not_sure_if_anyone_else_had_this_experience_but_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8l5iw/not_sure_if_anyone_else_had_this_experience_but_a/", "subreddit_subscribers": 737071, "created_utc": 1709784937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm scanning some old books and I'm struggling with \"huge\" PDF file sizes, i.e. ~ 200 MB.\n\nThis onto itself isn't that big of an issue except that my PDF editor struggles to edit / OCR such large PDFs.\n\nI've tried to use JPEG2000 instead of regular JPEG when making the PDFs but it was too much trouble for nearly no significant gain. I'm currently scanning the books at 300 DPI and saving the scanned files as JPEGs with a quality setting of 80.\n\nPart of me wants to try newer formats like HEIC, JXL and WebP but those are not acceptable inside PDFs from what I've understood from the PDF spec.\n\nSo, is there any other file format I could use? I just want a single file output that contains all the scanned pages in high quality plus OCR.", "author_fullname": "t2_1oy24ohu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File format for archiving scanned books?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8ft5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709770221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m scanning some old books and I&amp;#39;m struggling with &amp;quot;huge&amp;quot; PDF file sizes, i.e. ~ 200 MB.&lt;/p&gt;\n\n&lt;p&gt;This onto itself isn&amp;#39;t that big of an issue except that my PDF editor struggles to edit / OCR such large PDFs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to use JPEG2000 instead of regular JPEG when making the PDFs but it was too much trouble for nearly no significant gain. I&amp;#39;m currently scanning the books at 300 DPI and saving the scanned files as JPEGs with a quality setting of 80.&lt;/p&gt;\n\n&lt;p&gt;Part of me wants to try newer formats like HEIC, JXL and WebP but those are not acceptable inside PDFs from what I&amp;#39;ve understood from the PDF spec.&lt;/p&gt;\n\n&lt;p&gt;So, is there any other file format I could use? I just want a single file output that contains all the scanned pages in high quality plus OCR.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "noob (i.e. &lt; 1TB)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8ft5a", "is_robot_indexable": true, "report_reasons": null, "author": "gjvnq1", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b8ft5a/file_format_for_archiving_scanned_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8ft5a/file_format_for_archiving_scanned_books/", "subreddit_subscribers": 737071, "created_utc": 1709770221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First of all let me say that I've spent weeks digging through this sub's valuable information in terms of optical media archiving, and stumbled upon many of the Verbatim-CMC-HTL-LTH comments that were very insightful. However, I wasn't be able to read a definitive answer to the actual state of archiving data on off-the-shelf media in 2023.\n\nFirst and foremost, with all the *Blu-ray M-Disc not being real M-Discs anymore* things going on, what is a realistic expectation of data longevity, when discs are stored under normal everyday conditions (e.g. outside of the 10-25*C temperature and 40-60% humidity range)? Is it still worth considering optical media to get at least a decade of stress free archiving or does media produced in the recent years show significant fallout rates even after short periods of time? What's the actual return on invest on today's media, even speaking about regular \"cheap\" Blu-ray discs and what would be the recommended re-burn interval based on the little experience with post-2019 media that there might be?\n\nThank you!", "author_fullname": "t2_kigcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realistic expectation of Blu-ray M-Disc life in 2023 under everyday conditions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8xe10", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709825215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all let me say that I&amp;#39;ve spent weeks digging through this sub&amp;#39;s valuable information in terms of optical media archiving, and stumbled upon many of the Verbatim-CMC-HTL-LTH comments that were very insightful. However, I wasn&amp;#39;t be able to read a definitive answer to the actual state of archiving data on off-the-shelf media in 2023.&lt;/p&gt;\n\n&lt;p&gt;First and foremost, with all the &lt;em&gt;Blu-ray M-Disc not being real M-Discs anymore&lt;/em&gt; things going on, what is a realistic expectation of data longevity, when discs are stored under normal everyday conditions (e.g. outside of the 10-25*C temperature and 40-60% humidity range)? Is it still worth considering optical media to get at least a decade of stress free archiving or does media produced in the recent years show significant fallout rates even after short periods of time? What&amp;#39;s the actual return on invest on today&amp;#39;s media, even speaking about regular &amp;quot;cheap&amp;quot; Blu-ray discs and what would be the recommended re-burn interval based on the little experience with post-2019 media that there might be?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8xe10", "is_robot_indexable": true, "report_reasons": null, "author": "mrusme", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8xe10/realistic_expectation_of_bluray_mdisc_life_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8xe10/realistic_expectation_of_bluray_mdisc_life_in/", "subreddit_subscribers": 737071, "created_utc": 1709825215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nHello recently, i've bought a 2600$ online course, with different modules and videos with my friend, i paid for about 1000 among it and my friend payed the money left. My friend is the only one with the logins.\nAnd he dont want the staff to know that 2 people bought it, and potentially ban us  for it. \n\nWe would like to scrape the videos in the course and download it then transfer it to my pc in different folders so i can follow it peacefully, without having to lose a grand invested.\nI have no idea how to do that so far, and i havent found an answer googling it, but i belive it's doable, since i have seen similar things.\nCould a kind soul help me out, or redirect me to a ressource that learn me to do it?", "author_fullname": "t2_u9sssbeww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to scrape the videos of a private online course?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8e4my", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709766024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello recently, i&amp;#39;ve bought a 2600$ online course, with different modules and videos with my friend, i paid for about 1000 among it and my friend payed the money left. My friend is the only one with the logins.\nAnd he dont want the staff to know that 2 people bought it, and potentially ban us  for it. &lt;/p&gt;\n\n&lt;p&gt;We would like to scrape the videos in the course and download it then transfer it to my pc in different folders so i can follow it peacefully, without having to lose a grand invested.\nI have no idea how to do that so far, and i havent found an answer googling it, but i belive it&amp;#39;s doable, since i have seen similar things.\nCould a kind soul help me out, or redirect me to a ressource that learn me to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8e4my", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianFluid6425", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8e4my/how_to_scrape_the_videos_of_a_private_online/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8e4my/how_to_scrape_the_videos_of_a_private_online/", "subreddit_subscribers": 737071, "created_utc": 1709766024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In my university class we are doing a time capsule for our university to be opened 50 years from now. It is my job to figure out how to have data last that long. So far I've determined the best way to go about it would be to print our presentations on paper designed to last a long time. But in the event other presenters would like to have music or videos in their presentation I'd also like to prepare for a digital method that could last 50 years in a time capsule.\n\nFeedback would be greatly appreciated, thank you.", "author_fullname": "t2_2sjzxx2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ensure digital data lasts 50 years without interaction or additional cost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b963ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709847234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my university class we are doing a time capsule for our university to be opened 50 years from now. It is my job to figure out how to have data last that long. So far I&amp;#39;ve determined the best way to go about it would be to print our presentations on paper designed to last a long time. But in the event other presenters would like to have music or videos in their presentation I&amp;#39;d also like to prepare for a digital method that could last 50 years in a time capsule.&lt;/p&gt;\n\n&lt;p&gt;Feedback would be greatly appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b963ig", "is_robot_indexable": true, "report_reasons": null, "author": "PiggiesGoSqueal", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b963ig/how_to_ensure_digital_data_lasts_50_years_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b963ig/how_to_ensure_digital_data_lasts_50_years_without/", "subreddit_subscribers": 737071, "created_utc": 1709847234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Need to create a storage solution at remote sites. These will reside in awkward and/or smaller spaces without racks. A good-sized shed that's protected from the elements but no HVAC, for example. It will store surveillance footage, so 24/7 writing and occasional reading. One other caveat is that it has to run windows. Our VMS recording application is windows only. A pre-built NAS would obviously work but then it turns into two devices to manage instead of one. Since I'd still have a PC running the software. Because of that, I'd rather stick to a PC-only solution, but not married to it.\n\nOne option is simply building a PC with a bunch of spinners. But it needs some resiliency. I manage them, but I\u2019ve never built out a RAID device myself from scratch. I\u2019ve read not to use diskmgmt raid or storage spaces. Though some say if you're just running a mirror, no parity, storage spaces can be fine. This doesn't need to be fancy. A RAID 1 would satisfy the requirements. Maybe storage spaces is fine? Any other decent software raid options out there? Thoughts on Stablebit Drivepool? I would need the storage to show up in Windows as one drive.\n\nAnother option is to run a hardware raid card in the build. I've read that using some consumer level card like startech is basically useless. It looks like a decent LSI enterprise card with caching and BBU runs \\~$1,000? At that price, it may be better to get smaller Dell or HP server tower instead of trying to piece something together from scratch.\n\nAnd of course, the \"easiest\" solution is to just get a pre-built nas and call it day.\n\nKind of looking for a conversation here. Thoughts on the some of the assumptions made due to what I've read so far. Some real-world experience with any of the outlined options. Again, it's going to live in outdoor temperatures. Around here anywhere from 80-100 in the summer, down to 10-30 in the winter. Not really sure if any options are going to hold up better than others given that.", "author_fullname": "t2_qacpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CCTV Storage Solutions. Outside?!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b903d7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709838870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709832051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need to create a storage solution at remote sites. These will reside in awkward and/or smaller spaces without racks. A good-sized shed that&amp;#39;s protected from the elements but no HVAC, for example. It will store surveillance footage, so 24/7 writing and occasional reading. One other caveat is that it has to run windows. Our VMS recording application is windows only. A pre-built NAS would obviously work but then it turns into two devices to manage instead of one. Since I&amp;#39;d still have a PC running the software. Because of that, I&amp;#39;d rather stick to a PC-only solution, but not married to it.&lt;/p&gt;\n\n&lt;p&gt;One option is simply building a PC with a bunch of spinners. But it needs some resiliency. I manage them, but I\u2019ve never built out a RAID device myself from scratch. I\u2019ve read not to use diskmgmt raid or storage spaces. Though some say if you&amp;#39;re just running a mirror, no parity, storage spaces can be fine. This doesn&amp;#39;t need to be fancy. A RAID 1 would satisfy the requirements. Maybe storage spaces is fine? Any other decent software raid options out there? Thoughts on Stablebit Drivepool? I would need the storage to show up in Windows as one drive.&lt;/p&gt;\n\n&lt;p&gt;Another option is to run a hardware raid card in the build. I&amp;#39;ve read that using some consumer level card like startech is basically useless. It looks like a decent LSI enterprise card with caching and BBU runs ~$1,000? At that price, it may be better to get smaller Dell or HP server tower instead of trying to piece something together from scratch.&lt;/p&gt;\n\n&lt;p&gt;And of course, the &amp;quot;easiest&amp;quot; solution is to just get a pre-built nas and call it day.&lt;/p&gt;\n\n&lt;p&gt;Kind of looking for a conversation here. Thoughts on the some of the assumptions made due to what I&amp;#39;ve read so far. Some real-world experience with any of the outlined options. Again, it&amp;#39;s going to live in outdoor temperatures. Around here anywhere from 80-100 in the summer, down to 10-30 in the winter. Not really sure if any options are going to hold up better than others given that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b903d7", "is_robot_indexable": true, "report_reasons": null, "author": "sageRJ", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b903d7/cctv_storage_solutions_outside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b903d7/cctv_storage_solutions_outside/", "subreddit_subscribers": 737071, "created_utc": 1709832051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm in the process of rebuilding a media server. The common Plex, ARR stack with qBittorent. So seeding is also relevant, not just hardlinking. I will be using Docker for the stack. Full disclosure, I am new to pretty much all of it, Linux, mergerfs, etc. I have been reading docs extensively but am still confused on a couple concepts/config settings.\n\nThe point in the setup process I am at now is taking 2 storage HDDs with media and pooling them with mergerfs. Most mergerfs Google results seem to be from this sub so here I am!\n\nFor context, my mergerfs config looks like this:\n\n`/mnt/hdd1:/mnt/hdd2 /merge mergerfs cache.files=partial,dropcacheonclose=true,category.create=mfs`\n\n* /mnt/hdd1 - already has TV and Movies on it\n* /mnt/hdd2 - empty, but have set the same high level directory structure on it:\n   * /torrents\n      * /movies\n      * /tv\n   * /library\n      * /movies\n      * /tv\n\nThe part in the docs that made me start second guessing was reference to \"hardlinks do not work across branches in a pool\". I started thinking about what happens if mergerfs effectively puts the torrent file on one branch, but the Sonarr/Radarr hardlink on another branch (if that's even what it's doing)? *If this is completely irrelevant by just setting Sonarr/Radarr root to /merge then let me know.*\n\nI also want to avoid running into the issue mentioned in docs of \"all my files ending up on 1 filesystem?!\" since I have one drive that already has a bunch of files and directories on it, and one empty drive.\n\nSo, now I'm wondering if I need to modify my mergerfs options from the default to make sure hardlinks and seeding work.\n\n* Do I need to change to create policy to `epmfs`?\n* Do I need to create specific policies for something like tv series' to keep all seasons/episodes related to a show on the same branch?\n* Do I need to enable `moveonenospc`?\n* Would any of the other extensive options be relevant to this topic?\n\nAs far as Docker is concerned, my understanding is I need to mount the containers at the root /merge directory. *It's entirely possible that's all I need to do and I'm overthinking this.*", "author_fullname": "t2_1ajq44ub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make sure hardlinks and seeding work with mergerfs and Radarr/Sonarr?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b96ke4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709848329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in the process of rebuilding a media server. The common Plex, ARR stack with qBittorent. So seeding is also relevant, not just hardlinking. I will be using Docker for the stack. Full disclosure, I am new to pretty much all of it, Linux, mergerfs, etc. I have been reading docs extensively but am still confused on a couple concepts/config settings.&lt;/p&gt;\n\n&lt;p&gt;The point in the setup process I am at now is taking 2 storage HDDs with media and pooling them with mergerfs. Most mergerfs Google results seem to be from this sub so here I am!&lt;/p&gt;\n\n&lt;p&gt;For context, my mergerfs config looks like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;/mnt/hdd1:/mnt/hdd2 /merge mergerfs cache.files=partial,dropcacheonclose=true,category.create=mfs&lt;/code&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;/mnt/hdd1 - already has TV and Movies on it&lt;/li&gt;\n&lt;li&gt;/mnt/hdd2 - empty, but have set the same high level directory structure on it:\n\n&lt;ul&gt;\n&lt;li&gt;/torrents\n\n&lt;ul&gt;\n&lt;li&gt;/movies&lt;/li&gt;\n&lt;li&gt;/tv&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;/library\n\n&lt;ul&gt;\n&lt;li&gt;/movies&lt;/li&gt;\n&lt;li&gt;/tv&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The part in the docs that made me start second guessing was reference to &amp;quot;hardlinks do not work across branches in a pool&amp;quot;. I started thinking about what happens if mergerfs effectively puts the torrent file on one branch, but the Sonarr/Radarr hardlink on another branch (if that&amp;#39;s even what it&amp;#39;s doing)? &lt;em&gt;If this is completely irrelevant by just setting Sonarr/Radarr root to /merge then let me know.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I also want to avoid running into the issue mentioned in docs of &amp;quot;all my files ending up on 1 filesystem?!&amp;quot; since I have one drive that already has a bunch of files and directories on it, and one empty drive.&lt;/p&gt;\n\n&lt;p&gt;So, now I&amp;#39;m wondering if I need to modify my mergerfs options from the default to make sure hardlinks and seeding work.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do I need to change to create policy to &lt;code&gt;epmfs&lt;/code&gt;?&lt;/li&gt;\n&lt;li&gt;Do I need to create specific policies for something like tv series&amp;#39; to keep all seasons/episodes related to a show on the same branch?&lt;/li&gt;\n&lt;li&gt;Do I need to enable &lt;code&gt;moveonenospc&lt;/code&gt;?&lt;/li&gt;\n&lt;li&gt;Would any of the other extensive options be relevant to this topic?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As far as Docker is concerned, my understanding is I need to mount the containers at the root /merge directory. &lt;em&gt;It&amp;#39;s entirely possible that&amp;#39;s all I need to do and I&amp;#39;m overthinking this.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b96ke4", "is_robot_indexable": true, "report_reasons": null, "author": "GooeyDuck1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b96ke4/how_to_make_sure_hardlinks_and_seeding_work_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b96ke4/how_to_make_sure_hardlinks_and_seeding_work_with/", "subreddit_subscribers": 737071, "created_utc": 1709848329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello. I received this hard drive from a friend who bought it on ebay. The drive states it's a pre-production sample, but googling the model number doesn't provide any results. Does anyone know what model of hard drive this is a pre prod sample too? I've yet to get it to power on, but may be due to the 3.3 volt rail, still troubleshooting", "author_fullname": "t2_5vaqz7s0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 22TB \"Pre-production sample\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_1b95zv5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "domain": "i.redd.it", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/pucK1rJJsVfk4jWEpSrd0bP7gCJNr8-0IQCUuiIBbz0.jpg", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709847002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I received this hard drive from a friend who bought it on ebay. The drive states it&amp;#39;s a pre-production sample, but googling the model number doesn&amp;#39;t provide any results. Does anyone know what model of hard drive this is a pre prod sample too? I&amp;#39;ve yet to get it to power on, but may be due to the 3.3 volt rail, still troubleshooting&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/sxqkvx00dzmc1.jpeg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?auto=webp&amp;s=78871d0ada3111f7f622711c5c6faa35c90920db", "width": 2252, "height": 3776}, "resolutions": [{"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fbcee879ef51bf4f8b2f76c7ca7162890eb4566", "width": 108, "height": 181}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=248c073ecf7aa30cd1a6fbaec999a4663bf993c4", "width": 216, "height": 362}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=834e3354b38bb170fd0c77993ed23d84a157c3e7", "width": 320, "height": 536}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5efd78384da521ab375e112103deb8b9496ef9a", "width": 640, "height": 1073}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ceb69cb323788e00244b6374eb6f66c5114ee2c6", "width": 960, "height": 1609}, {"url": "https://preview.redd.it/sxqkvx00dzmc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49fe940f22b766c3fc08aef6f1efc412ca523d15", "width": 1080, "height": 1810}], "variants": {}, "id": "su-VYCZDt-hjhfHiJZ2juCLVAreXI3sNd1RI15mur8I"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b95zv5", "is_robot_indexable": true, "report_reasons": null, "author": "saints0963", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b95zv5/wd_22tb_preproduction_sample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/sxqkvx00dzmc1.jpeg", "subreddit_subscribers": 737071, "created_utc": 1709847002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on automating a script", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b95bof", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_ay7tp", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "scripting", "selftext": "I know nothing about coding but have messed around with plenty of command line tools. I'm looking to speed up some data hoarding. This is the script I use\n\n    ia search 'subject:\"lord of the rings\" collection:thingiverse' --itemlist &gt; lotr.txt\n    ia download --itemlist lotr.txt --no-directories -i --glob=\\*.zip\n\nNow how I'm currently doing it is manually changing the suject in question. So for example if I wanted to download things related to 'Tron' the script would be\n\n    ia search 'subject:\"tron\" collection:thingiverse' --itemlist &gt; tron.txt\n    ia download --itemlist tron.txt --no-directories -i --glob=\\*.zip\n\nI would like to speed things up by having a script or gui or something where I can just type 'Tron' and have the computer spit out a script I can then paste into terminal. Bonus points if I can have it mkdir then cd into that directory. I'm hoping this will speed things up and also be a learning experience so I can apply it to things like yt-dlp and other tools. This is all being done on linux mint or ubuntu btw", "author_fullname": "t2_ay7tp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on automating a script", "link_flair_richtext": [], "subreddit_name_prefixed": "r/scripting", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b95b2y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709845361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.scripting", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know nothing about coding but have messed around with plenty of command line tools. I&amp;#39;m looking to speed up some data hoarding. This is the script I use&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ia search &amp;#39;subject:&amp;quot;lord of the rings&amp;quot; collection:thingiverse&amp;#39; --itemlist &amp;gt; lotr.txt\nia download --itemlist lotr.txt --no-directories -i --glob=\\*.zip\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Now how I&amp;#39;m currently doing it is manually changing the suject in question. So for example if I wanted to download things related to &amp;#39;Tron&amp;#39; the script would be&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ia search &amp;#39;subject:&amp;quot;tron&amp;quot; collection:thingiverse&amp;#39; --itemlist &amp;gt; tron.txt\nia download --itemlist tron.txt --no-directories -i --glob=\\*.zip\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I would like to speed things up by having a script or gui or something where I can just type &amp;#39;Tron&amp;#39; and have the computer spit out a script I can then paste into terminal. Bonus points if I can have it mkdir then cd into that directory. I&amp;#39;m hoping this will speed things up and also be a learning experience so I can apply it to things like yt-dlp and other tools. This is all being done on linux mint or ubuntu btw&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh8s", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1b95b2y", "is_robot_indexable": true, "report_reasons": null, "author": "erik530195", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/scripting/comments/1b95b2y/looking_for_advice_on_automating_a_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/scripting/comments/1b95b2y/looking_for_advice_on_automating_a_script/", "subreddit_subscribers": 3882, "created_utc": 1709845361.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1709845400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.scripting", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/scripting/comments/1b95b2y/looking_for_advice_on_automating_a_script/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "244TB ZFS and Synology", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b95bof", "is_robot_indexable": true, "report_reasons": null, "author": "erik530195", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1b95b2y", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b95bof/looking_for_advice_on_automating_a_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/scripting/comments/1b95b2y/looking_for_advice_on_automating_a_script/", "subreddit_subscribers": 737071, "created_utc": 1709845400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many pics to scan. I had a software called I think HP easy scan that was very good by splitting one scan to several pics.\n\nNow I have a new HP printer (516) and the software I'm using doesn't split them. This is big downside and I don't know what to do. I can't go picture by picture, it's too much. If I can put 2-4 pictures in one scan I can handle it hahaha.\n\nThank you so much for the help!", "author_fullname": "t2_4xllun6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanning old pics to drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b90cvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709832667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many pics to scan. I had a software called I think HP easy scan that was very good by splitting one scan to several pics.&lt;/p&gt;\n\n&lt;p&gt;Now I have a new HP printer (516) and the software I&amp;#39;m using doesn&amp;#39;t split them. This is big downside and I don&amp;#39;t know what to do. I can&amp;#39;t go picture by picture, it&amp;#39;s too much. If I can put 2-4 pictures in one scan I can handle it hahaha.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b90cvg", "is_robot_indexable": true, "report_reasons": null, "author": "Direct_Check_3366", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b90cvg/scanning_old_pics_to_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b90cvg/scanning_old_pics_to_drive/", "subreddit_subscribers": 737071, "created_utc": 1709832667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm starting to learn about Synology/NAS but it seems like everything has to be accessed by Ethernet?  To me it would seem to slow things down (USB 3 at 5 Gbits/s is faster than usual Ethernet) and also adds another layer of potential failure (secondary concern).  \n\nI want to have Synology (or another NAS system) for the benefit of automatic redundancy management for hard drive failures.  However for me I just want it to function as a more reliable external hard drive. I don't want any automatic sync at least for a while because currently I have 2 decades worth of data stored and copied (it's a mess) in like 20 different external hard drives that I have to personally sort through.  Ideally I'm going to transfer things by wire/USB cable so it will be much faster than Ethernet as there is so much data I have to copy over and sort things a bit while copying over stuff.\n\nI don't mind it having Ethernet access, it would be nice to be able to see the files from other devices in home network, but to me this can be a bonus and not a main feature.\n\nSo is there a Synology or other NAS setup/system that would be the best for my scenario? Thanks!", "author_fullname": "t2_5dxtclyyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to have a simple NAS system that can be accessed by USB 3 cables offline if I want to? Functioning like a more reliable external hard drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8zlbu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709830862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting to learn about Synology/NAS but it seems like everything has to be accessed by Ethernet?  To me it would seem to slow things down (USB 3 at 5 Gbits/s is faster than usual Ethernet) and also adds another layer of potential failure (secondary concern).  &lt;/p&gt;\n\n&lt;p&gt;I want to have Synology (or another NAS system) for the benefit of automatic redundancy management for hard drive failures.  However for me I just want it to function as a more reliable external hard drive. I don&amp;#39;t want any automatic sync at least for a while because currently I have 2 decades worth of data stored and copied (it&amp;#39;s a mess) in like 20 different external hard drives that I have to personally sort through.  Ideally I&amp;#39;m going to transfer things by wire/USB cable so it will be much faster than Ethernet as there is so much data I have to copy over and sort things a bit while copying over stuff.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind it having Ethernet access, it would be nice to be able to see the files from other devices in home network, but to me this can be a bonus and not a main feature.&lt;/p&gt;\n\n&lt;p&gt;So is there a Synology or other NAS setup/system that would be the best for my scenario? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8zlbu", "is_robot_indexable": true, "report_reasons": null, "author": "AntarcticNightingale", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8zlbu/is_it_possible_to_have_a_simple_nas_system_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8zlbu/is_it_possible_to_have_a_simple_nas_system_that/", "subreddit_subscribers": 737071, "created_utc": 1709830862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I run RAID1 on Linux mdadm with spinning HDDs.\n\nI have started getting these warnings from mdstat:\n\n    mdstat mismatch cnt = 128 unsynchronized blocks\n\nDoes this mean that my disks/arrays are damaged for certain?\n\nI previously had this issue before. I could not figure out what was wrong but I ended up replacing the disks. \n\nNow after a short while, this happens again...\n\nI have tried initiating a resin. But this counter increases again after a while. Note that the value is a very round number of 128. It seems to be always a power of 2 value. On the previous disks, I think it was 256 or 512.\n\nSMART does not report anything of note. Neither on the old nor the new disks.", "author_fullname": "t2_nlx42kb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mdstat mismatch cnt: are my disks/data damaged?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8z4jy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709829765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run RAID1 on Linux mdadm with spinning HDDs.&lt;/p&gt;\n\n&lt;p&gt;I have started getting these warnings from mdstat:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;mdstat mismatch cnt = 128 unsynchronized blocks\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Does this mean that my disks/arrays are damaged for certain?&lt;/p&gt;\n\n&lt;p&gt;I previously had this issue before. I could not figure out what was wrong but I ended up replacing the disks. &lt;/p&gt;\n\n&lt;p&gt;Now after a short while, this happens again...&lt;/p&gt;\n\n&lt;p&gt;I have tried initiating a resin. But this counter increases again after a while. Note that the value is a very round number of 128. It seems to be always a power of 2 value. On the previous disks, I think it was 256 or 512.&lt;/p&gt;\n\n&lt;p&gt;SMART does not report anything of note. Neither on the old nor the new disks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8z4jy", "is_robot_indexable": true, "report_reasons": null, "author": "PM_Me_Food_Pics_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8z4jy/mdstat_mismatch_cnt_are_my_disksdata_damaged/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8z4jy/mdstat_mismatch_cnt_are_my_disksdata_damaged/", "subreddit_subscribers": 737071, "created_utc": 1709829765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 4TB Seagate drive with a multitude of issues and very conflicting test results. Basically, I have experienced data corruption, data loss, unexpected noises, and other issues that started out happening often and seriously enough to deem the HDD too unreliable for data storage, and yet, after completing the backup process and giving the HDD some time off (not using it at all, but keeping it turned on), the issues mysteriously improved dramatically, but the drive still feels off, as copied files were still corrupted during the copying process recently.  The SMART logs, Windows Dsk Chk, HD tune, are pretty much all green, as far as I can tell, and I was told these results seemed good on the TechSupport subreddit as well.\n\nBecause of these conflicting results, I would prefer to exchange my drive for a brand new one through Seagate's RMA process before the warranty expires, but I am worried if these conflicting results will cause issues with the RMA process. Does anyone have any experience with sending out a Seagate HDD to RMA, especially with more difficult to detect issues, or with less clear-cut HDD problems? If so, did they fix or send you a new HDD, or did they just returned it back to you without doing anything to fix, or resulted in having trouble/difficulties with Seagate due to them not detecting hardware defects?\n\nDue to the data loss (although only contained to newly-written files past the first 2TB at the time of the data writing) and all the issues experienced, I just can't trust this drive at all until I can find out what caused it to misbehave, so I vastly prefer for Seagate to send me a new one, since this is still covered by the warranty, but I have never RMA'ed a drive before, and since this isn't a clear-cut case, I'm worried if they will really come through, so it would be great to know if based on previous RMA experiences in this subreddit, if my attempt to RMA this untrustworthy and/or faulty drive will likely be successful or not.\n\nThanks!", "author_fullname": "t2_17ez6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Regarding Seagate's RMA process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8ywgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709835965.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709829019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 4TB Seagate drive with a multitude of issues and very conflicting test results. Basically, I have experienced data corruption, data loss, unexpected noises, and other issues that started out happening often and seriously enough to deem the HDD too unreliable for data storage, and yet, after completing the backup process and giving the HDD some time off (not using it at all, but keeping it turned on), the issues mysteriously improved dramatically, but the drive still feels off, as copied files were still corrupted during the copying process recently.  The SMART logs, Windows Dsk Chk, HD tune, are pretty much all green, as far as I can tell, and I was told these results seemed good on the TechSupport subreddit as well.&lt;/p&gt;\n\n&lt;p&gt;Because of these conflicting results, I would prefer to exchange my drive for a brand new one through Seagate&amp;#39;s RMA process before the warranty expires, but I am worried if these conflicting results will cause issues with the RMA process. Does anyone have any experience with sending out a Seagate HDD to RMA, especially with more difficult to detect issues, or with less clear-cut HDD problems? If so, did they fix or send you a new HDD, or did they just returned it back to you without doing anything to fix, or resulted in having trouble/difficulties with Seagate due to them not detecting hardware defects?&lt;/p&gt;\n\n&lt;p&gt;Due to the data loss (although only contained to newly-written files past the first 2TB at the time of the data writing) and all the issues experienced, I just can&amp;#39;t trust this drive at all until I can find out what caused it to misbehave, so I vastly prefer for Seagate to send me a new one, since this is still covered by the warranty, but I have never RMA&amp;#39;ed a drive before, and since this isn&amp;#39;t a clear-cut case, I&amp;#39;m worried if they will really come through, so it would be great to know if based on previous RMA experiences in this subreddit, if my attempt to RMA this untrustworthy and/or faulty drive will likely be successful or not.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8ywgm", "is_robot_indexable": true, "report_reasons": null, "author": "Xenofan23", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8ywgm/regarding_seagates_rma_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8ywgm/regarding_seagates_rma_process/", "subreddit_subscribers": 737071, "created_utc": 1709829019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Many years ago i manually copied all the photos and videos i had to a HDD. Ever since then i just cloned my hard drives with a clone dock and called it a day. Recently i found a external hard drive from my dad (which still works 12+ years later) which has a lot of pictures / videos i don't have already. So how would i go about copying those files onto my HDD?\n\nI thought doing the same thing i did before. Putting my drive into my drive bay and connecting the external drive through usb, then the old copy and paste. But is there a better option? A more secure option? A hopefully not expensive option? ", "author_fullname": "t2_kht4ovrc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying external HDD onto internal HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8w9nv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709822412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many years ago i manually copied all the photos and videos i had to a HDD. Ever since then i just cloned my hard drives with a clone dock and called it a day. Recently i found a external hard drive from my dad (which still works 12+ years later) which has a lot of pictures / videos i don&amp;#39;t have already. So how would i go about copying those files onto my HDD?&lt;/p&gt;\n\n&lt;p&gt;I thought doing the same thing i did before. Putting my drive into my drive bay and connecting the external drive through usb, then the old copy and paste. But is there a better option? A more secure option? A hopefully not expensive option? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8w9nv", "is_robot_indexable": true, "report_reasons": null, "author": "TengokuDaimakyo", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8w9nv/copying_external_hdd_onto_internal_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8w9nv/copying_external_hdd_onto_internal_hdd/", "subreddit_subscribers": 737071, "created_utc": 1709822412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On the tuto it says $ ia upload &lt;identifier&gt; file1 file2 --metadata=\"mediatype:texts\" --metadata=\"blah:arg\"\n\nBut does that upload the file to an existing archive?\n\nCan someone please show me how that line looks like normally? Because everything I try gives me an error. I know what the identifier is. I don't know how to write the file. Do I write the entire path? Do I include the .mp3? When I use &lt; and &gt; it says it was unexpected or something like that. So can you show me the line you would use to upload a file, or multiple, to an existing archive?\n\nEdit : And I can't (on the site itself) change the name of a file. It doesn't work. I change it, click on \"done editing\", and it doesn't change. What am I doing wrong?", "author_fullname": "t2_ymgr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Internet Archive) How to use the python to upload to existing archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8sakj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709810570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the tuto it says $ ia upload &amp;lt;identifier&amp;gt; file1 file2 --metadata=&amp;quot;mediatype:texts&amp;quot; --metadata=&amp;quot;blah:arg&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But does that upload the file to an existing archive?&lt;/p&gt;\n\n&lt;p&gt;Can someone please show me how that line looks like normally? Because everything I try gives me an error. I know what the identifier is. I don&amp;#39;t know how to write the file. Do I write the entire path? Do I include the .mp3? When I use &amp;lt; and &amp;gt; it says it was unexpected or something like that. So can you show me the line you would use to upload a file, or multiple, to an existing archive?&lt;/p&gt;\n\n&lt;p&gt;Edit : And I can&amp;#39;t (on the site itself) change the name of a file. It doesn&amp;#39;t work. I change it, click on &amp;quot;done editing&amp;quot;, and it doesn&amp;#39;t change. What am I doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8sakj", "is_robot_indexable": true, "report_reasons": null, "author": "PouffieEdc", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8sakj/internet_archive_how_to_use_the_python_to_upload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8sakj/internet_archive_how_to_use_the_python_to_upload/", "subreddit_subscribers": 737071, "created_utc": 1709810570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellow hoarders, long time lurker and first-time question asker here.\n\nI currently have \\~8tb of photos and video files from various travels, which I've been storing across a separate 2-bay NAS and a couple of RAID-1 drives on my PC. Once I've edited/printed/published the stuff that I want to keep, the rest is really just left in storage 'just in case', as I'm sure most of you understand!\n\nAnyway, storage space on these devices is now running low, and I probably need about \\~1.5TB/year, so I was looking at buying a shiny new DS923+ to keep my original solution going. However, this got me thinking: **instead, could I just buy a new PC and then fill my old PC with hard drives in RAID as a sort of cold storage backup, only powering it on when I need to backup a new set of photos/videos?** I don't need 24/7 access to the data over the network, and I could run data integrity checks every few months when I actually power it on (...couldn't I? Is there any software that does this in this way?)\n\nAm I missing something here, or would a new NAS (or something else) be the better solution? My thinking is that \u00a31k spent on a new NAS and HDDs is half way to a brand new computer, and I already have an existing NAS for films, music etc that I *do* want 24/7 access to. It wouldn't be the end of the world if the files were lost, so I'm not worried about a full 3/2/1 backup solution.\n\nSomewhat surprisingly, I just can't seem to find an answer online that covers my particular use-case, so any views/suggestions would be gratefully appreciated!", "author_fullname": "t2_v25tbbcqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using an old PC for cold(ish) backup storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8rtol", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709808900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow hoarders, long time lurker and first-time question asker here.&lt;/p&gt;\n\n&lt;p&gt;I currently have ~8tb of photos and video files from various travels, which I&amp;#39;ve been storing across a separate 2-bay NAS and a couple of RAID-1 drives on my PC. Once I&amp;#39;ve edited/printed/published the stuff that I want to keep, the rest is really just left in storage &amp;#39;just in case&amp;#39;, as I&amp;#39;m sure most of you understand!&lt;/p&gt;\n\n&lt;p&gt;Anyway, storage space on these devices is now running low, and I probably need about ~1.5TB/year, so I was looking at buying a shiny new DS923+ to keep my original solution going. However, this got me thinking: &lt;strong&gt;instead, could I just buy a new PC and then fill my old PC with hard drives in RAID as a sort of cold storage backup, only powering it on when I need to backup a new set of photos/videos?&lt;/strong&gt; I don&amp;#39;t need 24/7 access to the data over the network, and I could run data integrity checks every few months when I actually power it on (...couldn&amp;#39;t I? Is there any software that does this in this way?)&lt;/p&gt;\n\n&lt;p&gt;Am I missing something here, or would a new NAS (or something else) be the better solution? My thinking is that \u00a31k spent on a new NAS and HDDs is half way to a brand new computer, and I already have an existing NAS for films, music etc that I &lt;em&gt;do&lt;/em&gt; want 24/7 access to. It wouldn&amp;#39;t be the end of the world if the files were lost, so I&amp;#39;m not worried about a full 3/2/1 backup solution.&lt;/p&gt;\n\n&lt;p&gt;Somewhat surprisingly, I just can&amp;#39;t seem to find an answer online that covers my particular use-case, so any views/suggestions would be gratefully appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8rtol", "is_robot_indexable": true, "report_reasons": null, "author": "toasty_trifle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8rtol/using_an_old_pc_for_coldish_backup_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8rtol/using_an_old_pc_for_coldish_backup_storage/", "subreddit_subscribers": 737071, "created_utc": 1709808900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,\n\nI bought an IcyBox IB-3640SU3 DAS enclosure a few months ago and we just had a power outage and I noticed that the IB-3640SU3 didn't power back on once the power was restored. I thought it might have something to do with the SYNC setting so I simulated a power outage on all 3 sync modes (On, Off &amp; Hibernate) but it still didn't power on once power was restored. Each time I had to manually press the 'on' button in order to power it on.\n\nFor any of the guys out there with this disk enclosure, is this the same experience you have? Is there any way to have the box power on automatically? If not, this is a pretty terrible design flaw in my opinion.\n\nEdit: I just received a response from IcyBox support and it appears that there's no way to enable the enclosure to automatically power back on after a power outage.\n\n&amp;#x200B;\n\n&gt;Thank you very much for your question to us. If the device is turned off after a power outage, you will need to turn it on at the switch. There is no other way.\n\n&amp;#x200B;", "author_fullname": "t2_rsg05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IcyBox IB-3640SU3 - power on after power outage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8rmwp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709814428.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709808213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I bought an IcyBox IB-3640SU3 DAS enclosure a few months ago and we just had a power outage and I noticed that the IB-3640SU3 didn&amp;#39;t power back on once the power was restored. I thought it might have something to do with the SYNC setting so I simulated a power outage on all 3 sync modes (On, Off &amp;amp; Hibernate) but it still didn&amp;#39;t power on once power was restored. Each time I had to manually press the &amp;#39;on&amp;#39; button in order to power it on.&lt;/p&gt;\n\n&lt;p&gt;For any of the guys out there with this disk enclosure, is this the same experience you have? Is there any way to have the box power on automatically? If not, this is a pretty terrible design flaw in my opinion.&lt;/p&gt;\n\n&lt;p&gt;Edit: I just received a response from IcyBox support and it appears that there&amp;#39;s no way to enable the enclosure to automatically power back on after a power outage.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Thank you very much for your question to us. If the device is turned off after a power outage, you will need to turn it on at the switch. There is no other way.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8rmwp", "is_robot_indexable": true, "report_reasons": null, "author": "ozgyrex", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8rmwp/icybox_ib3640su3_power_on_after_power_outage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8rmwp/icybox_ib3640su3_power_on_after_power_outage/", "subreddit_subscribers": 737071, "created_utc": 1709808213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently got a WD Red Plus 12 TB and after running badblocks (no errors), I found that starting a SMART test using \"smartctl -t long /dev/sda\" resulted in a grinding noise. Is this the sound of the head being tested, or should I return my drive?\n\n[How it sounds](https://on.soundcloud.com/wr9LS)", "author_fullname": "t2_ocvat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Grinding-like noise (audio included) when starting SMART test on hard drive, is that normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8q64y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709802378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got a WD Red Plus 12 TB and after running badblocks (no errors), I found that starting a SMART test using &amp;quot;smartctl -t long /dev/sda&amp;quot; resulted in a grinding noise. Is this the sound of the head being tested, or should I return my drive?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://on.soundcloud.com/wr9LS\"&gt;How it sounds&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8q64y", "is_robot_indexable": true, "report_reasons": null, "author": "ELO_Space", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8q64y/grindinglike_noise_audio_included_when_starting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8q64y/grindinglike_noise_audio_included_when_starting/", "subreddit_subscribers": 737071, "created_utc": 1709802378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "TechPowerUp recorded two versions(20-82-10081 and 20-82-10048), and some people bought 20-82-10082 version.\n\nI've confirmed that they have different compatibility and performance.\n\nIs it possible that other WD DRAM-less Product... such as SN770 may have a similar situation?\n\n[https://www.techpowerup.com/ssd-specs/filter/?formfactor=4#sn570](https://www.techpowerup.com/ssd-specs/filter/?formfactor=4#sn570)\n\nhttps://preview.redd.it/cjk7nsqn8vmc1.png?width=921&amp;format=png&amp;auto=webp&amp;s=21b55cdd524a658a167a338e951484926e97a6fd\n\nhttps://preview.redd.it/jqng7wyg9vmc1.png?width=852&amp;format=png&amp;auto=webp&amp;s=c09e25bb652176ff8e7ede1733f708d6c8de4c7e\n\n&amp;#x200B;", "author_fullname": "t2_45ykgtmj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD SN570 has three different controller versions???", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cjk7nsqn8vmc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/cjk7nsqn8vmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=657f26dbbaed7d54da9e4af65331d4207e38474d"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/cjk7nsqn8vmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=086e739303ed2f2dff28f9d252014627acaa03be"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/cjk7nsqn8vmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e1ea584f555c4fce5b82e41ba0c167600cd707a"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/cjk7nsqn8vmc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0a50b90188acf04dd3a56842c730c74b80e1288d"}], "s": {"y": 404, "x": 921, "u": "https://preview.redd.it/cjk7nsqn8vmc1.png?width=921&amp;format=png&amp;auto=webp&amp;s=21b55cdd524a658a167a338e951484926e97a6fd"}, "id": "cjk7nsqn8vmc1"}, "jqng7wyg9vmc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/jqng7wyg9vmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fcc767db693354d3b1754dd42b45d78a935185f1"}, {"y": 171, "x": 216, "u": "https://preview.redd.it/jqng7wyg9vmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf1bbef9ed83c12d5f5c54272e79058d3032192d"}, {"y": 254, "x": 320, "u": "https://preview.redd.it/jqng7wyg9vmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f807b910494231fb39afce11786f3b808e7ad89c"}, {"y": 508, "x": 640, "u": "https://preview.redd.it/jqng7wyg9vmc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d575a858d6c3f1497eb4ca86321f2ea1273d062b"}], "s": {"y": 677, "x": 852, "u": "https://preview.redd.it/jqng7wyg9vmc1.png?width=852&amp;format=png&amp;auto=webp&amp;s=c09e25bb652176ff8e7ede1733f708d6c8de4c7e"}, "id": "jqng7wyg9vmc1"}}, "name": "t3_1b8oylv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a5BCfn9Sf0mvbZlC7lm-l3DfoDefd9GF-10PuNLnDfA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1709797655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TechPowerUp recorded two versions(20-82-10081 and 20-82-10048), and some people bought 20-82-10082 version.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve confirmed that they have different compatibility and performance.&lt;/p&gt;\n\n&lt;p&gt;Is it possible that other WD DRAM-less Product... such as SN770 may have a similar situation?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.techpowerup.com/ssd-specs/filter/?formfactor=4#sn570\"&gt;https://www.techpowerup.com/ssd-specs/filter/?formfactor=4#sn570&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cjk7nsqn8vmc1.png?width=921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b55cdd524a658a167a338e951484926e97a6fd\"&gt;https://preview.redd.it/cjk7nsqn8vmc1.png?width=921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b55cdd524a658a167a338e951484926e97a6fd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jqng7wyg9vmc1.png?width=852&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09e25bb652176ff8e7ede1733f708d6c8de4c7e\"&gt;https://preview.redd.it/jqng7wyg9vmc1.png?width=852&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09e25bb652176ff8e7ede1733f708d6c8de4c7e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NX-Fo528bOa2zJHt8p6Kf5ULUOxCLS83s45SNa8xtvQ.jpg?auto=webp&amp;s=8369bd9fa5a592ba7b36c7f403ae3f3457dd0fd3", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/NX-Fo528bOa2zJHt8p6Kf5ULUOxCLS83s45SNa8xtvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=31e2f9b3884d9e936bb3505dd431f80091291343", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/NX-Fo528bOa2zJHt8p6Kf5ULUOxCLS83s45SNa8xtvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=077287d4c74a83d596082b92a8a538645def6959", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/NX-Fo528bOa2zJHt8p6Kf5ULUOxCLS83s45SNa8xtvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=280189f0a8954d53da41c772581ab9260fa16952", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/NX-Fo528bOa2zJHt8p6Kf5ULUOxCLS83s45SNa8xtvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=32af20483c9dbac49728261e7818b42b5e471167", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/NX-Fo528bOa2zJHt8p6Kf5ULUOxCLS83s45SNa8xtvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=70df1331e8e33c8ee8ab004c1c55ce2b9f55abae", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/NX-Fo528bOa2zJHt8p6Kf5ULUOxCLS83s45SNa8xtvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79c9be269387c4b86ecd05b7c6785ef1f2e5e155", "width": 1080, "height": 567}], "variants": {}, "id": "I0Wif2uJchCz_ZJpL5sz7e_a9Rg7Wxj9SXIZHhtZRNk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8oylv", "is_robot_indexable": true, "report_reasons": null, "author": "SlianoXie", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8oylv/wd_sn570_has_three_different_controller_versions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8oylv/wd_sn570_has_three_different_controller_versions/", "subreddit_subscribers": 737071, "created_utc": 1709797655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nI\u2019ve been trying to get my LSI adapter 9211-8i (SAS2 2008 Falcon) to work with a HGST 7k6000 SAS drive. Both are recognized in the device manager, but when I attempt to initialize the drive in DiskPart, I encounter an \u201cincorrect function\u201d error.\n\nHere\u2019s what I\u2019ve tried so far:\n\n\t\u2022\tChecked cables and connections\n\t\u2022\tUpdated firmware and drivers for both the adapter and drive\n\t\u2022\tChecked compatibility (couldn\u2019t find clear info)\n\t\u2022\tAttempted to initialize through traditional means (disk manager) and MegaRAID Storage Manager, where \u201cInitialize\u201d is grayed out\n\t\u2022\tThe drive is listed as \u201cUnconfigured Good\u201d in MegaRAID, but I\u2019m unable to proceed with any configuration\n\nI\u2019ve also tried a lot more stuff and I\u2019m confused there is a super simple fix that I\u2019m just missing. I\u2019m reaching a bit of a dead-end and would appreciate any insights or advice. Thanks in advance! \n", "author_fullname": "t2_g379t2la", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LSI SAS card not Initializing HGST Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8odnm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709795539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been trying to get my LSI adapter 9211-8i (SAS2 2008 Falcon) to work with a HGST 7k6000 SAS drive. Both are recognized in the device manager, but when I attempt to initialize the drive in DiskPart, I encounter an \u201cincorrect function\u201d error.&lt;/p&gt;\n\n&lt;p&gt;Here\u2019s what I\u2019ve tried so far:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;\u2022 Checked cables and connections\n\u2022 Updated firmware and drivers for both the adapter and drive\n\u2022 Checked compatibility (couldn\u2019t find clear info)\n\u2022 Attempted to initialize through traditional means (disk manager) and MegaRAID Storage Manager, where \u201cInitialize\u201d is grayed out\n\u2022 The drive is listed as \u201cUnconfigured Good\u201d in MegaRAID, but I\u2019m unable to proceed with any configuration\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I\u2019ve also tried a lot more stuff and I\u2019m confused there is a super simple fix that I\u2019m just missing. I\u2019m reaching a bit of a dead-end and would appreciate any insights or advice. Thanks in advance! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8odnm", "is_robot_indexable": true, "report_reasons": null, "author": "Fast_Advertising_992", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8odnm/lsi_sas_card_not_initializing_hgst_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8odnm/lsi_sas_card_not_initializing_hgst_drive/", "subreddit_subscribers": 737071, "created_utc": 1709795539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Last year in May Crystal Disk Mark showed that my drive had some uncorrectable sectors and some reallocated sectors. I checked forums then and it was suggested that if they keep increasing then to ditch the drive. They didn't increase for about 6 months so I was happy. But now, after 10 months, reallocated sectors have increased from 30 to 83 but uncorrectable sectors have gone down to 4 from 6 (how is that possible?)  \nHDD is not in warranty. Should I retire the drive?", "author_fullname": "t2_1358x8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reallocated sector count increasing slowly, is it time to retire the drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8jngj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709780639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last year in May Crystal Disk Mark showed that my drive had some uncorrectable sectors and some reallocated sectors. I checked forums then and it was suggested that if they keep increasing then to ditch the drive. They didn&amp;#39;t increase for about 6 months so I was happy. But now, after 10 months, reallocated sectors have increased from 30 to 83 but uncorrectable sectors have gone down to 4 from 6 (how is that possible?)&lt;br/&gt;\nHDD is not in warranty. Should I retire the drive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8jngj", "is_robot_indexable": true, "report_reasons": null, "author": "BadlySynced", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8jngj/reallocated_sector_count_increasing_slowly_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8jngj/reallocated_sector_count_increasing_slowly_is_it/", "subreddit_subscribers": 737071, "created_utc": 1709780639.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}