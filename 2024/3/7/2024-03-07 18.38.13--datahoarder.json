{"kind": "Listing", "data": {"after": "t3_1b88b4r", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Small Radios Big Televisions* is being delisted in a few weeks.\n\nThere is also a comment from the game dev of *Fist Puncher:* \" I'm one of the creators and developers of Fist Puncher which was also published by Adult Swim on Steam. We received the same notice from Warner Bros. that Fist Puncher would be retired.\"\n\n[https://arstechnica.com/gaming/2024/03/its-kind-of-depressing-wb-discovery-pulls-indie-game-for-business-changes/?comments=1&amp;comments-page=2](https://arstechnica.com/gaming/2024/03/its-kind-of-depressing-wb-discovery-pulls-indie-game-for-business-changes/?comments=1&amp;comments-page=2)", "author_fullname": "t2_ropw64r3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This is gonna be overshadowed by Rooster Teeth, but Adult Swim Games are being delisted....", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8hvg4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 143, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 143, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709775737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Small Radios Big Televisions&lt;/em&gt; is being delisted in a few weeks.&lt;/p&gt;\n\n&lt;p&gt;There is also a comment from the game dev of &lt;em&gt;Fist Puncher:&lt;/em&gt; &amp;quot; I&amp;#39;m one of the creators and developers of Fist Puncher which was also published by Adult Swim on Steam. We received the same notice from Warner Bros. that Fist Puncher would be retired.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/gaming/2024/03/its-kind-of-depressing-wb-discovery-pulls-indie-game-for-business-changes/?comments=1&amp;amp;comments-page=2\"&gt;https://arstechnica.com/gaming/2024/03/its-kind-of-depressing-wb-discovery-pulls-indie-game-for-business-changes/?comments=1&amp;amp;comments-page=2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?auto=webp&amp;s=f8949da775fab798e377b9d4bda0eacdede93ab3", "width": 760, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c6fce87c2d1623d509d6631bd26e1e0a18f78d8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ca244099ee3e5eaa99da45934bbe88d66517244b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfba3aab374ae635218d146cf5c47c9298c2a6ea", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/64csLtKwU-UYc1qtT4IBvOpdiRURwNPI4thNez3ClP4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c1185bf8619f97099896dd10f5afc211b661cf4", "width": 640, "height": 320}], "variants": {}, "id": "y0PD7u2rWaG0Pe2POVjasiROH_byp5EUih62OTHGey8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8hvg4", "is_robot_indexable": true, "report_reasons": null, "author": "MiopicDream", "discussion_type": null, "num_comments": 14, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8hvg4/this_is_gonna_be_overshadowed_by_rooster_teeth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8hvg4/this_is_gonna_be_overshadowed_by_rooster_teeth/", "subreddit_subscribers": 737032, "created_utc": 1709775737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Never understood why Amazon send these in bags but sends toothpaste in a box!", "author_fullname": "t2_agzlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Amazon strikes again!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"p7ntdqmtasmc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/p7ntdqmtasmc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe6416744e5af1d34a8ca721286b57016e0a5192"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/p7ntdqmtasmc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e6674b86c50e88eddab3d07d68e0eaf13b5e53c"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/p7ntdqmtasmc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=64db90b3fabf75fcdc3e503569007e6d82b9ae3f"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/p7ntdqmtasmc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=47ed8a6ef510309b6f07c480cc45cb19aa76d635"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/p7ntdqmtasmc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b3566a09ca3b62231566f3292987aa7513740ba"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/p7ntdqmtasmc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e5caad3c476153a0c09de33aa7ce1415b71ec2d0"}], "s": {"y": 4096, "x": 1842, "u": "https://preview.redd.it/p7ntdqmtasmc1.jpg?width=1842&amp;format=pjpg&amp;auto=webp&amp;s=80c2d2cc400e4308109dbf776737af736f8623de"}, "id": "p7ntdqmtasmc1"}, "t1j8p8ptasmc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/t1j8p8ptasmc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ab41e2ad56357f72288697e88fbffe9c78b860a"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/t1j8p8ptasmc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=57cccd9cf78358be570f0638b271e1c521d60843"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/t1j8p8ptasmc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e688bb0da1ace7c21652560285a46ff996e65bd6"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/t1j8p8ptasmc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=68139729917d76bd51d502867db12a0ab0423aee"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/t1j8p8ptasmc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f949a92d3cdf271872762c80ea48ec1f67f75653"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/t1j8p8ptasmc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f099295453ac61f5eb7ca83c685dcc773c4455b"}], "s": {"y": 4096, "x": 1842, "u": "https://preview.redd.it/t1j8p8ptasmc1.jpg?width=1842&amp;format=pjpg&amp;auto=webp&amp;s=8e53fdff7af23612b619ea5e5647bbca36c2c4a6"}, "id": "t1j8p8ptasmc1"}}, "name": "t3_1b8c801", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 81, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "p7ntdqmtasmc1", "id": 416333164}, {"media_id": "t1j8p8ptasmc1", "id": 416333165}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 81, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KDFAL2pmqG2c9VmZa0Ncq_6NHUXO_imPtd64sHBpOvI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709761521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Never understood why Amazon send these in bags but sends toothpaste in a box!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/1b8c801", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8c801", "is_robot_indexable": true, "report_reasons": null, "author": "krobbinsit", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8c801/amazon_strikes_again/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/1b8c801", "subreddit_subscribers": 737032, "created_utc": 1709761521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have several WD Reds, WD Red Pros, and HGST Ultrastar He8 8TB hard drives running 24/7 for almost 8 years. Is it time to replace them?\n\nDo we know if there is going to be a WD Red Pro 24TB released this year?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/e509mgvzpsmc1.png?width=1274&amp;format=png&amp;auto=webp&amp;s=be051d31d85cc6b00cd7a152e616aa34ede04a76", "author_fullname": "t2_44voo4fk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When do I replace my drives? (7+ years)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"e509mgvzpsmc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f4b0c3e1a38bed632b16490750176ee7168d626"}, {"y": 77, "x": 216, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f0d81366bb5c11a3c7a6ee4566482399578c6747"}, {"y": 115, "x": 320, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=809f75a9a3c3684763cf124886d85c48454dfe76"}, {"y": 231, "x": 640, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c5fcadcbb6c17a7b3c8f73a22bd4454bf9ea2c9"}, {"y": 346, "x": 960, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4aeb830499067c0af98015d3798877825229a8da"}, {"y": 389, "x": 1080, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac7f92bc7250a4d0520e610f4fb4648f7e7bc480"}], "s": {"y": 460, "x": 1274, "u": "https://preview.redd.it/e509mgvzpsmc1.png?width=1274&amp;format=png&amp;auto=webp&amp;s=be051d31d85cc6b00cd7a152e616aa34ede04a76"}, "id": "e509mgvzpsmc1"}}, "name": "t3_1b8ee7w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3PxXpllm0CgcNPzqAo2BbFtI84F4N6xSG66AMfo27Xg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709766650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several WD Reds, WD Red Pros, and HGST Ultrastar He8 8TB hard drives running 24/7 for almost 8 years. Is it time to replace them?&lt;/p&gt;\n\n&lt;p&gt;Do we know if there is going to be a WD Red Pro 24TB released this year?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e509mgvzpsmc1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be051d31d85cc6b00cd7a152e616aa34ede04a76\"&gt;https://preview.redd.it/e509mgvzpsmc1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be051d31d85cc6b00cd7a152e616aa34ede04a76&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8ee7w", "is_robot_indexable": true, "report_reasons": null, "author": "tastethebean", "discussion_type": null, "num_comments": 32, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8ee7w/when_do_i_replace_my_drives_7_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8ee7w/when_do_i_replace_my_drives_7_years/", "subreddit_subscribers": 737032, "created_utc": 1709766650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": " Rooster Teeth is shutting down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b88prz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_l6hsy91", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "roosterteeth", "selftext": "Hey y\u2019all - today I\u2019m coming to y\u2019all with a pretty tough message. One that I need you to read all the way through. (Seriously. Please.)\n\nAt this moment, all Rooster Teeth staff and many contractors are in an All Hands company-wide meeting right now where some very important information is being shared to us. Important information that is now also being shared publicly through press outlets and various community spaces. \n\nPlease remember something as you begin to read the below message and DEFINITELY before you comment. We, all of us who work at Rooster Teeth, are **processing this in real time just like you**. Please be mindful that this is on the Rooster Teeth subreddit, a place where staff read what you write, and where other community members come to engage. If you have questions, please head to the [Rooster Teeth website and leave them on that post](https://roosterteeth.com/g/post/af39b172-be74-4398-94f8-908c3f4c5d6e) \\- and thank you for your patience. Continued updates will be posted on RoosterTeeth.com, but you can also reach out and submit questions or feelings to our Support Page. We will be hosting a livestream tomorrow, March 7, 2024 at 4pm CT on RoosterTeeth.com talking about this more.  \n\n\n&gt;*Dear Rooster Teeth,*   \n&gt;  \n&gt;*Since our founders created and uploaded their first video on the then-called World Wide Web in 2003, Rooster Teeth has been a source of creativity, laughter, and lasting innovation in the wildly volatile media industry.*   \n&gt;  \n&gt;*We\u2019ve read the headlines about industry-wide layoffs and closures, and you\u2019ve heard me give my perspective and updates on the rapidly changing state of media and entertainment during each of our monthly All Hands meetings.*   \n&gt;  \n&gt;*Since inheriting ownership and control of Rooster Teeth from AT&amp;T following its acquisition of TimeWarner, Warner Bros. Discovery continued its investment in our company, content and community. Now however, it\u2019s with a heavy heart I announce that Rooster Teeth is shutting down due to challenges facing digital media resulting from fundamental shifts in consumer behavior and monetization across platforms, advertising, and patronage.*  \n&gt;  \n&gt;*Please note, the Roost team is not currently impacted by this action as the Roost Podcast Network will continue operating and fulfilling its obligations while WBD evaluates outside interest in acquiring this growing asset.*  \n&gt;  \n&gt;*We have many questions to answer in the coming days and weeks, and the opportunity to work together to implement the best way to wind things down for us and our community. We\u2019re working through what comes next in real time, and we will be as open, direct, and accessible as possible.  Thank you all in advance for your patience and support of one another.*   \n&gt;  \n&gt;*Let's take a moment to celebrate our 21-year contribution to the zeitgeist, advancing creativity and outlasting many of our peers from the early days of online video and digital-first content.*   \n&gt;  \n&gt;***TO A CREATIVE LEGACY***  \n&gt;  \n&gt;*From a garage in Buda, TX, to global screens large and small, our teams of dreamers and doers have introduced and grown what made Rooster Teeth stand out: animation, comedy, and gaming. From new forms of animated comedy with machinima to countless viral memes, including the Immortal Snail (aka Snail Assassin), to a US-born animated series embraced by Japan as anime, and record-breaking (at the time) crowdfunded movies. You've accomplished so much and made dreams come true here. You've turned original IP into video games, comic books, and VTubers. You've directed short videos, mo-cap, and films. You've puppeteered, hosted podcasts, and have built a thriving community that spans the globe. Your creativity knows no bounds, and you'll continue contributing significantly to culture wherever your paths may take you.*  \n&gt;  \n&gt;***TO THOSE WHO COME FIRST***  \n&gt;  \n&gt;*Despite passing through many corporate owners, Rooster Teeth transcended a media business and was a dynamic movement that shaped the bond between communities, creators, and storytelling. Our founders didn\u2019t have a blueprint for a media empire, but they got close to building one alongside a community that fueled its remarkable growth. In its earliest days, RT relied on community sponsorship through time, dollars, and unwavering passion. Volunteers evolved into staff, and the snowball effect grew, resulting in new relationships, marriages, births, and shared experiences that have changed lives.*   \n&gt;  \n&gt;***TO TRAILBLAZING CONTENT CREATION***   \n*Our approach to content creation on emerging platforms paved the way for new media models. We inspired generations of creators across streaming, machinima, animation, let\u2019s plays, merch drops, touring, podcasting, and more. Companies like GameStop, YouTube, Facebook, Spotify, and TikTok asked us to collaborate with them in their earliest days because we set a standard for what a digital-first brand could be. We boldly took our content beyond screens and into community-driven experiences.*   \n&gt;  \n&gt;***TO A CHANGING INDUSTRY***  \n*Every story reaches its final pages. Rooster Teeth\u2019s closure isn\u2019t merely an end; it reflects broader business dynamics. Monetization shifts, platform algorithms, advertising challenges, and the ebb and flow of patronage\u2014all these converging factors have led to many closures in the industry. While we learn about updates on programming day by day, we will share our plans for shows, franchises, partnerships, and merch soon and share those updates with teams internally and with the community on RoosterTeeth.com*  \n&gt;  \n&gt;***TO OUR FINAL SEASON***  \n&gt;  \n&gt;*Though not intentional, It\u2019s only appropriate that our last season of \u201cRed vs. Blue\u201d coincides with us navigating this closure together. Our legacy is not just a collection of content but a history of pixels burned into our screens, minds, and hearts. Rooster Teeth has made an indelible mark on the media industry, and we should be so proud of the countless ways we pioneered a business connecting creators and content with a dedicated community.*   \n&gt;  \n&gt;*With respect, gratitude, and sincere appreciation,*   \n&gt;  \n&gt;*Jordan Levin*\n\n&amp;#x200B;\n\nI\u2019ll leave it in the capable hands of the Mods here to decide where conversation happens - whether it\u2019s here or a stickied post (we\u2019re using #goodbye-RT in the RT and DB Discords). More information will be shared on RoosterTeeth.com as they are decided. Take the time you need to process. Don\u2019t lash out, don\u2019t speculate. No one specific instance caused this. Every single person at Rooster Teeth is being affected and we are eternally grateful for the support and love that you have graced us with. \n\nMuch love,  \nChels   \nHead of Community &amp; Support Operations", "author_fullname": "t2_hofqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Important Information about Rooster Teeth", "link_flair_richtext": [{"e": "text", "t": "News"}], "subreddit_name_prefixed": "r/roosterteeth", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b84xhg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "#b8001f", "subreddit_type": "public", "ups": 1759, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "dd7ee9c4-6e9a-11ea-9ff8-0e7590e1268d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1759, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "staff special chattykinson", "author_flair_richtext": [{"a": ":Rooster17:", "e": "emoji", "u": "https://emoji.redditmedia.com/uk8duvyfa9u01_t5_2s7g9/Rooster17"}, {"e": "text", "t": " Chelsea Atkinson - Director of Community &amp; CS"}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709744608.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.roosterteeth", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y\u2019all - today I\u2019m coming to y\u2019all with a pretty tough message. One that I need you to read all the way through. (Seriously. Please.)&lt;/p&gt;\n\n&lt;p&gt;At this moment, all Rooster Teeth staff and many contractors are in an All Hands company-wide meeting right now where some very important information is being shared to us. Important information that is now also being shared publicly through press outlets and various community spaces. &lt;/p&gt;\n\n&lt;p&gt;Please remember something as you begin to read the below message and DEFINITELY before you comment. We, all of us who work at Rooster Teeth, are &lt;strong&gt;processing this in real time just like you&lt;/strong&gt;. Please be mindful that this is on the Rooster Teeth subreddit, a place where staff read what you write, and where other community members come to engage. If you have questions, please head to the &lt;a href=\"https://roosterteeth.com/g/post/af39b172-be74-4398-94f8-908c3f4c5d6e\"&gt;Rooster Teeth website and leave them on that post&lt;/a&gt; - and thank you for your patience. Continued updates will be posted on RoosterTeeth.com, but you can also reach out and submit questions or feelings to our Support Page. We will be hosting a livestream tomorrow, March 7, 2024 at 4pm CT on RoosterTeeth.com talking about this more.  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;Dear Rooster Teeth,&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Since our founders created and uploaded their first video on the then-called World Wide Web in 2003, Rooster Teeth has been a source of creativity, laughter, and lasting innovation in the wildly volatile media industry.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We\u2019ve read the headlines about industry-wide layoffs and closures, and you\u2019ve heard me give my perspective and updates on the rapidly changing state of media and entertainment during each of our monthly All Hands meetings.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Since inheriting ownership and control of Rooster Teeth from AT&amp;amp;T following its acquisition of TimeWarner, Warner Bros. Discovery continued its investment in our company, content and community. Now however, it\u2019s with a heavy heart I announce that Rooster Teeth is shutting down due to challenges facing digital media resulting from fundamental shifts in consumer behavior and monetization across platforms, advertising, and patronage.&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Please note, the Roost team is not currently impacted by this action as the Roost Podcast Network will continue operating and fulfilling its obligations while WBD evaluates outside interest in acquiring this growing asset.&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We have many questions to answer in the coming days and weeks, and the opportunity to work together to implement the best way to wind things down for us and our community. We\u2019re working through what comes next in real time, and we will be as open, direct, and accessible as possible.  Thank you all in advance for your patience and support of one another.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Let&amp;#39;s take a moment to celebrate our 21-year contribution to the zeitgeist, advancing creativity and outlasting many of our peers from the early days of online video and digital-first content.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;TO A CREATIVE LEGACY&lt;/em&gt;&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;From a garage in Buda, TX, to global screens large and small, our teams of dreamers and doers have introduced and grown what made Rooster Teeth stand out: animation, comedy, and gaming. From new forms of animated comedy with machinima to countless viral memes, including the Immortal Snail (aka Snail Assassin), to a US-born animated series embraced by Japan as anime, and record-breaking (at the time) crowdfunded movies. You&amp;#39;ve accomplished so much and made dreams come true here. You&amp;#39;ve turned original IP into video games, comic books, and VTubers. You&amp;#39;ve directed short videos, mo-cap, and films. You&amp;#39;ve puppeteered, hosted podcasts, and have built a thriving community that spans the globe. Your creativity knows no bounds, and you&amp;#39;ll continue contributing significantly to culture wherever your paths may take you.&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;TO THOSE WHO COME FIRST&lt;/em&gt;&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Despite passing through many corporate owners, Rooster Teeth transcended a media business and was a dynamic movement that shaped the bond between communities, creators, and storytelling. Our founders didn\u2019t have a blueprint for a media empire, but they got close to building one alongside a community that fueled its remarkable growth. In its earliest days, RT relied on community sponsorship through time, dollars, and unwavering passion. Volunteers evolved into staff, and the snowball effect grew, resulting in new relationships, marriages, births, and shared experiences that have changed lives.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;TO TRAILBLAZING CONTENT CREATION&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n&lt;em&gt;Our approach to content creation on emerging platforms paved the way for new media models. We inspired generations of creators across streaming, machinima, animation, let\u2019s plays, merch drops, touring, podcasting, and more. Companies like GameStop, YouTube, Facebook, Spotify, and TikTok asked us to collaborate with them in their earliest days because we set a standard for what a digital-first brand could be. We boldly took our content beyond screens and into community-driven experiences.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;TO A CHANGING INDUSTRY&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;\n&lt;em&gt;Every story reaches its final pages. Rooster Teeth\u2019s closure isn\u2019t merely an end; it reflects broader business dynamics. Monetization shifts, platform algorithms, advertising challenges, and the ebb and flow of patronage\u2014all these converging factors have led to many closures in the industry. While we learn about updates on programming day by day, we will share our plans for shows, franchises, partnerships, and merch soon and share those updates with teams internally and with the community on RoosterTeeth.com&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;TO OUR FINAL SEASON&lt;/em&gt;&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Though not intentional, It\u2019s only appropriate that our last season of \u201cRed vs. Blue\u201d coincides with us navigating this closure together. Our legacy is not just a collection of content but a history of pixels burned into our screens, minds, and hearts. Rooster Teeth has made an indelible mark on the media industry, and we should be so proud of the countless ways we pioneered a business connecting creators and content with a dedicated community.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;With respect, gratitude, and sincere appreciation,&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Jordan Levin&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll leave it in the capable hands of the Mods here to decide where conversation happens - whether it\u2019s here or a stickied post (we\u2019re using #goodbye-RT in the RT and DB Discords). More information will be shared on RoosterTeeth.com as they are decided. Take the time you need to process. Don\u2019t lash out, don\u2019t speculate. No one specific instance caused this. Every single person at Rooster Teeth is being affected and we are eternally grateful for the support and love that you have graced us with. &lt;/p&gt;\n\n&lt;p&gt;Much love,&lt;br/&gt;\nChels&lt;br/&gt;\nHead of Community &amp;amp; Support Operations&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?auto=webp&amp;s=5602b748c98c6f6e1958350c91ac33097b430f01", "width": 640, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e84381241a63128bea8646277d6170d23ea7efc8", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=672b9520834fde540ac3d7ddabc624232d9853d5", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8122627e4c52d4cfd0b3c1b74a9d076346a4f5e3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=15d77f7cea84294850aa4324ecec9a90858a15c7", "width": 640, "height": 360}], "variants": {}, "id": "4TPYj2uAtOrgSuN76FxBszwfamrY2t7MyFrlj09-3HE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "6c3ea6bc-5a34-11ee-8f29-120c9a5b7e40", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ":Rooster17: Chelsea Atkinson - Director of Community &amp; CS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2s7g9", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#373c3f", "id": "1b84xhg", "is_robot_indexable": true, "report_reasons": null, "author": "chattykinson", "discussion_type": null, "num_comments": 996, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/roosterteeth/comments/1b84xhg/important_information_about_rooster_teeth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/roosterteeth/comments/1b84xhg/important_information_about_rooster_teeth/", "subreddit_subscribers": 252859, "created_utc": 1709744608.0, "num_crossposts": 6, "media": null, "is_video": false}], "created": 1709753264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.roosterteeth", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/roosterteeth/comments/1b84xhg/important_information_about_rooster_teeth/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?auto=webp&amp;s=5602b748c98c6f6e1958350c91ac33097b430f01", "width": 640, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e84381241a63128bea8646277d6170d23ea7efc8", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=672b9520834fde540ac3d7ddabc624232d9853d5", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8122627e4c52d4cfd0b3c1b74a9d076346a4f5e3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/-rv4LAxUUQKrlSPY8u1JPkkQxug5KH98nYs3GtHXHpw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=15d77f7cea84294850aa4324ecec9a90858a15c7", "width": 640, "height": 360}], "variants": {}, "id": "4TPYj2uAtOrgSuN76FxBszwfamrY2t7MyFrlj09-3HE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b88prz", "is_robot_indexable": true, "report_reasons": null, "author": "Jacksharkben", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1b84xhg", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b88prz/rooster_teeth_is_shutting_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/roosterteeth/comments/1b84xhg/important_information_about_rooster_teeth/", "subreddit_subscribers": 737032, "created_utc": 1709753264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have nearly 5 terabytes of games, websites, music, documents and software on my Linux RAID. What is a good way to efficiently search through this mess? Simple file search (say via \\`find\\`) takes forever and isn't very fast when I enable regexes. Another type of query I want to perform is about the content of the files, I use \\`silversearcher-ag\\` for this and its reasonably fast but still a pain to use and sometimes very slow.\n\nIs there any tool that can index this properly and be better at searching both files with certain name and/or files with certain data. Bonus points if it has some web ui so that I can make it available over the network.", "author_fullname": "t2_bhm4t9j3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you efficiently search through your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8wqyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709823646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have nearly 5 terabytes of games, websites, music, documents and software on my Linux RAID. What is a good way to efficiently search through this mess? Simple file search (say via `find`) takes forever and isn&amp;#39;t very fast when I enable regexes. Another type of query I want to perform is about the content of the files, I use `silversearcher-ag` for this and its reasonably fast but still a pain to use and sometimes very slow.&lt;/p&gt;\n\n&lt;p&gt;Is there any tool that can index this properly and be better at searching both files with certain name and/or files with certain data. Bonus points if it has some web ui so that I can make it available over the network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8wqyd", "is_robot_indexable": true, "report_reasons": null, "author": "Emergency_Apricot_77", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8wqyd/how_do_you_efficiently_search_through_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8wqyd/how_do_you_efficiently_search_through_your_data/", "subreddit_subscribers": 737032, "created_utc": 1709823646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "happy leeching : P\n\n## opensubtitles.org.dump.9500000.to.9599999\n\n    magnet:?xt=urn:btih:287508f8acc0a5a060b940a83fbba68455ef2207&amp;dn=opensubtitles.org.dump.9500000.to.9599999.v20240306\n\n### subtitles server\n\na minimal subtitles server is running on http://milahuuuc3656fettsi3jjepqhhvnuml5hug3k7djtzlfe4dw6trivqd.onion/bin/get-subtitles\n\nbased on the full 150 GB database\n\ntakes a few seconds to return all subtitles for a movie in one language (currently only english is supported)\n\nsource: [get-subs.py](https://github.com/milahu/opensubtitles-scraper/raw/main/get-subs.py) and [lighttpd.conf](https://github.com/milahu/opensubtitles-scraper/raw/main/docs/lighttpd.conf)\n\nfeel free to run your own subtitles server : )\n\n### daily releases\n\ndaily releases are uploaded to this git repo\n\nhttps://github.com/milahu/opensubtitles-scraper-new-subs\n\n### future releases\n\nthere is one release every 100 days\n\nto automatically download future releases, subscribe to this RSS feed\n\nhttps://github.com/milahu/opensubtitles-scraper/raw/main/release/opensubtitles.org.dump.torrent.rss\n\n### continue\n\n* [subtitles from opensubtitles.org - subs 9500000 to 9799999](https://old.reddit.com/r/DataHoarder/comments/1azqwa4/subtitles_from_opensubtitlesorg_subs_9500000_to/) - subs 9500000 to 9799999\n* [5,719,123 subtitles from opensubtitles.org](https://www.reddit.com/r/DataHoarder/comments/w7sgcz/5719123_subtitles_from_opensubtitlesorg/) \\- subs 1 to 9180517\n* [opensubtitles.org dump - 1 million subtitles - 23 GB](https://www.reddit.com/r/DataHoarder/comments/12yxcoy/opensubtitlesorg_dump_1_million_subtitles_23_gb/) \\- subs 9180519 to 9521948", "author_fullname": "t2_naq15kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "subtitles from opensubtitles.org - subs 9500000 to 9599999", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8b3bi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709820938.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709758809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;happy leeching : P&lt;/p&gt;\n\n&lt;h2&gt;opensubtitles.org.dump.9500000.to.9599999&lt;/h2&gt;\n\n&lt;pre&gt;&lt;code&gt;magnet:?xt=urn:btih:287508f8acc0a5a060b940a83fbba68455ef2207&amp;amp;dn=opensubtitles.org.dump.9500000.to.9599999.v20240306\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;subtitles server&lt;/h3&gt;\n\n&lt;p&gt;a minimal subtitles server is running on &lt;a href=\"http://milahuuuc3656fettsi3jjepqhhvnuml5hug3k7djtzlfe4dw6trivqd.onion/bin/get-subtitles\"&gt;http://milahuuuc3656fettsi3jjepqhhvnuml5hug3k7djtzlfe4dw6trivqd.onion/bin/get-subtitles&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;based on the full 150 GB database&lt;/p&gt;\n\n&lt;p&gt;takes a few seconds to return all subtitles for a movie in one language (currently only english is supported)&lt;/p&gt;\n\n&lt;p&gt;source: &lt;a href=\"https://github.com/milahu/opensubtitles-scraper/raw/main/get-subs.py\"&gt;get-subs.py&lt;/a&gt; and &lt;a href=\"https://github.com/milahu/opensubtitles-scraper/raw/main/docs/lighttpd.conf\"&gt;lighttpd.conf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;feel free to run your own subtitles server : )&lt;/p&gt;\n\n&lt;h3&gt;daily releases&lt;/h3&gt;\n\n&lt;p&gt;daily releases are uploaded to this git repo&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/milahu/opensubtitles-scraper-new-subs\"&gt;https://github.com/milahu/opensubtitles-scraper-new-subs&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;future releases&lt;/h3&gt;\n\n&lt;p&gt;there is one release every 100 days&lt;/p&gt;\n\n&lt;p&gt;to automatically download future releases, subscribe to this RSS feed&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/milahu/opensubtitles-scraper/raw/main/release/opensubtitles.org.dump.torrent.rss\"&gt;https://github.com/milahu/opensubtitles-scraper/raw/main/release/opensubtitles.org.dump.torrent.rss&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;continue&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://old.reddit.com/r/DataHoarder/comments/1azqwa4/subtitles_from_opensubtitlesorg_subs_9500000_to/\"&gt;subtitles from opensubtitles.org - subs 9500000 to 9799999&lt;/a&gt; - subs 9500000 to 9799999&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/w7sgcz/5719123_subtitles_from_opensubtitlesorg/\"&gt;5,719,123 subtitles from opensubtitles.org&lt;/a&gt; - subs 1 to 9180517&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12yxcoy/opensubtitlesorg_dump_1_million_subtitles_23_gb/\"&gt;opensubtitles.org dump - 1 million subtitles - 23 GB&lt;/a&gt; - subs 9180519 to 9521948&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8b3bi", "is_robot_indexable": true, "report_reasons": null, "author": "milahu2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8b3bi/subtitles_from_opensubtitlesorg_subs_9500000_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8b3bi/subtitles_from_opensubtitlesorg_subs_9500000_to/", "subreddit_subscribers": 737032, "created_utc": 1709758809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently been backing up some of my favourite videos/channels and wondered how much you can download in a day without getting blocked by Google/YouTube (or throttled I guess?)\n\nHas anyone ever ran into this issue? I'm not talking about using multiple instances of Youtube-dl, YT-DLP either, just using one command to download an entire channel/playlist. \n\nI guess my question(s) are/is:\n\n1. What's the most amount of videos/GB you've downloaded in a single day / few hours?\n2. Has anyone ever had any issues due to downloading too much via YT-DLP/youtube-dl?", "author_fullname": "t2_hinnm58s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the most you've downloaded with YT-DLP before?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8cq5v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709762729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently been backing up some of my favourite videos/channels and wondered how much you can download in a day without getting blocked by Google/YouTube (or throttled I guess?)&lt;/p&gt;\n\n&lt;p&gt;Has anyone ever ran into this issue? I&amp;#39;m not talking about using multiple instances of Youtube-dl, YT-DLP either, just using one command to download an entire channel/playlist. &lt;/p&gt;\n\n&lt;p&gt;I guess my question(s) are/is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What&amp;#39;s the most amount of videos/GB you&amp;#39;ve downloaded in a single day / few hours?&lt;/li&gt;\n&lt;li&gt;Has anyone ever had any issues due to downloading too much via YT-DLP/youtube-dl?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8cq5v", "is_robot_indexable": true, "report_reasons": null, "author": "WonderingWhenSayHi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8cq5v/whats_the_most_youve_downloaded_with_ytdlp_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8cq5v/whats_the_most_youve_downloaded_with_ytdlp_before/", "subreddit_subscribers": 737032, "created_utc": 1709762729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "You may recall around Black Friday these drives went on sale briefly for around $335 or so... Newegg did, I believe Amazon matched and a few other stores.  I ordered one from Newegg and many were delayed.   I got the one from Newegg about 2 weeks later.   Not too bad, IMO.   But for my NAS I needed two and I scrambled and they had gone out of stock literally just as I had ordered from Newegg.   So I ordered from another store that had lowered their prices to match.   Well, they kept delaying and delaying, but sending emails.  Well, two weeks ago they sent an email saying it was discontinued and cancelled the order.   But it's not discontinued.  It's this drive:  WD221KFGX     Or more specifically, this one:  WD221KFGX-SPB9KN0  \n\n\nSo clue me in... am I wrong?  Is this discontinued?    It's in stock on their site,  WD seems to think it's alive and well on their product page.  But when I pointed that out, the store pointed to their SKU, not the one above.   But the email they sent points to this SKU as well as their own (both are on it).   The only place on the entire internet with the SKU they are using is their own store.    So I asked for them to explain that.    Hard for me to believe they are getting a sweet special SKU from WD when even Amazon doesn't.   ...and what exactly could be different.  It's almost certainly just their own item number.  \n\n If you are curious to know, it's Adorama.     Now I know Adorama is a mixed blessing at best, but still, dragging me for two months then cancelling the order is actually worse than just cancelling outright.  So I want them to honor their discount.   Or clarify their position.   I mean, it's about $80, give or take.  At this point I'm just pissed at them.   \n\nFeel free to offer any helpful thoughts.      It's late, I have work, I'm going to bed, so I'll check back in the morning so my apologies if I don't comment immediately.   I'll update though if I get feedback from their CS dept and people are interested.   ", "author_fullname": "t2_5jxdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not sure if anyone else had this experience, but a certain store is not wanting to honor their sale price for a 22TB internal WD Red Pro NAS drive.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8l5iw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709784937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You may recall around Black Friday these drives went on sale briefly for around $335 or so... Newegg did, I believe Amazon matched and a few other stores.  I ordered one from Newegg and many were delayed.   I got the one from Newegg about 2 weeks later.   Not too bad, IMO.   But for my NAS I needed two and I scrambled and they had gone out of stock literally just as I had ordered from Newegg.   So I ordered from another store that had lowered their prices to match.   Well, they kept delaying and delaying, but sending emails.  Well, two weeks ago they sent an email saying it was discontinued and cancelled the order.   But it&amp;#39;s not discontinued.  It&amp;#39;s this drive:  WD221KFGX     Or more specifically, this one:  WD221KFGX-SPB9KN0  &lt;/p&gt;\n\n&lt;p&gt;So clue me in... am I wrong?  Is this discontinued?    It&amp;#39;s in stock on their site,  WD seems to think it&amp;#39;s alive and well on their product page.  But when I pointed that out, the store pointed to their SKU, not the one above.   But the email they sent points to this SKU as well as their own (both are on it).   The only place on the entire internet with the SKU they are using is their own store.    So I asked for them to explain that.    Hard for me to believe they are getting a sweet special SKU from WD when even Amazon doesn&amp;#39;t.   ...and what exactly could be different.  It&amp;#39;s almost certainly just their own item number.  &lt;/p&gt;\n\n&lt;p&gt;If you are curious to know, it&amp;#39;s Adorama.     Now I know Adorama is a mixed blessing at best, but still, dragging me for two months then cancelling the order is actually worse than just cancelling outright.  So I want them to honor their discount.   Or clarify their position.   I mean, it&amp;#39;s about $80, give or take.  At this point I&amp;#39;m just pissed at them.   &lt;/p&gt;\n\n&lt;p&gt;Feel free to offer any helpful thoughts.      It&amp;#39;s late, I have work, I&amp;#39;m going to bed, so I&amp;#39;ll check back in the morning so my apologies if I don&amp;#39;t comment immediately.   I&amp;#39;ll update though if I get feedback from their CS dept and people are interested.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b8l5iw", "is_robot_indexable": true, "report_reasons": null, "author": "0ttr", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8l5iw/not_sure_if_anyone_else_had_this_experience_but_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8l5iw/not_sure_if_anyone_else_had_this_experience_but_a/", "subreddit_subscribers": 737032, "created_utc": 1709784937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm scanning some old books and I'm struggling with \"huge\" PDF file sizes, i.e. ~ 200 MB.\n\nThis onto itself isn't that big of an issue except that my PDF editor struggles to edit / OCR such large PDFs.\n\nI've tried to use JPEG2000 instead of regular JPEG when making the PDFs but it was too much trouble for nearly no significant gain. I'm currently scanning the books at 300 DPI and saving the scanned files as JPEGs with a quality setting of 80.\n\nPart of me wants to try newer formats like HEIC, JXL and WebP but those are not acceptable inside PDFs from what I've understood from the PDF spec.\n\nSo, is there any other file format I could use? I just want a single file output that contains all the scanned pages in high quality plus OCR.", "author_fullname": "t2_1oy24ohu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File format for archiving scanned books?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8ft5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709770221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m scanning some old books and I&amp;#39;m struggling with &amp;quot;huge&amp;quot; PDF file sizes, i.e. ~ 200 MB.&lt;/p&gt;\n\n&lt;p&gt;This onto itself isn&amp;#39;t that big of an issue except that my PDF editor struggles to edit / OCR such large PDFs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to use JPEG2000 instead of regular JPEG when making the PDFs but it was too much trouble for nearly no significant gain. I&amp;#39;m currently scanning the books at 300 DPI and saving the scanned files as JPEGs with a quality setting of 80.&lt;/p&gt;\n\n&lt;p&gt;Part of me wants to try newer formats like HEIC, JXL and WebP but those are not acceptable inside PDFs from what I&amp;#39;ve understood from the PDF spec.&lt;/p&gt;\n\n&lt;p&gt;So, is there any other file format I could use? I just want a single file output that contains all the scanned pages in high quality plus OCR.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "noob (i.e. &lt; 1TB)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8ft5a", "is_robot_indexable": true, "report_reasons": null, "author": "gjvnq1", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1b8ft5a/file_format_for_archiving_scanned_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8ft5a/file_format_for_archiving_scanned_books/", "subreddit_subscribers": 737032, "created_utc": 1709770221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nHello recently, i've bought a 2600$ online course, with different modules and videos with my friend, i paid for about 1000 among it and my friend payed the money left. My friend is the only one with the logins.\nAnd he dont want the staff to know that 2 people bought it, and potentially ban us  for it. \n\nWe would like to scrape the videos in the course and download it then transfer it to my pc in different folders so i can follow it peacefully, without having to lose a grand invested.\nI have no idea how to do that so far, and i havent found an answer googling it, but i belive it's doable, since i have seen similar things.\nCould a kind soul help me out, or redirect me to a ressource that learn me to do it?", "author_fullname": "t2_u9sssbeww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to scrape the videos of a private online course?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8e4my", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709766024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello recently, i&amp;#39;ve bought a 2600$ online course, with different modules and videos with my friend, i paid for about 1000 among it and my friend payed the money left. My friend is the only one with the logins.\nAnd he dont want the staff to know that 2 people bought it, and potentially ban us  for it. &lt;/p&gt;\n\n&lt;p&gt;We would like to scrape the videos in the course and download it then transfer it to my pc in different folders so i can follow it peacefully, without having to lose a grand invested.\nI have no idea how to do that so far, and i havent found an answer googling it, but i belive it&amp;#39;s doable, since i have seen similar things.\nCould a kind soul help me out, or redirect me to a ressource that learn me to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8e4my", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianFluid6425", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8e4my/how_to_scrape_the_videos_of_a_private_online/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8e4my/how_to_scrape_the_videos_of_a_private_online/", "subreddit_subscribers": 737032, "created_utc": 1709766024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many pics to scan. I had a software called I think HP easy scan that was very good by splitting one scan to several pics.\n\nNow I have a new HP printer (516) and the software I'm using doesn't split them. This is big downside and I don't know what to do. I can't go picture by picture, it's too much. If I can put 2-4 pictures in one scan I can handle it hahaha.\n\nThank you so much for the help!", "author_fullname": "t2_4xllun6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanning old pics to drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b90cvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709832667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many pics to scan. I had a software called I think HP easy scan that was very good by splitting one scan to several pics.&lt;/p&gt;\n\n&lt;p&gt;Now I have a new HP printer (516) and the software I&amp;#39;m using doesn&amp;#39;t split them. This is big downside and I don&amp;#39;t know what to do. I can&amp;#39;t go picture by picture, it&amp;#39;s too much. If I can put 2-4 pictures in one scan I can handle it hahaha.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b90cvg", "is_robot_indexable": true, "report_reasons": null, "author": "Direct_Check_3366", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b90cvg/scanning_old_pics_to_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b90cvg/scanning_old_pics_to_drive/", "subreddit_subscribers": 737032, "created_utc": 1709832667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Need to create a storage solution at remote sites. These will resides in awkward and/or smaller spaces without racks. A good sized shed that's protected from the elements but no HVAC, for example. It will store surveillance footage, so 24/7 writing and occasional reading. One other caveat is that it has to run windows. Our VMS recording application is windows only. A pre-built NAS would obviously work but then it turns into two devices to manage instead of one. Since I'd still have a PC running the software. Because of that, I'd rather stick to a PC-only solution, but not married to it.\n\nOne option is simply building a PC with a bunch of spinners. But it needs some resiliency. I manage them, but I\u2019ve never built out a RAID device myself from scratch. I\u2019ve read not to use diskmgmt raid or storage spaces. Though some say if you're just running a mirror, no parity, storage spaces can be fine. This doesn't need to be fancy. A RAID 1 would satisfy the requirements. Maybe storage spaces is fine? Any other decent software raid options out there? Thoughts on Stablebit Drivepool? I would need the storage to show up in Windows as one drive.\n\nAnother option is to run a hardware raid card in the build. I've read that using some consumer level card like startech is basically useless. It looks like a decent LSI enterprise card with caching and BBU runs \\~$1,000? At that price, it may be better to get smaller Dell or HP server tower instead of trying to piece something together from scratch.\n\nAnd of course, the \"easiest\" solution is to just get a pre-built nas and call it day.\n\nKind of looking for a conversation here. Thoughts on the some of the assumptions made due to what I've read so far. Some real-world experience with any of the outlined options. Again, it's going to live in outdoor temperatures. Around here anywhere from 80-100 in the summer, down to 10-30 in the winter. Not really sure if any options are going to hold up better than others given that.", "author_fullname": "t2_qacpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CCTV Storage Solutions. Outside?!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b903d7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709832051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need to create a storage solution at remote sites. These will resides in awkward and/or smaller spaces without racks. A good sized shed that&amp;#39;s protected from the elements but no HVAC, for example. It will store surveillance footage, so 24/7 writing and occasional reading. One other caveat is that it has to run windows. Our VMS recording application is windows only. A pre-built NAS would obviously work but then it turns into two devices to manage instead of one. Since I&amp;#39;d still have a PC running the software. Because of that, I&amp;#39;d rather stick to a PC-only solution, but not married to it.&lt;/p&gt;\n\n&lt;p&gt;One option is simply building a PC with a bunch of spinners. But it needs some resiliency. I manage them, but I\u2019ve never built out a RAID device myself from scratch. I\u2019ve read not to use diskmgmt raid or storage spaces. Though some say if you&amp;#39;re just running a mirror, no parity, storage spaces can be fine. This doesn&amp;#39;t need to be fancy. A RAID 1 would satisfy the requirements. Maybe storage spaces is fine? Any other decent software raid options out there? Thoughts on Stablebit Drivepool? I would need the storage to show up in Windows as one drive.&lt;/p&gt;\n\n&lt;p&gt;Another option is to run a hardware raid card in the build. I&amp;#39;ve read that using some consumer level card like startech is basically useless. It looks like a decent LSI enterprise card with caching and BBU runs ~$1,000? At that price, it may be better to get smaller Dell or HP server tower instead of trying to piece something together from scratch.&lt;/p&gt;\n\n&lt;p&gt;And of course, the &amp;quot;easiest&amp;quot; solution is to just get a pre-built nas and call it day.&lt;/p&gt;\n\n&lt;p&gt;Kind of looking for a conversation here. Thoughts on the some of the assumptions made due to what I&amp;#39;ve read so far. Some real-world experience with any of the outlined options. Again, it&amp;#39;s going to live in outdoor temperatures. Around here anywhere from 80-100 in the summer, down to 10-30 in the winter. Not really sure if any options are going to hold up better than others given that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b903d7", "is_robot_indexable": true, "report_reasons": null, "author": "sageRJ", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b903d7/cctv_storage_solutions_outside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b903d7/cctv_storage_solutions_outside/", "subreddit_subscribers": 737032, "created_utc": 1709832051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Many years ago i manually copied all the photos and videos i had to a HDD. Ever since then i just cloned my hard drives with a clone dock and called it a day. Recently i found a external hard drive from my dad (which still works 12+ years later) which has a lot of pictures / videos i don't have already. So how would i go about copying those files onto my HDD?\n\nI thought doing the same thing i did before. Putting my drive into my drive bay and connecting the external drive through usb, then the old copy and paste. But is there a better option? A more secure option? A hopefully not expensive option? ", "author_fullname": "t2_kht4ovrc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying external HDD onto internal HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8w9nv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709822412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many years ago i manually copied all the photos and videos i had to a HDD. Ever since then i just cloned my hard drives with a clone dock and called it a day. Recently i found a external hard drive from my dad (which still works 12+ years later) which has a lot of pictures / videos i don&amp;#39;t have already. So how would i go about copying those files onto my HDD?&lt;/p&gt;\n\n&lt;p&gt;I thought doing the same thing i did before. Putting my drive into my drive bay and connecting the external drive through usb, then the old copy and paste. But is there a better option? A more secure option? A hopefully not expensive option? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8w9nv", "is_robot_indexable": true, "report_reasons": null, "author": "TengokuDaimakyo", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8w9nv/copying_external_hdd_onto_internal_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8w9nv/copying_external_hdd_onto_internal_hdd/", "subreddit_subscribers": 737032, "created_utc": 1709822412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On the tuto it says $ ia upload &lt;identifier&gt; file1 file2 --metadata=\"mediatype:texts\" --metadata=\"blah:arg\"\n\nBut does that upload the file to an existing archive?\n\nCan someone please show me how that line looks like normally? Because everything I try gives me an error. I know what the identifier is. I don't know how to write the file. Do I write the entire path? Do I include the .mp3? When I use &lt; and &gt; it says it was unexpected or something like that. So can you show me the line you would use to upload a file, or multiple, to an existing archive?\n\nEdit : And I can't (on the site itself) change the name of a file. It doesn't work. I change it, click on \"done editing\", and it doesn't change. What am I doing wrong?", "author_fullname": "t2_ymgr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Internet Archive) How to use the python to upload to existing archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8sakj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709810570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the tuto it says $ ia upload &amp;lt;identifier&amp;gt; file1 file2 --metadata=&amp;quot;mediatype:texts&amp;quot; --metadata=&amp;quot;blah:arg&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But does that upload the file to an existing archive?&lt;/p&gt;\n\n&lt;p&gt;Can someone please show me how that line looks like normally? Because everything I try gives me an error. I know what the identifier is. I don&amp;#39;t know how to write the file. Do I write the entire path? Do I include the .mp3? When I use &amp;lt; and &amp;gt; it says it was unexpected or something like that. So can you show me the line you would use to upload a file, or multiple, to an existing archive?&lt;/p&gt;\n\n&lt;p&gt;Edit : And I can&amp;#39;t (on the site itself) change the name of a file. It doesn&amp;#39;t work. I change it, click on &amp;quot;done editing&amp;quot;, and it doesn&amp;#39;t change. What am I doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8sakj", "is_robot_indexable": true, "report_reasons": null, "author": "PouffieEdc", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8sakj/internet_archive_how_to_use_the_python_to_upload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8sakj/internet_archive_how_to_use_the_python_to_upload/", "subreddit_subscribers": 737032, "created_utc": 1709810570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,\n\nI bought an IcyBox IB-3640SU3 DAS enclosure a few months ago and we just had a power outage and I noticed that the IB-3640SU3 didn't power back on once the power was restored. I thought it might have something to do with the SYNC setting so I simulated a power outage on all 3 sync modes (On, Off &amp; Hibernate) but it still didn't power on once power was restored. Each time I had to manually press the 'on' button in order to power it on.\n\nFor any of the guys out there with this disk enclosure, is this the same experience you have? Is there any way to have the box power on automatically? If not, this is a pretty terrible design flaw in my opinion.\n\nEdit: I just received a response from IcyBox support and it appears that there's no way to enable the enclosure to automatically power back on after a power outage.\n\n&amp;#x200B;\n\n&gt;Thank you very much for your question to us. If the device is turned off after a power outage, you will need to turn it on at the switch. There is no other way.\n\n&amp;#x200B;", "author_fullname": "t2_rsg05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IcyBox IB-3640SU3 - power on after power outage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8rmwp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709814428.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709808213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I bought an IcyBox IB-3640SU3 DAS enclosure a few months ago and we just had a power outage and I noticed that the IB-3640SU3 didn&amp;#39;t power back on once the power was restored. I thought it might have something to do with the SYNC setting so I simulated a power outage on all 3 sync modes (On, Off &amp;amp; Hibernate) but it still didn&amp;#39;t power on once power was restored. Each time I had to manually press the &amp;#39;on&amp;#39; button in order to power it on.&lt;/p&gt;\n\n&lt;p&gt;For any of the guys out there with this disk enclosure, is this the same experience you have? Is there any way to have the box power on automatically? If not, this is a pretty terrible design flaw in my opinion.&lt;/p&gt;\n\n&lt;p&gt;Edit: I just received a response from IcyBox support and it appears that there&amp;#39;s no way to enable the enclosure to automatically power back on after a power outage.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Thank you very much for your question to us. If the device is turned off after a power outage, you will need to turn it on at the switch. There is no other way.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8rmwp", "is_robot_indexable": true, "report_reasons": null, "author": "ozgyrex", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8rmwp/icybox_ib3640su3_power_on_after_power_outage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8rmwp/icybox_ib3640su3_power_on_after_power_outage/", "subreddit_subscribers": 737032, "created_utc": 1709808213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Last year in May Crystal Disk Mark showed that my drive had some uncorrectable sectors and some reallocated sectors. I checked forums then and it was suggested that if they keep increasing then to ditch the drive. They didn't increase for about 6 months so I was happy. But now, after 10 months, reallocated sectors have increased from 30 to 83 but uncorrectable sectors have gone down to 4 from 6 (how is that possible?)  \nHDD is not in warranty. Should I retire the drive?", "author_fullname": "t2_1358x8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reallocated sector count increasing slowly, is it time to retire the drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8jngj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709780639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last year in May Crystal Disk Mark showed that my drive had some uncorrectable sectors and some reallocated sectors. I checked forums then and it was suggested that if they keep increasing then to ditch the drive. They didn&amp;#39;t increase for about 6 months so I was happy. But now, after 10 months, reallocated sectors have increased from 30 to 83 but uncorrectable sectors have gone down to 4 from 6 (how is that possible?)&lt;br/&gt;\nHDD is not in warranty. Should I retire the drive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8jngj", "is_robot_indexable": true, "report_reasons": null, "author": "BadlySynced", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8jngj/reallocated_sector_count_increasing_slowly_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8jngj/reallocated_sector_count_increasing_slowly_is_it/", "subreddit_subscribers": 737032, "created_utc": 1709780639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m curious to know what tools people use to track their datasets. Git is great for code but not so much binary files like media assets or unity assets. \n\nThis is more of an academic question than just plain storage but I\u2019m curious to hear from this community . ", "author_fullname": "t2_ey91h2hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you version control or keep track of data/files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8b2ue", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709758783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m curious to know what tools people use to track their datasets. Git is great for code but not so much binary files like media assets or unity assets. &lt;/p&gt;\n\n&lt;p&gt;This is more of an academic question than just plain storage but I\u2019m curious to hear from this community . &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8b2ue", "is_robot_indexable": true, "report_reasons": null, "author": "Royal_Difficulty_678", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8b2ue/how_do_you_version_control_or_keep_track_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8b2ue/how_do_you_version_control_or_keep_track_of/", "subreddit_subscribers": 737032, "created_utc": 1709758783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a bunch of high capacity samsung 512gb sd for a ridicoulsly low price (2\u20ac/tb), i would like to merge them with something like a multiple reader, like a USB hub but for sd, on the Internet I could not find anything, is there something like this or a DIY project, I have some soldering skills I could try to do something?", "author_fullname": "t2_eoa4e7zl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple sd reader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8xxn1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709826505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a bunch of high capacity samsung 512gb sd for a ridicoulsly low price (2\u20ac/tb), i would like to merge them with something like a multiple reader, like a USB hub but for sd, on the Internet I could not find anything, is there something like this or a DIY project, I have some soldering skills I could try to do something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8xxn1", "is_robot_indexable": true, "report_reasons": null, "author": "Loitering14", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8xxn1/multiple_sd_reader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8xxn1/multiple_sd_reader/", "subreddit_subscribers": 737032, "created_utc": 1709826505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First of all let me say that I've spent weeks digging through this sub's valuable information in terms of optical media archiving, and stumbled upon many of the Verbatim-CMC-HTL-LTH comments that were very insightful. However, I wasn't be able to read a definitive answer to the actual state of archiving data on off-the-shelf media in 2023.\n\nFirst and foremost, with all the *Blu-ray M-Disc not being real M-Discs anymore* things going on, what is a realistic expectation of data longevity, when discs are stored under normal everyday conditions (e.g. outside of the 10-25*C temperature and 40-60% humidity range)? Is it still worth considering optical media to get at least a decade of stress free archiving or does media produced in the recent years show significant fallout rates even after short periods of time? What's the actual return on invest on today's media, even speaking about regular \"cheap\" Blu-ray discs and what would be the recommended re-burn interval based on the little experience with post-2019 media that there might be?\n\nThank you!", "author_fullname": "t2_kigcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realistic expectation of Blu-ray M-Disc life in 2023 under everyday conditions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8xe10", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709825215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all let me say that I&amp;#39;ve spent weeks digging through this sub&amp;#39;s valuable information in terms of optical media archiving, and stumbled upon many of the Verbatim-CMC-HTL-LTH comments that were very insightful. However, I wasn&amp;#39;t be able to read a definitive answer to the actual state of archiving data on off-the-shelf media in 2023.&lt;/p&gt;\n\n&lt;p&gt;First and foremost, with all the &lt;em&gt;Blu-ray M-Disc not being real M-Discs anymore&lt;/em&gt; things going on, what is a realistic expectation of data longevity, when discs are stored under normal everyday conditions (e.g. outside of the 10-25*C temperature and 40-60% humidity range)? Is it still worth considering optical media to get at least a decade of stress free archiving or does media produced in the recent years show significant fallout rates even after short periods of time? What&amp;#39;s the actual return on invest on today&amp;#39;s media, even speaking about regular &amp;quot;cheap&amp;quot; Blu-ray discs and what would be the recommended re-burn interval based on the little experience with post-2019 media that there might be?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8xe10", "is_robot_indexable": true, "report_reasons": null, "author": "mrusme", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8xe10/realistic_expectation_of_bluray_mdisc_life_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8xe10/realistic_expectation_of_bluray_mdisc_life_in/", "subreddit_subscribers": 737032, "created_utc": 1709825215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nHello fellow DataHoarders,\n\nI recently subscribed to omar-sherbeni.com, a website offering educational courses. The videos on this site, often exceeding an hour in length, are streamed via an iframe from \"iframe.mediadelivery.net\". Due to my intermittent internet access, I am keen on downloading these videos for offline viewing.\n\nHowever, I've encountered difficulties in figuring out how to effectively download these videos. An  example of a video I wish to download is available at this link: \n\n[https://iframe.mediadelivery.net/embed/33074/f022ef5e-c772-4962-82e2-ade0b50dd164](https://iframe.mediadelivery.net/embed/33074/f022ef5e-c772-4962-82e2-ade0b50dd164)?\n\nsource video (403 error):\n\n[https://vz-595ffcda-b51.b-cdn.net/f022ef5e-c772-4962-82e2-ade0b50dd164/play\\_720p.mp4](https://vz-595ffcda-b51.b-cdn.net/f022ef5e-c772-4962-82e2-ade0b50dd164/play_720p.mp4)\n\nI have tried YT-DLP with no success\n\nI would greatly appreciate any guidance or suggestions from this community on how to approach this. Your expertise and knowledge in this area would be immensely helpful to me.\n\nThank you in advance for your assistance!", "author_fullname": "t2_47ti727n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "assistance downloading embedded video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8wsdz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709823733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DataHoarders,&lt;/p&gt;\n\n&lt;p&gt;I recently subscribed to omar-sherbeni.com, a website offering educational courses. The videos on this site, often exceeding an hour in length, are streamed via an iframe from &amp;quot;iframe.mediadelivery.net&amp;quot;. Due to my intermittent internet access, I am keen on downloading these videos for offline viewing.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;ve encountered difficulties in figuring out how to effectively download these videos. An  example of a video I wish to download is available at this link: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://iframe.mediadelivery.net/embed/33074/f022ef5e-c772-4962-82e2-ade0b50dd164\"&gt;https://iframe.mediadelivery.net/embed/33074/f022ef5e-c772-4962-82e2-ade0b50dd164&lt;/a&gt;?&lt;/p&gt;\n\n&lt;p&gt;source video (403 error):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://vz-595ffcda-b51.b-cdn.net/f022ef5e-c772-4962-82e2-ade0b50dd164/play_720p.mp4\"&gt;https://vz-595ffcda-b51.b-cdn.net/f022ef5e-c772-4962-82e2-ade0b50dd164/play_720p.mp4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have tried YT-DLP with no success&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any guidance or suggestions from this community on how to approach this. Your expertise and knowledge in this area would be immensely helpful to me.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/N6BiSsElFCm1BjEtMgdqGHZVns8oAcEgQzN17XJ0pfA.jpg?auto=webp&amp;s=9fbd32f7dd78dbf24bb3443c2c354752d3178969", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/N6BiSsElFCm1BjEtMgdqGHZVns8oAcEgQzN17XJ0pfA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa1dba74046ea6606a30ee215ec9f35ceafd0f0a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/N6BiSsElFCm1BjEtMgdqGHZVns8oAcEgQzN17XJ0pfA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c12b655a0528ed88c676c3c9021a29c712704da8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/N6BiSsElFCm1BjEtMgdqGHZVns8oAcEgQzN17XJ0pfA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=275b71a0579a080f1d867e6a5141419dd24b1176", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/N6BiSsElFCm1BjEtMgdqGHZVns8oAcEgQzN17XJ0pfA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed8c92de45afdbc2417b25e3939b97c1c8cd2439", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/N6BiSsElFCm1BjEtMgdqGHZVns8oAcEgQzN17XJ0pfA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a2c1565634c8e9d6603d90a336d41c2623b1212", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/N6BiSsElFCm1BjEtMgdqGHZVns8oAcEgQzN17XJ0pfA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5b00738506e6cfbf0152b6b4d33962f279e740a1", "width": 1080, "height": 607}], "variants": {}, "id": "sY4et3N69Lebm1lEJKxdtj6c6D-wPGxzxkMqE3H7_cc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8wsdz", "is_robot_indexable": true, "report_reasons": null, "author": "Im_IegaIIy_blind", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8wsdz/assistance_downloading_embedded_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8wsdz/assistance_downloading_embedded_video/", "subreddit_subscribers": 737032, "created_utc": 1709823733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI have some YouTube videos archived that are now down. Each video has .mkv, .description, .info.json, .webp, and some long funky filenames.\n\nI added all of the files to a .7z archive, setting the compression level to \"0 - Store\" (no point using compression since archival won't compress videos/images anyway), and then split the 10GB of files into 15 .7z.xxx files, each 700mb in size, the last being 200mb.\n\nI also added an information .txt file and a .sums checksum file from OpenHashTab. (Is this the right way to submit checksums?)\n\nI used https://archive.org/contribute.php (it said new beta uploader) in Firefox to upload the files, it was extremely slow, at about 200kbps, even though I have super fast internet. Tried a USA VPN, which people said would improve speeds, but no dice, anyways:\n\nI left it overnight, and I came back to a box saying \"There is a network problem\" (400 Bad Data). I clicked details and got this (ignore the censored part of the path, I put that there).\n\n\n    &lt;?xml version='1.0' encoding='UTF-8'?&gt;\n    &lt;Error&gt;&lt;Code&gt;BadContent&lt;/Code&gt;&lt;Message&gt;Uploaded content is unacceptable.&lt;/Message&gt;&lt;Resource&gt;Traceback (most recent call last):\n      File \"/petabox/sw/ias3/deploy/check_file.py\", line 123, in check_encrypted_archive\n        t = subprocess.check_output(command).decode(\"utf-8\")\n      File \"/usr/lib/python3.8/subprocess.py\", line 415, in check_output\n        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n      File \"/usr/lib/python3.8/subprocess.py\", line 516, in run\n        raise CalledProcessError(retcode, process.args,\n    subprocess.CalledProcessError: Command '['7z', 'l', '-slt', '-p', '', '--', '/3/incoming/REMOTE_SUBMIT/CENSORED']' returned non-zero exit status 2.\n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File \"/petabox/sw/ias3/deploy/check_file.py\", line 229, in main\n        problems = do_checks(name, path)\n      File \"/petabox/sw/ias3/deploy/check_file.py\", line 187, in do_checks\n        r = check(name, path)\n      File \"/petabox/sw/ias3/deploy/check_file.py\", line 125, in check_encrypted_archive\n        if 'Can not open encrypted archive' in e.output:\n    TypeError: a bytes-like object is required, not 'str'\n    \n    &lt;/Resource&gt;&lt;RequestId&gt;CENSORED&lt;/RequestId&gt;&lt;/Error&gt;\n\nI hit Resume, but it just re-does the last GB of the upload, then the same error appears, every time. It also does that part way too fast for some reason.\n\nThe created 7z opens and extracts just fine on my machine...\n\nIt's not like I had an interruption, I set my laptop to never sleep and the internet never really drops out. I figured its having trouble reading the split archives. \n\nSo, my questions:\n\n- How can I avoid this again? Is it because it's 7z or split? Strange because I came across these on archive.org many times before\n\n- Is there a way to fix the atrocious upload speed? \n\n- Would it be better to make this a .torrent on archive.org? If so I don't know how I feel about hosting it myself, does archive.org handle that?\n\n- Any guides out there on the python CLI method or BitTorrent upload method? I have QBitTorrent. Couldn't find much on google or their site.\n\n- Is there a \"correct\" standardised way to make a checksums file for my files?", "author_fullname": "t2_5tc3oklu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive.org - \"Uploaded content is unacceptable - Can not open encrypted archive\" when uploading a split .7z at \"Store\" level", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8wn1s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709823804.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709823379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have some YouTube videos archived that are now down. Each video has .mkv, .description, .info.json, .webp, and some long funky filenames.&lt;/p&gt;\n\n&lt;p&gt;I added all of the files to a .7z archive, setting the compression level to &amp;quot;0 - Store&amp;quot; (no point using compression since archival won&amp;#39;t compress videos/images anyway), and then split the 10GB of files into 15 .7z.xxx files, each 700mb in size, the last being 200mb.&lt;/p&gt;\n\n&lt;p&gt;I also added an information .txt file and a .sums checksum file from OpenHashTab. (Is this the right way to submit checksums?)&lt;/p&gt;\n\n&lt;p&gt;I used &lt;a href=\"https://archive.org/contribute.php\"&gt;https://archive.org/contribute.php&lt;/a&gt; (it said new beta uploader) in Firefox to upload the files, it was extremely slow, at about 200kbps, even though I have super fast internet. Tried a USA VPN, which people said would improve speeds, but no dice, anyways:&lt;/p&gt;\n\n&lt;p&gt;I left it overnight, and I came back to a box saying &amp;quot;There is a network problem&amp;quot; (400 Bad Data). I clicked details and got this (ignore the censored part of the path, I put that there).&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&amp;#39;1.0&amp;#39; encoding=&amp;#39;UTF-8&amp;#39;?&amp;gt;\n&amp;lt;Error&amp;gt;&amp;lt;Code&amp;gt;BadContent&amp;lt;/Code&amp;gt;&amp;lt;Message&amp;gt;Uploaded content is unacceptable.&amp;lt;/Message&amp;gt;&amp;lt;Resource&amp;gt;Traceback (most recent call last):\n  File &amp;quot;/petabox/sw/ias3/deploy/check_file.py&amp;quot;, line 123, in check_encrypted_archive\n    t = subprocess.check_output(command).decode(&amp;quot;utf-8&amp;quot;)\n  File &amp;quot;/usr/lib/python3.8/subprocess.py&amp;quot;, line 415, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &amp;quot;/usr/lib/python3.8/subprocess.py&amp;quot;, line 516, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command &amp;#39;[&amp;#39;7z&amp;#39;, &amp;#39;l&amp;#39;, &amp;#39;-slt&amp;#39;, &amp;#39;-p&amp;#39;, &amp;#39;&amp;#39;, &amp;#39;--&amp;#39;, &amp;#39;/3/incoming/REMOTE_SUBMIT/CENSORED&amp;#39;]&amp;#39; returned non-zero exit status 2.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &amp;quot;/petabox/sw/ias3/deploy/check_file.py&amp;quot;, line 229, in main\n    problems = do_checks(name, path)\n  File &amp;quot;/petabox/sw/ias3/deploy/check_file.py&amp;quot;, line 187, in do_checks\n    r = check(name, path)\n  File &amp;quot;/petabox/sw/ias3/deploy/check_file.py&amp;quot;, line 125, in check_encrypted_archive\n    if &amp;#39;Can not open encrypted archive&amp;#39; in e.output:\nTypeError: a bytes-like object is required, not &amp;#39;str&amp;#39;\n\n&amp;lt;/Resource&amp;gt;&amp;lt;RequestId&amp;gt;CENSORED&amp;lt;/RequestId&amp;gt;&amp;lt;/Error&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I hit Resume, but it just re-does the last GB of the upload, then the same error appears, every time. It also does that part way too fast for some reason.&lt;/p&gt;\n\n&lt;p&gt;The created 7z opens and extracts just fine on my machine...&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not like I had an interruption, I set my laptop to never sleep and the internet never really drops out. I figured its having trouble reading the split archives. &lt;/p&gt;\n\n&lt;p&gt;So, my questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;How can I avoid this again? Is it because it&amp;#39;s 7z or split? Strange because I came across these on archive.org many times before&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is there a way to fix the atrocious upload speed? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Would it be better to make this a .torrent on archive.org? If so I don&amp;#39;t know how I feel about hosting it myself, does archive.org handle that?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any guides out there on the python CLI method or BitTorrent upload method? I have QBitTorrent. Couldn&amp;#39;t find much on google or their site.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is there a &amp;quot;correct&amp;quot; standardised way to make a checksums file for my files?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8wn1s", "is_robot_indexable": true, "report_reasons": null, "author": "Gold-Advisor", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8wn1s/archiveorg_uploaded_content_is_unacceptable_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8wn1s/archiveorg_uploaded_content_is_unacceptable_can/", "subreddit_subscribers": 737032, "created_utc": 1709823379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A bit of context: A few months ago my computer SSD died, and with that I lost 2 years of college data, 15 years of family photos, and 5+ years of personal projects, I was very devastated by the loss, as I lost so many family photos that I loved and many projects that I worked on for years, even though most were archived.  \nEver since that I've been using Google Drive to store my data, or at least the data that I don't use in my day to day. However, I've already hit my 15GB limit, and paying for more data is way too expensive, Google offers good deals, but I don't think I want to pay that much just to store my data out of fear to lose it.   \nNow, I've been looking at some NAS models for a long time, although I always saw them as unnecessary, however, I think they should be very good for storing and archiving data; however, I've seen many posts here saying that a NAS is not a good backup, and that using a system like RAID 1 is also not necessarily good for protecting data, which is very confusing for me.  \nSo if a NAS is not an actual backup, neither is RAID 1, shouldn't I just pay for cloud storage like Google's? I see the appeal of a home server, but I could also have a home server with low storage for personal use and then archive everything on the cloud. So, should I even be considering a NAS at all?  \nIn other words, should I use a NAS for my archiving purposes? I mean, aside from family videos, I don't think I will be streaming movies from a NAS, I will primarily use it as a long term memory, maybe with some redundancy to ensure safety of my data, is that a bad idea?  \n", "author_fullname": "t2_11zqbc59", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving family and personal data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8bcc9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709759405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A bit of context: A few months ago my computer SSD died, and with that I lost 2 years of college data, 15 years of family photos, and 5+ years of personal projects, I was very devastated by the loss, as I lost so many family photos that I loved and many projects that I worked on for years, even though most were archived.&lt;br/&gt;\nEver since that I&amp;#39;ve been using Google Drive to store my data, or at least the data that I don&amp;#39;t use in my day to day. However, I&amp;#39;ve already hit my 15GB limit, and paying for more data is way too expensive, Google offers good deals, but I don&amp;#39;t think I want to pay that much just to store my data out of fear to lose it.&lt;br/&gt;\nNow, I&amp;#39;ve been looking at some NAS models for a long time, although I always saw them as unnecessary, however, I think they should be very good for storing and archiving data; however, I&amp;#39;ve seen many posts here saying that a NAS is not a good backup, and that using a system like RAID 1 is also not necessarily good for protecting data, which is very confusing for me.&lt;br/&gt;\nSo if a NAS is not an actual backup, neither is RAID 1, shouldn&amp;#39;t I just pay for cloud storage like Google&amp;#39;s? I see the appeal of a home server, but I could also have a home server with low storage for personal use and then archive everything on the cloud. So, should I even be considering a NAS at all?&lt;br/&gt;\nIn other words, should I use a NAS for my archiving purposes? I mean, aside from family videos, I don&amp;#39;t think I will be streaming movies from a NAS, I will primarily use it as a long term memory, maybe with some redundancy to ensure safety of my data, is that a bad idea?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1b8bcc9", "is_robot_indexable": true, "report_reasons": null, "author": "BOB5941", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8bcc9/archiving_family_and_personal_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8bcc9/archiving_family_and_personal_data/", "subreddit_subscribers": 737032, "created_utc": 1709759405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What would be the best, affordable way to scan a bunch of physical patches (think like police/firefighter uniforms) in high quality for digital use? I\u2019m thinking a flatbed scanner would be good, but I\u2019m not sure what to look for with it or where to start from.", "author_fullname": "t2_8cj4czo4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanning Patches", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8b5t5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709758964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What would be the best, affordable way to scan a bunch of physical patches (think like police/firefighter uniforms) in high quality for digital use? I\u2019m thinking a flatbed scanner would be good, but I\u2019m not sure what to look for with it or where to start from.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8b5t5", "is_robot_indexable": true, "report_reasons": null, "author": "srtj193529", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8b5t5/scanning_patches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8b5t5/scanning_patches/", "subreddit_subscribers": 737032, "created_utc": 1709758964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First post in this group so bear with me. From what I've read so far I suppose I'm probably a lightweight data hoarder compared to most people here.\n\nI'm retiring a 4 year old Seagate Barracuda 4TB that developed bad sectors (everything had been backed up so no data loss worth mentioning). Meanwhile my main WD 6TB drive containing most of my video data is filled to the brim. I have been looking into internal SATA drives that are at least 10TB in size. I'm sensitive to noise while power consumption and long lifespan are concerns as well (not mentioning prices, they are what they are nowadays). Since speed is of little concern to me I always stuck with 5400-5900rpm drives. Unfortunately those seem to be non-existent above 8TB. Unless I'm missing something.\n\nFrom what I found myself, and after reading this thread (https://www.reddit.com/r/DataHoarder/comments/15hqcla/who\\_makes\\_the\\_quietest\\_hard\\_drives\\_1014tb/), I suppose WD Red Plus are probably the best option for me, even though I wish their 10+TB drives weren't 7200rpm. Unless the difference in noise is really significant I wouldn't bother with 8TB since it wouldn't take long until those would be filled to the brim as well.\n\nI'm open to recommendations and suggestions. I'm EU-based, if it makes any difference.", "author_fullname": "t2_s00xs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quiet 10+TB HDD for NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8adbh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709758620.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709757104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First post in this group so bear with me. From what I&amp;#39;ve read so far I suppose I&amp;#39;m probably a lightweight data hoarder compared to most people here.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m retiring a 4 year old Seagate Barracuda 4TB that developed bad sectors (everything had been backed up so no data loss worth mentioning). Meanwhile my main WD 6TB drive containing most of my video data is filled to the brim. I have been looking into internal SATA drives that are at least 10TB in size. I&amp;#39;m sensitive to noise while power consumption and long lifespan are concerns as well (not mentioning prices, they are what they are nowadays). Since speed is of little concern to me I always stuck with 5400-5900rpm drives. Unfortunately those seem to be non-existent above 8TB. Unless I&amp;#39;m missing something.&lt;/p&gt;\n\n&lt;p&gt;From what I found myself, and after reading this thread (&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/15hqcla/who%5C_makes%5C_the%5C_quietest%5C_hard%5C_drives%5C_1014tb/\"&gt;https://www.reddit.com/r/DataHoarder/comments/15hqcla/who\\_makes\\_the\\_quietest\\_hard\\_drives\\_1014tb/&lt;/a&gt;), I suppose WD Red Plus are probably the best option for me, even though I wish their 10+TB drives weren&amp;#39;t 7200rpm. Unless the difference in noise is really significant I wouldn&amp;#39;t bother with 8TB since it wouldn&amp;#39;t take long until those would be filled to the brim as well.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to recommendations and suggestions. I&amp;#39;m EU-based, if it makes any difference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b8adbh", "is_robot_indexable": true, "report_reasons": null, "author": "LeadBounder", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b8adbh/quiet_10tb_hdd_for_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b8adbh/quiet_10tb_hdd_for_nas/", "subreddit_subscribers": 737032, "created_utc": 1709757104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, as my title states I'm looking for advice on my backup process. Here is my current hardware and what's stored on it-\n\n2TB internal SSD - Boot drive, has Google Drive backup on it, my documents, pictures, videos, music\n\n1TB internal SSD - backup of GDrive docs, all docs, pics, vids, music\n\n2TB external HDD - backup of everything on the 1TB SSD\n\nI recently picked up 4TB and 6TB internal drives because I was running out of space on my 1TB SSD. I use FreeFileSync for my backups. \n\nMy current plan is to move all my media files to the 4tb drive, and then backup those files and all my documents and GDrive files from my 2tb SSD onto the 6tb HDD. I'm also going to create an image of my system (settings, programs) to restore from, should I need it, and put it on my 1tb SSD or 2tb external HDD. Definitely not as thorough as some of the setups I've seen here but I would love some input/opinions on my setup. Thanks!\n\nedit: no comments and a downvote. what gives? just trying to get my stuff set up damn", "author_fullname": "t2_hz50o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner Data Hoarder looking for setup/backup advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b88b4r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709834415.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709752328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, as my title states I&amp;#39;m looking for advice on my backup process. Here is my current hardware and what&amp;#39;s stored on it-&lt;/p&gt;\n\n&lt;p&gt;2TB internal SSD - Boot drive, has Google Drive backup on it, my documents, pictures, videos, music&lt;/p&gt;\n\n&lt;p&gt;1TB internal SSD - backup of GDrive docs, all docs, pics, vids, music&lt;/p&gt;\n\n&lt;p&gt;2TB external HDD - backup of everything on the 1TB SSD&lt;/p&gt;\n\n&lt;p&gt;I recently picked up 4TB and 6TB internal drives because I was running out of space on my 1TB SSD. I use FreeFileSync for my backups. &lt;/p&gt;\n\n&lt;p&gt;My current plan is to move all my media files to the 4tb drive, and then backup those files and all my documents and GDrive files from my 2tb SSD onto the 6tb HDD. I&amp;#39;m also going to create an image of my system (settings, programs) to restore from, should I need it, and put it on my 1tb SSD or 2tb external HDD. Definitely not as thorough as some of the setups I&amp;#39;ve seen here but I would love some input/opinions on my setup. Thanks!&lt;/p&gt;\n\n&lt;p&gt;edit: no comments and a downvote. what gives? just trying to get my stuff set up damn&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1b88b4r", "is_robot_indexable": true, "report_reasons": null, "author": "SecretlyCarl", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1b88b4r/beginner_data_hoarder_looking_for_setupbackup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1b88b4r/beginner_data_hoarder_looking_for_setupbackup/", "subreddit_subscribers": 737032, "created_utc": 1709752328.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}