{"kind": "Listing", "data": {"after": "t3_1b8ajea", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I started my first project on Dbt and how boy, this tool is INSANE. I just feel like any tool similar to Azure Data Factory, or Talend Cloud Platform are LIGHT-YEARS away from the power of this tool. If you think about modularity, pricing, agility, time to market, documentation, versioning, frameworks with reusability, etc. Dbt is just SO MUCH better.\n\nIf you were about to start a new cloud project, why would you not choose Fivetran/Stitch + Dbt ?", "author_fullname": "t2_rz2xn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will Dbt just taker over the world ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8e72j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709768203.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709766176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I started my first project on Dbt and how boy, this tool is INSANE. I just feel like any tool similar to Azure Data Factory, or Talend Cloud Platform are LIGHT-YEARS away from the power of this tool. If you think about modularity, pricing, agility, time to market, documentation, versioning, frameworks with reusability, etc. Dbt is just SO MUCH better.&lt;/p&gt;\n\n&lt;p&gt;If you were about to start a new cloud project, why would you not choose Fivetran/Stitch + Dbt ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b8e72j", "is_robot_indexable": true, "report_reasons": null, "author": "Ownards", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8e72j/will_dbt_just_taker_over_the_world/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8e72j/will_dbt_just_taker_over_the_world/", "subreddit_subscribers": 166488, "created_utc": 1709766176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, recently I completed another personal project. Any suggestions are welcome.\n\n***Update 1: Add AWS EKS to the project.***\n\n[Github Repo](https://github.com/Zzdragon66/stock-streaming-project)\n\n## Project Description\n\n* This project leverages Python, Kafka, and Spark to process real-time streaming data from both stock markets and Reddit. It employs a Long Short-Term Memory (LSTM) deep learning model to conduct real-time predictions on SPY (S&amp;P 500 ETF) stock data. Additionally, the project utilizes Grafana for the real-time visualization of stock data, predictive analytics, and reddit data, providing a comprehensive and dynamic overview of market trends and sentiments.\n\n## Demo\n\n&amp;#x200B;\n\nhttps://i.redd.it/t85j4210dpmc1.gif\n\n## Project Structure\n\n&amp;#x200B;\n\nhttps://preview.redd.it/69o9uzhjaumc1.png?width=4084&amp;format=png&amp;auto=webp&amp;s=9a11bbd73c64d060aad8f96e75ef4df57d86c6c6\n\n## Tools\n\n1. Apache Airflow: Data pipeline orchestration\n2. Apache Kafka: Stream data handling\n3. Apache Spark: batch data processing\n4. Apache Cassandra: NoSQL database to store time series data\n5. Docker + Kubernets: Containerization and Docker Orchestration\n6. AWS: Amazon Elastic Kubernetes Service(EKS) to run Kubernets on cloud \n7. Pytorch: Deep learning model\n8. Grafna: Stream Data visualization\n9. Python: produce streaming data with multithreading\n\n## Project Design Choice\n\n## Kafka\n\n* Why Kafka?\n   * Kafak serves a stream data handler to feed data into spark and deep learning model\n* Design of kafka\n   * I utilize Python's multi-threading capabilities to simultaneously produce stock data, enhancing the throughput by exploiting parallelism. Consequently, I partition the topic according to the number of stocks, allowing each thread to direct its data into a distinct partition, thereby optimizing the data flow and maximizing efficiency\n\n## Cassandra Database Design\n\n* Stock data contains the data of `stock` symbol and `utc_timestamp`, which can be used to uniquely identify the single data point. Therefore I use those two features as the primary key\n* Use `utc_timestamp` as the clustering key to store the time series data in ascending order for efficient read(sequantial read for a time series data) and high throughput write(real-time data only appends to the end of parition)\n\n## Deep learning model Discussion\n\n* Data\n   * Train Data Dimension (N, T, D)\n      * N is number of data in a batch\n      * T=200 look back two hundred seconds data\n      * D=5 the features in the data (price, number of transactions, high price, low price, volumes)\n   * Prediction Data Dimension (1, 200, 5)\n* Data Preprocessing:\n   * Use MinMaxScaler to make sure each feature has similar scale\n* Model Structure:\n   * X-&gt;\\[LSTM \\* 5\\]-&gt;Linear-&gt;Price-Prediction\n* How the Model works:\n   * At current timestamp t, get latest 200 time sereis data before $t$ in ascending `utc_timestamp` order. Feed the data into deep learning model which will predict the current SPY stock prie at time t.\n* Due to the limited computational resources on my local machine, the \"real-time\" prediction lags behind actual time because of the long computation duration required.\n\n## Future Directions\n\n1. Use Terraform to initialize cloud infrastructure automatically\n\n2. Use kubeflow to train deep learning model automatically\n\n3. Train a better deep learning model to make prediction more accurate and faster", "author_fullname": "t2_5igde9z6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End-End Stock Streaming Project(K8S, Airflow, Kafka, Spark, Pytorch, Docker, Cassandra, Grafna)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"t85j4210dpmc1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=31a407bb797466af045aaacbc93e581ff4f9c604"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=3f984dbc901edba448538f4312ee5c65378d0457"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=b9d568e98b52007048fd496e3e88d056677ca222"}, {"y": 336, "x": 640, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=1f055006987c65e7d8a44ab5e684b7968448b785"}, {"y": 505, "x": 960, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=973819231667912f2bd709258a716265b1aae6b9"}, {"y": 568, "x": 1080, "u": "https://preview.redd.it/t85j4210dpmc1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=8729eaaf8bd74765e4bc387a1e38d29f3135397f"}], "s": {"y": 720, "gif": "https://i.redd.it/t85j4210dpmc1.gif", "mp4": "https://preview.redd.it/t85j4210dpmc1.gif?format=mp4&amp;s=86060fce964241e914ea0f962b15505fa701cb45", "x": 1368}, "id": "t85j4210dpmc1"}, "69o9uzhjaumc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/69o9uzhjaumc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fb2ec975103f58d53b1698d5c2be92f793dd264"}, {"y": 129, "x": 216, "u": "https://preview.redd.it/69o9uzhjaumc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b40a16d0a949b28b43e1c5d4a079660a665538df"}, {"y": 191, "x": 320, "u": "https://preview.redd.it/69o9uzhjaumc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf1ba99df926144ab23f04c6d26d1c0420ebfb7c"}, {"y": 383, "x": 640, "u": "https://preview.redd.it/69o9uzhjaumc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f7676f3c448772ba739d3a799706fdad2c443fbc"}, {"y": 575, "x": 960, "u": "https://preview.redd.it/69o9uzhjaumc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=770960dfd9fb78608a499d3d16993a1fb502999d"}, {"y": 647, "x": 1080, "u": "https://preview.redd.it/69o9uzhjaumc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db73dd11b2b426a9b3bb634715089e4b188d4108"}], "s": {"y": 2448, "x": 4084, "u": "https://preview.redd.it/69o9uzhjaumc1.png?width=4084&amp;format=png&amp;auto=webp&amp;s=9a11bbd73c64d060aad8f96e75ef4df57d86c6c6"}, "id": "69o9uzhjaumc1"}}, "name": "t3_1b7xuw3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Etb8Be2jAfqIOTCmcfwlxS9MSNP6YjwwGhd0i2ARTL8.jpg", "edited": 1709785675.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1709726056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, recently I completed another personal project. Any suggestions are welcome.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Update 1: Add AWS EKS to the project.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Zzdragon66/stock-streaming-project\"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Project Description&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;This project leverages Python, Kafka, and Spark to process real-time streaming data from both stock markets and Reddit. It employs a Long Short-Term Memory (LSTM) deep learning model to conduct real-time predictions on SPY (S&amp;amp;P 500 ETF) stock data. Additionally, the project utilizes Grafana for the real-time visualization of stock data, predictive analytics, and reddit data, providing a comprehensive and dynamic overview of market trends and sentiments.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Demo&lt;/h2&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/t85j4210dpmc1.gif\"&gt;https://i.redd.it/t85j4210dpmc1.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Project Structure&lt;/h2&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/69o9uzhjaumc1.png?width=4084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a11bbd73c64d060aad8f96e75ef4df57d86c6c6\"&gt;https://preview.redd.it/69o9uzhjaumc1.png?width=4084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a11bbd73c64d060aad8f96e75ef4df57d86c6c6&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Tools&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Apache Airflow: Data pipeline orchestration&lt;/li&gt;\n&lt;li&gt;Apache Kafka: Stream data handling&lt;/li&gt;\n&lt;li&gt;Apache Spark: batch data processing&lt;/li&gt;\n&lt;li&gt;Apache Cassandra: NoSQL database to store time series data&lt;/li&gt;\n&lt;li&gt;Docker + Kubernets: Containerization and Docker Orchestration&lt;/li&gt;\n&lt;li&gt;AWS: Amazon Elastic Kubernetes Service(EKS) to run Kubernets on cloud &lt;/li&gt;\n&lt;li&gt;Pytorch: Deep learning model&lt;/li&gt;\n&lt;li&gt;Grafna: Stream Data visualization&lt;/li&gt;\n&lt;li&gt;Python: produce streaming data with multithreading&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h2&gt;Project Design Choice&lt;/h2&gt;\n\n&lt;h2&gt;Kafka&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Why Kafka?\n\n&lt;ul&gt;\n&lt;li&gt;Kafak serves a stream data handler to feed data into spark and deep learning model&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Design of kafka\n\n&lt;ul&gt;\n&lt;li&gt;I utilize Python&amp;#39;s multi-threading capabilities to simultaneously produce stock data, enhancing the throughput by exploiting parallelism. Consequently, I partition the topic according to the number of stocks, allowing each thread to direct its data into a distinct partition, thereby optimizing the data flow and maximizing efficiency&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Cassandra Database Design&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Stock data contains the data of &lt;code&gt;stock&lt;/code&gt; symbol and &lt;code&gt;utc_timestamp&lt;/code&gt;, which can be used to uniquely identify the single data point. Therefore I use those two features as the primary key&lt;/li&gt;\n&lt;li&gt;Use &lt;code&gt;utc_timestamp&lt;/code&gt; as the clustering key to store the time series data in ascending order for efficient read(sequantial read for a time series data) and high throughput write(real-time data only appends to the end of parition)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Deep learning model Discussion&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data\n\n&lt;ul&gt;\n&lt;li&gt;Train Data Dimension (N, T, D)\n\n&lt;ul&gt;\n&lt;li&gt;N is number of data in a batch&lt;/li&gt;\n&lt;li&gt;T=200 look back two hundred seconds data&lt;/li&gt;\n&lt;li&gt;D=5 the features in the data (price, number of transactions, high price, low price, volumes)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Prediction Data Dimension (1, 200, 5)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Data Preprocessing:\n\n&lt;ul&gt;\n&lt;li&gt;Use MinMaxScaler to make sure each feature has similar scale&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Model Structure:\n\n&lt;ul&gt;\n&lt;li&gt;X-&amp;gt;[LSTM * 5]-&amp;gt;Linear-&amp;gt;Price-Prediction&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;How the Model works:\n\n&lt;ul&gt;\n&lt;li&gt;At current timestamp t, get latest 200 time sereis data before $t$ in ascending &lt;code&gt;utc_timestamp&lt;/code&gt; order. Feed the data into deep learning model which will predict the current SPY stock prie at time t.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Due to the limited computational resources on my local machine, the &amp;quot;real-time&amp;quot; prediction lags behind actual time because of the long computation duration required.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Future Directions&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Use Terraform to initialize cloud infrastructure automatically&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use kubeflow to train deep learning model automatically&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Train a better deep learning model to make prediction more accurate and faster&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NJJN1xrI0l6yhfaAlNEyBhQQQ2klY5lQKF9AWZ1c5hY.jpg?auto=webp&amp;s=39bafd82d6a7170a85a81e650dc93be53918bff7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/NJJN1xrI0l6yhfaAlNEyBhQQQ2klY5lQKF9AWZ1c5hY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e65a7e543ddf968c73c97ff318854c536b9be515", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/NJJN1xrI0l6yhfaAlNEyBhQQQ2klY5lQKF9AWZ1c5hY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b0ba1158d9b8e6e1cbe496a6e08b18fe7fc07e5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/NJJN1xrI0l6yhfaAlNEyBhQQQ2klY5lQKF9AWZ1c5hY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a810c43beed9171c5bbed7dc133789270b3febb4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/NJJN1xrI0l6yhfaAlNEyBhQQQ2klY5lQKF9AWZ1c5hY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9f88a48b1720f2cc96c4a9e3322d996bd27827fc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/NJJN1xrI0l6yhfaAlNEyBhQQQ2klY5lQKF9AWZ1c5hY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f5e959929b03e0c49e6f6a4096f75c271d783d12", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/NJJN1xrI0l6yhfaAlNEyBhQQQ2klY5lQKF9AWZ1c5hY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a48ac8f2d821efe98f6a6a6270a4cae09529d54", "width": 1080, "height": 540}], "variants": {}, "id": "HPYeRQeVbNWNCsJaE0BDNqi45GC2p75ZppAXI6G_KZQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1b7xuw3", "is_robot_indexable": true, "report_reasons": null, "author": "AffectionateEmu8146", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7xuw3/endend_stock_streaming_projectk8s_airflow_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7xuw3/endend_stock_streaming_projectk8s_airflow_kafka/", "subreddit_subscribers": 166488, "created_utc": 1709726056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_s1sqpvie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A tool to quickly extract data from websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7zezu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/94pkdfw2rpmc1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/94pkdfw2rpmc1/DASH_96.mp4", "dash_url": "https://v.redd.it/94pkdfw2rpmc1/DASHPlaylist.mpd?a=1712378296%2CNDA2YWU1MjI4NmY3ZjhlN2I4NDQzMjAwMjdiYjEzNTQzYWY4MjNmMGZlNWZhNmVlYWY4ZGRhM2M0ZmZmMjdlZA%3D%3D&amp;v=1&amp;f=sd", "duration": 53, "hls_url": "https://v.redd.it/94pkdfw2rpmc1/HLSPlaylist.m3u8?a=1712378296%2COTI4OGQ0ZTNmZDY5ZTFkMDdlYmI4NGM4NGIyN2YwYmIwZmZjOTliNWJjY2NkZWYyYjEwMmM2NDQxNTk5YmExYg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=fd3f9a9e3fa773dbf7d1f607766b00cdb0eb9644", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1709730925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/94pkdfw2rpmc1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?format=pjpg&amp;auto=webp&amp;s=0430c356c59c7aa0f15f767d9fead38ac1e60ae9", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ce8ea7ec58f56de5a5aaac23f675c954344c7f1", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b76caff515a156b91183bda6484e43c6bba0d59b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1cfbc805b2bfd0b7078fe23ee907fc05dff6adb2", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0c11fd20ec6c937c2f5bb29322c58e282eeef471", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=65e00eb3dd620fd51fb9237e8c128ac8de834ea4", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1382a8ecc535e238086087b11a6f25c2dc0d01f7", "width": 1080, "height": 607}], "variants": {}, "id": "MGEzMzI2Z3VycG1jMbgquC2yV37Gtqn3u1WBKlQegqusV3jYrF88fs9iNH6Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b7zezu", "is_robot_indexable": true, "report_reasons": null, "author": "GeekLifer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7zezu/a_tool_to_quickly_extract_data_from_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/94pkdfw2rpmc1", "subreddit_subscribers": 166488, "created_utc": 1709730925.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/94pkdfw2rpmc1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/94pkdfw2rpmc1/DASH_96.mp4", "dash_url": "https://v.redd.it/94pkdfw2rpmc1/DASHPlaylist.mpd?a=1712378296%2CNDA2YWU1MjI4NmY3ZjhlN2I4NDQzMjAwMjdiYjEzNTQzYWY4MjNmMGZlNWZhNmVlYWY4ZGRhM2M0ZmZmMjdlZA%3D%3D&amp;v=1&amp;f=sd", "duration": 53, "hls_url": "https://v.redd.it/94pkdfw2rpmc1/HLSPlaylist.m3u8?a=1712378296%2COTI4OGQ0ZTNmZDY5ZTFkMDdlYmI4NGM4NGIyN2YwYmIwZmZjOTliNWJjY2NkZWYyYjEwMmM2NDQxNTk5YmExYg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context: I graduated June 2021 from a good UC with a BS in Data Science, but took an opportunity as a contractor at MAANG to get some industry experience in Data Engineering.\n\nI\u2019ve been working as a contractor here for about 2 years, yet I feel like my imposter syndrome is still something I\u2019m struggling with. The title as a contractor also creates this negative perception of myself as well. At this point, I think it\u2019s hindering my progress.\n\nMany of the FTE I\u2019ve worked with have told me I have the skills to transfer to a full time position. I recently failed the full loop for transferring to a FTE. Consequently, this rejection adds to my imposter syndrome of not being enough.\n\nHow did you guys get over your imposter syndrome? How did you guys stop giving a fuck about what others thought and just do what you do to excel? Thanks!\n", "author_fullname": "t2_2r6kthl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Imposter Syndrome", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b82sdv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709739613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context: I graduated June 2021 from a good UC with a BS in Data Science, but took an opportunity as a contractor at MAANG to get some industry experience in Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been working as a contractor here for about 2 years, yet I feel like my imposter syndrome is still something I\u2019m struggling with. The title as a contractor also creates this negative perception of myself as well. At this point, I think it\u2019s hindering my progress.&lt;/p&gt;\n\n&lt;p&gt;Many of the FTE I\u2019ve worked with have told me I have the skills to transfer to a full time position. I recently failed the full loop for transferring to a FTE. Consequently, this rejection adds to my imposter syndrome of not being enough.&lt;/p&gt;\n\n&lt;p&gt;How did you guys get over your imposter syndrome? How did you guys stop giving a fuck about what others thought and just do what you do to excel? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b82sdv", "is_robot_indexable": true, "report_reasons": null, "author": "kabzthegang", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b82sdv/help_with_imposter_syndrome/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b82sdv/help_with_imposter_syndrome/", "subreddit_subscribers": 166488, "created_utc": 1709739613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Who's done it? What stories do you have? Preface - I just made a professional faux pas by reneging on an offer I accepted about a week ago. Totally my bad, had to apologize and have a very awkward conversation with the client company who, for the record, desperately wanted me to join their team. But upon announcing the news to my current employer that I was planning on leaving, they made me an offer I couldn't refuse. I know I know I know... never accept a counter offer!!! Well... I believe that's true most of the time but this was a unique situation and I felt like accepting the counter was definitely in my favor. I had to explain that to the company who had presented me with the offer a week previous and they went down the whole \"this puts us in a really bad spot\" (which was technically true) and \"how much more would it take for you to reconsider and join our company?\". Sometimes it's nice to be in a bidding war but it can be stressful too. Ultimately I felt like my current role was a better fit all things considered. I feel really stupid now for having accepted the other offer without first having a conversation with my current company. Lesson learned... who else has a good story?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reneging on a job offer...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8c7m3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709761496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Who&amp;#39;s done it? What stories do you have? Preface - I just made a professional faux pas by reneging on an offer I accepted about a week ago. Totally my bad, had to apologize and have a very awkward conversation with the client company who, for the record, desperately wanted me to join their team. But upon announcing the news to my current employer that I was planning on leaving, they made me an offer I couldn&amp;#39;t refuse. I know I know I know... never accept a counter offer!!! Well... I believe that&amp;#39;s true most of the time but this was a unique situation and I felt like accepting the counter was definitely in my favor. I had to explain that to the company who had presented me with the offer a week previous and they went down the whole &amp;quot;this puts us in a really bad spot&amp;quot; (which was technically true) and &amp;quot;how much more would it take for you to reconsider and join our company?&amp;quot;. Sometimes it&amp;#39;s nice to be in a bidding war but it can be stressful too. Ultimately I felt like my current role was a better fit all things considered. I feel really stupid now for having accepted the other offer without first having a conversation with my current company. Lesson learned... who else has a good story?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b8c7m3", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8c7m3/reneging_on_a_job_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8c7m3/reneging_on_a_job_offer/", "subreddit_subscribers": 166488, "created_utc": 1709761496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI have 2 years experience in the midwest.\n\nCurrent company (small consulting firm remote):\n\n* **Salary**: 83k (expecting small raise in may)\n* **PTO**: 15 days\n* **Bonus**: 3-8% but has always been the bare minimum\n* **Raises**: bare minimum because of low budget\n* **401k**: 1% match\n* **Other info:** Company has had 3 rounds of layoffs in the last year. Seems to be slightly struggling but my manager says our team shouldnt be worried.\n\nNew company (big health insurance remote):\n\n* **Salary**: 77k (could be negotiable)\n* **PTO**: 23 DAYS!!!!\n* **Bonus**: 10%\n* **Raises**: yearly depending on performance\n* **401k**: 4% match + employer also contributes 1.25% of annual salary\n* **Other info**: only one small layoff in the last 6 years. happened a month ago\n\nObviously the pay cut is not acceptable but I will try to negotiate. But the PTO they have is amazing. And I am sick of feeling like im on a sinking ship at the current company. This is more of a lateral move because I will be the same level at the new company as I am at the current company. What amount of base salary at the new company would convince you to make the switch?", "author_fullname": "t2_gyusc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you make this job switch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b894aq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709754198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 2 years experience in the midwest.&lt;/p&gt;\n\n&lt;p&gt;Current company (small consulting firm remote):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Salary&lt;/strong&gt;: 83k (expecting small raise in may)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PTO&lt;/strong&gt;: 15 days&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Bonus&lt;/strong&gt;: 3-8% but has always been the bare minimum&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Raises&lt;/strong&gt;: bare minimum because of low budget&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;401k&lt;/strong&gt;: 1% match&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Other info:&lt;/strong&gt; Company has had 3 rounds of layoffs in the last year. Seems to be slightly struggling but my manager says our team shouldnt be worried.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;New company (big health insurance remote):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Salary&lt;/strong&gt;: 77k (could be negotiable)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PTO&lt;/strong&gt;: 23 DAYS!!!!&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Bonus&lt;/strong&gt;: 10%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Raises&lt;/strong&gt;: yearly depending on performance&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;401k&lt;/strong&gt;: 4% match + employer also contributes 1.25% of annual salary&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Other info&lt;/strong&gt;: only one small layoff in the last 6 years. happened a month ago&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Obviously the pay cut is not acceptable but I will try to negotiate. But the PTO they have is amazing. And I am sick of feeling like im on a sinking ship at the current company. This is more of a lateral move because I will be the same level at the new company as I am at the current company. What amount of base salary at the new company would convince you to make the switch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b894aq", "is_robot_indexable": true, "report_reasons": null, "author": "spencedogg69", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b894aq/would_you_make_this_job_switch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b894aq/would_you_make_this_job_switch/", "subreddit_subscribers": 166488, "created_utc": 1709754198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most of the work at my job is focused around a web scrapping | ETL | ML pipeline, where almost everything is implemented in PySpark, using DataFrames. I've recently started learning Scala for my masters. My feeling is that I should be able to significantly improve the performance of some kind of processes by migrating to Scala, but I don't know what to look for. \n\nMy guess is that any code that makes a significant use of udfs could use a migration. Does anyone have any specific benchmark numbers on this? Do you know of any other use cases, apart from this?", "author_fullname": "t2_2q8p8adc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any use cases for Scala over Python, using Spark with the DataFrame API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b85yc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709746912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of the work at my job is focused around a web scrapping | ETL | ML pipeline, where almost everything is implemented in PySpark, using DataFrames. I&amp;#39;ve recently started learning Scala for my masters. My feeling is that I should be able to significantly improve the performance of some kind of processes by migrating to Scala, but I don&amp;#39;t know what to look for. &lt;/p&gt;\n\n&lt;p&gt;My guess is that any code that makes a significant use of udfs could use a migration. Does anyone have any specific benchmark numbers on this? Do you know of any other use cases, apart from this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b85yc5", "is_robot_indexable": true, "report_reasons": null, "author": "sepes_15", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b85yc5/are_there_any_use_cases_for_scala_over_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b85yc5/are_there_any_use_cases_for_scala_over_python/", "subreddit_subscribers": 166488, "created_utc": 1709746912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " To provide a deeper understanding of how the modern, open-source data stack consisting of Iceberg, dbt, Trino, and Hive operates within a music streaming platform, let\u2019s delve into the detailed workflow and benefits of each component. \n\n[https://medium.com/@stefentaime\\_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597](https://medium.com/@stefentaime_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg + Dbt + Trino + Hive : modern, open-source data stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8hrsh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1709775460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To provide a deeper understanding of how the modern, open-source data stack consisting of Iceberg, dbt, Trino, and Hive operates within a music streaming platform, let\u2019s delve into the detailed workflow and benefits of each component. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597\"&gt;https://medium.com/@stefentaime_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AtJoeVVVljqoOzuTleobD77TCZ4r8faS4-cyQylUZrc.jpg?auto=webp&amp;s=be934a56decacf8eca219589b84f2339b0243906", "width": 1200, "height": 479}, "resolutions": [{"url": "https://external-preview.redd.it/AtJoeVVVljqoOzuTleobD77TCZ4r8faS4-cyQylUZrc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=059afe8c79099bb6170d8b58f98f8fe355385c1b", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/AtJoeVVVljqoOzuTleobD77TCZ4r8faS4-cyQylUZrc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12444d6eaec1853f3a879b0994e5a4a22f4ecee7", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/AtJoeVVVljqoOzuTleobD77TCZ4r8faS4-cyQylUZrc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=588e90aa7e68c9cd1a5062959a04a41d450334de", "width": 320, "height": 127}, {"url": "https://external-preview.redd.it/AtJoeVVVljqoOzuTleobD77TCZ4r8faS4-cyQylUZrc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8eed3210bcd2f026a70c6655e89fed53139d4ef1", "width": 640, "height": 255}, {"url": "https://external-preview.redd.it/AtJoeVVVljqoOzuTleobD77TCZ4r8faS4-cyQylUZrc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bbeee0807b81b29f17fdf00e3c3abd4fdf4be11b", "width": 960, "height": 383}, {"url": "https://external-preview.redd.it/AtJoeVVVljqoOzuTleobD77TCZ4r8faS4-cyQylUZrc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0526b30f1c518f45424092fa6379115a3f8c83a", "width": 1080, "height": 431}], "variants": {}, "id": "hAKDFD0uRIYI0zrRA57P0iNFGx9Q16Q3r57HOJ2V69c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1b8hrsh", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8hrsh/iceberg_dbt_trino_hive_modern_opensource_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8hrsh/iceberg_dbt_trino_hive_modern_opensource_data/", "subreddit_subscribers": 166488, "created_utc": 1709775460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently moved to a new company where we are trying to create a pipeline for a downstream team that are going to use it for analytics. The upstream is the team directly involved with \"creating\" the dataset and their whole department works with sharepoint and google sheets. They create and maintain everything in excel manually because that's what they have been doing. So, for our pipeline we have to manually get the excel file from them  through slack or through email before we start the pipeline. I have tried having discussion with them where the file they create should be pushed to lake periodically but they don't want to move forward with it. We volunteered to build it from their side the integration to your lake but they don't want it. Now, my manager doesn't want me talking to them and do it as-is. Do you guys have experience how to deal with this situation and what can i do from side that can make this process a bit more automated? Any advice is appreciated. ", "author_fullname": "t2_3dailfeq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving away from google sheets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8c1dk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709761056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently moved to a new company where we are trying to create a pipeline for a downstream team that are going to use it for analytics. The upstream is the team directly involved with &amp;quot;creating&amp;quot; the dataset and their whole department works with sharepoint and google sheets. They create and maintain everything in excel manually because that&amp;#39;s what they have been doing. So, for our pipeline we have to manually get the excel file from them  through slack or through email before we start the pipeline. I have tried having discussion with them where the file they create should be pushed to lake periodically but they don&amp;#39;t want to move forward with it. We volunteered to build it from their side the integration to your lake but they don&amp;#39;t want it. Now, my manager doesn&amp;#39;t want me talking to them and do it as-is. Do you guys have experience how to deal with this situation and what can i do from side that can make this process a bit more automated? Any advice is appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b8c1dk", "is_robot_indexable": true, "report_reasons": null, "author": "dekardar", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8c1dk/moving_away_from_google_sheets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8c1dk/moving_away_from_google_sheets/", "subreddit_subscribers": 166488, "created_utc": 1709761056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi everyone! I hope you're all doing well.\n\nI'm working on a pipeline to scrape data from SofaScore. On average, I'll need to scrape 36 matches daily, requiring 72 total requests (two per match \u2013 one for statistics and one for highlights). To optimize this process, I'd like to parallelize the DAG execution.\n\nCould you advise on the best practices for this? Should I define a task for each match, or create two separate DAGs (one for statistics and one for highlights) and trigger them both for each match?", "author_fullname": "t2_b95a7eew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parallelize Tasks execution in Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b86dyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709747926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I hope you&amp;#39;re all doing well.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a pipeline to scrape data from SofaScore. On average, I&amp;#39;ll need to scrape 36 matches daily, requiring 72 total requests (two per match \u2013 one for statistics and one for highlights). To optimize this process, I&amp;#39;d like to parallelize the DAG execution.&lt;/p&gt;\n\n&lt;p&gt;Could you advise on the best practices for this? Should I define a task for each match, or create two separate DAGs (one for statistics and one for highlights) and trigger them both for each match?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b86dyj", "is_robot_indexable": true, "report_reasons": null, "author": "Ordinary_Run_2513", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b86dyj/parallelize_tasks_execution_in_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b86dyj/parallelize_tasks_execution_in_airflow/", "subreddit_subscribers": 166488, "created_utc": 1709747926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working with a company who has several locations, each with a separate Remote Desktop, using the same software to manage their business. I\u2019d like to replicate that data from the MySQL databases to a unified location in Snowflake. Seems like options include fivetran / air byte etc. are there any more custom solutions that are easy-ish, reliable and cost effective?", "author_fullname": "t2_63dasqqa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replicate Remote Desktop MySQL instances to snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8hpn3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709775297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with a company who has several locations, each with a separate Remote Desktop, using the same software to manage their business. I\u2019d like to replicate that data from the MySQL databases to a unified location in Snowflake. Seems like options include fivetran / air byte etc. are there any more custom solutions that are easy-ish, reliable and cost effective?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b8hpn3", "is_robot_indexable": true, "report_reasons": null, "author": "skiyogagolfbeer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8hpn3/replicate_remote_desktop_mysql_instances_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8hpn3/replicate_remote_desktop_mysql_instances_to/", "subreddit_subscribers": 166488, "created_utc": 1709775297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a use case where the best thing is to build a pipeline but I am not sure how to go about choosing the correct technologies and architecture. Can someone please help with suggestions and verifying the thought process?\n\nI have data in an elastic search index which I want to read, enrich, perform aggregations and then use for dashboards or ad-hoc analytical questions. \n\nI was thinking of medallion architecture to store the retrieved data in the bronze layer, store the enriched data in the silver and the refined &amp; aggregated one in the gold. Those will be stored as Iceberg tables with parquet as the underlying storage format on S3. I can then use AWS Glue crawler to create a data catalog on top of the gold table and be able to query it with AWS Athena.\n\n1. Is this correct or falls into best practice? \n2. Also I am confused how to integrate everything together and where does the job which connects to elastic search lie? I want to run this EOD everyday.\n3. How do I apply this medallion architecture in AWS S3?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_grh3q8ab1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with architecture for project AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8g6jv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709771187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a use case where the best thing is to build a pipeline but I am not sure how to go about choosing the correct technologies and architecture. Can someone please help with suggestions and verifying the thought process?&lt;/p&gt;\n\n&lt;p&gt;I have data in an elastic search index which I want to read, enrich, perform aggregations and then use for dashboards or ad-hoc analytical questions. &lt;/p&gt;\n\n&lt;p&gt;I was thinking of medallion architecture to store the retrieved data in the bronze layer, store the enriched data in the silver and the refined &amp;amp; aggregated one in the gold. Those will be stored as Iceberg tables with parquet as the underlying storage format on S3. I can then use AWS Glue crawler to create a data catalog on top of the gold table and be able to query it with AWS Athena.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is this correct or falls into best practice? &lt;/li&gt;\n&lt;li&gt;Also I am confused how to integrate everything together and where does the job which connects to elastic search lie? I want to run this EOD everyday.&lt;/li&gt;\n&lt;li&gt;How do I apply this medallion architecture in AWS S3?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b8g6jv", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial_Track915", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8g6jv/help_with_architecture_for_project_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8g6jv/help_with_architecture_for_project_aws/", "subreddit_subscribers": 166488, "created_utc": 1709771187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I'm struggling with scraping over 1200 graphs from a server. For context, I'm using Playwright in Python, and I used Chromium to scrap data of a single graph (by the way, is a server in which I have to login with my username and password).  Scaling this for a 1200+ graphs is driving me crazy, I do not even know if this is even possible (for every graph I have to scrap, I launch either a new Chromium windows or tab).  Obviously the best way to do it is asking for the data itself to the webmaster, but this is off the table at the moment.\n\nAny idea how can I do this? ", "author_fullname": "t2_3h43racm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping over 1200+ links.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8e24b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709765867.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m struggling with scraping over 1200 graphs from a server. For context, I&amp;#39;m using Playwright in Python, and I used Chromium to scrap data of a single graph (by the way, is a server in which I have to login with my username and password).  Scaling this for a 1200+ graphs is driving me crazy, I do not even know if this is even possible (for every graph I have to scrap, I launch either a new Chromium windows or tab).  Obviously the best way to do it is asking for the data itself to the webmaster, but this is off the table at the moment.&lt;/p&gt;\n\n&lt;p&gt;Any idea how can I do this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b8e24b", "is_robot_indexable": true, "report_reasons": null, "author": "fmoralesh", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8e24b/scraping_over_1200_links/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8e24b/scraping_over_1200_links/", "subreddit_subscribers": 166488, "created_utc": 1709765867.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are moving from a data lake of sorts, or better put, a bowl of spaghetti, to creating a new data warehouse. Our base system is  Dynamics 365 Business central. Open source or Fabric? Or is there a better alternative?", "author_fullname": "t2_7kpight7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouse - Fabric vs Opensource vs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b88yj3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709754072.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709753823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are moving from a data lake of sorts, or better put, a bowl of spaghetti, to creating a new data warehouse. Our base system is  Dynamics 365 Business central. Open source or Fabric? Or is there a better alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b88yj3", "is_robot_indexable": true, "report_reasons": null, "author": "DutchN8G8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b88yj3/data_warehouse_fabric_vs_opensource_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b88yj3/data_warehouse_fabric_vs_opensource_vs/", "subreddit_subscribers": 166488, "created_utc": 1709753823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do we have any resources to prepare for DE system design where a list covers most design, architectural patterns ?\n\nIf not, is anyone interested in building one ?", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blind75 for DE system design?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b864dd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709747297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do we have any resources to prepare for DE system design where a list covers most design, architectural patterns ?&lt;/p&gt;\n\n&lt;p&gt;If not, is anyone interested in building one ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b864dd", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b864dd/blind75_for_de_system_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b864dd/blind75_for_de_system_design/", "subreddit_subscribers": 166488, "created_utc": 1709747297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nwe have data stored in an S3 bucket in the form of raw CSV files. Adobe is currently accessing this data directly from the S3 bucket. However, these files contain sensitive information. I\u2019m exploring options to ensure that we maintain ownership of the data while allowing Adobe to access it securely.\n\nAny suggestions or use cases or right way to get this done propely . We are having mulitple third parties using the data not only adobe. Just asking what are my options here please help.", "author_fullname": "t2_3sqs3uub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expose data from S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b81nx5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709736863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;we have data stored in an S3 bucket in the form of raw CSV files. Adobe is currently accessing this data directly from the S3 bucket. However, these files contain sensitive information. I\u2019m exploring options to ensure that we maintain ownership of the data while allowing Adobe to access it securely.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or use cases or right way to get this done propely . We are having mulitple third parties using the data not only adobe. Just asking what are my options here please help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b81nx5", "is_robot_indexable": true, "report_reasons": null, "author": "priyasweety1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b81nx5/expose_data_from_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b81nx5/expose_data_from_s3/", "subreddit_subscribers": 166488, "created_utc": 1709736863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI have parquet files arriving at S3 regularly. So I created folder structure as root/yyyy/mm/dd/ and in the day I have multiple small parquet files.\nThe parquet file contain time series data.\nI want to generate a histogram in grafana for one of the columns.  How can efficiently achieve this. I cannot use grafana histogram plot because it requires all the rows. \nFor couple of days it's fine. But when I want to calculate over multiple months I get timeout error. And even in Athena it takes more than a minute to calculate the buckets and counts. What I was thinking is to use ctas command to create a table with  statistics for each column and insert or update the fields every day with stats from new data. Is this the correct approach?", "author_fullname": "t2_40nv3bcr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions on using Athena ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b7vmpd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709717500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI have parquet files arriving at S3 regularly. So I created folder structure as root/yyyy/mm/dd/ and in the day I have multiple small parquet files.\nThe parquet file contain time series data.\nI want to generate a histogram in grafana for one of the columns.  How can efficiently achieve this. I cannot use grafana histogram plot because it requires all the rows. \nFor couple of days it&amp;#39;s fine. But when I want to calculate over multiple months I get timeout error. And even in Athena it takes more than a minute to calculate the buckets and counts. What I was thinking is to use ctas command to create a table with  statistics for each column and insert or update the fields every day with stats from new data. Is this the correct approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b7vmpd", "is_robot_indexable": true, "report_reasons": null, "author": "nanosuituser", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b7vmpd/need_suggestions_on_using_athena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b7vmpd/need_suggestions_on_using_athena/", "subreddit_subscribers": 166488, "created_utc": 1709717500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. \nPreparing for interviews. Need to learn pyspark but need some platform so that I can focus on the learning without thinking too much about the set up. ", "author_fullname": "t2_v5a9tzi96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn Spark for interviews with minimal or no setup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1b8k3em", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709781861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. \nPreparing for interviews. Need to learn pyspark but need some platform so that I can focus on the learning without thinking too much about the set up. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b8k3em", "is_robot_indexable": true, "report_reasons": null, "author": "BinaryBass", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8k3em/how_to_learn_spark_for_interviews_with_minimal_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8k3em/how_to_learn_spark_for_interviews_with_minimal_or/", "subreddit_subscribers": 166488, "created_utc": 1709781861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not here to downplay Zoomcamp, just to explore new options and make a more informed decision.\n\nThe community strongly recommends Data Talk Zoomcamp for DE, and taking it is probably the next step I should take in my studies.\n\nAre there any other alternatives that the community recommends at a similar level of:\n\n- content quality\n- preparation for real-world Data Engineering problems / jobs\n- financial accessibility\n\nThanks", "author_fullname": "t2_4n5pg9gw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which courses compete with or are alternatives to Data Talk Zoomcamp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8it55", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709778332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not here to downplay Zoomcamp, just to explore new options and make a more informed decision.&lt;/p&gt;\n\n&lt;p&gt;The community strongly recommends Data Talk Zoomcamp for DE, and taking it is probably the next step I should take in my studies.&lt;/p&gt;\n\n&lt;p&gt;Are there any other alternatives that the community recommends at a similar level of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;content quality&lt;/li&gt;\n&lt;li&gt;preparation for real-world Data Engineering problems / jobs&lt;/li&gt;\n&lt;li&gt;financial accessibility&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b8it55", "is_robot_indexable": true, "report_reasons": null, "author": "fjellen", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8it55/which_courses_compete_with_or_are_alternatives_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8it55/which_courses_compete_with_or_are_alternatives_to/", "subreddit_subscribers": 166488, "created_utc": 1709778332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is anyone doing this? \n\nI need a scalable backend on azure for transformation where the \"large\" datasets are a couple of gigs. Jobs are on customer demand, so enough work that I want something that can scale, but maybe not quite enough to justify using something spark. \n\nI have a prototype of this that seems to work but before I build a whole system, curious if anyone is doing or has tried this and has any insight. I'm not a data person, just a dumb programmer. This is for part of the backend for a large SAAS system.", "author_fullname": "t2_622qq2ku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDb + Azure functions + Azure blob storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8gl8w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709772281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone doing this? &lt;/p&gt;\n\n&lt;p&gt;I need a scalable backend on azure for transformation where the &amp;quot;large&amp;quot; datasets are a couple of gigs. Jobs are on customer demand, so enough work that I want something that can scale, but maybe not quite enough to justify using something spark. &lt;/p&gt;\n\n&lt;p&gt;I have a prototype of this that seems to work but before I build a whole system, curious if anyone is doing or has tried this and has any insight. I&amp;#39;m not a data person, just a dumb programmer. This is for part of the backend for a large SAAS system.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b8gl8w", "is_robot_indexable": true, "report_reasons": null, "author": "WMMMMMMMMMMMMMMMMMMW", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8gl8w/duckdb_azure_functions_azure_blob_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8gl8w/duckdb_azure_functions_azure_blob_storage/", "subreddit_subscribers": 166488, "created_utc": 1709772281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I\u2019m a student that will graduate in a few months and I\u2019m attracted by the research field. I was wonder if any of you know about research groups (universities/colleges or also professors) in EU or UK that are focused on the data engineering/management field. Topics that I\u2019m passionate about are parallel computation, like anything about spark/dask/ray/etc., and something like the work of the DataLearning group (the only group that I know of).\nThank you all in advance ", "author_fullname": "t2_7ty03r5f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "European research groups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8fibk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709769383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I\u2019m a student that will graduate in a few months and I\u2019m attracted by the research field. I was wonder if any of you know about research groups (universities/colleges or also professors) in EU or UK that are focused on the data engineering/management field. Topics that I\u2019m passionate about are parallel computation, like anything about spark/dask/ray/etc., and something like the work of the DataLearning group (the only group that I know of).\nThank you all in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1b8fibk", "is_robot_indexable": true, "report_reasons": null, "author": "itstimeoclock", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8fibk/european_research_groups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8fibk/european_research_groups/", "subreddit_subscribers": 166488, "created_utc": 1709769383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am presently using [draw.io](https://draw.io), and I have been unable to locate icons resembling those in the provided image. I would greatly appreciate your assistance. Thank you.\n\nhttps://preview.redd.it/qo3c75lflsmc1.png?width=1018&amp;format=png&amp;auto=webp&amp;s=131a2a872005cd7f3543497c0d7c8e21b80f9505", "author_fullname": "t2_56mgz5y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make this system design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 125, "top_awarded_type": null, "hide_score": false, "media_metadata": {"qo3c75lflsmc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 96, "x": 108, "u": "https://preview.redd.it/qo3c75lflsmc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d43f19aa2bcf4151dc1401339785a58d15e28ce"}, {"y": 192, "x": 216, "u": "https://preview.redd.it/qo3c75lflsmc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=79e890ad6d0b6378b1c5fc20f7edbee33e0baa34"}, {"y": 285, "x": 320, "u": "https://preview.redd.it/qo3c75lflsmc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=288adb6ea9f520f8d7928f221ae608f1ac3d2ff8"}, {"y": 571, "x": 640, "u": "https://preview.redd.it/qo3c75lflsmc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fcf4793acd0eb4ece591f2d10ed275b0d793c93a"}, {"y": 857, "x": 960, "u": "https://preview.redd.it/qo3c75lflsmc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=401f582b9fbcf89d82facaa4fc6ea7aee8894f2a"}], "s": {"y": 909, "x": 1018, "u": "https://preview.redd.it/qo3c75lflsmc1.png?width=1018&amp;format=png&amp;auto=webp&amp;s=131a2a872005cd7f3543497c0d7c8e21b80f9505"}, "id": "qo3c75lflsmc1"}}, "name": "t3_1b8duhr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fcFWibtFICYuQonmtDm0K6pkuHY1-KfjIuO2ISJPo8Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709765357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am presently using &lt;a href=\"https://draw.io\"&gt;draw.io&lt;/a&gt;, and I have been unable to locate icons resembling those in the provided image. I would greatly appreciate your assistance. Thank you.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qo3c75lflsmc1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=131a2a872005cd7f3543497c0d7c8e21b80f9505\"&gt;https://preview.redd.it/qo3c75lflsmc1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=131a2a872005cd7f3543497c0d7c8e21b80f9505&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b8duhr", "is_robot_indexable": true, "report_reasons": null, "author": "wh1t3bl3", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8duhr/how_to_make_this_system_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8duhr/how_to_make_this_system_design/", "subreddit_subscribers": 166488, "created_utc": 1709765357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: With Spark 3.4, Spark Connect was released allowing individuals to remotely connect to remote spark clusters using the DataFrame API. They claim that it can be embedded in modern data apps, and even be able to use IDEs like VS Code. I think it's cool.  \n\n\nLink: [https://spark.apache.org/docs/latest/spark-connect-overview.html](https://spark.apache.org/docs/latest/spark-connect-overview.html)\n\nQuestion: I want to be able to do this with AWS EMR. Did anyone try this and was successful or mind sharing pointers on how to set it up? ", "author_fullname": "t2_8n77vtsfm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Connect + EMR (Remote connectivity to spark clusters)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8bscy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709760472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: With Spark 3.4, Spark Connect was released allowing individuals to remotely connect to remote spark clusters using the DataFrame API. They claim that it can be embedded in modern data apps, and even be able to use IDEs like VS Code. I think it&amp;#39;s cool.  &lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://spark.apache.org/docs/latest/spark-connect-overview.html\"&gt;https://spark.apache.org/docs/latest/spark-connect-overview.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Question: I want to be able to do this with AWS EMR. Did anyone try this and was successful or mind sharing pointers on how to set it up? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1b8bscy", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Net-1111", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8bscy/spark_connect_emr_remote_connectivity_to_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8bscy/spark_connect_emr_remote_connectivity_to_spark/", "subreddit_subscribers": 166488, "created_utc": 1709760472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. \n\nI work at a company that despite being relatively large, is about twenty years behind the times. I\u2019m talking most folks have been at this org their entire careers and they\u2019ve never even dealt with pivot tables. \n\nAll of that to say this: there\u2019s barely any infrastructure at all. I need to build some but the only software that\u2019s compatible with what is in use is Visual Studio. I was hoping to use SSIS. We don\u2019t need a lot of things done, just a simple small data warehouse. \n\nThing is\u2026 I\u2019ve installed visual studio 2022. I have a license. I\u2019ve gotten SSDT &amp; SSIS installed and I can\u2019t create a new SSIS project. \n\nThe internet seems to waiver between SSIS being unsupported in VStudio and the need to use Azure. We\u2019re not using Azure any where else. \n\nWhat are my options?  Does Microsoft want me to hate everything? ", "author_fullname": "t2_7fqx3yyx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visual Studio Data Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8bjwt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1709760297.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709759915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. &lt;/p&gt;\n\n&lt;p&gt;I work at a company that despite being relatively large, is about twenty years behind the times. I\u2019m talking most folks have been at this org their entire careers and they\u2019ve never even dealt with pivot tables. &lt;/p&gt;\n\n&lt;p&gt;All of that to say this: there\u2019s barely any infrastructure at all. I need to build some but the only software that\u2019s compatible with what is in use is Visual Studio. I was hoping to use SSIS. We don\u2019t need a lot of things done, just a simple small data warehouse. &lt;/p&gt;\n\n&lt;p&gt;Thing is\u2026 I\u2019ve installed visual studio 2022. I have a license. I\u2019ve gotten SSDT &amp;amp; SSIS installed and I can\u2019t create a new SSIS project. &lt;/p&gt;\n\n&lt;p&gt;The internet seems to waiver between SSIS being unsupported in VStudio and the need to use Azure. We\u2019re not using Azure any where else. &lt;/p&gt;\n\n&lt;p&gt;What are my options?  Does Microsoft want me to hate everything? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b8bjwt", "is_robot_indexable": true, "report_reasons": null, "author": "TodosLosPomegranates", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8bjwt/visual_studio_data_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8bjwt/visual_studio_data_tools/", "subreddit_subscribers": 166488, "created_utc": 1709759915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In dbt, I am trying to make sure that all our fact tables have relationship tests on their dimension fields.\n\nIs there a tool which would do this? So far I've only seen packages which check the coverage for all fields, but I do not want to enforce that, only for the dim\\_\\*\\_key patterned fields.\n\nDo you have any similar solutions?", "author_fullname": "t2_81a79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Test coverage in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1b8ajea", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1709757508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In dbt, I am trying to make sure that all our fact tables have relationship tests on their dimension fields.&lt;/p&gt;\n\n&lt;p&gt;Is there a tool which would do this? So far I&amp;#39;ve only seen packages which check the coverage for all fields, but I do not want to enforce that, only for the dim_*_key patterned fields.&lt;/p&gt;\n\n&lt;p&gt;Do you have any similar solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1b8ajea", "is_robot_indexable": true, "report_reasons": null, "author": "crepitation", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1b8ajea/test_coverage_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1b8ajea/test_coverage_in_dbt/", "subreddit_subscribers": 166488, "created_utc": 1709757508.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}