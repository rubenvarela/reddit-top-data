{"kind": "Listing", "data": {"after": "t3_1bhprz9", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently hosted the \"NBA Data Modeling Challenge,\" where over 100 participants modeled\u2014yes, you guessed it\u2014historical NBA data!\n\nLeveraging SQL and dbt, participants went above and beyond to uncover NBA insights and compete for a big prize: $1,500!\n\nIn this blog post, I've compiled my favorite insights generated by the participants, such as:\n\n* The dramatic impact of the 3-pointer on the NBA over the last decade\n* The most consistent playoff performers of all time\n* The players who should have been awarded MVP in each season\n* The most clutch NBA players of all time\n* After adjusting for inflation, the highest-paid NBA players ever\n* The most overvalued players in the 2022-23 season\n\nIt's a must-read if you're an NBA fan or just love high-quality SQL, dbt, data analysis, and data visualization!\n\n[Check out the blog here!](https://www.paradime.io/blog/basketball-by-the-numbers-insights-from-paradimes-nba-data-modeling-challenge)", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key Insights from NBA Data Modeling Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhutin", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710778985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently hosted the &amp;quot;NBA Data Modeling Challenge,&amp;quot; where over 100 participants modeled\u2014yes, you guessed it\u2014historical NBA data!&lt;/p&gt;\n\n&lt;p&gt;Leveraging SQL and dbt, participants went above and beyond to uncover NBA insights and compete for a big prize: $1,500!&lt;/p&gt;\n\n&lt;p&gt;In this blog post, I&amp;#39;ve compiled my favorite insights generated by the participants, such as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The dramatic impact of the 3-pointer on the NBA over the last decade&lt;/li&gt;\n&lt;li&gt;The most consistent playoff performers of all time&lt;/li&gt;\n&lt;li&gt;The players who should have been awarded MVP in each season&lt;/li&gt;\n&lt;li&gt;The most clutch NBA players of all time&lt;/li&gt;\n&lt;li&gt;After adjusting for inflation, the highest-paid NBA players ever&lt;/li&gt;\n&lt;li&gt;The most overvalued players in the 2022-23 season&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s a must-read if you&amp;#39;re an NBA fan or just love high-quality SQL, dbt, data analysis, and data visualization!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.paradime.io/blog/basketball-by-the-numbers-insights-from-paradimes-nba-data-modeling-challenge\"&gt;Check out the blog here!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?auto=webp&amp;s=25f017d68d0cf4c258efa3e609d72159ed14aac6", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6fbac571ba0ce966cbcec335f160a91ac7ced94", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb36f934fd4b8620ed8f27f64b10972193ec4f35", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0be76566051f49336667a35d8613b7e9e075336e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6b200f5d42ed3944e618b8233d94f7bcc0dc47f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=65158b0818951a2c049382d98f68aa710e693c4c", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d6aaf46c2cc14d1be0078f00af273aa676c29fa", "width": 1080, "height": 607}], "variants": {}, "id": "hdEr2Ol9ohYczAT_H4Yxt1eeFjwqm5W_afbNmG9q8NQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhutin", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhutin/key_insights_from_nba_data_modeling_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhutin/key_insights_from_nba_data_modeling_challenge/", "subreddit_subscribers": 170049, "created_utc": 1710778985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I usually work with Databricks and I just started learning how Data Factory works. From my understanding, Data Factory can be used for data transformations, as well as for the Extract and Load parts of an ETL process. But I don\u2019t see it used for transformations by my client.\n\nMe and my colleagues use Data Factory for this client, but from what I can see (since this project started years before me arriving in the company) the pipelines 90% of the time run notebooks and send emails when the notebooks fail. Is this the norm?", "author_fullname": "t2_k97u3vqd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhnh1m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710757729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually work with Databricks and I just started learning how Data Factory works. From my understanding, Data Factory can be used for data transformations, as well as for the Extract and Load parts of an ETL process. But I don\u2019t see it used for transformations by my client.&lt;/p&gt;\n\n&lt;p&gt;Me and my colleagues use Data Factory for this client, but from what I can see (since this project started years before me arriving in the company) the pipelines 90% of the time run notebooks and send emails when the notebooks fail. Is this the norm?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhnh1m", "is_robot_indexable": true, "report_reasons": null, "author": "IlMagodelLusso", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhnh1m/azure_data_factory_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhnh1m/azure_data_factory_use/", "subreddit_subscribers": 170049, "created_utc": 1710757729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey reddit, I'm a data architect with 10 years of experience and for the past few years I've been [mentoring data engineers](https://mentorcruise.com/mentor/lewisgavin/) on a site called MentorCruise.\n\nI've not long become a new dad so I can't take on as many mentees as I used to. But a lot of the advice I'm giving applies to most people looking to progress in their DE career. (I even saw a post here yesterday asking [about becoming a good engineer](https://www.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/))\n\nI get an overwhelming number of applications but can't accept as many as I'd like due to just becoming a dad for the first time. So I've decided to create a course.\n\n**This is where you come in.** I'd love to get your feedback on the course as a fellow data engineer to ensure I'm on the right track.\n\nHere is the link to the course site: [https://next-level-data.mailchimpsites.com/](https://next-level-data.mailchimpsites.com/)\n\nI'd love your feedback on:\n\n* the course contents\n* the course price\n* the website/experience in general\n\nThis may seem like I'm farming for signups but I'm genuinely just getting started with this and all I'm looking for is feedback.\n\nHowever, if you do look at the course and think it will be of interest to you then let me know. I can definitely setup some special referral discounts for Redditors who bring friends/colleagues along too.\n\nThanks in advance :)", "author_fullname": "t2_9kwrl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking feedback on data engineering career course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhm35q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710797404.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710751932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey reddit, I&amp;#39;m a data architect with 10 years of experience and for the past few years I&amp;#39;ve been &lt;a href=\"https://mentorcruise.com/mentor/lewisgavin/\"&gt;mentoring data engineers&lt;/a&gt; on a site called MentorCruise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve not long become a new dad so I can&amp;#39;t take on as many mentees as I used to. But a lot of the advice I&amp;#39;m giving applies to most people looking to progress in their DE career. (I even saw a post here yesterday asking &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/\"&gt;about becoming a good engineer&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;I get an overwhelming number of applications but can&amp;#39;t accept as many as I&amp;#39;d like due to just becoming a dad for the first time. So I&amp;#39;ve decided to create a course.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This is where you come in.&lt;/strong&gt; I&amp;#39;d love to get your feedback on the course as a fellow data engineer to ensure I&amp;#39;m on the right track.&lt;/p&gt;\n\n&lt;p&gt;Here is the link to the course site: &lt;a href=\"https://next-level-data.mailchimpsites.com/\"&gt;https://next-level-data.mailchimpsites.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love your feedback on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the course contents&lt;/li&gt;\n&lt;li&gt;the course price&lt;/li&gt;\n&lt;li&gt;the website/experience in general&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This may seem like I&amp;#39;m farming for signups but I&amp;#39;m genuinely just getting started with this and all I&amp;#39;m looking for is feedback.&lt;/p&gt;\n\n&lt;p&gt;However, if you do look at the course and think it will be of interest to you then let me know. I can definitely setup some special referral discounts for Redditors who bring friends/colleagues along too.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?auto=webp&amp;s=826577a6dff9c2822defccc6abeb8a3791a4f378", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5f6652f61b1a0e56833d08f0686d30e2329eb05", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f365e7495e0645aa0c8774a5cfed8409b5674b4f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16de7d5daef6611323ac7642d9f5f287849b64a3", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f0b36b08ab5346200cd441068e98c3ac852d9ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c93bf08bf7fa6625c90d6c3f183341ce83f477e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/l1YxhjhFTzfdvIDgCB2khrS0hUd5iby_jRTojW8KtL8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6cb598cce81c0d8d0949be2acb444bf84ad2b82", "width": 1080, "height": 567}], "variants": {}, "id": "zCh4-DKpuT8tl8-SzPXCdRevHjz5DJZ0d8U0sLHbmXw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bhm35q", "is_robot_indexable": true, "report_reasons": null, "author": "gavlaaaaaaaa", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhm35q/seeking_feedback_on_data_engineering_career_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhm35q/seeking_feedback_on_data_engineering_career_course/", "subreddit_subscribers": 170049, "created_utc": 1710751932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you normally build APIs?\n\nI have good gasp of reading and parsing data from APIs but I have never build any. Not sure if building APIs is common for hedge fund DEs? Thank you!\n\nWhat are the common data ETL sources in Hedge funds?\n\nI can think of files, apis, sql servers, ftp. Anything else?", "author_fullname": "t2_9od6j04g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any data engineers working at a hedge fund? I got a couple job interviews coming and would like some insights.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi5fni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710804649.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710804381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you normally build APIs?&lt;/p&gt;\n\n&lt;p&gt;I have good gasp of reading and parsing data from APIs but I have never build any. Not sure if building APIs is common for hedge fund DEs? Thank you!&lt;/p&gt;\n\n&lt;p&gt;What are the common data ETL sources in Hedge funds?&lt;/p&gt;\n\n&lt;p&gt;I can think of files, apis, sql servers, ftp. Anything else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bi5fni", "is_robot_indexable": true, "report_reasons": null, "author": "Tall-Skin5800", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi5fni/any_data_engineers_working_at_a_hedge_fund_i_got/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi5fni/any_data_engineers_working_at_a_hedge_fund_i_got/", "subreddit_subscribers": 170049, "created_utc": 1710804381.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am sure many others have been in my shoes before. \n\nI have been with my org for about a year, and it\u2019s approach to data management is extremely primitive. I am trying to do some dash boarding of various KPIs, but I\u2019m finding that everywhere I look the approach has been that some random person enters data into an excel sheet somewhere without any documentation.\n\nAgain, there\u2019s no documentation on where this data lives or how it comes to be. Over the course of my role, I\u2019ve discovered that much of this data entry could just be automated by creating some views off our main production database. I\u2019ve resolved about 70% of the necessary data processing by creating a view and loading it to a PowerBi data model, but I\u2019m trying to figure out how to handle the missing ends. \n\nI\u2019ve considered asking our operations manager if we could drive people to host their excel workbooks in share point. That way, I\u2019d have access to the files and could take a periodic snapshot by just reading it to a pandas dataframe before loading it to a history table in a data warehouse. \n\nHow have others managed this issue with undocumented excel workbooks floating all over the place? \n\nIt\u2019s tough because I get the sense that many are refusing to share information because they fear their job being automated. At the same time, it is simply not feasible for me to spend my whole day constantly chasing down spreadsheets everywhere.", "author_fullname": "t2_6hsp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excel files sprinkled across organization with no documentation ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi6dus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710807128.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710806762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sure many others have been in my shoes before. &lt;/p&gt;\n\n&lt;p&gt;I have been with my org for about a year, and it\u2019s approach to data management is extremely primitive. I am trying to do some dash boarding of various KPIs, but I\u2019m finding that everywhere I look the approach has been that some random person enters data into an excel sheet somewhere without any documentation.&lt;/p&gt;\n\n&lt;p&gt;Again, there\u2019s no documentation on where this data lives or how it comes to be. Over the course of my role, I\u2019ve discovered that much of this data entry could just be automated by creating some views off our main production database. I\u2019ve resolved about 70% of the necessary data processing by creating a view and loading it to a PowerBi data model, but I\u2019m trying to figure out how to handle the missing ends. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve considered asking our operations manager if we could drive people to host their excel workbooks in share point. That way, I\u2019d have access to the files and could take a periodic snapshot by just reading it to a pandas dataframe before loading it to a history table in a data warehouse. &lt;/p&gt;\n\n&lt;p&gt;How have others managed this issue with undocumented excel workbooks floating all over the place? &lt;/p&gt;\n\n&lt;p&gt;It\u2019s tough because I get the sense that many are refusing to share information because they fear their job being automated. At the same time, it is simply not feasible for me to spend my whole day constantly chasing down spreadsheets everywhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi6dus", "is_robot_indexable": true, "report_reasons": null, "author": "suitupyo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi6dus/excel_files_sprinkled_across_organization_with_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi6dus/excel_files_sprinkled_across_organization_with_no/", "subreddit_subscribers": 170049, "created_utc": 1710806762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So i have a project where basically data needs to be ingested into a big snowflake table from multiple csv files that are sent to AWS SFTP (and forwarded to S3). The project itself is pretty simple tbh. The high level architecture and workflow is as follows: CSV gets send by customer applications into S3 --&gt; loading into snowflake load table --&gt; some small/basic transformations (joins across data of multiple files) --&gt; database with clean data --&gt; some customers will require export jobs like nested json or csv to their own s3 or whatever.\n\nTo be a little bit more specific: the data we are talking about here is very small like 30mb etc. the ingestion is very basic as well maybe some edge cases but overall just parsing CSV.\n\nNow we have a debate in the team. Our AWS architect made an architecture i will call \"aws first\": Basically whenever anything has to be done he wants me to do it with AWS Lambda and python3. Meaning: Ingestion made with s3 events that triggers lambda that will do simple ingestion. Then the copy process and joins also in a lambda and the export jobs as well.\n\nThe second approach i would call snowflake first is skipping AWS Lambda all together and just using Snowpipe that will listen to s3 events and then some UFD functions or stored procedures. For the Export jobs we can use snowpipe and snowflake tasks as well maybe in some cases we would need external functions and then some kind of AWS lambda service but it's probably unlikely this will be a real use-case. The snowpipes and storage integration i would create with terraform snowflake provider.\n\nSo i would like to ask the community what approach they would choose here. In my opinion both approaches have disadvantages and advantages:\n\nadvantages aws first approach (aws lambda and python)\n\n- most of the project is in aws so aws lambda would integrate very well (IaC) etc.\n\n- patching of software is required anyways since our project has some other tools and apps\n\n- we could implement some additional stuff in the lambda like idempotency\n\n- very easy to understand if the project wants to hire low cost devs in the future for programing integrations\n\n- data is pretty small probably even fits into a python3 dataframe so all the optimisation we would get from focusing on snowflake features probably not worth it\n\nadvantages \"snowflake first\" approach (terraform and snowpipe, tasks, ufd etc.)\n\n- leveraging the tools of a platform we already paying for anyways\n\n- less operational overhead, less changes needed in the future\n\nMaybe i can get some guidance on this topic. It just feels counterintuitive to me to build so many custom lambda functions on the other hand there isn't really a good argument to do most of the workloads in snowflake. What do you think? :) Thanks!\n\n", "author_fullname": "t2_ficwvf44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance: Snowflake first approach vs AWS first approach data ingestion and export jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi195y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710794665.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710794349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i have a project where basically data needs to be ingested into a big snowflake table from multiple csv files that are sent to AWS SFTP (and forwarded to S3). The project itself is pretty simple tbh. The high level architecture and workflow is as follows: CSV gets send by customer applications into S3 --&amp;gt; loading into snowflake load table --&amp;gt; some small/basic transformations (joins across data of multiple files) --&amp;gt; database with clean data --&amp;gt; some customers will require export jobs like nested json or csv to their own s3 or whatever.&lt;/p&gt;\n\n&lt;p&gt;To be a little bit more specific: the data we are talking about here is very small like 30mb etc. the ingestion is very basic as well maybe some edge cases but overall just parsing CSV.&lt;/p&gt;\n\n&lt;p&gt;Now we have a debate in the team. Our AWS architect made an architecture i will call &amp;quot;aws first&amp;quot;: Basically whenever anything has to be done he wants me to do it with AWS Lambda and python3. Meaning: Ingestion made with s3 events that triggers lambda that will do simple ingestion. Then the copy process and joins also in a lambda and the export jobs as well.&lt;/p&gt;\n\n&lt;p&gt;The second approach i would call snowflake first is skipping AWS Lambda all together and just using Snowpipe that will listen to s3 events and then some UFD functions or stored procedures. For the Export jobs we can use snowpipe and snowflake tasks as well maybe in some cases we would need external functions and then some kind of AWS lambda service but it&amp;#39;s probably unlikely this will be a real use-case. The snowpipes and storage integration i would create with terraform snowflake provider.&lt;/p&gt;\n\n&lt;p&gt;So i would like to ask the community what approach they would choose here. In my opinion both approaches have disadvantages and advantages:&lt;/p&gt;\n\n&lt;p&gt;advantages aws first approach (aws lambda and python)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;most of the project is in aws so aws lambda would integrate very well (IaC) etc.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;patching of software is required anyways since our project has some other tools and apps&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;we could implement some additional stuff in the lambda like idempotency&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;very easy to understand if the project wants to hire low cost devs in the future for programing integrations&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;data is pretty small probably even fits into a python3 dataframe so all the optimisation we would get from focusing on snowflake features probably not worth it&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;advantages &amp;quot;snowflake first&amp;quot; approach (terraform and snowpipe, tasks, ufd etc.)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;leveraging the tools of a platform we already paying for anyways&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;less operational overhead, less changes needed in the future&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Maybe i can get some guidance on this topic. It just feels counterintuitive to me to build so many custom lambda functions on the other hand there isn&amp;#39;t really a good argument to do most of the workloads in snowflake. What do you think? :) Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi195y", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Interaction_5701", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi195y/guidance_snowflake_first_approach_vs_aws_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi195y/guidance_snowflake_first_approach_vs_aws_first/", "subreddit_subscribers": 170049, "created_utc": 1710794349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently got my first offer as a Data Engineer coming out of school and wanted to understand the possibilities of moving out of DE into SWE at some point. Has any body made this jump before? Was it hard or easy? What kinds of skills did you have to pick up either on the job or by yourself?\n\nThanks!", "author_fullname": "t2_hfl4w59z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Going from DE to SWE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhzgyi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710790126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got my first offer as a Data Engineer coming out of school and wanted to understand the possibilities of moving out of DE into SWE at some point. Has any body made this jump before? Was it hard or easy? What kinds of skills did you have to pick up either on the job or by yourself?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bhzgyi", "is_robot_indexable": true, "report_reasons": null, "author": "digging_for_memories", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhzgyi/going_from_de_to_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhzgyi/going_from_de_to_swe/", "subreddit_subscribers": 170049, "created_utc": 1710790126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is optimized by DBMS: logical or physical plans? or both? If only logical plans, how DBMS can calculate costs without knowing of exact physical operations behind nodes of query tree? I also doubt that only physical plans are optimized.  \nI have this picture (CMU DB group lectures) where given logical plan is optimized but I dunno how this may happen if logical plan is to my knowledge high-level DAG of operators without any physical specifics.\n\nTo me it seems that optimizer should optimize both logical plans (iterate through different query plan trees and choose best plan using *heuristics*) and physical plans (iterate through different options of executing the same query plan tree (different joins, access methods, etc.) and choose best plan using *costs-based approach*) but I'm definitely not sure about that.\n\nUPD. Found docs of specific DB [https://docs.pingcap.com/tidb/stable/sql-optimization-concepts](https://docs.pingcap.com/tidb/stable/sql-optimization-concepts) . In this docs there is point about doing some \"logically equivalent changes to the query\" and they name it \"[Logical Optimization](https://docs.pingcap.com/tidb/stable/sql-logical-optimization)\". Also there is about \"obtaining a final execution plan based on the data distribution and the specific execution cost of an operator\" and they name it \"[Physical Optimization](https://docs.pingcap.com/tidb/stable/sql-physical-optimization)\".   \nSo to me it seems that in modern DB systems there is actually similar process to one I described earlier.\n\nhttps://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;format=png&amp;auto=webp&amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a", "author_fullname": "t2_b3ayn5hsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When does query optimization in DBMS happen?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3p9o5illm5pc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f31e57669dba4a7a2b7b285720441bb9fdb4f7f"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c9c84776ba8b8b2e4e94043039331ca3d0a690e"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=631e3dae7cbd3a70a36f697e530ad1d9dcd4185b"}], "s": {"y": 270, "x": 537, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;format=png&amp;auto=webp&amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a"}, "id": "3p9o5illm5pc1"}}, "name": "t3_1bi1et9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/N-7Eb17SVssj45oho_0YGsxARVCoWdtSC2M938LmapM.jpg", "edited": 1710800187.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1710794728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is optimized by DBMS: logical or physical plans? or both? If only logical plans, how DBMS can calculate costs without knowing of exact physical operations behind nodes of query tree? I also doubt that only physical plans are optimized.&lt;br/&gt;\nI have this picture (CMU DB group lectures) where given logical plan is optimized but I dunno how this may happen if logical plan is to my knowledge high-level DAG of operators without any physical specifics.&lt;/p&gt;\n\n&lt;p&gt;To me it seems that optimizer should optimize both logical plans (iterate through different query plan trees and choose best plan using &lt;em&gt;heuristics&lt;/em&gt;) and physical plans (iterate through different options of executing the same query plan tree (different joins, access methods, etc.) and choose best plan using &lt;em&gt;costs-based approach&lt;/em&gt;) but I&amp;#39;m definitely not sure about that.&lt;/p&gt;\n\n&lt;p&gt;UPD. Found docs of specific DB &lt;a href=\"https://docs.pingcap.com/tidb/stable/sql-optimization-concepts\"&gt;https://docs.pingcap.com/tidb/stable/sql-optimization-concepts&lt;/a&gt; . In this docs there is point about doing some &amp;quot;logically equivalent changes to the query&amp;quot; and they name it &amp;quot;&lt;a href=\"https://docs.pingcap.com/tidb/stable/sql-logical-optimization\"&gt;Logical Optimization&lt;/a&gt;&amp;quot;. Also there is about &amp;quot;obtaining a final execution plan based on the data distribution and the specific execution cost of an operator&amp;quot; and they name it &amp;quot;&lt;a href=\"https://docs.pingcap.com/tidb/stable/sql-physical-optimization\"&gt;Physical Optimization&lt;/a&gt;&amp;quot;.&lt;br/&gt;\nSo to me it seems that in modern DB systems there is actually similar process to one I described earlier.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a\"&gt;https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?auto=webp&amp;s=8150913c159f903cc5577b744799db42db187d9c", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30824ff14765ae039e3b6d7d2d5ee2dc0f24c652", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=00db47a9520ff588fdf82bcaf30a82abe2c3a3f2", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a231e7f510167f41a4a7f55c51d42ce8b4451acf", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1987b6c83ef844cb83414743dd68ff938f1c110", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5df7f9263b806d2009a7df7ab1303f86671967dc", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c005210e0451b258fbb29ae1ec082d7cd9752d5f", "width": 1080, "height": 1080}], "variants": {}, "id": "ZQDvKgaiNIaaiEfaM14MidCgI9LagV_KOlnvnr_bxa0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bi1et9", "is_robot_indexable": true, "report_reasons": null, "author": "isk14yo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1et9/when_does_query_optimization_in_dbms_happen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1et9/when_does_query_optimization_in_dbms_happen/", "subreddit_subscribers": 170049, "created_utc": 1710794728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "agenda is to extract the data from Redshift and need to load into SQL server\n\n\nWhat are someone of the best approachs to do this , fast and cost efficient.", "author_fullname": "t2_iiiqo30a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestion on etl", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhv9to", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710780087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;agenda is to extract the data from Redshift and need to load into SQL server&lt;/p&gt;\n\n&lt;p&gt;What are someone of the best approachs to do this , fast and cost efficient.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhv9to", "is_robot_indexable": true, "report_reasons": null, "author": "BOOBINDERxKK", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhv9to/suggestion_on_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhv9to/suggestion_on_etl/", "subreddit_subscribers": 170049, "created_utc": 1710780087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nTo be very up front: I run a small saas focused on real-time metrics. I recently announced a new feature/product I'd love some feedback on (and potentially beta testers).  [Blog Post](https://aggregations.io/blog/autodocs-coming-soon).\n\nThe idea is simple: you forward your event stream and you get a searchable schema of your events, &amp; their properties along with statistics/distributions of the field values. \n\nThe other element comes in the form of a per-version changelog, with alerting for things like type changes, cardinality fluctuations, etc. \n\nI won't go too far into the technical details, unless people are interested -- but building this is obviously has been a very intricate and complex project, I'm pretty happy with the result so far :)\n\n&amp;#x200B;\n\nI've built a system like this multiple times in the past at larger companies, so I know the value it can provide -- I'm just not sure (1) how to express it well and (2) what other scenarios/ features might be useful.\n\nFor example, post launch I know I want to add in more collaboration/annotation features (to make it more of a \"documentation hub\" for analysts, data producers, etc) -- but other things I've built before, may not be super applicable. \n\nIn the past, I enabled \"generate SQL to fetch this property in different languages\" because JSON functions can be tricky and some payloads are gnarly. I don't know if that is widely applicable? \n\n&amp;#x200B;\n\nIs this a thing that would help you? What struggles do you have with documenting your events, etc?\n\nAny thoughts or feedback would be greatly appreciated! ", "author_fullname": "t2_vpdufq3pm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics Events Documentation &amp; Monitoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhpxx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710766234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;To be very up front: I run a small saas focused on real-time metrics. I recently announced a new feature/product I&amp;#39;d love some feedback on (and potentially beta testers).  &lt;a href=\"https://aggregations.io/blog/autodocs-coming-soon\"&gt;Blog Post&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The idea is simple: you forward your event stream and you get a searchable schema of your events, &amp;amp; their properties along with statistics/distributions of the field values. &lt;/p&gt;\n\n&lt;p&gt;The other element comes in the form of a per-version changelog, with alerting for things like type changes, cardinality fluctuations, etc. &lt;/p&gt;\n\n&lt;p&gt;I won&amp;#39;t go too far into the technical details, unless people are interested -- but building this is obviously has been a very intricate and complex project, I&amp;#39;m pretty happy with the result so far :)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a system like this multiple times in the past at larger companies, so I know the value it can provide -- I&amp;#39;m just not sure (1) how to express it well and (2) what other scenarios/ features might be useful.&lt;/p&gt;\n\n&lt;p&gt;For example, post launch I know I want to add in more collaboration/annotation features (to make it more of a &amp;quot;documentation hub&amp;quot; for analysts, data producers, etc) -- but other things I&amp;#39;ve built before, may not be super applicable. &lt;/p&gt;\n\n&lt;p&gt;In the past, I enabled &amp;quot;generate SQL to fetch this property in different languages&amp;quot; because JSON functions can be tricky and some payloads are gnarly. I don&amp;#39;t know if that is widely applicable? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this a thing that would help you? What struggles do you have with documenting your events, etc?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts or feedback would be greatly appreciated! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?auto=webp&amp;s=d82e0bdcb768bd94828b24f022cf6710f5fd134f", "width": 1024, "height": 649}, "resolutions": [{"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df99c7f6cc2a6e1b07f115e66572f9d72a9b8c92", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b006cd92fff54c42cf968e704fd21d6aaedd845d", "width": 216, "height": 136}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=03d22aa565ec345cb9aa8586fb78133fe3868106", "width": 320, "height": 202}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8739eb0b61a84385b26c321db0d6125c4c1f1835", "width": 640, "height": 405}, {"url": "https://external-preview.redd.it/QexzLy9ugNWGTrQnTHX7vpn21TrDZaAoQXk5q0cQnDc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=216181d41f4abfddef7cbbcc1b76ad8259974862", "width": 960, "height": 608}], "variants": {}, "id": "-IguNufVGIqW9ENdLi7KpG31ptOTdRhW0AL9zjARo1g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhpxx2", "is_robot_indexable": true, "report_reasons": null, "author": "jsneedles", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhpxx2/analytics_events_documentation_monitoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhpxx2/analytics_events_documentation_monitoring/", "subreddit_subscribers": 170049, "created_utc": 1710766234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Get the knowledge required to excel in the realms of data engineering, data science, and a host of related in-demand fields with this collection of books from O\u2019Reilly! Deciphering Data Architectures provides a guided tour of today\u2019s most common architectures\u2014from data lakehouses to data meshes\u2014to help you understand the pros and cons of each. Data Science: The Hard Parts is a handy guidebook of techniques and best practices that are generally overlooked when teaching this wide-ranging discipline.\n\n\n", "author_fullname": "t2_6b9o0e5i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Humble Tech Book Bundle: Pipelines and NoSQL by O'Reilly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhzg06", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kRC4x_HcbhsgiwleL2XqINxbXymSn_3C1EO3omqF0og.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1710790063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "humblebundle.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Get the knowledge required to excel in the realms of data engineering, data science, and a host of related in-demand fields with this collection of books from O\u2019Reilly! Deciphering Data Architectures provides a guided tour of today\u2019s most common architectures\u2014from data lakehouses to data meshes\u2014to help you understand the pros and cons of each. Data Science: The Hard Parts is a handy guidebook of techniques and best practices that are generally overlooked when teaching this wide-ranging discipline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_threes_tile_index_1_c_pipelinesandnosqloreilly_bookbundle", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?auto=webp&amp;s=b50f3a7282d32dcfa0e059b0b506a19daa6b7df9", "width": 1120, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8df569a17eb586e2b0feb844c8ddd80e2c53523e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc1d0f218d319d03ff9fb045ea5a017098e1c402", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4db0e3979f4fd989e036859dfa1257d15a01f34", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0c5aa5216466749f2ffd89e4f6dda46502974b1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92b050a7f148a6395bed48099df1cca2307178a1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abb37e101932d1c5791c9ad599433a01b56b311e", "width": 1080, "height": 607}], "variants": {}, "id": "FbykrjGMgm2863qufl2xVlkGdUUd4rQbUPblShEhFV4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1bhzg06", "is_robot_indexable": true, "report_reasons": null, "author": "serious_frank", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhzg06/humble_tech_book_bundle_pipelines_and_nosql_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_threes_tile_index_1_c_pipelinesandnosqloreilly_bookbundle", "subreddit_subscribers": 170049, "created_utc": 1710790063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Practical Data Engineering: A Hands-On Real-Estate Project Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi4oww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uV26zp7PCWtK7MY6N1K57dVho0aLyS0OzFGWG5NlO_g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710802548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sspaeti-com/practical-data-engineering/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?auto=webp&amp;s=992c374da0b7cef563bce125b40f7dd70f1be59c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc278d38d5b68c3d58fd1c83d91f3d4a5ed5d00d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c9c7c2427d27bbb7a3472b4e6add60124f18c80", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b3d352a98a4461d3372f0b5e4ccb805371c183a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f9137f00a5567761222bb5535d7e204dabc16e4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d31d1ad32e4fd19da504743c6205ca1bc880371e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e58b4a64203c0b621797d896f3a0ef28fa8ed7e", "width": 1080, "height": 540}], "variants": {}, "id": "4LE-hJ8pfXAPB3TmqsVS2HmP-B727Eo4P_1rtYcMipw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bi4oww", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bi4oww/practical_data_engineering_a_handson_realestate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sspaeti-com/practical-data-engineering/", "subreddit_subscribers": 170049, "created_utc": 1710802548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says I'm looking for any messy datasets that would require some significant transformation for a personal project I'm doing. I have already set the architecture which involves Kafka, RDS Postgres, Docker, Debezium and Pyspark. I specify some tables from which any new-entries are captured and published to Kafka topic(s) through a debezium-postgres-connector. The next step is to ingest these real-time entries in Pyspark, perform some transformation, and publish it to another topic for subsequent processing (I haven't decided what I'm going to do yet \u2014 maybe some visualization/analytics?)\n\n&amp;#x200B;\n\nEverything is working fine and I'm getting the new entries in real time. But the data that is currently present in the database is overall clean and doesn't require any major transformation. Maybe dropping a column or two, or filtering for rows that don't meet a specific criteria but that's it. Neither is the data really huge.\n\n&amp;#x200B;\n\nSo I'm looking for any datasets that can span multiple tables so I can do some meaningful transformation on them. I could upload the datasets to the db and proceed from there. \n\n&amp;#x200B;\n\nI'll also take any suggestions regarding the project itself. Maybe there's a flaw in the architecture I'm missing. Who knows?", "author_fullname": "t2_4w6ebksa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any messy datasets for Pyspark practice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bibu4z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710822578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says I&amp;#39;m looking for any messy datasets that would require some significant transformation for a personal project I&amp;#39;m doing. I have already set the architecture which involves Kafka, RDS Postgres, Docker, Debezium and Pyspark. I specify some tables from which any new-entries are captured and published to Kafka topic(s) through a debezium-postgres-connector. The next step is to ingest these real-time entries in Pyspark, perform some transformation, and publish it to another topic for subsequent processing (I haven&amp;#39;t decided what I&amp;#39;m going to do yet \u2014 maybe some visualization/analytics?)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Everything is working fine and I&amp;#39;m getting the new entries in real time. But the data that is currently present in the database is overall clean and doesn&amp;#39;t require any major transformation. Maybe dropping a column or two, or filtering for rows that don&amp;#39;t meet a specific criteria but that&amp;#39;s it. Neither is the data really huge.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m looking for any datasets that can span multiple tables so I can do some meaningful transformation on them. I could upload the datasets to the db and proceed from there. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll also take any suggestions regarding the project itself. Maybe there&amp;#39;s a flaw in the architecture I&amp;#39;m missing. Who knows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bibu4z", "is_robot_indexable": true, "report_reasons": null, "author": "SAAD_3XK", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bibu4z/any_messy_datasets_for_pyspark_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bibu4z/any_messy_datasets_for_pyspark_practice/", "subreddit_subscribers": 170049, "created_utc": 1710822578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a client that insists they want to see normalized data in a denormalized csv. If I were to do this it would be hundreds of columns and the number of columns could change so they would need to be dynamically generated.  \n\n\nI don't suppose a tool exists to do this?", "author_fullname": "t2_mo4lb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Denormalize Tables into CSV?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi1uwp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710795746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a client that insists they want to see normalized data in a denormalized csv. If I were to do this it would be hundreds of columns and the number of columns could change so they would need to be dynamically generated.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t suppose a tool exists to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bi1uwp", "is_robot_indexable": true, "report_reasons": null, "author": "Phantazein", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1uwp/denormalize_tables_into_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1uwp/denormalize_tables_into_csv/", "subreddit_subscribers": 170049, "created_utc": 1710795746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have athena partitions for date like:\n\nyear=2012/month=01/day=01 and inside each folder around 30 parquet files\n\nThe schema of the parquet files does not have the columns year, month, and day, it has a single column called 'date' which is a timestamp, when I created the folders just did it by filtering on the spark df and then writing on the S3 folder. Does this affect performance in how Athena queries this data? Should I have within the .parquet files a schema with the year, month, and day? should I change my partitions to be 2012/01/01 2012/01/02 and create a column within the .parquet files called just \"actual\\_date\" and keep \"date\" timestamp? any thoughts? :) TIA!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena Partitions and .parquet schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi1qnn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710795935.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710795487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have athena partitions for date like:&lt;/p&gt;\n\n&lt;p&gt;year=2012/month=01/day=01 and inside each folder around 30 parquet files&lt;/p&gt;\n\n&lt;p&gt;The schema of the parquet files does not have the columns year, month, and day, it has a single column called &amp;#39;date&amp;#39; which is a timestamp, when I created the folders just did it by filtering on the spark df and then writing on the S3 folder. Does this affect performance in how Athena queries this data? Should I have within the .parquet files a schema with the year, month, and day? should I change my partitions to be 2012/01/01 2012/01/02 and create a column within the .parquet files called just &amp;quot;actual_date&amp;quot; and keep &amp;quot;date&amp;quot; timestamp? any thoughts? :) TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi1qnn", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1qnn/athena_partitions_and_parquet_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1qnn/athena_partitions_and_parquet_schema/", "subreddit_subscribers": 170049, "created_utc": 1710795487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey!\n\nNot sure if this is the best place for this question, but I thought it would be interesting to hear how people are building models with require joining across large impression and conversion tables to calculate stats.\n\nThe main issue I am trying to figure out best practice for right now is how to handle the fact that conversions can happen up 30 days days in the future past impressions. This means each time I run my jobs, I need to select a whole 30 days of data from the tables. This seems extremely inefficient, especially when running this hourly.\n\nWould love to hear how people are tackling this!\n\nI am using DBT + Bigquery.", "author_fullname": "t2_8eusc3v3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are people building a impression and conversion tracking system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhwr9o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710783670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;Not sure if this is the best place for this question, but I thought it would be interesting to hear how people are building models with require joining across large impression and conversion tables to calculate stats.&lt;/p&gt;\n\n&lt;p&gt;The main issue I am trying to figure out best practice for right now is how to handle the fact that conversions can happen up 30 days days in the future past impressions. This means each time I run my jobs, I need to select a whole 30 days of data from the tables. This seems extremely inefficient, especially when running this hourly.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how people are tackling this!&lt;/p&gt;\n\n&lt;p&gt;I am using DBT + Bigquery.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhwr9o", "is_robot_indexable": true, "report_reasons": null, "author": "Suspicious_Dress_350", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhwr9o/how_are_people_building_a_impression_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhwr9o/how_are_people_building_a_impression_and/", "subreddit_subscribers": 170049, "created_utc": 1710783670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to integrate OpenLineage to my airflow server, but I wanna know if I can use other metadata visualization tools instead of marquez", "author_fullname": "t2_dwxrwyrv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to use DataHub as UI for OpenLineage backend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhsa88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710772659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to integrate OpenLineage to my airflow server, but I wanna know if I can use other metadata visualization tools instead of marquez&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhsa88", "is_robot_indexable": true, "report_reasons": null, "author": "nhawlao", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhsa88/is_it_possible_to_use_datahub_as_ui_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhsa88/is_it_possible_to_use_datahub_as_ui_for/", "subreddit_subscribers": 170049, "created_utc": 1710772659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Experts,\n\nPutting it here as its more related to data. We are using AWS cloud for multiple applications and using multiple databases like AWS aurora postgres, Mysql, redshift, Dynamo, Snowflake etc. and also in many cases data is stored S3 storage simply for long term retention and occasional querying purpose. \n\nWe want to implement data archival and retention strategies across all the databases or data stores. Want to know, if we already have any common framework available for data archival and retention across all the databases or data stores in these AWS databases or data stores.\n\n Or \n\nIs it possible to build some common framework like having a common config table(something as below) and then invoke either DELETE command (if the table is non partitioned) or else drop partition (if table is partitioned). The lambda with the business logic can be scheduled (through aws event bridge) will connect to the database and then read the config table and do the data archival and purge as per defined setup?\n\n  \"Table\\_name\"\n\n \"retention\\_days\\_number\"\n\n \"retention\\_column\\_name\"\n\n \"Partition\\_type\\_name\"\n\n \"partitioned\\_yes\\_no\"\n\n \"partition\\_column\\_name\" ", "author_fullname": "t2_cxqpzxgel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data purging strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bidc92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710828155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Experts,&lt;/p&gt;\n\n&lt;p&gt;Putting it here as its more related to data. We are using AWS cloud for multiple applications and using multiple databases like AWS aurora postgres, Mysql, redshift, Dynamo, Snowflake etc. and also in many cases data is stored S3 storage simply for long term retention and occasional querying purpose. &lt;/p&gt;\n\n&lt;p&gt;We want to implement data archival and retention strategies across all the databases or data stores. Want to know, if we already have any common framework available for data archival and retention across all the databases or data stores in these AWS databases or data stores.&lt;/p&gt;\n\n&lt;p&gt;Or &lt;/p&gt;\n\n&lt;p&gt;Is it possible to build some common framework like having a common config table(something as below) and then invoke either DELETE command (if the table is non partitioned) or else drop partition (if table is partitioned). The lambda with the business logic can be scheduled (through aws event bridge) will connect to the database and then read the config table and do the data archival and purge as per defined setup?&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Table_name&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;retention_days_number&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;retention_column_name&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Partition_type_name&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;partitioned_yes_no&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;partition_column_name&amp;quot; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bidc92", "is_robot_indexable": true, "report_reasons": null, "author": "Upper-Lifeguard-8478", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bidc92/data_purging_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bidc92/data_purging_strategy/", "subreddit_subscribers": 170049, "created_utc": 1710828155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, \n\n&amp;#x200B;\n\nI am a senior data engineer in Germany and would like to try out contracting jobs (on the side of my main job) just to explore new opportunities. \n\n&amp;#x200B;\n\nI have been applying to contracting remove jobs in EU (including UK) and USA via websites like Reed, Hays, Harnham but with no luck. \n\n&amp;#x200B;\n\nDoes anyone have any experience in such setup? Would love to hear your thoughts", "author_fullname": "t2_xm0tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you find contracting jobs? [senior DE]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi9yt8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710816737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am a senior data engineer in Germany and would like to try out contracting jobs (on the side of my main job) just to explore new opportunities. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have been applying to contracting remove jobs in EU (including UK) and USA via websites like Reed, Hays, Harnham but with no luck. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience in such setup? Would love to hear your thoughts&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bi9yt8", "is_robot_indexable": true, "report_reasons": null, "author": "elephantail", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi9yt8/how_do_you_find_contracting_jobs_senior_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi9yt8/how_do_you_find_contracting_jobs_senior_de/", "subreddit_subscribers": 170049, "created_utc": 1710816737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Starting a new project, I'm grappling with some debugging challenges:\n\n1. **Dim tables referencing fact tables:** Is this standard practice or a potential pattern to reconsider?\n2. **Multiple fact tables referencing other fact tables:** How do we effectively manage this complexity?\n\nSeeking practical advice: Are these situations acceptable, or should we prioritize a data model refactor within our team? Your insights would be invaluable!", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding Dimension and Fact Table References: Seeking Practical Insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi83x0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710811395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Starting a new project, I&amp;#39;m grappling with some debugging challenges:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Dim tables referencing fact tables:&lt;/strong&gt; Is this standard practice or a potential pattern to reconsider?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multiple fact tables referencing other fact tables:&lt;/strong&gt; How do we effectively manage this complexity?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Seeking practical advice: Are these situations acceptable, or should we prioritize a data model refactor within our team? Your insights would be invaluable!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi83x0", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi83x0/understanding_dimension_and_fact_table_references/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi83x0/understanding_dimension_and_fact_table_references/", "subreddit_subscribers": 170049, "created_utc": 1710811395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nHope you're all doing well!   I'm trying to learn how to scrape data from job sites like Glassdoor and Monster using Python. \n\nI tried to use BeautifulSoup and request but I'm unable to find the correct class and parse the data.\n\nAny tips or advice on where to start? Would appreciate any help!\n\nThankyou", "author_fullname": "t2_ec8yqztxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice from my veteran brothers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi30ja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710798459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Hope you&amp;#39;re all doing well!   I&amp;#39;m trying to learn how to scrape data from job sites like Glassdoor and Monster using Python. &lt;/p&gt;\n\n&lt;p&gt;I tried to use BeautifulSoup and request but I&amp;#39;m unable to find the correct class and parse the data.&lt;/p&gt;\n\n&lt;p&gt;Any tips or advice on where to start? Would appreciate any help!&lt;/p&gt;\n\n&lt;p&gt;Thankyou&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bi30ja", "is_robot_indexable": true, "report_reasons": null, "author": "Southern-Pilot-804", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi30ja/seeking_advice_from_my_veteran_brothers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi30ja/seeking_advice_from_my_veteran_brothers/", "subreddit_subscribers": 170049, "created_utc": 1710798459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**TL;DR**\nWhat processes or tools do you use to create logging specs, get the specs implemented, validate their implementation, and then maintain documentation for that logging?\n\n**Background**\n\nAs a Data Engineer, I spend a lot of time with my software engineers asking them to implement new or modify existing logging. In order to do so, I provide a logging spec to them with instructions on what I'm looking for.  \n\n**My Process**\n\nMy company currently uses unstructured google docs to collaborate on the spec, but then there's no way to validate the implementation and google docs are easily lost over time and documentation for these events become non-existent.", "author_fullname": "t2_fo0y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the most effective way to create, implement, validate, and document logging specifications?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi1mny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710795238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;\nWhat processes or tools do you use to create logging specs, get the specs implemented, validate their implementation, and then maintain documentation for that logging?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As a Data Engineer, I spend a lot of time with my software engineers asking them to implement new or modify existing logging. In order to do so, I provide a logging spec to them with instructions on what I&amp;#39;m looking for.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Process&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My company currently uses unstructured google docs to collaborate on the spec, but then there&amp;#39;s no way to validate the implementation and google docs are easily lost over time and documentation for these events become non-existent.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi1mny", "is_robot_indexable": true, "report_reasons": null, "author": "sharpchicity", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1mny/what_is_the_most_effective_way_to_create/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1mny/what_is_the_most_effective_way_to_create/", "subreddit_subscribers": 170049, "created_utc": 1710795238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain\\_like\\_im\\_5\\_databricks\\_photon/](https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/)\n\nHey guys I saw this post the other day and we have a similar set up. except I want something open source instead of a full fledge product like Photon. from what ive read, spark3 already converts data frames into columnar formatting in the catalytic optimizer. \n\nso im even wondering apart from the C++ translation if I were to use photon would there be any other benefits? \n\n&amp;#x200B;\n\nalso if you know of any open source products that can be used in OP's tech stack to accelerate my project lmk please\n\n&amp;#x200B;", "author_fullname": "t2_i28c8i3vu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photon alternatives? I need something open source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhvpyv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710781181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/\"&gt;https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey guys I saw this post the other day and we have a similar set up. except I want something open source instead of a full fledge product like Photon. from what ive read, spark3 already converts data frames into columnar formatting in the catalytic optimizer. &lt;/p&gt;\n\n&lt;p&gt;so im even wondering apart from the C++ translation if I were to use photon would there be any other benefits? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;also if you know of any open source products that can be used in OP&amp;#39;s tech stack to accelerate my project lmk please&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bhvpyv", "is_robot_indexable": true, "report_reasons": null, "author": "Stunning_Wolf_4595", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhvpyv/photon_alternatives_i_need_something_open_source/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhvpyv/photon_alternatives_i_need_something_open_source/", "subreddit_subscribers": 170049, "created_utc": 1710781181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have delta live table which reads the CSV Files from ADLS and loads into bronze and silver. Your typical Data Pipeline.\n\nIt\u2019s not very complicated pipeline. With auto loader it instantly picks the files and loads to bronze in Unity Catalog but in between I am doing two operations -\n\n1) Usual Deduplication based on the Hash value \n\n2) Based on a configuration CSV file , I pick operations like convert a elements in a  column to Upper case , or trim the elements of the column or you replace all null/empty values with NA. Small operations which I don\u2019t think will take much time for like whole 77K records I have for just prototyping \n\nMy question is , loading the Silver table just take a lot of time , 15 minutes and that too for 77K record. \n\nI am very new to spark , being from your  traditional Software engineer background.\nI would expect that any kind of transformation can be done parallel in workers but I see only 1 worker running in Spark UI. I see DLT uses structured streaming so does it just run jobs in serial way like one after another and then gather all data set in one data frame and return ? \n\nCan\u2019t see logs in workers because of whole Unity Catalog and I don\u2019t know how to enable those. Gave some configs but it dint work.  \n\n\nIf you guys have any alternative to this or pattern which is better than this , please suggest. That will be a great help for me. I don\u2019t seem to find a lot on internet about this particular use case. \n\n\n", "author_fullname": "t2_89nifnvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help needed on Delta Live Table ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhumcc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710778500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have delta live table which reads the CSV Files from ADLS and loads into bronze and silver. Your typical Data Pipeline.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s not very complicated pipeline. With auto loader it instantly picks the files and loads to bronze in Unity Catalog but in between I am doing two operations -&lt;/p&gt;\n\n&lt;p&gt;1) Usual Deduplication based on the Hash value &lt;/p&gt;\n\n&lt;p&gt;2) Based on a configuration CSV file , I pick operations like convert a elements in a  column to Upper case , or trim the elements of the column or you replace all null/empty values with NA. Small operations which I don\u2019t think will take much time for like whole 77K records I have for just prototyping &lt;/p&gt;\n\n&lt;p&gt;My question is , loading the Silver table just take a lot of time , 15 minutes and that too for 77K record. &lt;/p&gt;\n\n&lt;p&gt;I am very new to spark , being from your  traditional Software engineer background.\nI would expect that any kind of transformation can be done parallel in workers but I see only 1 worker running in Spark UI. I see DLT uses structured streaming so does it just run jobs in serial way like one after another and then gather all data set in one data frame and return ? &lt;/p&gt;\n\n&lt;p&gt;Can\u2019t see logs in workers because of whole Unity Catalog and I don\u2019t know how to enable those. Gave some configs but it dint work.  &lt;/p&gt;\n\n&lt;p&gt;If you guys have any alternative to this or pattern which is better than this , please suggest. That will be a great help for me. I don\u2019t seem to find a lot on internet about this particular use case. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhumcc", "is_robot_indexable": true, "report_reasons": null, "author": "voucherwolves", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhumcc/help_needed_on_delta_live_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhumcc/help_needed_on_delta_live_table/", "subreddit_subscribers": 170049, "created_utc": 1710778500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm seeking advice on automating the transformation process for ERP data from raw (csv) to silver layer in Microsoft Fabrics. Here are some factors I'm considering for automation:\n\n-Incremental load\n\n-SCD2 handling\n\n-Automatic adjustment for column changes (Insert, Delete) in the underlying data structure while still supporting SCD2\n\n-Generating artificial surrogate keys for streamlined joins in the gold layer (any optimization suggestions, like z-order for delta tables?\n\n-Automatic adaptation of column metadata from ERP\n\n\nWhat are your thoughts on which of these factors can be effectively automated and which might require manual intervention?\n\nLooking forward to your insights and recommendations. Thanks in advance!", "author_fullname": "t2_2q1g4lkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Automating ERP Data Transformation to Silver Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhprz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710765699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking advice on automating the transformation process for ERP data from raw (csv) to silver layer in Microsoft Fabrics. Here are some factors I&amp;#39;m considering for automation:&lt;/p&gt;\n\n&lt;p&gt;-Incremental load&lt;/p&gt;\n\n&lt;p&gt;-SCD2 handling&lt;/p&gt;\n\n&lt;p&gt;-Automatic adjustment for column changes (Insert, Delete) in the underlying data structure while still supporting SCD2&lt;/p&gt;\n\n&lt;p&gt;-Generating artificial surrogate keys for streamlined joins in the gold layer (any optimization suggestions, like z-order for delta tables?&lt;/p&gt;\n\n&lt;p&gt;-Automatic adaptation of column metadata from ERP&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on which of these factors can be effectively automated and which might require manual intervention?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your insights and recommendations. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhprz9", "is_robot_indexable": true, "report_reasons": null, "author": "tomdg4", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhprz9/seeking_advice_on_automating_erp_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhprz9/seeking_advice_on_automating_erp_data/", "subreddit_subscribers": 170049, "created_utc": 1710765699.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}