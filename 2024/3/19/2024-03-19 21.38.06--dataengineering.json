{"kind": "Listing", "data": {"after": "t3_1binect", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hope this post is ok, as I don't work for either O'Reilly or Humble Bundle. Given the number of questions on this thread for getting books on the topic, thought maybe some of you might be interested in this too! Personally, I'd been wanting to get, \"Data Algorithms with Spark,\" but had been hesitating due to the price. I was super thrilled seeing this included in the book bundle.\n\nThis is an organization that partners with others to offer books (and games) at a super low price. Part of the proceeds goes to charity. I've been a huge fan of them since discovering them a while ago.\n\n[https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books](https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books)", "author_fullname": "t2_97dp6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "O\u2019Reilly data engineering reference books on sale! (Includes reference books on pyspark and scaling up pipelines)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bifhj9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 94, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 94, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710837462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hope this post is ok, as I don&amp;#39;t work for either O&amp;#39;Reilly or Humble Bundle. Given the number of questions on this thread for getting books on the topic, thought maybe some of you might be interested in this too! Personally, I&amp;#39;d been wanting to get, &amp;quot;Data Algorithms with Spark,&amp;quot; but had been hesitating due to the price. I was super thrilled seeing this included in the book bundle.&lt;/p&gt;\n\n&lt;p&gt;This is an organization that partners with others to offer books (and games) at a super low price. Part of the proceeds goes to charity. I&amp;#39;ve been a huge fan of them since discovering them a while ago.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books\"&gt;https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?auto=webp&amp;s=b50f3a7282d32dcfa0e059b0b506a19daa6b7df9", "width": 1120, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8df569a17eb586e2b0feb844c8ddd80e2c53523e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc1d0f218d319d03ff9fb045ea5a017098e1c402", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4db0e3979f4fd989e036859dfa1257d15a01f34", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0c5aa5216466749f2ffd89e4f6dda46502974b1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92b050a7f148a6395bed48099df1cca2307178a1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abb37e101932d1c5791c9ad599433a01b56b311e", "width": 1080, "height": 607}], "variants": {}, "id": "FbykrjGMgm2863qufl2xVlkGdUUd4rQbUPblShEhFV4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bifhj9", "is_robot_indexable": true, "report_reasons": null, "author": "truckbot101", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bifhj9/oreilly_data_engineering_reference_books_on_sale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bifhj9/oreilly_data_engineering_reference_books_on_sale/", "subreddit_subscribers": 170209, "created_utc": 1710837462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am sure many others have been in my shoes before. \n\nI have been with my org for about a year, and it\u2019s approach to data management is extremely primitive. I am trying to do some dash boarding of various KPIs, but I\u2019m finding that everywhere I look the approach has been that some random person enters data into an excel sheet somewhere without any documentation.\n\nAgain, there\u2019s no documentation on where this data lives or how it comes to be. Over the course of my role, I\u2019ve discovered that much of this data entry could just be automated by creating some views off our main production database. I\u2019ve resolved about 70% of the necessary data processing by creating a view and loading it to a PowerBi data model, but I\u2019m trying to figure out how to handle the missing ends. \n\nI\u2019ve considered asking our operations manager if we could drive people to host their excel workbooks in share point. That way, I\u2019d have access to the files and could take a periodic snapshot by just reading it to a pandas dataframe before loading it to a history table in a data warehouse. \n\nHow have others managed this issue with undocumented excel workbooks floating all over the place? \n\nIt\u2019s tough because I get the sense that many are refusing to share information because they fear their job being automated. At the same time, it is simply not feasible for me to spend my whole day constantly chasing down spreadsheets everywhere.", "author_fullname": "t2_6hsp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excel files sprinkled across organization with no documentation ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi6dus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710807128.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710806762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sure many others have been in my shoes before. &lt;/p&gt;\n\n&lt;p&gt;I have been with my org for about a year, and it\u2019s approach to data management is extremely primitive. I am trying to do some dash boarding of various KPIs, but I\u2019m finding that everywhere I look the approach has been that some random person enters data into an excel sheet somewhere without any documentation.&lt;/p&gt;\n\n&lt;p&gt;Again, there\u2019s no documentation on where this data lives or how it comes to be. Over the course of my role, I\u2019ve discovered that much of this data entry could just be automated by creating some views off our main production database. I\u2019ve resolved about 70% of the necessary data processing by creating a view and loading it to a PowerBi data model, but I\u2019m trying to figure out how to handle the missing ends. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve considered asking our operations manager if we could drive people to host their excel workbooks in share point. That way, I\u2019d have access to the files and could take a periodic snapshot by just reading it to a pandas dataframe before loading it to a history table in a data warehouse. &lt;/p&gt;\n\n&lt;p&gt;How have others managed this issue with undocumented excel workbooks floating all over the place? &lt;/p&gt;\n\n&lt;p&gt;It\u2019s tough because I get the sense that many are refusing to share information because they fear their job being automated. At the same time, it is simply not feasible for me to spend my whole day constantly chasing down spreadsheets everywhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi6dus", "is_robot_indexable": true, "report_reasons": null, "author": "suitupyo", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi6dus/excel_files_sprinkled_across_organization_with_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi6dus/excel_files_sprinkled_across_organization_with_no/", "subreddit_subscribers": 170209, "created_utc": 1710806762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I write data engineering ETL scripts that are meant to be used in the back end for major reports in Tableau or Power BI, looker, things like that. Over the past 6 months, We have been working on this absolutely, atrociously huge Tableau back end ETL script, because the VP that owns the solution that we are making it for once it created as a table. So they want the ETL script to take a huge amount of data and put it into a table, and they want us to directly retrieve off the table. No crazy SQL inside the Power BI report. However, our data engineering manager said to heck with that, we're not doing that, we should do it a completely different way, we should use custom SQL inside the Power bi. So now, we are writing much smaller and very specific custom SQL that is focused on each and every little piece of the dashboard complete 180 from what we were doing before. So before we were creating a data Mart, basically. Now, we're just writing all the custom SQL piecemeal for everything that anyone might ever want to see. \n\n\n\nFrustrating part is my manager acts a little bit gruff about it taking so long, like what is taking you so long to do this? Why is it not done yet? Well, we've changed directions like three times now because you have this idea about what you think is best, and the VP has the idea of what they actually want, the two of you don't agree, so I did it their way, you told me to change and do it your way, now we're doing it a completely different way again. She just doesn't understand that it's really hard to determine who to listen to when there are two leaders giving instructions", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's really annoying when leaders can't make up their mind about what they want", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1birem3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710872947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I write data engineering ETL scripts that are meant to be used in the back end for major reports in Tableau or Power BI, looker, things like that. Over the past 6 months, We have been working on this absolutely, atrociously huge Tableau back end ETL script, because the VP that owns the solution that we are making it for once it created as a table. So they want the ETL script to take a huge amount of data and put it into a table, and they want us to directly retrieve off the table. No crazy SQL inside the Power BI report. However, our data engineering manager said to heck with that, we&amp;#39;re not doing that, we should do it a completely different way, we should use custom SQL inside the Power bi. So now, we are writing much smaller and very specific custom SQL that is focused on each and every little piece of the dashboard complete 180 from what we were doing before. So before we were creating a data Mart, basically. Now, we&amp;#39;re just writing all the custom SQL piecemeal for everything that anyone might ever want to see. &lt;/p&gt;\n\n&lt;p&gt;Frustrating part is my manager acts a little bit gruff about it taking so long, like what is taking you so long to do this? Why is it not done yet? Well, we&amp;#39;ve changed directions like three times now because you have this idea about what you think is best, and the VP has the idea of what they actually want, the two of you don&amp;#39;t agree, so I did it their way, you told me to change and do it your way, now we&amp;#39;re doing it a completely different way again. She just doesn&amp;#39;t understand that it&amp;#39;s really hard to determine who to listen to when there are two leaders giving instructions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1birem3", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1birem3/its_really_annoying_when_leaders_cant_make_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1birem3/its_really_annoying_when_leaders_cant_make_up/", "subreddit_subscribers": 170209, "created_utc": 1710872947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently working as a mix of DA/DE, and I believe my skills with SQL are not upto the mark to make a move to DE.\nI understand there is just \"normal\" sql that data analysts, BI analysts use which is typically:\n\n&gt; Select xyz from abc where ysd = '123'\n\nYes this can get more complex with joins, ctes, window functions etc.\n\nWhat is the difference between complex SQL that a DE would write, compared to the SQL a data analyst writes?", "author_fullname": "t2_mvubfxcg0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Level of SQL for DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bil95m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710857622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working as a mix of DA/DE, and I believe my skills with SQL are not upto the mark to make a move to DE.\nI understand there is just &amp;quot;normal&amp;quot; sql that data analysts, BI analysts use which is typically:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Select xyz from abc where ysd = &amp;#39;123&amp;#39;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yes this can get more complex with joins, ctes, window functions etc.&lt;/p&gt;\n\n&lt;p&gt;What is the difference between complex SQL that a DE would write, compared to the SQL a data analyst writes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bil95m", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous-Kale9545", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bil95m/level_of_sql_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bil95m/level_of_sql_for_de/", "subreddit_subscribers": 170209, "created_utc": 1710857622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to work an enterprise workflow and am trying to think of what the right stack looks like.\n\nCloud at the moment is not a possibility - but the organization will go there one day. It\u2019s just a money thing.\n\nWe have contracts with Microsoft and so my front end is forced to be power apps and power BI. We do have licenses for power automate etc. \n\nWe have a very small allocation of dataverse and a decent sized share point. \n\nMy organization has 0 APIs, but are going to build them.\n\nWe have a few large databases and a gateway.\n\nMy original plan was to do data collection through power apps, power automate into a json, build a rest API with fast API to receive post, route traffic to mongo, store into a Postgres staging area to run Python transforms and into another Postgres as long term structured data, put an API in front of that into an MDM, an API off the MDM into an analytical database, and then an API off of that into the various BI customers and generally run data services.\n\nI\u2019ve recently heard about Apache Cassandra and thought it was interesting. I\u2019ve also heard of people building lakehouses and warehouses with duck DB.\n\nUltimately I\u2019m trying to figure out what the best scalable databases are and the easiest to interact with. Also.. am I doing my flow right?", "author_fullname": "t2_4dovkjca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database choices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bij7z5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710851881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to work an enterprise workflow and am trying to think of what the right stack looks like.&lt;/p&gt;\n\n&lt;p&gt;Cloud at the moment is not a possibility - but the organization will go there one day. It\u2019s just a money thing.&lt;/p&gt;\n\n&lt;p&gt;We have contracts with Microsoft and so my front end is forced to be power apps and power BI. We do have licenses for power automate etc. &lt;/p&gt;\n\n&lt;p&gt;We have a very small allocation of dataverse and a decent sized share point. &lt;/p&gt;\n\n&lt;p&gt;My organization has 0 APIs, but are going to build them.&lt;/p&gt;\n\n&lt;p&gt;We have a few large databases and a gateway.&lt;/p&gt;\n\n&lt;p&gt;My original plan was to do data collection through power apps, power automate into a json, build a rest API with fast API to receive post, route traffic to mongo, store into a Postgres staging area to run Python transforms and into another Postgres as long term structured data, put an API in front of that into an MDM, an API off the MDM into an analytical database, and then an API off of that into the various BI customers and generally run data services.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve recently heard about Apache Cassandra and thought it was interesting. I\u2019ve also heard of people building lakehouses and warehouses with duck DB.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I\u2019m trying to figure out what the best scalable databases are and the easiest to interact with. Also.. am I doing my flow right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bij7z5", "is_robot_indexable": true, "report_reasons": null, "author": "necrohobo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bij7z5/database_choices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bij7z5/database_choices/", "subreddit_subscribers": 170209, "created_utc": 1710851881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, dlt (the data ingestion library) cofounder here,   \n\n\nI want to showcase our event ingestion setup. We put this behind cloudflare, to lower latency in different geographies.\n\nMany of our users use dlt for event ingestion. We were using Segment ourselves as we had free credits, but on credit expiration the bill is not pretty. So we moved to dlt on serverless gcp cloud functions with pub sub.\n\nWe like Segment, but we like 18x cost saving more :)\n\nHere's our setup  \n[https://dlthub.com/docs/blog/dlt-segment-migration](https://dlthub.com/docs/blog/dlt-segment-migration)\n\nMore streaming setups done by our users here: [https://dlthub.com/docs/blog/tags/streaming](https://dlthub.com/docs/blog/tags/streaming)  \n\n\n&amp;#x200B;", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Event ingestion on GCP terraform template + blog (18x cost saving over Segment)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bigwrv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710843634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, dlt (the data ingestion library) cofounder here,   &lt;/p&gt;\n\n&lt;p&gt;I want to showcase our event ingestion setup. We put this behind cloudflare, to lower latency in different geographies.&lt;/p&gt;\n\n&lt;p&gt;Many of our users use dlt for event ingestion. We were using Segment ourselves as we had free credits, but on credit expiration the bill is not pretty. So we moved to dlt on serverless gcp cloud functions with pub sub.&lt;/p&gt;\n\n&lt;p&gt;We like Segment, but we like 18x cost saving more :)&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s our setup&lt;br/&gt;\n&lt;a href=\"https://dlthub.com/docs/blog/dlt-segment-migration\"&gt;https://dlthub.com/docs/blog/dlt-segment-migration&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More streaming setups done by our users here: &lt;a href=\"https://dlthub.com/docs/blog/tags/streaming\"&gt;https://dlthub.com/docs/blog/tags/streaming&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?auto=webp&amp;s=7f542da7e01be864731a87a5966f1da32fb1c0c6", "width": 3483, "height": 1148}, "resolutions": [{"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e43620cee4872bff896e6e91e21a8b05be75fb6c", "width": 108, "height": 35}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e7bf904f616dabb756e6dfc370782f91bb6e0c7", "width": 216, "height": 71}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eeac09672bbbc25f987aaa496dc5565036f1da3b", "width": 320, "height": 105}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea4b58ab320d7d12631b04de23f15a300aecdd7b", "width": 640, "height": 210}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a6fc2854f5627b9b2ff29e9e4745b940c2d8ac7", "width": 960, "height": 316}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eeb7f45ca1bd06a2122e72974c60807e1bedf33a", "width": 1080, "height": 355}], "variants": {}, "id": "_iRCnnKFUj1iGmaRU1XQdxy12fe92yHsgkY8JGxjpyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bigwrv", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bigwrv/event_ingestion_on_gcp_terraform_template_blog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bigwrv/event_ingestion_on_gcp_terraform_template_blog/", "subreddit_subscribers": 170209, "created_utc": 1710843634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says I'm looking for any messy datasets that would require some significant transformation for a personal project I'm doing. I have already set the architecture which involves Kafka, RDS Postgres, Docker, Debezium and Pyspark. I specify some tables from which any new-entries are captured and published to Kafka topic(s) through a debezium-postgres-connector. The next step is to ingest these real-time entries in Pyspark, perform some transformation, and publish it to another topic for subsequent processing (I haven't decided what I'm going to do yet \u2014 maybe some visualization/analytics?)\n\n&amp;#x200B;\n\nEverything is working fine and I'm getting the new entries in real time. But the data that is currently present in the database is overall clean and doesn't require any major transformation. Maybe dropping a column or two, or filtering for rows that don't meet a specific criteria but that's it. Neither is the data really huge.\n\n&amp;#x200B;\n\nSo I'm looking for any datasets that can span multiple tables so I can do some meaningful transformation on them. I could upload the datasets to the db and proceed from there. \n\n&amp;#x200B;\n\nI'll also take any suggestions regarding the project itself. Maybe there's a flaw in the architecture I'm missing. Who knows?", "author_fullname": "t2_4w6ebksa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any messy datasets for Pyspark practice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bibu4z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710822578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says I&amp;#39;m looking for any messy datasets that would require some significant transformation for a personal project I&amp;#39;m doing. I have already set the architecture which involves Kafka, RDS Postgres, Docker, Debezium and Pyspark. I specify some tables from which any new-entries are captured and published to Kafka topic(s) through a debezium-postgres-connector. The next step is to ingest these real-time entries in Pyspark, perform some transformation, and publish it to another topic for subsequent processing (I haven&amp;#39;t decided what I&amp;#39;m going to do yet \u2014 maybe some visualization/analytics?)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Everything is working fine and I&amp;#39;m getting the new entries in real time. But the data that is currently present in the database is overall clean and doesn&amp;#39;t require any major transformation. Maybe dropping a column or two, or filtering for rows that don&amp;#39;t meet a specific criteria but that&amp;#39;s it. Neither is the data really huge.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m looking for any datasets that can span multiple tables so I can do some meaningful transformation on them. I could upload the datasets to the db and proceed from there. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll also take any suggestions regarding the project itself. Maybe there&amp;#39;s a flaw in the architecture I&amp;#39;m missing. Who knows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bibu4z", "is_robot_indexable": true, "report_reasons": null, "author": "SAAD_3XK", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bibu4z/any_messy_datasets_for_pyspark_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bibu4z/any_messy_datasets_for_pyspark_practice/", "subreddit_subscribers": 170209, "created_utc": 1710822578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learning Airflow for the first time. Have been using Prefect.\n\nLearned about xcoms today. The syntax for passing things seems very verbose and somewhat cumbersome, but I get it. However, the 48kb limit means xcoms have some limited application.\n\nIn researching posts, I saw on multiple occasions people stating (paraphrasing here) \"*back when Airflow was still using xcoms*\".\n\nWhat does this mean?\n\n* In airflow, can a task be a function that calls other functions that are not tasks themselves?\n* Could your task push data to something like S3 where the next task can pick it up?\n* Are xcoms still an integral part of Airflow?", "author_fullname": "t2_qhsi5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do people say xcoms are no longer the default in airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bimv59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710861874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learning Airflow for the first time. Have been using Prefect.&lt;/p&gt;\n\n&lt;p&gt;Learned about xcoms today. The syntax for passing things seems very verbose and somewhat cumbersome, but I get it. However, the 48kb limit means xcoms have some limited application.&lt;/p&gt;\n\n&lt;p&gt;In researching posts, I saw on multiple occasions people stating (paraphrasing here) &amp;quot;&lt;em&gt;back when Airflow was still using xcoms&lt;/em&gt;&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What does this mean?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In airflow, can a task be a function that calls other functions that are not tasks themselves?&lt;/li&gt;\n&lt;li&gt;Could your task push data to something like S3 where the next task can pick it up?&lt;/li&gt;\n&lt;li&gt;Are xcoms still an integral part of Airflow?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bimv59", "is_robot_indexable": true, "report_reasons": null, "author": "NoUsernames1eft", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bimv59/why_do_people_say_xcoms_are_no_longer_the_default/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bimv59/why_do_people_say_xcoms_are_no_longer_the_default/", "subreddit_subscribers": 170209, "created_utc": 1710861874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, \n\n&amp;#x200B;\n\nI am a senior data engineer in Germany and would like to try out contracting jobs (on the side of my main job) just to explore new opportunities. \n\n&amp;#x200B;\n\nI have been applying to contracting remove jobs in EU (including UK) and USA via websites like Reed, Hays, Harnham but with no luck. \n\n&amp;#x200B;\n\nDoes anyone have any experience in such setup? Would love to hear your thoughts", "author_fullname": "t2_xm0tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you find contracting jobs? [senior DE]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi9yt8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710816737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am a senior data engineer in Germany and would like to try out contracting jobs (on the side of my main job) just to explore new opportunities. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have been applying to contracting remove jobs in EU (including UK) and USA via websites like Reed, Hays, Harnham but with no luck. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience in such setup? Would love to hear your thoughts&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bi9yt8", "is_robot_indexable": true, "report_reasons": null, "author": "elephantail", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi9yt8/how_do_you_find_contracting_jobs_senior_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi9yt8/how_do_you_find_contracting_jobs_senior_de/", "subreddit_subscribers": 170209, "created_utc": 1710816737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Starting a new project, I'm grappling with some debugging challenges:\n\n1. **Dim tables referencing fact tables:** Is this standard practice or a potential pattern to reconsider?\n2. **Multiple fact tables referencing other fact tables:** How do we effectively manage this complexity?\n\nSeeking practical advice: Are these situations acceptable, or should we prioritize a data model refactor within our team? Your insights would be invaluable!", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding Dimension and Fact Table References: Seeking Practical Insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi83x0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710811395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Starting a new project, I&amp;#39;m grappling with some debugging challenges:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Dim tables referencing fact tables:&lt;/strong&gt; Is this standard practice or a potential pattern to reconsider?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multiple fact tables referencing other fact tables:&lt;/strong&gt; How do we effectively manage this complexity?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Seeking practical advice: Are these situations acceptable, or should we prioritize a data model refactor within our team? Your insights would be invaluable!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi83x0", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi83x0/understanding_dimension_and_fact_table_references/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi83x0/understanding_dimension_and_fact_table_references/", "subreddit_subscribers": 170209, "created_utc": 1710811395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone ever stretched the truth about them being a DE? I was reading a post on Reddit where someone was a Data Analyst, and just did cloud work and projects on his own. He said he put his job title down as \"Data Engineer\" and ended up getting a DE job.  \n\n\nDoes that sound like something common in the job field? I have heard horror stories of people being hired in and not showing competencies. ", "author_fullname": "t2_tpf6owzl8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stretching the truth about being a \"Data Engineer\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bit81p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710877335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever stretched the truth about them being a DE? I was reading a post on Reddit where someone was a Data Analyst, and just did cloud work and projects on his own. He said he put his job title down as &amp;quot;Data Engineer&amp;quot; and ended up getting a DE job.  &lt;/p&gt;\n\n&lt;p&gt;Does that sound like something common in the job field? I have heard horror stories of people being hired in and not showing competencies. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bit81p", "is_robot_indexable": true, "report_reasons": null, "author": "DarkPaladin67", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bit81p/stretching_the_truth_about_being_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bit81p/stretching_the_truth_about_being_a_data_engineer/", "subreddit_subscribers": 170209, "created_utc": 1710877335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_cbh6ollo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Kafka connector to send kafka topics data to 200+ destinations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1biqsae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Dcu6VBDsnTInxOse8U-R-5-yKyByVMnBqCVSKZnCVVI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710871426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/rudderlabs/rudder-kafka-sink-connector", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?auto=webp&amp;s=2236b72c38c1ffdb3ef00bc2c4dae5fbd4a116cb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab3990f924bd9bf0da88fb79a89075e6cad33a01", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4df0cc65b132eb2a2f60895b37c4f59f705b5cd7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8847d43ea3f647be0e9a2cb39a1ebc6a8277a728", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e6253a85bf7e130c92e6bf21615e7bf385246f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c854fd5dd1329ce2cb94a4e60d5d3324969aeb54", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/WHKVfhj3QKTkIVEax-7bSvrfqzSVhtUTW03pN5eqW7c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97c03bdc700f1d9114a39bb80c449226c6882b10", "width": 1080, "height": 540}], "variants": {}, "id": "HHCapiHt6RX89jBT8ceKmiW73oQ_kfiILwHJ-WwYTqg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1biqsae", "is_robot_indexable": true, "report_reasons": null, "author": "ephemeral404", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biqsae/open_source_kafka_connector_to_send_kafka_topics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/rudderlabs/rudder-kafka-sink-connector", "subreddit_subscribers": 170209, "created_utc": 1710871426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Practical Data Engineering: A Hands-On Real-Estate Project Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi4oww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uV26zp7PCWtK7MY6N1K57dVho0aLyS0OzFGWG5NlO_g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710802548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sspaeti-com/practical-data-engineering/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?auto=webp&amp;s=992c374da0b7cef563bce125b40f7dd70f1be59c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc278d38d5b68c3d58fd1c83d91f3d4a5ed5d00d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c9c7c2427d27bbb7a3472b4e6add60124f18c80", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b3d352a98a4461d3372f0b5e4ccb805371c183a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f9137f00a5567761222bb5535d7e204dabc16e4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d31d1ad32e4fd19da504743c6205ca1bc880371e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e58b4a64203c0b621797d896f3a0ef28fa8ed7e", "width": 1080, "height": 540}], "variants": {}, "id": "4LE-hJ8pfXAPB3TmqsVS2HmP-B727Eo4P_1rtYcMipw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bi4oww", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bi4oww/practical_data_engineering_a_handson_realestate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sspaeti-com/practical-data-engineering/", "subreddit_subscribers": 170209, "created_utc": 1710802548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Guys! I need help\n\nIn my business, we typically don't rely on concrete facts, but rather on daily snapshots. We operate within the educational system, focusing on aspects like \"Students enrolled in courses on date X\". These student records may remain constant (I lack a specific date column for updates, only a cancellation date). Therefore, every day we capture a snapshot at 11:59 PM, which results in a significant amount of duplicate data, given the minimal changes from day to day. Perhaps one or two cancellations and two or three new enrollments might occur.\n\nIts something like 50K rows per day, it is not so much but we are on prem \n\nHow can i handle this type of fact?  Usually directors request to me like \"How many enrolled students we have on day X\"", "author_fullname": "t2_8jc0mwfh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Problem to design a fact table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1biua2g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710879883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Guys! I need help&lt;/p&gt;\n\n&lt;p&gt;In my business, we typically don&amp;#39;t rely on concrete facts, but rather on daily snapshots. We operate within the educational system, focusing on aspects like &amp;quot;Students enrolled in courses on date X&amp;quot;. These student records may remain constant (I lack a specific date column for updates, only a cancellation date). Therefore, every day we capture a snapshot at 11:59 PM, which results in a significant amount of duplicate data, given the minimal changes from day to day. Perhaps one or two cancellations and two or three new enrollments might occur.&lt;/p&gt;\n\n&lt;p&gt;Its something like 50K rows per day, it is not so much but we are on prem &lt;/p&gt;\n\n&lt;p&gt;How can i handle this type of fact?  Usually directors request to me like &amp;quot;How many enrolled students we have on day X&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1biua2g", "is_robot_indexable": true, "report_reasons": null, "author": "Andremallmann", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biua2g/problem_to_design_a_fact_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biua2g/problem_to_design_a_fact_table/", "subreddit_subscribers": 170209, "created_utc": 1710879883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Engineers, I am kinda starting to learn and use dbt and have used delta live tables from databricks(dlt) in the past. I am trying to understand the point of using dbt when I am already in databricks environment.\n\nCould someone who has the experience point out some of the scenarios where you found dbt more useful than using dlt. I assume dlt will be bit costlier but would love to hear your thoughts.", "author_fullname": "t2_szxdhbt3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt vs databricks dot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bige78", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710841541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Engineers, I am kinda starting to learn and use dbt and have used delta live tables from databricks(dlt) in the past. I am trying to understand the point of using dbt when I am already in databricks environment.&lt;/p&gt;\n\n&lt;p&gt;Could someone who has the experience point out some of the scenarios where you found dbt more useful than using dlt. I assume dlt will be bit costlier but would love to hear your thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bige78", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum__Gold", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bige78/dbt_vs_databricks_dot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bige78/dbt_vs_databricks_dot/", "subreddit_subscribers": 170209, "created_utc": 1710841541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there! \ud83d\udc4b Don't miss out on this amazing opportunity to join the MIT's Data Science and Analytics course at a special discount. Seize this chance to boost your skills and advance your career. Sign up now and don't let this unique opportunity pass you by! \ud83d\ude80\n\nhttps://executive-ed.xpro.mit.edu/professional-certificate-in-data-science-and-analytics?advocate_program=01tAy000000EpbkIAC&amp;advocate_source=RAEmailer&amp;coupon=JAIME:2-8JF42DN&amp;utm_campaign=incentivized_referrals&amp;utm_content=SO%20-%20MIT%20xPRO%20-%20Prof%20Certificate%20in%20Data%20Science&amp;utm_medium=personal_url&amp;utm_placement=RAEmailer_190324&amp;utm_source=referral&amp;utm_term=0wMJmkv0Xu/o8ddY+B3A57SYwlP3ymwT7cGliwqr2Pc=#referrals-email-capture-modal", "author_fullname": "t2_q5oohxl0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MIT Data Science and Analytics ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1biv43f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710881883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there! \ud83d\udc4b Don&amp;#39;t miss out on this amazing opportunity to join the MIT&amp;#39;s Data Science and Analytics course at a special discount. Seize this chance to boost your skills and advance your career. Sign up now and don&amp;#39;t let this unique opportunity pass you by! \ud83d\ude80&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://executive-ed.xpro.mit.edu/professional-certificate-in-data-science-and-analytics?advocate_program=01tAy000000EpbkIAC&amp;amp;advocate_source=RAEmailer&amp;amp;coupon=JAIME:2-8JF42DN&amp;amp;utm_campaign=incentivized_referrals&amp;amp;utm_content=SO%20-%20MIT%20xPRO%20-%20Prof%20Certificate%20in%20Data%20Science&amp;amp;utm_medium=personal_url&amp;amp;utm_placement=RAEmailer_190324&amp;amp;utm_source=referral&amp;amp;utm_term=0wMJmkv0Xu/o8ddY+B3A57SYwlP3ymwT7cGliwqr2Pc=#referrals-email-capture-modal\"&gt;https://executive-ed.xpro.mit.edu/professional-certificate-in-data-science-and-analytics?advocate_program=01tAy000000EpbkIAC&amp;amp;advocate_source=RAEmailer&amp;amp;coupon=JAIME:2-8JF42DN&amp;amp;utm_campaign=incentivized_referrals&amp;amp;utm_content=SO%20-%20MIT%20xPRO%20-%20Prof%20Certificate%20in%20Data%20Science&amp;amp;utm_medium=personal_url&amp;amp;utm_placement=RAEmailer_190324&amp;amp;utm_source=referral&amp;amp;utm_term=0wMJmkv0Xu/o8ddY+B3A57SYwlP3ymwT7cGliwqr2Pc=#referrals-email-capture-modal&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1biv43f", "is_robot_indexable": true, "report_reasons": null, "author": "EnriquePesca", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biv43f/mit_data_science_and_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biv43f/mit_data_science_and_analytics/", "subreddit_subscribers": 170209, "created_utc": 1710881883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need to remove the meta data date from the meta data and don\u2019t know how to do that?", "author_fullname": "t2_9l2plnma", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question META DATA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1biuvo1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710881331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need to remove the meta data date from the meta data and don\u2019t know how to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1biuvo1", "is_robot_indexable": true, "report_reasons": null, "author": "Final-Guard6388", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biuvo1/question_meta_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biuvo1/question_meta_data/", "subreddit_subscribers": 170209, "created_utc": 1710881331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been asked to make a plan/road map to migrate my job\u2019s pipelines off airflow + Spark + Redshift to airflow + dbt core + Redshift\n\nCurrently our keep raw data  in S3 to use DBT I would need to load that raw data into using the COPY command in Redshift.\n\nSo I\u2019ve been tinkering with DBT but it doesn\u2019t let me run CREATE, COPY, and UNLOAD anywhere except the pre-hook and post-hook of models, it expects everything to be a SELECT statement.\n\nThis seems like a terrible practice because the SQL \u201chooks\u201d in these needs to be a quoted string meaning you get no sql linting,no syntax highlighting or checking etc.\n\nIs there any way to schedule arbitrary SQL that isn\u2019t models with dbt?\n\n I can roll my own but it just seems like a tool like this that\u2019s literally for running SQL should have something more than these pre and post hooks to run DDL SQL commands.\n\n**tl,dr:** We have over 200+ raw data tables in S3. Making empty models with only pre and post hook to use the CREAT, COPY, and UNLOAD commands to load the data into Redshift seems like a bad idea. Is there a way to run DDL from DBT that\u2019s not pre and post hooks.\n", "author_fullname": "t2_160eq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DDL for COPY and UNLOAD commands in DBT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1biukdk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710880824.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710880575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been asked to make a plan/road map to migrate my job\u2019s pipelines off airflow + Spark + Redshift to airflow + dbt core + Redshift&lt;/p&gt;\n\n&lt;p&gt;Currently our keep raw data  in S3 to use DBT I would need to load that raw data into using the COPY command in Redshift.&lt;/p&gt;\n\n&lt;p&gt;So I\u2019ve been tinkering with DBT but it doesn\u2019t let me run CREATE, COPY, and UNLOAD anywhere except the pre-hook and post-hook of models, it expects everything to be a SELECT statement.&lt;/p&gt;\n\n&lt;p&gt;This seems like a terrible practice because the SQL \u201chooks\u201d in these needs to be a quoted string meaning you get no sql linting,no syntax highlighting or checking etc.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to schedule arbitrary SQL that isn\u2019t models with dbt?&lt;/p&gt;\n\n&lt;p&gt;I can roll my own but it just seems like a tool like this that\u2019s literally for running SQL should have something more than these pre and post hooks to run DDL SQL commands.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;tl,dr:&lt;/strong&gt; We have over 200+ raw data tables in S3. Making empty models with only pre and post hook to use the CREAT, COPY, and UNLOAD commands to load the data into Redshift seems like a bad idea. Is there a way to run DDL from DBT that\u2019s not pre and post hooks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1biukdk", "is_robot_indexable": true, "report_reasons": null, "author": "SirAutismx7", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biukdk/ddl_for_copy_and_unload_commands_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biukdk/ddl_for_copy_and_unload_commands_in_dbt/", "subreddit_subscribers": 170209, "created_utc": 1710880575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm currently tasked with setting up an open-source ingestion tool for two main use cases within my organization:\n1. Self-service capabilities: for our non-technical users to easily ingest data.\n2. ELT data pipeline: construction to streamline our data processing workflows.\n\nWe were initially considering **Airbyte** as the primary tool due to its versatility and broad connector support. However, we've encountered a significant compatibility issue with our current architecture, which heavily relies on **Delta Lake tables** and **Spark** for data processing. Specifically, we're facing challenges with Airbyte's integration with this setup, as discussed in this GitHub issue: https://github.com/airbytehq/airbyte/issues/16322\n\nGiven this context, I'm reaching out to the community for advice:\n- Has anyone successfully found a workaround for integrating Airbyte with Delta Lake and Spark, as per the mentioned issue?\n- Alternatively, are there any other open-source tools you would recommend that could meet our needs and seamlessly fit into our Delta Lake and Spark-centric architecture?\n\nAny insights, experiences, or suggestions you could share would be immensely appreciated. Our goal is to find a reliable, open-source solution that can accommodate our specific requirements without compromising on functionality or ease of use.\n\nThank you in advance for your help and looking forward to your recommendations!\n", "author_fullname": "t2_hffs9quu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Open Source Ingestion Tool Compatible with Delta Lake and Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1biu10m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710879253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently tasked with setting up an open-source ingestion tool for two main use cases within my organization:\n1. Self-service capabilities: for our non-technical users to easily ingest data.\n2. ELT data pipeline: construction to streamline our data processing workflows.&lt;/p&gt;\n\n&lt;p&gt;We were initially considering &lt;strong&gt;Airbyte&lt;/strong&gt; as the primary tool due to its versatility and broad connector support. However, we&amp;#39;ve encountered a significant compatibility issue with our current architecture, which heavily relies on &lt;strong&gt;Delta Lake tables&lt;/strong&gt; and &lt;strong&gt;Spark&lt;/strong&gt; for data processing. Specifically, we&amp;#39;re facing challenges with Airbyte&amp;#39;s integration with this setup, as discussed in this GitHub issue: &lt;a href=\"https://github.com/airbytehq/airbyte/issues/16322\"&gt;https://github.com/airbytehq/airbyte/issues/16322&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Given this context, I&amp;#39;m reaching out to the community for advice:\n- Has anyone successfully found a workaround for integrating Airbyte with Delta Lake and Spark, as per the mentioned issue?\n- Alternatively, are there any other open-source tools you would recommend that could meet our needs and seamlessly fit into our Delta Lake and Spark-centric architecture?&lt;/p&gt;\n\n&lt;p&gt;Any insights, experiences, or suggestions you could share would be immensely appreciated. Our goal is to find a reliable, open-source solution that can accommodate our specific requirements without compromising on functionality or ease of use.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help and looking forward to your recommendations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?auto=webp&amp;s=40569cb9bca9b656aab79a7caaf2aa15cecb1528", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=316467b66d8db1e6a7898aacaca13779a4ea3b08", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25f37ba73d36d0d9037a0fcd4924c33469d1d770", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=da0f588fcc80b504fc9396d7b2ab057b97a3e4de", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d1e94e73dedf8b3380a50400e5aef29a6266815", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=970f88a9bdd47a0ef61e9ea80d52ce826e0d3a86", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/pLGN3TzxDTAb7r7K0KCgXSZ27cnQJRwzFzkjuhcm668.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef746c904e388dc56ef058a6d4e645f17e9caacf", "width": 1080, "height": 540}], "variants": {}, "id": "tVtrXdHx-ZiNTu5lwWtBhHdKSdGFT2DM2lQxn6Ffg38"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1biu10m", "is_robot_indexable": true, "report_reasons": null, "author": "Ecstatic-Zucchini-53", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1biu10m/seeking_advice_on_open_source_ingestion_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1biu10m/seeking_advice_on_open_source_ingestion_tool/", "subreddit_subscribers": 170209, "created_utc": 1710879253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI have $500 to use as part of L&amp;D from my company.\n\nJust wondering what are the best resources to invest in.\n\nI read most of my books on Kindle and my computer, so not really a big of physical books.\n\nThinking of educative.io membership?\n\nAny suggestions?\n\nThanks!", "author_fullname": "t2_hnhd87tx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to use learning budget?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bisyjw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710876684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have $500 to use as part of L&amp;amp;D from my company.&lt;/p&gt;\n\n&lt;p&gt;Just wondering what are the best resources to invest in.&lt;/p&gt;\n\n&lt;p&gt;I read most of my books on Kindle and my computer, so not really a big of physical books.&lt;/p&gt;\n\n&lt;p&gt;Thinking of educative.io membership?&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bisyjw", "is_robot_indexable": true, "report_reasons": null, "author": "StoicResearcher", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bisyjw/how_to_use_learning_budget/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bisyjw/how_to_use_learning_budget/", "subreddit_subscribers": 170209, "created_utc": 1710876684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello data engineers,\n\nI'm seeking advice on migrating our on-premises Oracle data warehouse to the azure cloud. Our on-premises oracle db is 80tb with thousands of etl information jobs . \n\n * What was your approach (business-driven vs. replicating on-premises processes)?\n\n * Any recommended migration strategies or tools?\nOur current approach feels scattered. \n\nAny tips or lessons learned would be greatly appreciated!\n", "author_fullname": "t2_7f74h3uy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Legacy Oracle Data Warehouse to Azure Cloud Migration Strategies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bistcf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710876341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello data engineers,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking advice on migrating our on-premises Oracle data warehouse to the azure cloud. Our on-premises oracle db is 80tb with thousands of etl information jobs . &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;What was your approach (business-driven vs. replicating on-premises processes)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any recommended migration strategies or tools?\nOur current approach feels scattered. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any tips or lessons learned would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bistcf", "is_robot_indexable": true, "report_reasons": null, "author": "CountNo9037", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bistcf/legacy_oracle_data_warehouse_to_azure_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bistcf/legacy_oracle_data_warehouse_to_azure_cloud/", "subreddit_subscribers": 170209, "created_utc": 1710876341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company currently has separate dev/test/prod environments for our data warehouse. Using one data source as an example:\n\nWe have scripts on a test VM that extract data from the data source's test server and loads the data to our test environment. We have the same scripts on a prod VM that extracts data from the data source's prod server and loads to our prod environment.\n\nProblem: \n\n1. The test server on the data source isn't routinely updated, and is thus out of sync with prod and makes a 1 to 1 validation of data for new dev work difficult when users look at the prod data on the data source application. \n2. It seems redundant to have scripts extract data from both test and prod servers, when we could alternatively extract just from prod data source server into our prod data warehouse and then push straight from our prod environment to our test environment so that minimal resources are used for ETL and test data aligns perfectly with prod data.\n\nQuestion:\n\nHow do you guys typically manage ETL for your test environment? Am I right that it would be better to only load data into the prod data warehouse and push from there into the test data warehouse?", "author_fullname": "t2_j3ecksk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Load Test Environment Data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bisp92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710876069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company currently has separate dev/test/prod environments for our data warehouse. Using one data source as an example:&lt;/p&gt;\n\n&lt;p&gt;We have scripts on a test VM that extract data from the data source&amp;#39;s test server and loads the data to our test environment. We have the same scripts on a prod VM that extracts data from the data source&amp;#39;s prod server and loads to our prod environment.&lt;/p&gt;\n\n&lt;p&gt;Problem: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The test server on the data source isn&amp;#39;t routinely updated, and is thus out of sync with prod and makes a 1 to 1 validation of data for new dev work difficult when users look at the prod data on the data source application. &lt;/li&gt;\n&lt;li&gt;It seems redundant to have scripts extract data from both test and prod servers, when we could alternatively extract just from prod data source server into our prod data warehouse and then push straight from our prod environment to our test environment so that minimal resources are used for ETL and test data aligns perfectly with prod data.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;How do you guys typically manage ETL for your test environment? Am I right that it would be better to only load data into the prod data warehouse and push from there into the test data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bisp92", "is_robot_indexable": true, "report_reasons": null, "author": "SellGameRent", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bisp92/how_to_load_test_environment_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bisp92/how_to_load_test_environment_data/", "subreddit_subscribers": 170209, "created_utc": 1710876069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Datadogs, \n\nYour expert opinion would be appreciated if you could provide me with book recommendations on Streaming systems, NRT (near-time pipelines ) and related literature.\n\n&amp;#x200B;", "author_fullname": "t2_snzbmfolo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Recommendations on Streaming Systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1birk8h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710873313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Datadogs, &lt;/p&gt;\n\n&lt;p&gt;Your expert opinion would be appreciated if you could provide me with book recommendations on Streaming systems, NRT (near-time pipelines ) and related literature.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1birk8h", "is_robot_indexable": true, "report_reasons": null, "author": "loomingdale", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1birk8h/book_recommendations_on_streaming_systems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1birk8h/book_recommendations_on_streaming_systems/", "subreddit_subscribers": 170209, "created_utc": 1710873313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Do you normally build APIs?\n\nI have good gasp of reading and parsing data from APIs but I have never build any. Not sure if building APIs is common for hedge fund DEs? Thank you!\n\nWhat are the common data sources where DE pull data from?  Besides files, APIs, ftps, sql servers, what else? Thank you.  I posted this yesterday but not sure why the content is no longer available. I am kind of new here. Not sure If I am breaking any rules? ", "author_fullname": "t2_9od6j04g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any data engineers working at a hedge fund? I got a couple job interviews coming and would like some insights.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bipbf7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710867845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you normally build APIs?&lt;/p&gt;\n\n&lt;p&gt;I have good gasp of reading and parsing data from APIs but I have never build any. Not sure if building APIs is common for hedge fund DEs? Thank you!&lt;/p&gt;\n\n&lt;p&gt;What are the common data sources where DE pull data from?  Besides files, APIs, ftps, sql servers, what else? Thank you.  I posted this yesterday but not sure why the content is no longer available. I am kind of new here. Not sure If I am breaking any rules? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bipbf7", "is_robot_indexable": true, "report_reasons": null, "author": "Tall-Skin5800", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bipbf7/any_data_engineers_working_at_a_hedge_fund_i_got/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bipbf7/any_data_engineers_working_at_a_hedge_fund_i_got/", "subreddit_subscribers": 170209, "created_utc": 1710867845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wozut7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Text to Teradata SQL with LangChain and OpenAI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1binect", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3Bt4dXoyv9LpR83xzMwbB4oMHw_w-qTJS7KF_MeLGCw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710863226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/teradata/using-natural-language-to-query-teradata-vantagecloud-with-llms-a1f9bfa1688b", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ewc05IuLWgofqH0Yu3Pypt2IAza71uN1sIiyGmvt4Cs.jpg?auto=webp&amp;s=8656c7bc5a3298e323984d334b76a2d5db78f038", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Ewc05IuLWgofqH0Yu3Pypt2IAza71uN1sIiyGmvt4Cs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=770863f7f55dce5dca2c6ad16091235001b6ece2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Ewc05IuLWgofqH0Yu3Pypt2IAza71uN1sIiyGmvt4Cs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8aa418d82b94af1dde1c4864f3bcf054394cbb4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Ewc05IuLWgofqH0Yu3Pypt2IAza71uN1sIiyGmvt4Cs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0a3212764ce7d06a649dc21b8f1251f58e1181a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Ewc05IuLWgofqH0Yu3Pypt2IAza71uN1sIiyGmvt4Cs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c7e85215a65fc0a9fc8a53d52ccf493d13aaf7b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Ewc05IuLWgofqH0Yu3Pypt2IAza71uN1sIiyGmvt4Cs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a81ed9f32085b686becf0b73f222ba24e2cda38b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Ewc05IuLWgofqH0Yu3Pypt2IAza71uN1sIiyGmvt4Cs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad1b4ed91074636074036ef9a4703ef84c98d885", "width": 1080, "height": 540}], "variants": {}, "id": "oEZ5WsycoKJMTv0PRVi_gzHAW5h9BulpibxfpTP4WMY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1binect", "is_robot_indexable": true, "report_reasons": null, "author": "JanethL", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1binect/text_to_teradata_sql_with_langchain_and_openai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/teradata/using-natural-language-to-query-teradata-vantagecloud-with-llms-a1f9bfa1688b", "subreddit_subscribers": 170209, "created_utc": 1710863226.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}