{"kind": "Listing", "data": {"after": "t3_1bi1uwp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently hosted the \"NBA Data Modeling Challenge,\" where over 100 participants modeled\u2014yes, you guessed it\u2014historical NBA data!\n\nLeveraging SQL and dbt, participants went above and beyond to uncover NBA insights and compete for a big prize: $1,500!\n\nIn this blog post, I've compiled my favorite insights generated by the participants, such as:\n\n* The dramatic impact of the 3-pointer on the NBA over the last decade\n* The most consistent playoff performers of all time\n* The players who should have been awarded MVP in each season\n* The most clutch NBA players of all time\n* After adjusting for inflation, the highest-paid NBA players ever\n* The most overvalued players in the 2022-23 season\n\nIt's a must-read if you're an NBA fan or just love high-quality SQL, dbt, data analysis, and data visualization!\n\n[Check out the blog here!](https://www.paradime.io/blog/basketball-by-the-numbers-insights-from-paradimes-nba-data-modeling-challenge)", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key Insights from NBA Data Modeling Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhutin", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710778985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently hosted the &amp;quot;NBA Data Modeling Challenge,&amp;quot; where over 100 participants modeled\u2014yes, you guessed it\u2014historical NBA data!&lt;/p&gt;\n\n&lt;p&gt;Leveraging SQL and dbt, participants went above and beyond to uncover NBA insights and compete for a big prize: $1,500!&lt;/p&gt;\n\n&lt;p&gt;In this blog post, I&amp;#39;ve compiled my favorite insights generated by the participants, such as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The dramatic impact of the 3-pointer on the NBA over the last decade&lt;/li&gt;\n&lt;li&gt;The most consistent playoff performers of all time&lt;/li&gt;\n&lt;li&gt;The players who should have been awarded MVP in each season&lt;/li&gt;\n&lt;li&gt;The most clutch NBA players of all time&lt;/li&gt;\n&lt;li&gt;After adjusting for inflation, the highest-paid NBA players ever&lt;/li&gt;\n&lt;li&gt;The most overvalued players in the 2022-23 season&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s a must-read if you&amp;#39;re an NBA fan or just love high-quality SQL, dbt, data analysis, and data visualization!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.paradime.io/blog/basketball-by-the-numbers-insights-from-paradimes-nba-data-modeling-challenge\"&gt;Check out the blog here!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?auto=webp&amp;s=25f017d68d0cf4c258efa3e609d72159ed14aac6", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6fbac571ba0ce966cbcec335f160a91ac7ced94", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb36f934fd4b8620ed8f27f64b10972193ec4f35", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0be76566051f49336667a35d8613b7e9e075336e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6b200f5d42ed3944e618b8233d94f7bcc0dc47f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=65158b0818951a2c049382d98f68aa710e693c4c", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/UB1-maZSOynQH2GqzzWNIdWykBuC7A-u0hrbMOX5i1Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d6aaf46c2cc14d1be0078f00af273aa676c29fa", "width": 1080, "height": 607}], "variants": {}, "id": "hdEr2Ol9ohYczAT_H4Yxt1eeFjwqm5W_afbNmG9q8NQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bhutin", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhutin/key_insights_from_nba_data_modeling_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhutin/key_insights_from_nba_data_modeling_challenge/", "subreddit_subscribers": 170105, "created_utc": 1710778985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hope this post is ok, as I don't work for either O'Reilly or Humble Bundle. Given the number of questions on this thread for getting books on the topic, thought maybe some of you might be interested in this too! Personally, I'd been wanting to get, \"Data Algorithms with Spark,\" but had been hesitating due to the price. I was super thrilled seeing this included in the book bundle.\n\nThis is an organization that partners with others to offer books (and games) at a super low price. Part of the proceeds goes to charity. I've been a huge fan of them since discovering them a while ago.\n\n[https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books](https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books)", "author_fullname": "t2_97dp6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "O\u2019Reilly data engineering reference books on sale! (Includes reference books on pyspark and scaling up pipelines)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bifhj9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710837462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hope this post is ok, as I don&amp;#39;t work for either O&amp;#39;Reilly or Humble Bundle. Given the number of questions on this thread for getting books on the topic, thought maybe some of you might be interested in this too! Personally, I&amp;#39;d been wanting to get, &amp;quot;Data Algorithms with Spark,&amp;quot; but had been hesitating due to the price. I was super thrilled seeing this included in the book bundle.&lt;/p&gt;\n\n&lt;p&gt;This is an organization that partners with others to offer books (and games) at a super low price. Part of the proceeds goes to charity. I&amp;#39;ve been a huge fan of them since discovering them a while ago.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books\"&gt;https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?auto=webp&amp;s=b50f3a7282d32dcfa0e059b0b506a19daa6b7df9", "width": 1120, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8df569a17eb586e2b0feb844c8ddd80e2c53523e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc1d0f218d319d03ff9fb045ea5a017098e1c402", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4db0e3979f4fd989e036859dfa1257d15a01f34", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0c5aa5216466749f2ffd89e4f6dda46502974b1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92b050a7f148a6395bed48099df1cca2307178a1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abb37e101932d1c5791c9ad599433a01b56b311e", "width": 1080, "height": 607}], "variants": {}, "id": "FbykrjGMgm2863qufl2xVlkGdUUd4rQbUPblShEhFV4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bifhj9", "is_robot_indexable": true, "report_reasons": null, "author": "truckbot101", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bifhj9/oreilly_data_engineering_reference_books_on_sale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bifhj9/oreilly_data_engineering_reference_books_on_sale/", "subreddit_subscribers": 170105, "created_utc": 1710837462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am sure many others have been in my shoes before. \n\nI have been with my org for about a year, and it\u2019s approach to data management is extremely primitive. I am trying to do some dash boarding of various KPIs, but I\u2019m finding that everywhere I look the approach has been that some random person enters data into an excel sheet somewhere without any documentation.\n\nAgain, there\u2019s no documentation on where this data lives or how it comes to be. Over the course of my role, I\u2019ve discovered that much of this data entry could just be automated by creating some views off our main production database. I\u2019ve resolved about 70% of the necessary data processing by creating a view and loading it to a PowerBi data model, but I\u2019m trying to figure out how to handle the missing ends. \n\nI\u2019ve considered asking our operations manager if we could drive people to host their excel workbooks in share point. That way, I\u2019d have access to the files and could take a periodic snapshot by just reading it to a pandas dataframe before loading it to a history table in a data warehouse. \n\nHow have others managed this issue with undocumented excel workbooks floating all over the place? \n\nIt\u2019s tough because I get the sense that many are refusing to share information because they fear their job being automated. At the same time, it is simply not feasible for me to spend my whole day constantly chasing down spreadsheets everywhere.", "author_fullname": "t2_6hsp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excel files sprinkled across organization with no documentation ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi6dus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710807128.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710806762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sure many others have been in my shoes before. &lt;/p&gt;\n\n&lt;p&gt;I have been with my org for about a year, and it\u2019s approach to data management is extremely primitive. I am trying to do some dash boarding of various KPIs, but I\u2019m finding that everywhere I look the approach has been that some random person enters data into an excel sheet somewhere without any documentation.&lt;/p&gt;\n\n&lt;p&gt;Again, there\u2019s no documentation on where this data lives or how it comes to be. Over the course of my role, I\u2019ve discovered that much of this data entry could just be automated by creating some views off our main production database. I\u2019ve resolved about 70% of the necessary data processing by creating a view and loading it to a PowerBi data model, but I\u2019m trying to figure out how to handle the missing ends. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve considered asking our operations manager if we could drive people to host their excel workbooks in share point. That way, I\u2019d have access to the files and could take a periodic snapshot by just reading it to a pandas dataframe before loading it to a history table in a data warehouse. &lt;/p&gt;\n\n&lt;p&gt;How have others managed this issue with undocumented excel workbooks floating all over the place? &lt;/p&gt;\n\n&lt;p&gt;It\u2019s tough because I get the sense that many are refusing to share information because they fear their job being automated. At the same time, it is simply not feasible for me to spend my whole day constantly chasing down spreadsheets everywhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi6dus", "is_robot_indexable": true, "report_reasons": null, "author": "suitupyo", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi6dus/excel_files_sprinkled_across_organization_with_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi6dus/excel_files_sprinkled_across_organization_with_no/", "subreddit_subscribers": 170105, "created_utc": 1710806762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently got my first offer as a Data Engineer coming out of school and wanted to understand the possibilities of moving out of DE into SWE at some point. Has any body made this jump before? Was it hard or easy? What kinds of skills did you have to pick up either on the job or by yourself?\n\nThanks!", "author_fullname": "t2_hfl4w59z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Going from DE to SWE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhzgyi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710790126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got my first offer as a Data Engineer coming out of school and wanted to understand the possibilities of moving out of DE into SWE at some point. Has any body made this jump before? Was it hard or easy? What kinds of skills did you have to pick up either on the job or by yourself?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bhzgyi", "is_robot_indexable": true, "report_reasons": null, "author": "digging_for_memories", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhzgyi/going_from_de_to_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhzgyi/going_from_de_to_swe/", "subreddit_subscribers": 170105, "created_utc": 1710790126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So i have a project where basically data needs to be ingested into a big snowflake table from multiple csv files that are sent to AWS SFTP (and forwarded to S3). The project itself is pretty simple tbh. The high level architecture and workflow is as follows: CSV gets send by customer applications into S3 --&gt; loading into snowflake load table --&gt; some small/basic transformations (joins across data of multiple files) --&gt; database with clean data --&gt; some customers will require export jobs like nested json or csv to their own s3 or whatever.\n\nTo be a little bit more specific: the data we are talking about here is very small like 30mb etc. the ingestion is very basic as well maybe some edge cases but overall just parsing CSV.\n\nOur AWS architect made an architecture i will call \"aws first\": Basically whenever anything has to be done he wants me to do it with AWS Lambda and python3. Meaning: Ingestion made with s3 events that triggers lambda that will do simple ingestion. Then the copy process and joins also in a lambda and the export jobs as well.\n\nThe second approach i would call snowflake first is skipping AWS Lambda all together and just using Snowpipe that will listen to s3 events and then some UFD functions or stored procedures. For the Export jobs we can use snowpipe and snowflake tasks as well maybe in some cases we would need external functions and then some kind of AWS lambda service but it's probably unlikely this will be a real use-case. The snowpipes and storage integration i would create with terraform snowflake provider.\n\nSo i would like to ask the community what approach they would choose here. In my opinion both approaches have disadvantages and advantages:\n\nadvantages aws first approach (aws lambda and python)\n\n- most of the project is in aws so aws lambda would integrate very well (IaC) etc.\n\n- patching of software is required anyways since our project has some other tools and apps\n\n- we could implement some additional stuff in the lambda like idempotency\n\n- very easy to understand if the project wants to hire low cost devs in the future for programing integrations\n\n- data is pretty small probably even fits into a python3 dataframe so all the optimisation we would get from focusing on snowflake features probably not worth it\n\nadvantages \"snowflake first\" approach (terraform and snowpipe, tasks, ufd etc.)\n\n- leveraging the tools of a platform we already paying for anyways\n\n- less operational overhead, less changes needed in the future\n\nMaybe i can get some guidance on this topic. It just feels counterintuitive to me to build so many custom lambda functions on the other hand there isn't really a good argument to do most of the workloads in snowflake. What do you think? :) Thanks!\n\n", "author_fullname": "t2_ficwvf44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance: Snowflake first approach vs AWS first approach data ingestion and export jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi195y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710840028.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710794349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i have a project where basically data needs to be ingested into a big snowflake table from multiple csv files that are sent to AWS SFTP (and forwarded to S3). The project itself is pretty simple tbh. The high level architecture and workflow is as follows: CSV gets send by customer applications into S3 --&amp;gt; loading into snowflake load table --&amp;gt; some small/basic transformations (joins across data of multiple files) --&amp;gt; database with clean data --&amp;gt; some customers will require export jobs like nested json or csv to their own s3 or whatever.&lt;/p&gt;\n\n&lt;p&gt;To be a little bit more specific: the data we are talking about here is very small like 30mb etc. the ingestion is very basic as well maybe some edge cases but overall just parsing CSV.&lt;/p&gt;\n\n&lt;p&gt;Our AWS architect made an architecture i will call &amp;quot;aws first&amp;quot;: Basically whenever anything has to be done he wants me to do it with AWS Lambda and python3. Meaning: Ingestion made with s3 events that triggers lambda that will do simple ingestion. Then the copy process and joins also in a lambda and the export jobs as well.&lt;/p&gt;\n\n&lt;p&gt;The second approach i would call snowflake first is skipping AWS Lambda all together and just using Snowpipe that will listen to s3 events and then some UFD functions or stored procedures. For the Export jobs we can use snowpipe and snowflake tasks as well maybe in some cases we would need external functions and then some kind of AWS lambda service but it&amp;#39;s probably unlikely this will be a real use-case. The snowpipes and storage integration i would create with terraform snowflake provider.&lt;/p&gt;\n\n&lt;p&gt;So i would like to ask the community what approach they would choose here. In my opinion both approaches have disadvantages and advantages:&lt;/p&gt;\n\n&lt;p&gt;advantages aws first approach (aws lambda and python)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;most of the project is in aws so aws lambda would integrate very well (IaC) etc.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;patching of software is required anyways since our project has some other tools and apps&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;we could implement some additional stuff in the lambda like idempotency&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;very easy to understand if the project wants to hire low cost devs in the future for programing integrations&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;data is pretty small probably even fits into a python3 dataframe so all the optimisation we would get from focusing on snowflake features probably not worth it&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;advantages &amp;quot;snowflake first&amp;quot; approach (terraform and snowpipe, tasks, ufd etc.)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;leveraging the tools of a platform we already paying for anyways&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;less operational overhead, less changes needed in the future&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Maybe i can get some guidance on this topic. It just feels counterintuitive to me to build so many custom lambda functions on the other hand there isn&amp;#39;t really a good argument to do most of the workloads in snowflake. What do you think? :) Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi195y", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Interaction_5701", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi195y/guidance_snowflake_first_approach_vs_aws_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi195y/guidance_snowflake_first_approach_vs_aws_first/", "subreddit_subscribers": 170105, "created_utc": 1710794349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "agenda is to extract the data from Redshift and need to load into SQL server\n\n\nWhat are someone of the best approachs to do this , fast and cost efficient.", "author_fullname": "t2_iiiqo30a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestion on etl", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhv9to", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710780087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;agenda is to extract the data from Redshift and need to load into SQL server&lt;/p&gt;\n\n&lt;p&gt;What are someone of the best approachs to do this , fast and cost efficient.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhv9to", "is_robot_indexable": true, "report_reasons": null, "author": "BOOBINDERxKK", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhv9to/suggestion_on_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhv9to/suggestion_on_etl/", "subreddit_subscribers": 170105, "created_utc": 1710780087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is optimized by DBMS: logical or physical plans? or both? If only logical plans, how DBMS can calculate costs without knowing of exact physical operations behind nodes of query tree? I also doubt that only physical plans are optimized.  \nI have this picture (CMU DB group lectures) where given logical plan is optimized but I dunno how this may happen if logical plan is to my knowledge high-level DAG of operators without any physical specifics.\n\nTo me it seems that optimizer should optimize both logical plans (iterate through different query plan trees and choose best plan using *heuristics*) and physical plans (iterate through different options of executing the same query plan tree (different joins, access methods, etc.) and choose best plan using *costs-based approach*) but I'm definitely not sure about that.\n\nUPD. Found docs of specific DB [https://docs.pingcap.com/tidb/stable/sql-optimization-concepts](https://docs.pingcap.com/tidb/stable/sql-optimization-concepts) . In this docs there is point about doing some \"logically equivalent changes to the query\" and they name it \"[Logical Optimization](https://docs.pingcap.com/tidb/stable/sql-logical-optimization)\". Also there is about \"obtaining a final execution plan based on the data distribution and the specific execution cost of an operator\" and they name it \"[Physical Optimization](https://docs.pingcap.com/tidb/stable/sql-physical-optimization)\".   \nSo to me it seems that in modern DB systems there is actually similar process to one I described earlier.\n\nhttps://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;format=png&amp;auto=webp&amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a", "author_fullname": "t2_b3ayn5hsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When does query optimization in DBMS happen?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3p9o5illm5pc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f31e57669dba4a7a2b7b285720441bb9fdb4f7f"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c9c84776ba8b8b2e4e94043039331ca3d0a690e"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=631e3dae7cbd3a70a36f697e530ad1d9dcd4185b"}], "s": {"y": 270, "x": 537, "u": "https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;format=png&amp;auto=webp&amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a"}, "id": "3p9o5illm5pc1"}}, "name": "t3_1bi1et9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/N-7Eb17SVssj45oho_0YGsxARVCoWdtSC2M938LmapM.jpg", "edited": 1710800187.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1710794728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is optimized by DBMS: logical or physical plans? or both? If only logical plans, how DBMS can calculate costs without knowing of exact physical operations behind nodes of query tree? I also doubt that only physical plans are optimized.&lt;br/&gt;\nI have this picture (CMU DB group lectures) where given logical plan is optimized but I dunno how this may happen if logical plan is to my knowledge high-level DAG of operators without any physical specifics.&lt;/p&gt;\n\n&lt;p&gt;To me it seems that optimizer should optimize both logical plans (iterate through different query plan trees and choose best plan using &lt;em&gt;heuristics&lt;/em&gt;) and physical plans (iterate through different options of executing the same query plan tree (different joins, access methods, etc.) and choose best plan using &lt;em&gt;costs-based approach&lt;/em&gt;) but I&amp;#39;m definitely not sure about that.&lt;/p&gt;\n\n&lt;p&gt;UPD. Found docs of specific DB &lt;a href=\"https://docs.pingcap.com/tidb/stable/sql-optimization-concepts\"&gt;https://docs.pingcap.com/tidb/stable/sql-optimization-concepts&lt;/a&gt; . In this docs there is point about doing some &amp;quot;logically equivalent changes to the query&amp;quot; and they name it &amp;quot;&lt;a href=\"https://docs.pingcap.com/tidb/stable/sql-logical-optimization\"&gt;Logical Optimization&lt;/a&gt;&amp;quot;. Also there is about &amp;quot;obtaining a final execution plan based on the data distribution and the specific execution cost of an operator&amp;quot; and they name it &amp;quot;&lt;a href=\"https://docs.pingcap.com/tidb/stable/sql-physical-optimization\"&gt;Physical Optimization&lt;/a&gt;&amp;quot;.&lt;br/&gt;\nSo to me it seems that in modern DB systems there is actually similar process to one I described earlier.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a\"&gt;https://preview.redd.it/3p9o5illm5pc1.png?width=537&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=683f91e65fed6d4cc615379eb13cb35997f2578a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?auto=webp&amp;s=8150913c159f903cc5577b744799db42db187d9c", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30824ff14765ae039e3b6d7d2d5ee2dc0f24c652", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=00db47a9520ff588fdf82bcaf30a82abe2c3a3f2", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a231e7f510167f41a4a7f55c51d42ce8b4451acf", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1987b6c83ef844cb83414743dd68ff938f1c110", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5df7f9263b806d2009a7df7ab1303f86671967dc", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/qwZv8hypQb7RFYHa3U4CglTUpF0TmfrRn5LGonrSIDg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c005210e0451b258fbb29ae1ec082d7cd9752d5f", "width": 1080, "height": 1080}], "variants": {}, "id": "ZQDvKgaiNIaaiEfaM14MidCgI9LagV_KOlnvnr_bxa0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bi1et9", "is_robot_indexable": true, "report_reasons": null, "author": "isk14yo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1et9/when_does_query_optimization_in_dbms_happen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1et9/when_does_query_optimization_in_dbms_happen/", "subreddit_subscribers": 170105, "created_utc": 1710794728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Get the knowledge required to excel in the realms of data engineering, data science, and a host of related in-demand fields with this collection of books from O\u2019Reilly! Deciphering Data Architectures provides a guided tour of today\u2019s most common architectures\u2014from data lakehouses to data meshes\u2014to help you understand the pros and cons of each. Data Science: The Hard Parts is a handy guidebook of techniques and best practices that are generally overlooked when teaching this wide-ranging discipline.\n\n\n", "author_fullname": "t2_6b9o0e5i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Humble Tech Book Bundle: Pipelines and NoSQL by O'Reilly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhzg06", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kRC4x_HcbhsgiwleL2XqINxbXymSn_3C1EO3omqF0og.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1710790063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "humblebundle.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Get the knowledge required to excel in the realms of data engineering, data science, and a host of related in-demand fields with this collection of books from O\u2019Reilly! Deciphering Data Architectures provides a guided tour of today\u2019s most common architectures\u2014from data lakehouses to data meshes\u2014to help you understand the pros and cons of each. Data Science: The Hard Parts is a handy guidebook of techniques and best practices that are generally overlooked when teaching this wide-ranging discipline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_threes_tile_index_1_c_pipelinesandnosqloreilly_bookbundle", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?auto=webp&amp;s=b50f3a7282d32dcfa0e059b0b506a19daa6b7df9", "width": 1120, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8df569a17eb586e2b0feb844c8ddd80e2c53523e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc1d0f218d319d03ff9fb045ea5a017098e1c402", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4db0e3979f4fd989e036859dfa1257d15a01f34", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0c5aa5216466749f2ffd89e4f6dda46502974b1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92b050a7f148a6395bed48099df1cca2307178a1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ksfkmMLrK-O0CsI43Gee_aHJNaFDz6JcvyRb6MkH6ps.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abb37e101932d1c5791c9ad599433a01b56b311e", "width": 1080, "height": 607}], "variants": {}, "id": "FbykrjGMgm2863qufl2xVlkGdUUd4rQbUPblShEhFV4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1bhzg06", "is_robot_indexable": true, "report_reasons": null, "author": "serious_frank", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhzg06/humble_tech_book_bundle_pipelines_and_nosql_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books?hmb_source=&amp;hmb_medium=product_tile&amp;hmb_campaign=mosaic_section_1_layout_index_1_layout_type_threes_tile_index_1_c_pipelinesandnosqloreilly_bookbundle", "subreddit_subscribers": 170105, "created_utc": 1710790063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, dlt (the data ingestion library) cofounder here,   \n\n\nI want to showcase our event ingestion setup. We put this behind cloudflare, to lower latency in different geographies.\n\nMany of our users use dlt for event ingestion. We were using Segment ourselves as we had free credits, but on credit expiration the bill is not pretty. So we moved to dlt on serverless gcp cloud functions with pub sub.\n\nWe like Segment, but we like 18x cost saving more :)\n\nHere's our setup  \n[https://dlthub.com/docs/blog/dlt-segment-migration](https://dlthub.com/docs/blog/dlt-segment-migration)\n\nMore streaming setups done by our users here: [https://dlthub.com/docs/blog/tags/streaming](https://dlthub.com/docs/blog/tags/streaming)  \n\n\n&amp;#x200B;", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Event ingestion on GCP terraform template + blog (18x cost saving over Segment)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bigwrv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710843634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, dlt (the data ingestion library) cofounder here,   &lt;/p&gt;\n\n&lt;p&gt;I want to showcase our event ingestion setup. We put this behind cloudflare, to lower latency in different geographies.&lt;/p&gt;\n\n&lt;p&gt;Many of our users use dlt for event ingestion. We were using Segment ourselves as we had free credits, but on credit expiration the bill is not pretty. So we moved to dlt on serverless gcp cloud functions with pub sub.&lt;/p&gt;\n\n&lt;p&gt;We like Segment, but we like 18x cost saving more :)&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s our setup&lt;br/&gt;\n&lt;a href=\"https://dlthub.com/docs/blog/dlt-segment-migration\"&gt;https://dlthub.com/docs/blog/dlt-segment-migration&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More streaming setups done by our users here: &lt;a href=\"https://dlthub.com/docs/blog/tags/streaming\"&gt;https://dlthub.com/docs/blog/tags/streaming&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?auto=webp&amp;s=7f542da7e01be864731a87a5966f1da32fb1c0c6", "width": 3483, "height": 1148}, "resolutions": [{"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e43620cee4872bff896e6e91e21a8b05be75fb6c", "width": 108, "height": 35}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e7bf904f616dabb756e6dfc370782f91bb6e0c7", "width": 216, "height": 71}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eeac09672bbbc25f987aaa496dc5565036f1da3b", "width": 320, "height": 105}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea4b58ab320d7d12631b04de23f15a300aecdd7b", "width": 640, "height": 210}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a6fc2854f5627b9b2ff29e9e4745b940c2d8ac7", "width": 960, "height": 316}, {"url": "https://external-preview.redd.it/vvcyd0J-74fslNI97qJpP3Qs5O-jkC6sczFyt1KoXk0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eeb7f45ca1bd06a2122e72974c60807e1bedf33a", "width": 1080, "height": 355}], "variants": {}, "id": "_iRCnnKFUj1iGmaRU1XQdxy12fe92yHsgkY8JGxjpyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bigwrv", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bigwrv/event_ingestion_on_gcp_terraform_template_blog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bigwrv/event_ingestion_on_gcp_terraform_template_blog/", "subreddit_subscribers": 170105, "created_utc": 1710843634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Starting a new project, I'm grappling with some debugging challenges:\n\n1. **Dim tables referencing fact tables:** Is this standard practice or a potential pattern to reconsider?\n2. **Multiple fact tables referencing other fact tables:** How do we effectively manage this complexity?\n\nSeeking practical advice: Are these situations acceptable, or should we prioritize a data model refactor within our team? Your insights would be invaluable!", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding Dimension and Fact Table References: Seeking Practical Insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi83x0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710811395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Starting a new project, I&amp;#39;m grappling with some debugging challenges:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Dim tables referencing fact tables:&lt;/strong&gt; Is this standard practice or a potential pattern to reconsider?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multiple fact tables referencing other fact tables:&lt;/strong&gt; How do we effectively manage this complexity?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Seeking practical advice: Are these situations acceptable, or should we prioritize a data model refactor within our team? Your insights would be invaluable!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi83x0", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi83x0/understanding_dimension_and_fact_table_references/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi83x0/understanding_dimension_and_fact_table_references/", "subreddit_subscribers": 170105, "created_utc": 1710811395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Practical Data Engineering: A Hands-On Real-Estate Project Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi4oww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": "transparent", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uV26zp7PCWtK7MY6N1K57dVho0aLyS0OzFGWG5NlO_g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710802548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sspaeti-com/practical-data-engineering/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?auto=webp&amp;s=992c374da0b7cef563bce125b40f7dd70f1be59c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc278d38d5b68c3d58fd1c83d91f3d4a5ed5d00d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c9c7c2427d27bbb7a3472b4e6add60124f18c80", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b3d352a98a4461d3372f0b5e4ccb805371c183a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f9137f00a5567761222bb5535d7e204dabc16e4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d31d1ad32e4fd19da504743c6205ca1bc880371e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/cto0MXFFkuWS_oW9hogx2CyLOb1BoIb0TVtfPQwGQdc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e58b4a64203c0b621797d896f3a0ef28fa8ed7e", "width": 1080, "height": 540}], "variants": {}, "id": "4LE-hJ8pfXAPB3TmqsVS2HmP-B727Eo4P_1rtYcMipw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bi4oww", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bi4oww/practical_data_engineering_a_handson_realestate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sspaeti-com/practical-data-engineering/", "subreddit_subscribers": 170105, "created_utc": 1710802548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey!\n\nNot sure if this is the best place for this question, but I thought it would be interesting to hear how people are building models with require joining across large impression and conversion tables to calculate stats.\n\nThe main issue I am trying to figure out best practice for right now is how to handle the fact that conversions can happen up 30 days days in the future past impressions. This means each time I run my jobs, I need to select a whole 30 days of data from the tables. This seems extremely inefficient, especially when running this hourly.\n\nWould love to hear how people are tackling this!\n\nI am using DBT + Bigquery.", "author_fullname": "t2_8eusc3v3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are people building a impression and conversion tracking system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhwr9o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710783670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;Not sure if this is the best place for this question, but I thought it would be interesting to hear how people are building models with require joining across large impression and conversion tables to calculate stats.&lt;/p&gt;\n\n&lt;p&gt;The main issue I am trying to figure out best practice for right now is how to handle the fact that conversions can happen up 30 days days in the future past impressions. This means each time I run my jobs, I need to select a whole 30 days of data from the tables. This seems extremely inefficient, especially when running this hourly.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how people are tackling this!&lt;/p&gt;\n\n&lt;p&gt;I am using DBT + Bigquery.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bhwr9o", "is_robot_indexable": true, "report_reasons": null, "author": "Suspicious_Dress_350", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhwr9o/how_are_people_building_a_impression_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhwr9o/how_are_people_building_a_impression_and/", "subreddit_subscribers": 170105, "created_utc": 1710783670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to work an enterprise workflow and am trying to think of what the right stack looks like.\n\nCloud at the moment is not a possibility - but the organization will go there one day. It\u2019s just a money thing.\n\nWe have contracts with Microsoft and so my front end is forced to be power apps and power BI. We do have licenses for power automate etc. \n\nWe have a very small allocation of dataverse and a decent sized share point. \n\nMy organization has 0 APIs, but are going to build them.\n\nWe have a few large databases and a gateway.\n\nMy original plan was to do data collection through power apps, power automate into a json, build a rest API with fast API to receive post, route traffic to mongo, store into a Postgres staging area to run Python transforms and into another Postgres as long term structured data, put an API in front of that into an MDM, an API off the MDM into an analytical database, and then an API off of that into the various BI customers and generally run data services.\n\nI\u2019ve recently heard about Apache Cassandra and thought it was interesting. I\u2019ve also heard of people building lakehouses and warehouses with duck DB.\n\nUltimately I\u2019m trying to figure out what the best scalable databases are and the easiest to interact with. Also.. am I doing my flow right?", "author_fullname": "t2_4dovkjca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database choices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bij7z5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710851881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to work an enterprise workflow and am trying to think of what the right stack looks like.&lt;/p&gt;\n\n&lt;p&gt;Cloud at the moment is not a possibility - but the organization will go there one day. It\u2019s just a money thing.&lt;/p&gt;\n\n&lt;p&gt;We have contracts with Microsoft and so my front end is forced to be power apps and power BI. We do have licenses for power automate etc. &lt;/p&gt;\n\n&lt;p&gt;We have a very small allocation of dataverse and a decent sized share point. &lt;/p&gt;\n\n&lt;p&gt;My organization has 0 APIs, but are going to build them.&lt;/p&gt;\n\n&lt;p&gt;We have a few large databases and a gateway.&lt;/p&gt;\n\n&lt;p&gt;My original plan was to do data collection through power apps, power automate into a json, build a rest API with fast API to receive post, route traffic to mongo, store into a Postgres staging area to run Python transforms and into another Postgres as long term structured data, put an API in front of that into an MDM, an API off the MDM into an analytical database, and then an API off of that into the various BI customers and generally run data services.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve recently heard about Apache Cassandra and thought it was interesting. I\u2019ve also heard of people building lakehouses and warehouses with duck DB.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I\u2019m trying to figure out what the best scalable databases are and the easiest to interact with. Also.. am I doing my flow right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bij7z5", "is_robot_indexable": true, "report_reasons": null, "author": "necrohobo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bij7z5/database_choices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bij7z5/database_choices/", "subreddit_subscribers": 170105, "created_utc": 1710851881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Engineers, I am kinda starting to learn and use dbt and have used delta live tables from databricks(dlt) in the past. I am trying to understand the point of using dbt when I am already in databricks environment.\n\nCould someone who has the experience point out some of the scenarios where you found dbt more useful than using dlt. I assume dlt will be bit costlier but would love to hear your thoughts.", "author_fullname": "t2_szxdhbt3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt vs databricks dot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bige78", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710841541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Engineers, I am kinda starting to learn and use dbt and have used delta live tables from databricks(dlt) in the past. I am trying to understand the point of using dbt when I am already in databricks environment.&lt;/p&gt;\n\n&lt;p&gt;Could someone who has the experience point out some of the scenarios where you found dbt more useful than using dlt. I assume dlt will be bit costlier but would love to hear your thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bige78", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum__Gold", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bige78/dbt_vs_databricks_dot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bige78/dbt_vs_databricks_dot/", "subreddit_subscribers": 170105, "created_utc": 1710841541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says I'm looking for any messy datasets that would require some significant transformation for a personal project I'm doing. I have already set the architecture which involves Kafka, RDS Postgres, Docker, Debezium and Pyspark. I specify some tables from which any new-entries are captured and published to Kafka topic(s) through a debezium-postgres-connector. The next step is to ingest these real-time entries in Pyspark, perform some transformation, and publish it to another topic for subsequent processing (I haven't decided what I'm going to do yet \u2014 maybe some visualization/analytics?)\n\n&amp;#x200B;\n\nEverything is working fine and I'm getting the new entries in real time. But the data that is currently present in the database is overall clean and doesn't require any major transformation. Maybe dropping a column or two, or filtering for rows that don't meet a specific criteria but that's it. Neither is the data really huge.\n\n&amp;#x200B;\n\nSo I'm looking for any datasets that can span multiple tables so I can do some meaningful transformation on them. I could upload the datasets to the db and proceed from there. \n\n&amp;#x200B;\n\nI'll also take any suggestions regarding the project itself. Maybe there's a flaw in the architecture I'm missing. Who knows?", "author_fullname": "t2_4w6ebksa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any messy datasets for Pyspark practice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bibu4z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710822578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says I&amp;#39;m looking for any messy datasets that would require some significant transformation for a personal project I&amp;#39;m doing. I have already set the architecture which involves Kafka, RDS Postgres, Docker, Debezium and Pyspark. I specify some tables from which any new-entries are captured and published to Kafka topic(s) through a debezium-postgres-connector. The next step is to ingest these real-time entries in Pyspark, perform some transformation, and publish it to another topic for subsequent processing (I haven&amp;#39;t decided what I&amp;#39;m going to do yet \u2014 maybe some visualization/analytics?)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Everything is working fine and I&amp;#39;m getting the new entries in real time. But the data that is currently present in the database is overall clean and doesn&amp;#39;t require any major transformation. Maybe dropping a column or two, or filtering for rows that don&amp;#39;t meet a specific criteria but that&amp;#39;s it. Neither is the data really huge.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m looking for any datasets that can span multiple tables so I can do some meaningful transformation on them. I could upload the datasets to the db and proceed from there. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll also take any suggestions regarding the project itself. Maybe there&amp;#39;s a flaw in the architecture I&amp;#39;m missing. Who knows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bibu4z", "is_robot_indexable": true, "report_reasons": null, "author": "SAAD_3XK", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bibu4z/any_messy_datasets_for_pyspark_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bibu4z/any_messy_datasets_for_pyspark_practice/", "subreddit_subscribers": 170105, "created_utc": 1710822578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, \n\n&amp;#x200B;\n\nI am a senior data engineer in Germany and would like to try out contracting jobs (on the side of my main job) just to explore new opportunities. \n\n&amp;#x200B;\n\nI have been applying to contracting remove jobs in EU (including UK) and USA via websites like Reed, Hays, Harnham but with no luck. \n\n&amp;#x200B;\n\nDoes anyone have any experience in such setup? Would love to hear your thoughts", "author_fullname": "t2_xm0tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you find contracting jobs? [senior DE]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi9yt8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710816737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am a senior data engineer in Germany and would like to try out contracting jobs (on the side of my main job) just to explore new opportunities. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have been applying to contracting remove jobs in EU (including UK) and USA via websites like Reed, Hays, Harnham but with no luck. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience in such setup? Would love to hear your thoughts&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bi9yt8", "is_robot_indexable": true, "report_reasons": null, "author": "elephantail", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi9yt8/how_do_you_find_contracting_jobs_senior_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi9yt8/how_do_you_find_contracting_jobs_senior_de/", "subreddit_subscribers": 170105, "created_utc": 1710816737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have athena partitions for date like:\n\nyear=2012/month=01/day=01 and inside each folder around 30 parquet files\n\nThe schema of the parquet files does not have the columns year, month, and day, it has a single column called 'date' which is a timestamp, when I created the folders just did it by filtering on the spark df and then writing on the S3 folder. Does this affect performance in how Athena queries this data? Should I have within the .parquet files a schema with the year, month, and day? should I change my partitions to be 2012/01/01 2012/01/02 and create a column within the .parquet files called just \"actual\\_date\" and keep \"date\" timestamp? any thoughts? :) TIA!", "author_fullname": "t2_42yrzhea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena Partitions and .parquet schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi1qnn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710795935.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710795487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have athena partitions for date like:&lt;/p&gt;\n\n&lt;p&gt;year=2012/month=01/day=01 and inside each folder around 30 parquet files&lt;/p&gt;\n\n&lt;p&gt;The schema of the parquet files does not have the columns year, month, and day, it has a single column called &amp;#39;date&amp;#39; which is a timestamp, when I created the folders just did it by filtering on the spark df and then writing on the S3 folder. Does this affect performance in how Athena queries this data? Should I have within the .parquet files a schema with the year, month, and day? should I change my partitions to be 2012/01/01 2012/01/02 and create a column within the .parquet files called just &amp;quot;actual_date&amp;quot; and keep &amp;quot;date&amp;quot; timestamp? any thoughts? :) TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bi1qnn", "is_robot_indexable": true, "report_reasons": null, "author": "josejo9423", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1qnn/athena_partitions_and_parquet_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1qnn/athena_partitions_and_parquet_schema/", "subreddit_subscribers": 170105, "created_utc": 1710795487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to integrate OpenLineage to my airflow server, but I wanna know if I can use other metadata visualization tools instead of marquez", "author_fullname": "t2_dwxrwyrv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to use DataHub as UI for OpenLineage backend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bhsa88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710772659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to integrate OpenLineage to my airflow server, but I wanna know if I can use other metadata visualization tools instead of marquez&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bhsa88", "is_robot_indexable": true, "report_reasons": null, "author": "nhawlao", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bhsa88/is_it_possible_to_use_datahub_as_ui_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bhsa88/is_it_possible_to_use_datahub_as_ui_for/", "subreddit_subscribers": 170105, "created_utc": 1710772659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a DE for 2 years or so and would like a degree as it'll help me get a job in germany.\nThere doesn't tend to be a degree in data engineering itself. Only cloud computing or data science/data analytics. \nAnalytics seems to be the most desirable from German companies.\n\nI want to do an open university but unsure whether to look at analytics or data science for the degree. I do have an interest in both and machine learning is a cool concept but understand that data science/ML can be extremely complex mathematically. \n\nAny advice? \n", "author_fullname": "t2_7zcn2i3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for which Data Msc in UK? Planning to move to Germany and a qualification would help.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bigye8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710843810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a DE for 2 years or so and would like a degree as it&amp;#39;ll help me get a job in germany.\nThere doesn&amp;#39;t tend to be a degree in data engineering itself. Only cloud computing or data science/data analytics. \nAnalytics seems to be the most desirable from German companies.&lt;/p&gt;\n\n&lt;p&gt;I want to do an open university but unsure whether to look at analytics or data science for the degree. I do have an interest in both and machine learning is a cool concept but understand that data science/ML can be extremely complex mathematically. &lt;/p&gt;\n\n&lt;p&gt;Any advice? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bigye8", "is_robot_indexable": true, "report_reasons": null, "author": "Material_Direction_1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bigye8/recommendations_for_which_data_msc_in_uk_planning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bigye8/recommendations_for_which_data_msc_in_uk_planning/", "subreddit_subscribers": 170105, "created_utc": 1710843810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll be working on a migration project and I have a ton of etl pipelines with 2k+ lines of sql and I was wondering if anyone knows a tool that can gate the sql script and turn it into a sort of flow diagram to understand all the tables that are being created and their relation ", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Turning sql pipeline into flowchart", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1big8k5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710840826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll be working on a migration project and I have a ton of etl pipelines with 2k+ lines of sql and I was wondering if anyone knows a tool that can gate the sql script and turn it into a sort of flow diagram to understand all the tables that are being created and their relation &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1big8k5", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1big8k5/turning_sql_pipeline_into_flowchart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1big8k5/turning_sql_pipeline_into_flowchart/", "subreddit_subscribers": 170105, "created_utc": 1710840826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " As the sole data analyst within the finance team, I rely on Alteryx for conducting ETL operations, culminating in the loading of data into MS-SQL and the creation of CSV files as output. Across our operations, we manage 33 distinct ETL workflows, each meticulously tailored to accommodate various vendor datasets, each with its own set of independent tables. Despite this diversity, all data eventually coalesces into a standardized master table.\n\nOur infrastructure is strictly on-premise, devoid of any cloud integration, and notably lacks a Data Warehouse (DWH), relying solely on standalone tables.\n\nThe data flow follows this pattern:\n\n* txt files --&gt; Alteryx --&gt; Source tables (33)\n* Alteryx --&gt; Masterdata table (1)\n* Alteryx --&gt; Output tables (24) --&gt; CSVs\n\nWhile our current process meets requirements adequately, the burgeoning volume of data is escalating, although it hasn't yet reached the realm of big data.\n\nI am keen to implement mechanisms for data validation notifications, alerts, and sharing succinct product summaries with the team. Furthermore, I am eager to optimize and fortify our existing data pipeline. Any insights or recommendations for enhancements would be greatly valued. I'm also open to acquiring proficiency in new technologies if they are deemed beneficial for our operations.", "author_fullname": "t2_i3ww3xavg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Enhancing Data Operations: Streamlining Processes and Implementing Innovations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1big613", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710840518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the sole data analyst within the finance team, I rely on Alteryx for conducting ETL operations, culminating in the loading of data into MS-SQL and the creation of CSV files as output. Across our operations, we manage 33 distinct ETL workflows, each meticulously tailored to accommodate various vendor datasets, each with its own set of independent tables. Despite this diversity, all data eventually coalesces into a standardized master table.&lt;/p&gt;\n\n&lt;p&gt;Our infrastructure is strictly on-premise, devoid of any cloud integration, and notably lacks a Data Warehouse (DWH), relying solely on standalone tables.&lt;/p&gt;\n\n&lt;p&gt;The data flow follows this pattern:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;txt files --&amp;gt; Alteryx --&amp;gt; Source tables (33)&lt;/li&gt;\n&lt;li&gt;Alteryx --&amp;gt; Masterdata table (1)&lt;/li&gt;\n&lt;li&gt;Alteryx --&amp;gt; Output tables (24) --&amp;gt; CSVs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While our current process meets requirements adequately, the burgeoning volume of data is escalating, although it hasn&amp;#39;t yet reached the realm of big data.&lt;/p&gt;\n\n&lt;p&gt;I am keen to implement mechanisms for data validation notifications, alerts, and sharing succinct product summaries with the team. Furthermore, I am eager to optimize and fortify our existing data pipeline. Any insights or recommendations for enhancements would be greatly valued. I&amp;#39;m also open to acquiring proficiency in new technologies if they are deemed beneficial for our operations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1big613", "is_robot_indexable": true, "report_reasons": null, "author": "CapLevi_1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1big613/enhancing_data_operations_streamlining_processes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1big613/enhancing_data_operations_streamlining_processes/", "subreddit_subscribers": 170105, "created_utc": 1710840518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, founder of [Godel Grid Connect](https://www.godelgrid.com/) platform here. We run data pipelines and transfer data in user's own cloud. This is faster and more secure than traditional vendors. Currently we have integrated AWS on our platform and transfer data from postgres to s3. We're taking requests for new cloud integrations and connectors. What are your suggestions and opinions on this. What areas I can work on to stand out from similar services (AWS DMS in particular). What do think is a weak area in AWS DMS service.\n\nI am also looking for potential clients. If you think its useful to you message me on chat.", "author_fullname": "t2_i7k1oclt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Godel Grid Connect platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bidrvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710829930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, founder of &lt;a href=\"https://www.godelgrid.com/\"&gt;Godel Grid Connect&lt;/a&gt; platform here. We run data pipelines and transfer data in user&amp;#39;s own cloud. This is faster and more secure than traditional vendors. Currently we have integrated AWS on our platform and transfer data from postgres to s3. We&amp;#39;re taking requests for new cloud integrations and connectors. What are your suggestions and opinions on this. What areas I can work on to stand out from similar services (AWS DMS in particular). What do think is a weak area in AWS DMS service.&lt;/p&gt;\n\n&lt;p&gt;I am also looking for potential clients. If you think its useful to you message me on chat.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JHnLuJEdYyc3d7DetAqow9Hssb7thlOz0st7n8uuJ4Y.jpg?auto=webp&amp;s=aaa57e97851dd19c98654f21d7dba3b6fe84342a", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/JHnLuJEdYyc3d7DetAqow9Hssb7thlOz0st7n8uuJ4Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30045a1603180bfda706c750d1ea6dfc957c4078", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JHnLuJEdYyc3d7DetAqow9Hssb7thlOz0st7n8uuJ4Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b368d69d96e0d4cd60b25181c1e3d14cdab1831d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/JHnLuJEdYyc3d7DetAqow9Hssb7thlOz0st7n8uuJ4Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bd2b12beeb3f359111847c7ef46bc0193049464", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/JHnLuJEdYyc3d7DetAqow9Hssb7thlOz0st7n8uuJ4Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e476009fea6cb73acdf89690fb171a2054bcec3c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/JHnLuJEdYyc3d7DetAqow9Hssb7thlOz0st7n8uuJ4Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5468587b790f71f7938463e3efd5fb1d3f103b7f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/JHnLuJEdYyc3d7DetAqow9Hssb7thlOz0st7n8uuJ4Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0b76ce56b82a98c46886cd08b4a88cd2c10c2996", "width": 1080, "height": 567}], "variants": {}, "id": "40bKW5GR2-HKNRwqlSUJrbPsu8as1QjlbuSwOk10yrY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bidrvp", "is_robot_indexable": true, "report_reasons": null, "author": "Mediocre-Focus-3042", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bidrvp/godel_grid_connect_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bidrvp/godel_grid_connect_platform/", "subreddit_subscribers": 170105, "created_utc": 1710829930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Experts,\n\nPutting it here as its more related to data. We are using AWS cloud for multiple applications and using multiple databases like AWS aurora postgres, Mysql, redshift, Dynamo, Snowflake etc. and also in many cases data is stored S3 storage simply for long term retention and occasional querying purpose. \n\nWe want to implement data archival and retention strategies across all the databases or data stores. Want to know, if we already have any common framework available for data archival and retention across all the databases or data stores in these AWS databases or data stores.\n\n Or \n\nIs it possible to build some common framework like having a common config table(something as below) and then invoke either DELETE command (if the table is non partitioned) or else drop partition (if table is partitioned). The lambda with the business logic can be scheduled (through aws event bridge) will connect to the database and then read the config table and do the data archival and purge as per defined setup?\n\n  \"Table\\_name\"\n\n \"retention\\_days\\_number\"\n\n \"retention\\_column\\_name\"\n\n \"Partition\\_type\\_name\"\n\n \"partitioned\\_yes\\_no\"\n\n \"partition\\_column\\_name\" ", "author_fullname": "t2_cxqpzxgel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data purging strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bidc92", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710828155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Experts,&lt;/p&gt;\n\n&lt;p&gt;Putting it here as its more related to data. We are using AWS cloud for multiple applications and using multiple databases like AWS aurora postgres, Mysql, redshift, Dynamo, Snowflake etc. and also in many cases data is stored S3 storage simply for long term retention and occasional querying purpose. &lt;/p&gt;\n\n&lt;p&gt;We want to implement data archival and retention strategies across all the databases or data stores. Want to know, if we already have any common framework available for data archival and retention across all the databases or data stores in these AWS databases or data stores.&lt;/p&gt;\n\n&lt;p&gt;Or &lt;/p&gt;\n\n&lt;p&gt;Is it possible to build some common framework like having a common config table(something as below) and then invoke either DELETE command (if the table is non partitioned) or else drop partition (if table is partitioned). The lambda with the business logic can be scheduled (through aws event bridge) will connect to the database and then read the config table and do the data archival and purge as per defined setup?&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Table_name&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;retention_days_number&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;retention_column_name&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Partition_type_name&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;partitioned_yes_no&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;partition_column_name&amp;quot; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bidc92", "is_robot_indexable": true, "report_reasons": null, "author": "Upper-Lifeguard-8478", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bidc92/data_purging_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bidc92/data_purging_strategy/", "subreddit_subscribers": 170105, "created_utc": 1710828155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nHope you're all doing well!   I'm trying to learn how to scrape data from job sites like Glassdoor and Monster using Python. \n\nI tried to use BeautifulSoup and request but I'm unable to find the correct class and parse the data.\n\nAny tips or advice on where to start? Would appreciate any help!\n\nThankyou", "author_fullname": "t2_ec8yqztxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice from my veteran brothers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi30ja", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710798459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Hope you&amp;#39;re all doing well!   I&amp;#39;m trying to learn how to scrape data from job sites like Glassdoor and Monster using Python. &lt;/p&gt;\n\n&lt;p&gt;I tried to use BeautifulSoup and request but I&amp;#39;m unable to find the correct class and parse the data.&lt;/p&gt;\n\n&lt;p&gt;Any tips or advice on where to start? Would appreciate any help!&lt;/p&gt;\n\n&lt;p&gt;Thankyou&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bi30ja", "is_robot_indexable": true, "report_reasons": null, "author": "Southern-Pilot-804", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi30ja/seeking_advice_from_my_veteran_brothers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi30ja/seeking_advice_from_my_veteran_brothers/", "subreddit_subscribers": 170105, "created_utc": 1710798459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a client that insists they want to see normalized data in a denormalized csv. If I were to do this it would be hundreds of columns and the number of columns could change so they would need to be dynamically generated.  \n\n\nI don't suppose a tool exists to do this?", "author_fullname": "t2_mo4lb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Denormalize Tables into CSV?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bi1uwp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710795746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a client that insists they want to see normalized data in a denormalized csv. If I were to do this it would be hundreds of columns and the number of columns could change so they would need to be dynamically generated.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t suppose a tool exists to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bi1uwp", "is_robot_indexable": true, "report_reasons": null, "author": "Phantazein", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bi1uwp/denormalize_tables_into_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bi1uwp/denormalize_tables_into_csv/", "subreddit_subscribers": 170105, "created_utc": 1710795746.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}