{"kind": "Listing", "data": {"after": "t3_1bflawu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AI at this point is inevitable and it\u2019s become quite clear to me that the roles and responsibilities of a data engineer today will significantly change as AI tools become more common place. At this point it\u2019s  all speculative but my questions are\nA) what does the data engineer of tomorrow look like\nB) how can I adapt to a changing landscape and essentially future proof my career\n\nAny advice will be greatly appreciated!", "author_fullname": "t2_7iiccjhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I future proof my career as a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf4aft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710472745.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI at this point is inevitable and it\u2019s become quite clear to me that the roles and responsibilities of a data engineer today will significantly change as AI tools become more common place. At this point it\u2019s  all speculative but my questions are\nA) what does the data engineer of tomorrow look like\nB) how can I adapt to a changing landscape and essentially future proof my career&lt;/p&gt;\n\n&lt;p&gt;Any advice will be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bf4aft", "is_robot_indexable": true, "report_reasons": null, "author": "Ill-Advisor-8235", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf4aft/how_do_i_future_proof_my_career_as_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf4aft/how_do_i_future_proof_my_career_as_a_data_engineer/", "subreddit_subscribers": 169197, "created_utc": 1710472745.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I have been working on a personal project regarding data engineering. This project has to do with retrieving steam games prices for different games in different countries, and plotting the price difference in a world map.\n\nThis project is made up of 2 ETLs: One that retrieves price data and the other plots it using a world map.\n\nI would like some feedback on what I couldve done better. I tried using design pattern builder, using abstractions for different external resources and parametrization with Yaml.\n\nThis project uses 3 APIs and an S3 bucket for its internal processing.\n\n[here you have the project link](https://github.com/edseldim/steam_prices_data_engineering)\n\nThis is the final result\n\nhttps://preview.redd.it/139jgrlemeoc1.png?width=824&amp;format=png&amp;auto=webp&amp;s=204769ad503885eef0153d565f9243e5c5f56add", "author_fullname": "t2_kvl7dwa6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Steam Prices ETL (Personal Project)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"139jgrlemeoc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=39f8277d318238505cfb88cafb4380090c78efe2"}, {"y": 97, "x": 216, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0514ed0826dbe3dd540c824819241088fe1cb4c4"}, {"y": 144, "x": 320, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f84b4e44b907f687ada00592be8ea06d900ad8f"}, {"y": 289, "x": 640, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=42ecf2b7b21d10c5ef96b59d2218080c4023a1d9"}], "s": {"y": 373, "x": 824, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;format=png&amp;auto=webp&amp;s=204769ad503885eef0153d565f9243e5c5f56add"}, "id": "139jgrlemeoc1"}}, "name": "t3_1bf2ntf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/-JOn9CR7ml0xwwpJP5LtTt8IOTeg6FNt_1CnSruPpM4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1710467816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I have been working on a personal project regarding data engineering. This project has to do with retrieving steam games prices for different games in different countries, and plotting the price difference in a world map.&lt;/p&gt;\n\n&lt;p&gt;This project is made up of 2 ETLs: One that retrieves price data and the other plots it using a world map.&lt;/p&gt;\n\n&lt;p&gt;I would like some feedback on what I couldve done better. I tried using design pattern builder, using abstractions for different external resources and parametrization with Yaml.&lt;/p&gt;\n\n&lt;p&gt;This project uses 3 APIs and an S3 bucket for its internal processing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/edseldim/steam_prices_data_engineering\"&gt;here you have the project link&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the final result&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=204769ad503885eef0153d565f9243e5c5f56add\"&gt;https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=204769ad503885eef0153d565f9243e5c5f56add&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?auto=webp&amp;s=60d7f614a02fe266ad949dba81febe514ae1f75f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f474ec194a50612a3656981c46a5fd9df70dcee", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=214a43d5c075f07697fc5217ed7ec740397633f6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6f4e7b39d92036757989f827e590bd6c21fae2d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=59d55affc4a0264288094c3c6b4aa06c40899d13", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92a6161e090c6063dfa644e4dd0c3546ce422244", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9ae6c30b86adcad5ef672246cc4997b99783452", "width": 1080, "height": 540}], "variants": {}, "id": "M9rR2_qFAuxZC-bF3f95cChjYc5baZxoQBUTgp2zRSg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bf2ntf", "is_robot_indexable": true, "report_reasons": null, "author": "Confident_Watch8207", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf2ntf/steam_prices_etl_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf2ntf/steam_prices_etl_personal_project/", "subreddit_subscribers": 169197, "created_utc": 1710467816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently received an export from a client\u2019s previous vendor which contained 5,463 columns of Un-normalized data\u2026 I was also given a timeframe of less than a week to build tooling for and migrate this data. \n\nDoes anyone have any tools they\u2019ve used in the past to process this kind of thing? I mainly use Python, pandas, SQLite, Google sheets to extract and transform data (we don\u2019t have infrastructure built yet for streamlined migrations). So far, I\u2019ve removed empty columns and split it into two data frames in order to meet the limit of SQLite 2,000 column max. Still, the data is a mess\u2026 each record, it seems ,was flattened from several tables into a single row for each unique case. \n\nSometimes this isn\u2019t fun anymore lol", "author_fullname": "t2_5am908px", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flat file with over 5,000 columns\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfevg2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710511921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently received an export from a client\u2019s previous vendor which contained 5,463 columns of Un-normalized data\u2026 I was also given a timeframe of less than a week to build tooling for and migrate this data. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any tools they\u2019ve used in the past to process this kind of thing? I mainly use Python, pandas, SQLite, Google sheets to extract and transform data (we don\u2019t have infrastructure built yet for streamlined migrations). So far, I\u2019ve removed empty columns and split it into two data frames in order to meet the limit of SQLite 2,000 column max. Still, the data is a mess\u2026 each record, it seems ,was flattened from several tables into a single row for each unique case. &lt;/p&gt;\n\n&lt;p&gt;Sometimes this isn\u2019t fun anymore lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bfevg2", "is_robot_indexable": true, "report_reasons": null, "author": "iambatmanman", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfevg2/flat_file_with_over_5000_columns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfevg2/flat_file_with_over_5000_columns/", "subreddit_subscribers": 169197, "created_utc": 1710511921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there,\n\nI've been working as a Data Engineer for almost 5 years. I mean it was my title, but I think that most of this work was kind of just normal Python development work e.g. building api's with fast API, python library for data manipulation, dashboard app with flask/dash. Of course, 2nd part or even more was building pipelines with Airflow, Kafka, Spark, SQL, Python, and many others. But in the end, my title is Data Engineer, and now I am searching for a new role, and I am thinking about what's a better option. On one side there are fewer DE jobs, and I think these have a better salary, but on the other side: in one company DE = clicking AWS Glue, or writing SQL, whereas in other company is a heavy complex project where you join many other components, build your own, what involves a lot of coding.  \nI see, that for me the most fun is where I have a lot of coding and that's why I am thinking maybe it's better to move into SWE in Python, as I already have a solid foundation. I also love data engineering, but it really depends on the project, cause some are kind of drag and drop, and some really challenging.\n\nBut I wonder if it's no step back, and in the end, it will be more boring. Do you think that DE is a better, future-proof career path? Or Python Developer in CV will give more more possibilities :)\n\n", "author_fullname": "t2_2llofc3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which is more future-proof and more secure in terms of work. Python Dev vs Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf8d4j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710489569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710487868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working as a Data Engineer for almost 5 years. I mean it was my title, but I think that most of this work was kind of just normal Python development work e.g. building api&amp;#39;s with fast API, python library for data manipulation, dashboard app with flask/dash. Of course, 2nd part or even more was building pipelines with Airflow, Kafka, Spark, SQL, Python, and many others. But in the end, my title is Data Engineer, and now I am searching for a new role, and I am thinking about what&amp;#39;s a better option. On one side there are fewer DE jobs, and I think these have a better salary, but on the other side: in one company DE = clicking AWS Glue, or writing SQL, whereas in other company is a heavy complex project where you join many other components, build your own, what involves a lot of coding.&lt;br/&gt;\nI see, that for me the most fun is where I have a lot of coding and that&amp;#39;s why I am thinking maybe it&amp;#39;s better to move into SWE in Python, as I already have a solid foundation. I also love data engineering, but it really depends on the project, cause some are kind of drag and drop, and some really challenging.&lt;/p&gt;\n\n&lt;p&gt;But I wonder if it&amp;#39;s no step back, and in the end, it will be more boring. Do you think that DE is a better, future-proof career path? Or Python Developer in CV will give more more possibilities :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bf8d4j", "is_robot_indexable": true, "report_reasons": null, "author": "masek94", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf8d4j/which_is_more_futureproof_and_more_secure_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf8d4j/which_is_more_futureproof_and_more_secure_in/", "subreddit_subscribers": 169197, "created_utc": 1710487868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m curious to find out whether you use the BI tool or rely on SQL to get insights from data. In my experience, the BI tools can be complex and also quite limited in their utility for detailed analytics tasks. What do you currently use today for your analytics workflow, and what do you love and hate about it?", "author_fullname": "t2_ez4cm3m01", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BI tools!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf5ij4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710476663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m curious to find out whether you use the BI tool or rely on SQL to get insights from data. In my experience, the BI tools can be complex and also quite limited in their utility for detailed analytics tasks. What do you currently use today for your analytics workflow, and what do you love and hate about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf5ij4", "is_robot_indexable": true, "report_reasons": null, "author": "glinter777", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf5ij4/bi_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf5ij4/bi_tools/", "subreddit_subscribers": 169197, "created_utc": 1710476663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nI am currently going over streaming chapter in DE zoomcamp. I have an idea for a project, where I would like to do clickstream analitics. I would like to excersise a heavy load where my single Kafka topic would be bombarded with 10 million messages per minute.\n\nAs part of this process, I would to create a single consumer that would ingest that data and throw it to memory database, eg redis where i would run some queries on top of it.\n\nMy question is, is there an open source framework that has a python api, that would me allow to ingest this much data and throw it into memory db?\nThanks!", "author_fullname": "t2_fw1zu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting around 10M events per minute from Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bff92l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710512957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I am currently going over streaming chapter in DE zoomcamp. I have an idea for a project, where I would like to do clickstream analitics. I would like to excersise a heavy load where my single Kafka topic would be bombarded with 10 million messages per minute.&lt;/p&gt;\n\n&lt;p&gt;As part of this process, I would to create a single consumer that would ingest that data and throw it to memory database, eg redis where i would run some queries on top of it.&lt;/p&gt;\n\n&lt;p&gt;My question is, is there an open source framework that has a python api, that would me allow to ingest this much data and throw it into memory db?\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bff92l", "is_robot_indexable": true, "report_reasons": null, "author": "saif3r", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bff92l/ingesting_around_10m_events_per_minute_from_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bff92l/ingesting_around_10m_events_per_minute_from_kafka/", "subreddit_subscribers": 169197, "created_utc": 1710512957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I saw some reddit posts on this subreddit and read some articles but I have no clue what photon is. I currently use Apache Airflow, Spark3, and Scala. Python works with airflow to schedule DAG tasks which do very heavy dataframe computations and each of the tasks are ran on the Scala Jars.\n\nI'm pretty new to scala and spark so im basically a noob, can someone explain how Photon will help accelerate my pipeline and dag tasks? I understand that somehow things get re-written in C++. From my understanding once a Scala code gets compiled it gets turned into byte code instead of object code which means scala will run slower compared to C/C++. But I also read on Databricks' website that there would be 0 code changes required so how on earth does that work.\n\nI also read somewhere on this subreddit that it was mostly made for SQL and not for data frames. is this true? If so would this render it useless for my application?\n\nAlso are there other alternatives? I want to increase speed while reducing compute costs", "author_fullname": "t2_tz6ii805", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Explain like im 5: Databricks Photon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf8k8q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710488800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw some reddit posts on this subreddit and read some articles but I have no clue what photon is. I currently use Apache Airflow, Spark3, and Scala. Python works with airflow to schedule DAG tasks which do very heavy dataframe computations and each of the tasks are ran on the Scala Jars.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pretty new to scala and spark so im basically a noob, can someone explain how Photon will help accelerate my pipeline and dag tasks? I understand that somehow things get re-written in C++. From my understanding once a Scala code gets compiled it gets turned into byte code instead of object code which means scala will run slower compared to C/C++. But I also read on Databricks&amp;#39; website that there would be 0 code changes required so how on earth does that work.&lt;/p&gt;\n\n&lt;p&gt;I also read somewhere on this subreddit that it was mostly made for SQL and not for data frames. is this true? If so would this render it useless for my application?&lt;/p&gt;\n\n&lt;p&gt;Also are there other alternatives? I want to increase speed while reducing compute costs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf8k8q", "is_robot_indexable": true, "report_reasons": null, "author": "bleak-terminal", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/", "subreddit_subscribers": 169197, "created_utc": 1710488800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nPreface: I\u2019m autistic. I can perceive things a little differently, feel things a little differently, having conversations burns me out, \u2026 I don\u2019t mention this to people I work with though because I don\u2019t want to come off as requiring accommodation. I\u2019m such a high performer, people often look at me funny when I bring up my own issues. I think that\u2019s because I internalize them so well. I bring this up because sometimes my perception can be flawed and I can act in selfish ways. I want to make sure that\u2019s not what\u2019s happening here.\n\nI\u2019m about 6 months into a new role at a construction company. It\u2019s been going really well. I have been given an opportunity to manage my own live AWS account. I see my own projects through entirely, from requirement gathering to deployment and maintenance. I provision the servers, document everything, back up the code with git, \u2026 To add, I schedule meetings with most people on the workforce: from directors to super intendants, so that I can understand the full scope of what they do and build better tools for them. Hell, I recently spent 2.5 hours auditing a construction site to get more familiar with how I can streamline their process.\n\nRecently, my company moved into a new office. Once I arrived, I realized that I\u2019d be working from a cubicle and not an office. This is my first time out of my own office in 2 years spanning 2 jobs and to be honest I hate it. I get distracted so easily, my eyes are constantly bouncing around the room when people walk by and catch my attention, and I end up getting lost in my work. I have these moments now where I need to like re-organize myself because I get overwhelmed and can\u2019t remember my train of thought.\n\nThere\u2019s also the issue of my whiteboard. I bring my own whiteboard in on a stand to help me. It\u2019s like 2.5ft wide by 4ft tall on a stand. I\u2019ve also had this for 2 years spanning 2 jobs and it helps me immensely. I keep it by me for a large canvas for temporary notes and model rough drafts- so that I can quickly put my ideas down. However, I\u2019m not allowed to bring it in because the new building needs to stay tidy. Fine\u2026 but every single individual office in the building has a whiteboard mounted on the wall, and I was given a cubical\u2026\n\nThis transition out of an office has made me feel quite undervalued. I wasn\u2019t asked if I\u2019d be okay with this, somebody made that choice for me. However, I am conflicted because I do receive a LOT of positive verbal feedback- to include promises of bonuses once I hit my year in. So, by words it feels like I\u2019m valued but by actions I guess it does not.\n\nI reached out to raise the concern, asking to either have partial work from home or my own office. This request was denied, they asked that I used these \u201cdynamic offices\u201d that are positioned throughout the building and are available on a first-come first-serve basis. I tried this out, but these rooms are finished yet. No monitors, no whiteboard, I\u2019m just sitting in there with my laptop. I also need to get up and go there, which means I\u2019m still not able to just quickly model my thoughts at any time. I\u2019m considering raising the concern again, but now I worry I may become overbearing because I\u2019ve already spoke up on this once.\n\nRecently the owner of the company asked that I move over to their section of the company. The idea was that everyone over there would stand to benefit by my presence, having me as a resource and I leading projects with more upper-level stake holders. He also said the area would be quieter, though I have yet to realize that statement in reality.\n\nTo be honest, I\u2019m wondering if this is just a subtle reminder that I should not be getting attached to these jobs. I\u2019m getting paid $85K, which I know is low. I know that I\u2019ll be looking for a new job if I don\u2019t feel my first annual raise is adequate, which they typically are not in this field (hence the job hopping trend). Maybe I should see my situation as some kind of lesson?\n\n\nWhat do you guys think? Thanks! ", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Here\u2019s my whine post\u2026 Curious if you\u2019d think this is unfair.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfdkwb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710508151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Preface: I\u2019m autistic. I can perceive things a little differently, feel things a little differently, having conversations burns me out, \u2026 I don\u2019t mention this to people I work with though because I don\u2019t want to come off as requiring accommodation. I\u2019m such a high performer, people often look at me funny when I bring up my own issues. I think that\u2019s because I internalize them so well. I bring this up because sometimes my perception can be flawed and I can act in selfish ways. I want to make sure that\u2019s not what\u2019s happening here.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m about 6 months into a new role at a construction company. It\u2019s been going really well. I have been given an opportunity to manage my own live AWS account. I see my own projects through entirely, from requirement gathering to deployment and maintenance. I provision the servers, document everything, back up the code with git, \u2026 To add, I schedule meetings with most people on the workforce: from directors to super intendants, so that I can understand the full scope of what they do and build better tools for them. Hell, I recently spent 2.5 hours auditing a construction site to get more familiar with how I can streamline their process.&lt;/p&gt;\n\n&lt;p&gt;Recently, my company moved into a new office. Once I arrived, I realized that I\u2019d be working from a cubicle and not an office. This is my first time out of my own office in 2 years spanning 2 jobs and to be honest I hate it. I get distracted so easily, my eyes are constantly bouncing around the room when people walk by and catch my attention, and I end up getting lost in my work. I have these moments now where I need to like re-organize myself because I get overwhelmed and can\u2019t remember my train of thought.&lt;/p&gt;\n\n&lt;p&gt;There\u2019s also the issue of my whiteboard. I bring my own whiteboard in on a stand to help me. It\u2019s like 2.5ft wide by 4ft tall on a stand. I\u2019ve also had this for 2 years spanning 2 jobs and it helps me immensely. I keep it by me for a large canvas for temporary notes and model rough drafts- so that I can quickly put my ideas down. However, I\u2019m not allowed to bring it in because the new building needs to stay tidy. Fine\u2026 but every single individual office in the building has a whiteboard mounted on the wall, and I was given a cubical\u2026&lt;/p&gt;\n\n&lt;p&gt;This transition out of an office has made me feel quite undervalued. I wasn\u2019t asked if I\u2019d be okay with this, somebody made that choice for me. However, I am conflicted because I do receive a LOT of positive verbal feedback- to include promises of bonuses once I hit my year in. So, by words it feels like I\u2019m valued but by actions I guess it does not.&lt;/p&gt;\n\n&lt;p&gt;I reached out to raise the concern, asking to either have partial work from home or my own office. This request was denied, they asked that I used these \u201cdynamic offices\u201d that are positioned throughout the building and are available on a first-come first-serve basis. I tried this out, but these rooms are finished yet. No monitors, no whiteboard, I\u2019m just sitting in there with my laptop. I also need to get up and go there, which means I\u2019m still not able to just quickly model my thoughts at any time. I\u2019m considering raising the concern again, but now I worry I may become overbearing because I\u2019ve already spoke up on this once.&lt;/p&gt;\n\n&lt;p&gt;Recently the owner of the company asked that I move over to their section of the company. The idea was that everyone over there would stand to benefit by my presence, having me as a resource and I leading projects with more upper-level stake holders. He also said the area would be quieter, though I have yet to realize that statement in reality.&lt;/p&gt;\n\n&lt;p&gt;To be honest, I\u2019m wondering if this is just a subtle reminder that I should not be getting attached to these jobs. I\u2019m getting paid $85K, which I know is low. I know that I\u2019ll be looking for a new job if I don\u2019t feel my first annual raise is adequate, which they typically are not in this field (hence the job hopping trend). Maybe I should see my situation as some kind of lesson?&lt;/p&gt;\n\n&lt;p&gt;What do you guys think? Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bfdkwb", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfdkwb/heres_my_whine_post_curious_if_youd_think_this_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfdkwb/heres_my_whine_post_curious_if_youd_think_this_is/", "subreddit_subscribers": 169197, "created_utc": 1710508151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_tb9gsv8ek", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Figma's Databases Team Lived to Tell the Scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf5csb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8xbuhoCS2I4KUMWrLfS95pUZth0IhtqhuZrGN49f2HA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710476129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "figma.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?auto=webp&amp;s=b02d84f548d7016532bde698df50f6ad2964cdde", "width": 1200, "height": 899}, "resolutions": [{"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb49ef81566c7f3362de32f19bcbb0503eab41d3", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a47aeec8959e0854c7645af187400ef2b2b8d778", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff4a41b271855634f3f31a1a95d0a1923e5663d3", "width": 320, "height": 239}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf08e010042ff3adc093c447f05b01d35617f4f8", "width": 640, "height": 479}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=64c428fb6bb53fc271e92487969781f20a0cf5c2", "width": 960, "height": 719}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b089381d255a531b9dfc063c300fc4d51d3ba692", "width": 1080, "height": 809}], "variants": {}, "id": "Sh9ywQbgHtUAXOpENNdmfZpxlwzzyYcC3r9bH8mkHRA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bf5csb", "is_robot_indexable": true, "report_reasons": null, "author": "Rollstack", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf5csb/how_figmas_databases_team_lived_to_tell_the_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/", "subreddit_subscribers": 169197, "created_utc": 1710476129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a job where all I do is enter the $ amount, vendor name, and invoice # from a pdf invoice that is received in an outlook inbox into a Access database, There has to be a way to make it less manual.\n\nFor context I work for a government organization that receives 100s of invoices from many hundreds of different vendors every day.\n\nWhere should I start?", "author_fullname": "t2_g45v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I manually enter invoices in access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf57d2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710475636.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a job where all I do is enter the $ amount, vendor name, and invoice # from a pdf invoice that is received in an outlook inbox into a Access database, There has to be a way to make it less manual.&lt;/p&gt;\n\n&lt;p&gt;For context I work for a government organization that receives 100s of invoices from many hundreds of different vendors every day.&lt;/p&gt;\n\n&lt;p&gt;Where should I start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bf57d2", "is_robot_indexable": true, "report_reasons": null, "author": "KingCharlemange", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf57d2/i_manually_enter_invoices_in_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf57d2/i_manually_enter_invoices_in_access/", "subreddit_subscribers": 169197, "created_utc": 1710475636.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So for some context, I\u2019m about to graduate with my degree in industrial engineering and I\u2019ve known that I wanted to pursue data engineering for about a year and a half now. Since then I\u2019ve been trying to build myself towards that. I have two internships:\n\nShell (not data related was more research engineering)\n\nRMS (a smaller less known company that I worked as a database administrator for)\n\nI\u2019m about to take my associate cloud engineer exam for GCP and after I get the certificate for that I plan on working towards the professional data engineering certificate. \n\nSo my question here is as a fresh graduate, what jobs should I be applying for if my end goal is data engineer and if I can\u2019t find an entry level data engineering position?", "author_fullname": "t2_2dbrp66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some of the best entry level jobs to apply for if I want to be a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfh00d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710519364.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710517476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So for some context, I\u2019m about to graduate with my degree in industrial engineering and I\u2019ve known that I wanted to pursue data engineering for about a year and a half now. Since then I\u2019ve been trying to build myself towards that. I have two internships:&lt;/p&gt;\n\n&lt;p&gt;Shell (not data related was more research engineering)&lt;/p&gt;\n\n&lt;p&gt;RMS (a smaller less known company that I worked as a database administrator for)&lt;/p&gt;\n\n&lt;p&gt;I\u2019m about to take my associate cloud engineer exam for GCP and after I get the certificate for that I plan on working towards the professional data engineering certificate. &lt;/p&gt;\n\n&lt;p&gt;So my question here is as a fresh graduate, what jobs should I be applying for if my end goal is data engineer and if I can\u2019t find an entry level data engineering position?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bfh00d", "is_robot_indexable": true, "report_reasons": null, "author": "iBortex", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfh00d/what_are_some_of_the_best_entry_level_jobs_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfh00d/what_are_some_of_the_best_entry_level_jobs_to/", "subreddit_subscribers": 169197, "created_utc": 1710517476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team's tech stack is completely Azure based and we use ADF for orchestration with pipelines in Databricks and the data sitting in a delta lake in a Storage Account. \n\nThis works well for the most part but for the majority of our pipelines, running Spark is overkill and it could be done using Python or Polars. Many other teams in the firm access the data using their own Databricks Workspaces so I want to keep the data in delta lake but move over some of the DE pipelines into something more suitable and cheaper. I also think certain things such as web scraping using Selenium is overly complex in Databricks and would prefer to develop something like that locally and probably execute elsewhere too.\n\nHow would you recommend going about doing something like this? I'm not too sure of the best architecture for this. My initial thoughts are that we can use VMs to execute our code that we don't want to use Databricks for but I have very little experience with this so some learning recommendations would be helpful too.", "author_fullname": "t2_n937n0g6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I move pipelines away from Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfbbqt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710500593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team&amp;#39;s tech stack is completely Azure based and we use ADF for orchestration with pipelines in Databricks and the data sitting in a delta lake in a Storage Account. &lt;/p&gt;\n\n&lt;p&gt;This works well for the most part but for the majority of our pipelines, running Spark is overkill and it could be done using Python or Polars. Many other teams in the firm access the data using their own Databricks Workspaces so I want to keep the data in delta lake but move over some of the DE pipelines into something more suitable and cheaper. I also think certain things such as web scraping using Selenium is overly complex in Databricks and would prefer to develop something like that locally and probably execute elsewhere too.&lt;/p&gt;\n\n&lt;p&gt;How would you recommend going about doing something like this? I&amp;#39;m not too sure of the best architecture for this. My initial thoughts are that we can use VMs to execute our code that we don&amp;#39;t want to use Databricks for but I have very little experience with this so some learning recommendations would be helpful too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bfbbqt", "is_robot_indexable": true, "report_reasons": null, "author": "piri9825", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfbbqt/how_do_i_move_pipelines_away_from_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfbbqt/how_do_i_move_pipelines_away_from_databricks/", "subreddit_subscribers": 169197, "created_utc": 1710500593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello r/dataengineering,\n\nI've been deeply involved with Apache AGE, a graph database extension that's proving to be a game-changer for data engineering. As a core contributor, I've seen its potential to revolutionize the way we approach data relationships and analysis, especially in environments that traditionally rely on SQL databases.\n\nApache AGE enables the integration of graph database functionalities directly into PostgreSQL, allowing for complex data relationship queries without the need for a separate graph database system. This opens up new avenues for data modeling, querying, and analysis within the familiar PostgreSQL ecosystem.\n\nWhat's particularly exciting for data engineers is AGE's ability to handle complex, interconnected data scenarios more naturally and efficiently than traditional relational databases. This capability is invaluable for applications in social networking, fraud detection, recommendation systems, and more, where understanding the relationships between data points is key.\n\nI'm curious to hear from the community:\n\n* Have you integrated graph database features into your data engineering projects?\n* What challenges and opportunities do you see in adopting Apache AGE for your data workflows?\n\nLet's explore how Apache AGE can fit into our data engineering toolkit, share experiences, and discuss best practices for leveraging graph database capabilities in our projects.\n\nFor a deep dive into the technical workings, documentation, and to join our growing community, visit our [Apache AGE GitHub](https://github.com/apache/age) and [official website](https://age.apache.org/).", "author_fullname": "t2_ru7nh9tc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrating Apache AGE for Enhanced Data Engineering Workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bezk3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710459215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been deeply involved with Apache AGE, a graph database extension that&amp;#39;s proving to be a game-changer for data engineering. As a core contributor, I&amp;#39;ve seen its potential to revolutionize the way we approach data relationships and analysis, especially in environments that traditionally rely on SQL databases.&lt;/p&gt;\n\n&lt;p&gt;Apache AGE enables the integration of graph database functionalities directly into PostgreSQL, allowing for complex data relationship queries without the need for a separate graph database system. This opens up new avenues for data modeling, querying, and analysis within the familiar PostgreSQL ecosystem.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s particularly exciting for data engineers is AGE&amp;#39;s ability to handle complex, interconnected data scenarios more naturally and efficiently than traditional relational databases. This capability is invaluable for applications in social networking, fraud detection, recommendation systems, and more, where understanding the relationships between data points is key.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear from the community:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have you integrated graph database features into your data engineering projects?&lt;/li&gt;\n&lt;li&gt;What challenges and opportunities do you see in adopting Apache AGE for your data workflows?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Let&amp;#39;s explore how Apache AGE can fit into our data engineering toolkit, share experiences, and discuss best practices for leveraging graph database capabilities in our projects.&lt;/p&gt;\n\n&lt;p&gt;For a deep dive into the technical workings, documentation, and to join our growing community, visit our &lt;a href=\"https://github.com/apache/age\"&gt;Apache AGE GitHub&lt;/a&gt; and &lt;a href=\"https://age.apache.org/\"&gt;official website&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?auto=webp&amp;s=15f4c564dfb24842422fd91a488b20fa861c6ced", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82c39c26b2a4d0e656e8ed1368eef5e209fed31a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fb5dd8df8123d4cb3385c164770dc2f23d769", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0553dd3915d07598c17fea34ac7391746f060d6d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a85dd628f6980076c2a417ab108fd22292ef2789", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=426a3a01c2b2ad2624ca11ec382bfbc174a72df1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a50d70173c809efbe08ac98a2a0c511a63b56ae", "width": 1080, "height": 540}], "variants": {}, "id": "SGYMkpNq_z1nMJ_lmsYzVDtGYJK2yoPXMSe-sgg4NEQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bezk3y", "is_robot_indexable": true, "report_reasons": null, "author": "Eya_AGE", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bezk3y/integrating_apache_age_for_enhanced_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bezk3y/integrating_apache_age_for_enhanced_data/", "subreddit_subscribers": 169197, "created_utc": 1710459215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any old timers here remember this *ETL* tool called Ab Initio ? \n\nhttps://www.abinitio.com/en/\n\nBack in 00s, it was a holy grail of ETL tool for F50 banks, retail, insurance etc. Heck even Netflix used it for etl !\n\nWhat happened?\n\nToday in Linkedin there is no developers jobs for this tool.\n\nA few of the jobs are mostly outsourced maintenance.\n\nI know old tools/Mpps like Informatica, Teradata adapted to change and offer cloud/spark/hdfs adapters etc.\n\nAnybody has any idea what happened with Ab Initio and how is any DE with this skill doing ?", "author_fullname": "t2_5ifseipu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A lament for Ab Initio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfk3bf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710525407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any old timers here remember this &lt;em&gt;ETL&lt;/em&gt; tool called Ab Initio ? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.abinitio.com/en/\"&gt;https://www.abinitio.com/en/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Back in 00s, it was a holy grail of ETL tool for F50 banks, retail, insurance etc. Heck even Netflix used it for etl !&lt;/p&gt;\n\n&lt;p&gt;What happened?&lt;/p&gt;\n\n&lt;p&gt;Today in Linkedin there is no developers jobs for this tool.&lt;/p&gt;\n\n&lt;p&gt;A few of the jobs are mostly outsourced maintenance.&lt;/p&gt;\n\n&lt;p&gt;I know old tools/Mpps like Informatica, Teradata adapted to change and offer cloud/spark/hdfs adapters etc.&lt;/p&gt;\n\n&lt;p&gt;Anybody has any idea what happened with Ab Initio and how is any DE with this skill doing ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pDI4LVMNOdGs0gmmTtLv8A0QsObIBS-drCTxMCm0bzQ.jpg?auto=webp&amp;s=02708e2cfaede6d2327df1e5c33219acd7f54270", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/pDI4LVMNOdGs0gmmTtLv8A0QsObIBS-drCTxMCm0bzQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9627f8dc32e46147963d66ac2754a6add9467b56", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/pDI4LVMNOdGs0gmmTtLv8A0QsObIBS-drCTxMCm0bzQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c86035c35c10a431bcdf6c3b29aae238955ce422", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/pDI4LVMNOdGs0gmmTtLv8A0QsObIBS-drCTxMCm0bzQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e1961bc1aae2eb95aa64e2fb996bd7db7942330", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/pDI4LVMNOdGs0gmmTtLv8A0QsObIBS-drCTxMCm0bzQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dbd85495fc1e6206cb601a413631b19d27df5864", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/pDI4LVMNOdGs0gmmTtLv8A0QsObIBS-drCTxMCm0bzQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f5a99669e366db277031916d639b75107a3d7133", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/pDI4LVMNOdGs0gmmTtLv8A0QsObIBS-drCTxMCm0bzQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=254e6be292a4eb897ce4c62833657ddfedf28422", "width": 1080, "height": 567}], "variants": {}, "id": "pwS9vZT6VYVQjA6ilz_wG4J9ycuTInRmfDBQCdP6b0I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bfk3bf", "is_robot_indexable": true, "report_reasons": null, "author": "Smart-Weird", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfk3bf/a_lament_for_ab_initio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfk3bf/a_lament_for_ab_initio/", "subreddit_subscribers": 169197, "created_utc": 1710525407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, data lakes unlikes data warehouses let you store unstructured data (text, binary) - does it mean that they are mostly used for this type of data? Or are data lakes mostly used for structured and semi-structured data? But then why choose data lake if we can use data warehouse. What is your experence?", "author_fullname": "t2_34tfcgtu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data lakes mostly used for sturctured and semi-structured data or unstructured data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfaon5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710498132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, data lakes unlikes data warehouses let you store unstructured data (text, binary) - does it mean that they are mostly used for this type of data? Or are data lakes mostly used for structured and semi-structured data? But then why choose data lake if we can use data warehouse. What is your experence?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bfaon5", "is_robot_indexable": true, "report_reasons": null, "author": "rental_car_abuse", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfaon5/are_data_lakes_mostly_used_for_sturctured_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfaon5/are_data_lakes_mostly_used_for_sturctured_and/", "subreddit_subscribers": 169197, "created_utc": 1710498132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm primarily a frontend engineer. When I need to work on backend/infra, I often run into the issue of not knowing how to aggregate data in a scaleable way. E.g. pre-computing Reddit's upvote counts so you don't have to aggregate at read time. Aggregating for the top posts feed is easy because it's ok if it's stale by several minutes. However, if you upvote a post and refresh a second later, ideally the post count should include your upvote.\n\nIs there a standard way to do this? I don't know why it's so hard to find discussions around this problem. Afaik some ways of doing this are:\n\n1) When inserting the \"upvote\" record, also increment an \"upvoteCounts\" record\n\nIf you do this in a transaction, it would make the insertions slower. If you don't use a transaction, the upvote counts would go out of sync. I think some companies don't use transactions, but have a job to continuously recompute the counts. Another problem is if you need to mutate multiple aggregations per insertion, it'll be confusing to maintain.\n\n2) Use a trigger\n\nIt's basically the same as 1) with transactions, but handled by the DB instead of the application.\n\n3) Aggregate the event stream\n\nStream changes, then use something like Flink/ksqlDB to aggregate the stream, then stream the aggregation to somewhere for the application to read from. This has much higher latency than 1) with transactions. The benefit is that your aggregations are declarative, so they're easier to reason about.\n\n4) Have a batch job for old aggregations, then handle real-time aggregations in memory\n\nAssuming batch aggregation will take a long time, you can store real-time aggregations in memory. E.g. when someone upvotes, increment a count in memory. When fetching the count, sum the count from memory with the count from the batch job. However, it seems tricky to not double count or miss upvotes.\n\n5) Handling the aggregation at read-time with aggressive caching\n\nThis works when fetching the upvote count for a small set of posts. However, you wouldn't be able to sort posts by upvote count. Also, the max staleness is the cache timeout, which could be a long time.\n\n6) Real-time or incremental materialized views\n\nIdeally, we'd have materialized views that could quickly automatically respond to changes in the source tables. However, afaik the existing real-time materialized view systems are either in memory (e.g. Materialize) or places severe limitations on the source table (e.g. AnalyticDB). Materialize uses too much memory to be scaleable and I've never tried AnalyticDB, but it seems like it's too limiting on the source tables.\n\nAre there any resources to learn more about which approaches large companies use? Have you tried some of these and know first-hand about the pros and cons?", "author_fullname": "t2_4pdki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Different methods for real-time aggregation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf2jbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710467450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m primarily a frontend engineer. When I need to work on backend/infra, I often run into the issue of not knowing how to aggregate data in a scaleable way. E.g. pre-computing Reddit&amp;#39;s upvote counts so you don&amp;#39;t have to aggregate at read time. Aggregating for the top posts feed is easy because it&amp;#39;s ok if it&amp;#39;s stale by several minutes. However, if you upvote a post and refresh a second later, ideally the post count should include your upvote.&lt;/p&gt;\n\n&lt;p&gt;Is there a standard way to do this? I don&amp;#39;t know why it&amp;#39;s so hard to find discussions around this problem. Afaik some ways of doing this are:&lt;/p&gt;\n\n&lt;p&gt;1) When inserting the &amp;quot;upvote&amp;quot; record, also increment an &amp;quot;upvoteCounts&amp;quot; record&lt;/p&gt;\n\n&lt;p&gt;If you do this in a transaction, it would make the insertions slower. If you don&amp;#39;t use a transaction, the upvote counts would go out of sync. I think some companies don&amp;#39;t use transactions, but have a job to continuously recompute the counts. Another problem is if you need to mutate multiple aggregations per insertion, it&amp;#39;ll be confusing to maintain.&lt;/p&gt;\n\n&lt;p&gt;2) Use a trigger&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s basically the same as 1) with transactions, but handled by the DB instead of the application.&lt;/p&gt;\n\n&lt;p&gt;3) Aggregate the event stream&lt;/p&gt;\n\n&lt;p&gt;Stream changes, then use something like Flink/ksqlDB to aggregate the stream, then stream the aggregation to somewhere for the application to read from. This has much higher latency than 1) with transactions. The benefit is that your aggregations are declarative, so they&amp;#39;re easier to reason about.&lt;/p&gt;\n\n&lt;p&gt;4) Have a batch job for old aggregations, then handle real-time aggregations in memory&lt;/p&gt;\n\n&lt;p&gt;Assuming batch aggregation will take a long time, you can store real-time aggregations in memory. E.g. when someone upvotes, increment a count in memory. When fetching the count, sum the count from memory with the count from the batch job. However, it seems tricky to not double count or miss upvotes.&lt;/p&gt;\n\n&lt;p&gt;5) Handling the aggregation at read-time with aggressive caching&lt;/p&gt;\n\n&lt;p&gt;This works when fetching the upvote count for a small set of posts. However, you wouldn&amp;#39;t be able to sort posts by upvote count. Also, the max staleness is the cache timeout, which could be a long time.&lt;/p&gt;\n\n&lt;p&gt;6) Real-time or incremental materialized views&lt;/p&gt;\n\n&lt;p&gt;Ideally, we&amp;#39;d have materialized views that could quickly automatically respond to changes in the source tables. However, afaik the existing real-time materialized view systems are either in memory (e.g. Materialize) or places severe limitations on the source table (e.g. AnalyticDB). Materialize uses too much memory to be scaleable and I&amp;#39;ve never tried AnalyticDB, but it seems like it&amp;#39;s too limiting on the source tables.&lt;/p&gt;\n\n&lt;p&gt;Are there any resources to learn more about which approaches large companies use? Have you tried some of these and know first-hand about the pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf2jbk", "is_robot_indexable": true, "report_reasons": null, "author": "linksku", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf2jbk/different_methods_for_realtime_aggregation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf2jbk/different_methods_for_realtime_aggregation/", "subreddit_subscribers": 169197, "created_utc": 1710467450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've a pipeline run like this:\n\nextract data from data source -&gt; store into raw table in PostgreSQL -&gt; run another SQL transformation script to load into staging table.\n\n&amp;#x200B;\n\nNow for the third step, I would like to create an automation method to execute the SQL transformation script instead running manually. I expect it to be executed whenever an insertion occurs in the raw table.\n\nI know SQL trigger functions can react to insertion, and I'm not sure if DBT can react to the insertion yet.\n\nMy expectation is: can evoke to run transformation SQL script whenever insertion happens in raw table, and I'm able to record the transformation log to trace the action.\n\n&amp;#x200B;\n\nAny suggestion is appreciated! THanks", "author_fullname": "t2_11cquw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is SQL trigger functions or DBT execution more preferred for data transformation in PostgreSQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfe06k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710509411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve a pipeline run like this:&lt;/p&gt;\n\n&lt;p&gt;extract data from data source -&amp;gt; store into raw table in PostgreSQL -&amp;gt; run another SQL transformation script to load into staging table.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now for the third step, I would like to create an automation method to execute the SQL transformation script instead running manually. I expect it to be executed whenever an insertion occurs in the raw table.&lt;/p&gt;\n\n&lt;p&gt;I know SQL trigger functions can react to insertion, and I&amp;#39;m not sure if DBT can react to the insertion yet.&lt;/p&gt;\n\n&lt;p&gt;My expectation is: can evoke to run transformation SQL script whenever insertion happens in raw table, and I&amp;#39;m able to record the transformation log to trace the action.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestion is appreciated! THanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bfe06k", "is_robot_indexable": true, "report_reasons": null, "author": "Laurence-Lin", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfe06k/is_sql_trigger_functions_or_dbt_execution_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfe06k/is_sql_trigger_functions_or_dbt_execution_more/", "subreddit_subscribers": 169197, "created_utc": 1710509411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been using talend open studio for our ETL needs but with that going out of support we are looking for a new tool. \n\nWe have about 8 ETLs that extract data from various sources: powershell scripts, python scripts, MySQL database, sql server database and do some pretty basic transformations into an on prem oracle database. After that we build dashboards in power bi. \n\nI think cloud tooling could be good as there is less maintenance (server patching, app upgrades) but it must be able to write to on prem data warehouse. I\u2019m not against on prem tool if it makes more sense cost wise. \n\nI think cloud tools are usually priced based on computation. \n\nOne big benefit for us would be amount of resources on the tool online. If you run into issues are there lots of forums that have similar issues with resolutions? \n\nAny good paid tools or open source tools you recommend or don\u2019t recommend? ", "author_fullname": "t2_p8gsv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL tool recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf13x5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710463407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been using talend open studio for our ETL needs but with that going out of support we are looking for a new tool. &lt;/p&gt;\n\n&lt;p&gt;We have about 8 ETLs that extract data from various sources: powershell scripts, python scripts, MySQL database, sql server database and do some pretty basic transformations into an on prem oracle database. After that we build dashboards in power bi. &lt;/p&gt;\n\n&lt;p&gt;I think cloud tooling could be good as there is less maintenance (server patching, app upgrades) but it must be able to write to on prem data warehouse. I\u2019m not against on prem tool if it makes more sense cost wise. &lt;/p&gt;\n\n&lt;p&gt;I think cloud tools are usually priced based on computation. &lt;/p&gt;\n\n&lt;p&gt;One big benefit for us would be amount of resources on the tool online. If you run into issues are there lots of forums that have similar issues with resolutions? &lt;/p&gt;\n\n&lt;p&gt;Any good paid tools or open source tools you recommend or don\u2019t recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf13x5", "is_robot_indexable": true, "report_reasons": null, "author": "CyclingMonkey", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf13x5/etl_tool_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf13x5/etl_tool_recommendations/", "subreddit_subscribers": 169197, "created_utc": 1710463407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive Guide to Optimize Spark Data Workloads | Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1beza3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wSYuyjW9sHHw5_cAGIUIr_nm5ZCApMM3rIrUZvhd8S8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710458480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/discover/pages/optimize-data-workloads-guide#intro", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?auto=webp&amp;s=1888946a217a566b570f4afbb9c6cda7dae09653", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91375d0ca6cf9fd6186eb43a977e12bb7ac60794", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23b01c6ba19a97c4f454897dc3111177fc2598b1", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fcc73d3f10ecd1b06c22917d735a7eed72814572", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e0dac9968155d941a82fb9777c5cb692978475c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc43e0b2a699224e3c7e07d302187e9dbaed3f56", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0eb50fd84794eb10a4bcbffe0ec00c5545ad25f", "width": 1080, "height": 567}], "variants": {}, "id": "RS3lr2nqDF5sgLvhBJ8QfWyKVgI629watKirGMnYEBk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1beza3c", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beza3c/comprehensive_guide_to_optimize_spark_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/discover/pages/optimize-data-workloads-guide#intro", "subreddit_subscribers": 169197, "created_utc": 1710458480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nI recently hit this scenario -\n\nPipelines expect a certain schema to exist, this schema changes because life (out of DEs control (prod db?)) =&gt; pipeline breaks =&gt; downtime \n\nDoes this happen to you?\n\nWhat can I do about it?", "author_fullname": "t2_io93l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preventing pipelines from breaking because of upstream changes ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfdjvj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710508064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I recently hit this scenario -&lt;/p&gt;\n\n&lt;p&gt;Pipelines expect a certain schema to exist, this schema changes because life (out of DEs control (prod db?)) =&amp;gt; pipeline breaks =&amp;gt; downtime &lt;/p&gt;\n\n&lt;p&gt;Does this happen to you?&lt;/p&gt;\n\n&lt;p&gt;What can I do about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bfdjvj", "is_robot_indexable": true, "report_reasons": null, "author": "rotemtam", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfdjvj/preventing_pipelines_from_breaking_because_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfdjvj/preventing_pipelines_from_breaking_because_of/", "subreddit_subscribers": 169197, "created_utc": 1710508064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa5dw92do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake FinOps Center - Control and monitor costs with this free Streamlit app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1bfevv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cztW99v4JlBmYy-f1aqD8w5xM3DnxuEgoHCTvi0Ms2E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710511953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "app.snowflake.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://app.snowflake.com/marketplace/listing/GZT8Z2123RJ/baselit-finops-center", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rOWeAyEuSxiFWnBPRT99X2dPvg2PDvOn9QLrGwPi2jE.jpg?auto=webp&amp;s=b3c8ef12a1fc8834c8f8e5f91b8776e02bd65c7d", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/rOWeAyEuSxiFWnBPRT99X2dPvg2PDvOn9QLrGwPi2jE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=067b0f0d8b39972752e8df648542a0eb0f44c78e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/rOWeAyEuSxiFWnBPRT99X2dPvg2PDvOn9QLrGwPi2jE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b35c87c001a900e0d86832e2ae6adf3ae714f7d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/rOWeAyEuSxiFWnBPRT99X2dPvg2PDvOn9QLrGwPi2jE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84bc30978b9730b3b342290e0244412fcf9765f1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/rOWeAyEuSxiFWnBPRT99X2dPvg2PDvOn9QLrGwPi2jE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b28126ea2a6858923a2d8a6c0bb0b5a1e2d033bc", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/rOWeAyEuSxiFWnBPRT99X2dPvg2PDvOn9QLrGwPi2jE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c29935bdd59366f9dddeca916478a805c364d832", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/rOWeAyEuSxiFWnBPRT99X2dPvg2PDvOn9QLrGwPi2jE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce18c83f97bc129111ff9672366d1f1c14e13524", "width": 1080, "height": 567}], "variants": {}, "id": "6pwGgdLLEhyeygBB9mofhn0ySjQDLUFfIQfzKSexUVU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bfevv2", "is_robot_indexable": true, "report_reasons": null, "author": "sahil_singla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfevv2/snowflake_finops_center_control_and_monitor_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://app.snowflake.com/marketplace/listing/GZT8Z2123RJ/baselit-finops-center", "subreddit_subscribers": 169197, "created_utc": 1710511953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. We have a spark cluster on emr to preprocess the data and move from staging to production layer( as part of lakehouse architecture on s3). The data in staging is on s3 partitioned by downloaded date and hour (date at which the data comes to our system) in increments every hour. We need to preprocess the data and write it to production partitioned by report date (date at which the event occurred). \n\nThe challenge is the source system may update the previously sent data. I.e, for the event recognised by event id which happened last week and was ingested last week, may come again with updated values. We need to update it in out production layer. \n\nWe are currently trying to do it in spark. Every hour we get the staging data (one hour incremental data), get all the data from production layer by distinct report dates and upsert in spark and re write back to prod layer\n\n&amp;#x200B;\n\nData volume.\n\naround 100 million records per day in raw layer (10 gb parquet files)\n\naround 30 million records per day in prod layer (2-3 gb parquet files) (after preprocessing)\n\nEveryday we get around 60 dates older dates data (or 40 dates older dates data per day) which we have to upsert\n\nSo we have to read in spark for every hour data in staging layer (30-40 report dates data from production to upsert which amounts to 180-200GB compressed parquet files, on de-serialization will expand to 1000GB-2000GB)\n\n&amp;#x200B;\n\nThe spark job takes lot of time to complete (1:30 hours to 2 hours on a 7 node cluster with 32 gb machines and 8 core per node). Most of the time is on reads and writes. We have done all the possible optimisations from the spark code and config perspective\n\nFor us it's too long and costly and also not scalable\n\nIs there a better solution for this? How does industry approach this problem? Should we do upsert over any datawarehouse instead of spark?", "author_fullname": "t2_suqqio5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "scalable, performant and cost optimised approach needed for data preprocessing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf7wdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710485861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. We have a spark cluster on emr to preprocess the data and move from staging to production layer( as part of lakehouse architecture on s3). The data in staging is on s3 partitioned by downloaded date and hour (date at which the data comes to our system) in increments every hour. We need to preprocess the data and write it to production partitioned by report date (date at which the event occurred). &lt;/p&gt;\n\n&lt;p&gt;The challenge is the source system may update the previously sent data. I.e, for the event recognised by event id which happened last week and was ingested last week, may come again with updated values. We need to update it in out production layer. &lt;/p&gt;\n\n&lt;p&gt;We are currently trying to do it in spark. Every hour we get the staging data (one hour incremental data), get all the data from production layer by distinct report dates and upsert in spark and re write back to prod layer&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Data volume.&lt;/p&gt;\n\n&lt;p&gt;around 100 million records per day in raw layer (10 gb parquet files)&lt;/p&gt;\n\n&lt;p&gt;around 30 million records per day in prod layer (2-3 gb parquet files) (after preprocessing)&lt;/p&gt;\n\n&lt;p&gt;Everyday we get around 60 dates older dates data (or 40 dates older dates data per day) which we have to upsert&lt;/p&gt;\n\n&lt;p&gt;So we have to read in spark for every hour data in staging layer (30-40 report dates data from production to upsert which amounts to 180-200GB compressed parquet files, on de-serialization will expand to 1000GB-2000GB)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The spark job takes lot of time to complete (1:30 hours to 2 hours on a 7 node cluster with 32 gb machines and 8 core per node). Most of the time is on reads and writes. We have done all the possible optimisations from the spark code and config perspective&lt;/p&gt;\n\n&lt;p&gt;For us it&amp;#39;s too long and costly and also not scalable&lt;/p&gt;\n\n&lt;p&gt;Is there a better solution for this? How does industry approach this problem? Should we do upsert over any datawarehouse instead of spark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bf7wdx", "is_robot_indexable": true, "report_reasons": null, "author": "sud004", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf7wdx/scalable_performant_and_cost_optimised_approach/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf7wdx/scalable_performant_and_cost_optimised_approach/", "subreddit_subscribers": 169197, "created_utc": 1710485861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Okay I need some advice for choosing a training program that will get me job ready for data engineering. I prefer actual training over theory. Are there any good programs that have virtual training for real world examples? Such as building pipelines and using Python.\n\n\nBackground:\n\nWorked as a business analyst, systems analyst and a data analyst for 6 years.\n\nBeen using T-SQL for over 6 years. Need to make the jump into data engineering. Have a little experience in Python.\n\n\n\n\n\n\n", "author_fullname": "t2_exo31rtq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineers...what's a great learning program?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bfppjz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710539879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Okay I need some advice for choosing a training program that will get me job ready for data engineering. I prefer actual training over theory. Are there any good programs that have virtual training for real world examples? Such as building pipelines and using Python.&lt;/p&gt;\n\n&lt;p&gt;Background:&lt;/p&gt;\n\n&lt;p&gt;Worked as a business analyst, systems analyst and a data analyst for 6 years.&lt;/p&gt;\n\n&lt;p&gt;Been using T-SQL for over 6 years. Need to make the jump into data engineering. Have a little experience in Python.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bfppjz", "is_robot_indexable": true, "report_reasons": null, "author": "Dante_leigh", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfppjz/data_engineerswhats_a_great_learning_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfppjz/data_engineerswhats_a_great_learning_program/", "subreddit_subscribers": 169197, "created_utc": 1710539879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can you please suggest what pathways one could pursue after doing a masters in data science do data engineering?\n\nThanks for all your suggestions.\n\n&amp;#x200B;", "author_fullname": "t2_143jjw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science to data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bfpn8q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710539715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can you please suggest what pathways one could pursue after doing a masters in data science do data engineering?&lt;/p&gt;\n\n&lt;p&gt;Thanks for all your suggestions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bfpn8q", "is_robot_indexable": true, "report_reasons": null, "author": "Jay12a", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bfpn8q/data_science_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bfpn8q/data_science_to_data_engineering/", "subreddit_subscribers": 169197, "created_utc": 1710539715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m playing with streaming data from f1 games. \n\nBasically I get telemetry from the car via udp, and parse the bytes to float.\n\nWhat solution would you use to:\n\n1) Store this data\n2) Create realtime dashboards ?\n3) Create historical analysis?", "author_fullname": "t2_24zoub6u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming + Realtime + Dashboard ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bflawu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710528496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m playing with streaming data from f1 games. &lt;/p&gt;\n\n&lt;p&gt;Basically I get telemetry from the car via udp, and parse the bytes to float.&lt;/p&gt;\n\n&lt;p&gt;What solution would you use to:&lt;/p&gt;\n\n&lt;p&gt;1) Store this data\n2) Create realtime dashboards ?\n3) Create historical analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bflawu", "is_robot_indexable": true, "report_reasons": null, "author": "theuzz1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bflawu/streaming_realtime_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bflawu/streaming_realtime_dashboard/", "subreddit_subscribers": 169197, "created_utc": 1710528496.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}