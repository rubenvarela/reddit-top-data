{"kind": "Listing", "data": {"after": "t3_1beqtw9", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF's of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.\n\nI work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.", "author_fullname": "t2_t1bnfnc4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the hardest you have ever seen someone work manually?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beltlg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 154, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 154, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710425040.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710424759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF&amp;#39;s of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.&lt;/p&gt;\n\n&lt;p&gt;I work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1beltlg", "is_robot_indexable": true, "report_reasons": null, "author": "bjogc42069", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/", "subreddit_subscribers": 168935, "created_utc": 1710424759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the  project.\n\nThe dataset had over 5000 tables that were used across the board. \n\nMost of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don\u2019t help.\n\nIs there a way to restore it because we might lose the client at this rate? \n\nSample id of a table: \u2018project.Master.table1\u2019", "author_fullname": "t2_5hvalgp8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to recover a deleted dataset from BigQuery? (Urgent)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bekue2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 87, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 87, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710421925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the  project.&lt;/p&gt;\n\n&lt;p&gt;The dataset had over 5000 tables that were used across the board. &lt;/p&gt;\n\n&lt;p&gt;Most of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don\u2019t help.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to restore it because we might lose the client at this rate? &lt;/p&gt;\n\n&lt;p&gt;Sample id of a table: \u2018project.Master.table1\u2019&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bekue2", "is_robot_indexable": true, "report_reasons": null, "author": "honpra", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/", "subreddit_subscribers": 168935, "created_utc": 1710421925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, founder at Latitude here.\n\nWe spent the last 2 years building software for data teams. After many iterations, we've decided to rebuild everything from scratch and open-source it for the entire community.\n\nLatitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.\n\nYou can check out the repo here: [https://github.com/latitude-dev/latitude](https://github.com/latitude-dev/latitude)\n\nWe're actively looking for feedback and contributors. Let me know your thoughts!", "author_fullname": "t2_o4qnw2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Latitude: an open-source web framework to build data apps using SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bej704", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710416485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, founder at Latitude here.&lt;/p&gt;\n\n&lt;p&gt;We spent the last 2 years building software for data teams. After many iterations, we&amp;#39;ve decided to rebuild everything from scratch and open-source it for the entire community.&lt;/p&gt;\n\n&lt;p&gt;Latitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.&lt;/p&gt;\n\n&lt;p&gt;You can check out the repo here: &lt;a href=\"https://github.com/latitude-dev/latitude\"&gt;https://github.com/latitude-dev/latitude&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re actively looking for feedback and contributors. Let me know your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?auto=webp&amp;s=6eebca997ea4d4b55aeb0f760233c0415a0bdf63", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d827756cdde837af4ec07d99725da141bd435f68", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5612dbc3c955b0133b686b2df347ee7160426a8d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d57471adc017d17799740c2e311be154ac893f0c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbdd7b25a13222445010fda16e55f3b9df18a71b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ff1193415b2a1522056645b8cd2f1bfb564810b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba65964d68ee85a55125b4e94d704ac87a36d2e3", "width": 1080, "height": 540}], "variants": {}, "id": "zRu4yaIX9sS06soWCMlaUqVBLjk_mkvd6UlhzEJIqNY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bej704", "is_robot_indexable": true, "report_reasons": null, "author": "EloquentPickle", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bej704/latitude_an_opensource_web_framework_to_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bej704/latitude_an_opensource_web_framework_to_build/", "subreddit_subscribers": 168935, "created_utc": 1710416485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4ymkgdql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python requests best practices for data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1begg9k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ugbrL_EtU52ai4ErXvB2sZD09kEziLG1QVKocyJn93c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710405308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/python-requests-best-practices-for-data-engineers", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?auto=webp&amp;s=fdae695fd5fe2cee64e1fb4a7ec4483c6443e584", "width": 2912, "height": 1632}, "resolutions": [{"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f31260549e9d62ab9efa9beedc3fb6de9b637b1", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=930ed37cb47a4999c9f98a4e2bdb99166c32c43e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0669238db69cae8dcc36a03fc19af9df9df8b54b", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db93ea8d2e717d1b325f1c01272dad51892ecf90", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b7dcf25cde15dce4e116d09646fd78936820785", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77bd0feb4c744910ad476681f1a10765dff24509", "width": 1080, "height": 605}], "variants": {}, "id": "dW0cXVdRG0AiMhGU9NGc_dayd_CcrOfYkH0KamIBEUQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1begg9k", "is_robot_indexable": true, "report_reasons": null, "author": "SnooBeans3890", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1begg9k/python_requests_best_practices_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/python-requests-best-practices-for-data-engineers", "subreddit_subscribers": 168935, "created_utc": 1710405308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you showing visibility if you are part of a non tech management and org in general\nCurrent team structure is of an analyst and data scientist.\nDA and DS roles are front facing/stakeholder facing so they naturally get visibility. In a team structure like this, how do you make sure your work\u2019s visible and adds value", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solo DEs : How are you making sure your work is visible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes4bx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you showing visibility if you are part of a non tech management and org in general\nCurrent team structure is of an analyst and data scientist.\nDA and DS roles are front facing/stakeholder facing so they naturally get visibility. In a team structure like this, how do you make sure your work\u2019s visible and adds value&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes4bx", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes4bx/solo_des_how_are_you_making_sure_your_work_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes4bx/solo_des_how_are_you_making_sure_your_work_is/", "subreddit_subscribers": 168935, "created_utc": 1710440720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing research on open source data quality tools, and I've found these so far:\n\n1. dbt core\n2. Apache Griffin\n3. Soda Core\n4. Deequ\n5. Tensorflow Data Validation\n6. Moby DQ\n7. Great Expectatons\n\nI've been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I'm missing here? \n\n(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).", "author_fullname": "t2_mc935wf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-Source Data Quality Tools Abound", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bemv7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710427511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing research on open source data quality tools, and I&amp;#39;ve found these so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;dbt core&lt;/li&gt;\n&lt;li&gt;Apache Griffin&lt;/li&gt;\n&lt;li&gt;Soda Core&lt;/li&gt;\n&lt;li&gt;Deequ&lt;/li&gt;\n&lt;li&gt;Tensorflow Data Validation&lt;/li&gt;\n&lt;li&gt;Moby DQ&lt;/li&gt;\n&lt;li&gt;Great Expectatons&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I&amp;#39;m missing here? &lt;/p&gt;\n\n&lt;p&gt;(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bemv7l", "is_robot_indexable": true, "report_reasons": null, "author": "ValidInternetCitizen", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bemv7l/opensource_data_quality_tools_abound/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bemv7l/opensource_data_quality_tools_abound/", "subreddit_subscribers": 168935, "created_utc": 1710427511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I have been working on a personal project regarding data engineering. This project has to do with retrieving steam games prices for different games in different countries, and plotting the price difference in a world map.\n\nThis project is made up of 2 ETLs: One that retrieves price data and the other plots it using a world map.\n\nI would like some feedback on what I couldve done better. I tried using design pattern builder, using abstractions for different external resources and parametrization with Yaml.\n\nThis project uses 3 APIs and an S3 bucket for its internal processing.\n\n[here you have the project link](https://github.com/edseldim/steam_prices_data_engineering)\n\nThis is the final result\n\nhttps://preview.redd.it/139jgrlemeoc1.png?width=824&amp;format=png&amp;auto=webp&amp;s=204769ad503885eef0153d565f9243e5c5f56add", "author_fullname": "t2_kvl7dwa6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Steam Prices ETL (Personal Project)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"139jgrlemeoc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=39f8277d318238505cfb88cafb4380090c78efe2"}, {"y": 97, "x": 216, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0514ed0826dbe3dd540c824819241088fe1cb4c4"}, {"y": 144, "x": 320, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f84b4e44b907f687ada00592be8ea06d900ad8f"}, {"y": 289, "x": 640, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=42ecf2b7b21d10c5ef96b59d2218080c4023a1d9"}], "s": {"y": 373, "x": 824, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;format=png&amp;auto=webp&amp;s=204769ad503885eef0153d565f9243e5c5f56add"}, "id": "139jgrlemeoc1"}}, "name": "t3_1bf2ntf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/-JOn9CR7ml0xwwpJP5LtTt8IOTeg6FNt_1CnSruPpM4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1710467816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I have been working on a personal project regarding data engineering. This project has to do with retrieving steam games prices for different games in different countries, and plotting the price difference in a world map.&lt;/p&gt;\n\n&lt;p&gt;This project is made up of 2 ETLs: One that retrieves price data and the other plots it using a world map.&lt;/p&gt;\n\n&lt;p&gt;I would like some feedback on what I couldve done better. I tried using design pattern builder, using abstractions for different external resources and parametrization with Yaml.&lt;/p&gt;\n\n&lt;p&gt;This project uses 3 APIs and an S3 bucket for its internal processing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/edseldim/steam_prices_data_engineering\"&gt;here you have the project link&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the final result&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=204769ad503885eef0153d565f9243e5c5f56add\"&gt;https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=204769ad503885eef0153d565f9243e5c5f56add&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?auto=webp&amp;s=60d7f614a02fe266ad949dba81febe514ae1f75f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f474ec194a50612a3656981c46a5fd9df70dcee", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=214a43d5c075f07697fc5217ed7ec740397633f6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6f4e7b39d92036757989f827e590bd6c21fae2d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=59d55affc4a0264288094c3c6b4aa06c40899d13", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92a6161e090c6063dfa644e4dd0c3546ce422244", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9ae6c30b86adcad5ef672246cc4997b99783452", "width": 1080, "height": 540}], "variants": {}, "id": "M9rR2_qFAuxZC-bF3f95cChjYc5baZxoQBUTgp2zRSg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bf2ntf", "is_robot_indexable": true, "report_reasons": null, "author": "Confident_Watch8207", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf2ntf/steam_prices_etl_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf2ntf/steam_prices_etl_personal_project/", "subreddit_subscribers": 168935, "created_utc": 1710467816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently using Windows Task Scheduler to schedule scripts on a Windows Server but it is too simply.\n\nI need software where I can easily manage and monitor 100+ jobs/scripts, including a good GUI.\n\nWhat would you recommend? It does not have to be free.", "author_fullname": "t2_b54vgte16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you recommend good enterprise job scheduling software for Windows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bevpcl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710449513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently using Windows Task Scheduler to schedule scripts on a Windows Server but it is too simply.&lt;/p&gt;\n\n&lt;p&gt;I need software where I can easily manage and monitor 100+ jobs/scripts, including a good GUI.&lt;/p&gt;\n\n&lt;p&gt;What would you recommend? It does not have to be free.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bevpcl", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Resource9267", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bevpcl/can_you_recommend_good_enterprise_job_scheduling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bevpcl/can_you_recommend_good_enterprise_job_scheduling/", "subreddit_subscribers": 168935, "created_utc": 1710449513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Link to blog post here](https://www.y42.com/blog/gitops-for-data-2) \\- feedback welcome!  \n\n\nDo you test all your changes in prod? \ud83e\udd26\u200d\u2642\ufe0f Let's borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.\n\nJust released a blog post explaining it and showing how to make sure you're:\n\n* Always working on production data in an isolated environment (dev/staging/prod environments)\n* Collaborating securely with custom approval flows (GitOps)\n* Preventing faulty builds from going into production (CI/CD)\n\nCheck it out and share your thoughts :)", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitOps for Data - the Write-Audit-Publish (WAP) pattern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ben94r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710428522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.y42.com/blog/gitops-for-data-2\"&gt;Link to blog post here&lt;/a&gt; - feedback welcome!  &lt;/p&gt;\n\n&lt;p&gt;Do you test all your changes in prod? \ud83e\udd26\u200d\u2642\ufe0f Let&amp;#39;s borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.&lt;/p&gt;\n\n&lt;p&gt;Just released a blog post explaining it and showing how to make sure you&amp;#39;re:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Always working on production data in an isolated environment (dev/staging/prod environments)&lt;/li&gt;\n&lt;li&gt;Collaborating securely with custom approval flows (GitOps)&lt;/li&gt;\n&lt;li&gt;Preventing faulty builds from going into production (CI/CD)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Check it out and share your thoughts :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?auto=webp&amp;s=856e48bb6457d70b7d36615e87154df2ab80dd27", "width": 1456, "height": 816}, "resolutions": [{"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97a8117a8e7bd9d6e64bdfd71298da59725bee8a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c843ee7f0815e03799e7130789dd887ba7e04c3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71d797e7294077f0703e7de512fcdddfdbe6c3a4", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7947c9ce6d187c11827e95b5a17ee667fa4ea581", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d844f43d666e13ad989844094ff4c0fadb5f220", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e0246833a8cb6ab3a92145d268ba71b2d94d98d", "width": 1080, "height": 605}], "variants": {}, "id": "VPfZqKH060z4qwGQaqgebyoFVaPD4Rohnv811YzZ5WM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ben94r", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ben94r/gitops_for_data_the_writeauditpublish_wap_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ben94r/gitops_for_data_the_writeauditpublish_wap_pattern/", "subreddit_subscribers": 168935, "created_utc": 1710428522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently joined a company that stores all Primary Keys for all tables as UUIDs instead of integers.\n\nAnd for some reason all these columns' dtypes are `VARCHAR(64)`. I know that storing them as `CHAR(36)` would be better.\n\nI've tried to run some benchmarking to backup my decision, but found almost no storage benefit, and minimal query performance improvement (for simple selects, and also for joins).\n\nWould it really make a difference or should I disregard this initiative?\n\nPS: we're using mariadb, and I've benchmarked by creating three identical 1M rows table with varchar64, varchar36, and char36.", "author_fullname": "t2_apdhbvv7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Var vs varchar for uuids", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bernd3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710439544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently joined a company that stores all Primary Keys for all tables as UUIDs instead of integers.&lt;/p&gt;\n\n&lt;p&gt;And for some reason all these columns&amp;#39; dtypes are &lt;code&gt;VARCHAR(64)&lt;/code&gt;. I know that storing them as &lt;code&gt;CHAR(36)&lt;/code&gt; would be better.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to run some benchmarking to backup my decision, but found almost no storage benefit, and minimal query performance improvement (for simple selects, and also for joins).&lt;/p&gt;\n\n&lt;p&gt;Would it really make a difference or should I disregard this initiative?&lt;/p&gt;\n\n&lt;p&gt;PS: we&amp;#39;re using mariadb, and I&amp;#39;ve benchmarked by creating three identical 1M rows table with varchar64, varchar36, and char36.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bernd3", "is_robot_indexable": true, "report_reasons": null, "author": "Agile-Scene-2465", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bernd3/var_vs_varchar_for_uuids/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bernd3/var_vs_varchar_for_uuids/", "subreddit_subscribers": 168935, "created_utc": 1710439544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nAs a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here's the concept:\n\nWe need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:\n\n* Static (meaning they update or add their rows every *n* minutes or hours).\n* Temporally ordered (i.e., Time Series).\n\nIt is highly likely that the observations, which are to be fetched every *n* seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by 'Real Time', I mean that it gets updated every *n* seconds).\n\nAlso, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.\n\nI am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.\n\nThank you in advance", "author_fullname": "t2_c0ghewdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What architecture would you suggest for a Real Time Streaming project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beo2yq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710430878.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710430666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;As a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here&amp;#39;s the concept:&lt;/p&gt;\n\n&lt;p&gt;We need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Static (meaning they update or add their rows every &lt;em&gt;n&lt;/em&gt; minutes or hours).&lt;/li&gt;\n&lt;li&gt;Temporally ordered (i.e., Time Series).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It is highly likely that the observations, which are to be fetched every &lt;em&gt;n&lt;/em&gt; seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by &amp;#39;Real Time&amp;#39;, I mean that it gets updated every &lt;em&gt;n&lt;/em&gt; seconds).&lt;/p&gt;\n\n&lt;p&gt;Also, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.&lt;/p&gt;\n\n&lt;p&gt;I am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1beo2yq", "is_robot_indexable": true, "report_reasons": null, "author": "hasty-beaver", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beo2yq/what_architecture_would_you_suggest_for_a_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beo2yq/what_architecture_would_you_suggest_for_a_real/", "subreddit_subscribers": 168935, "created_utc": 1710430666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AI at this point is inevitable and it\u2019s become quite clear to me that the roles and responsibilities of a data engineer today will significantly change as AI tools become more common place. At this point it\u2019s  all speculative but my questions are\nA) what does the data engineer of tomorrow look like\nB) how can I adapt to a changing landscape and essentially future proof my career\n\nAny advice will be greatly appreciated!", "author_fullname": "t2_7iiccjhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I future proof my career as a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf4aft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710472745.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI at this point is inevitable and it\u2019s become quite clear to me that the roles and responsibilities of a data engineer today will significantly change as AI tools become more common place. At this point it\u2019s  all speculative but my questions are\nA) what does the data engineer of tomorrow look like\nB) how can I adapt to a changing landscape and essentially future proof my career&lt;/p&gt;\n\n&lt;p&gt;Any advice will be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bf4aft", "is_robot_indexable": true, "report_reasons": null, "author": "Ill-Advisor-8235", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf4aft/how_do_i_future_proof_my_career_as_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf4aft/how_do_i_future_proof_my_career_as_a_data_engineer/", "subreddit_subscribers": 168935, "created_utc": 1710472745.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for nothing more than anecdotal data here. I\u2019m looking at these two options and am wondering what people end up paying at scale. If you could provide a bit about your configuration and use-cases, even better \ud83d\ude00", "author_fullname": "t2_5bf8p6qg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much do you pay for your Clickhouse or Apache Pinot implementations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes9nf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710441071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for nothing more than anecdotal data here. I\u2019m looking at these two options and am wondering what people end up paying at scale. If you could provide a bit about your configuration and use-cases, even better \ud83d\ude00&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes9nf", "is_robot_indexable": true, "report_reasons": null, "author": "gorba004", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes9nf/how_much_do_you_pay_for_your_clickhouse_or_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes9nf/how_much_do_you_pay_for_your_clickhouse_or_apache/", "subreddit_subscribers": 168935, "created_utc": 1710441071.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_tb9gsv8ek", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Figma's Databases Team Lived to Tell the Scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": true, "name": "t3_1bf5csb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8xbuhoCS2I4KUMWrLfS95pUZth0IhtqhuZrGN49f2HA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710476129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "figma.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?auto=webp&amp;s=b02d84f548d7016532bde698df50f6ad2964cdde", "width": 1200, "height": 899}, "resolutions": [{"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb49ef81566c7f3362de32f19bcbb0503eab41d3", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a47aeec8959e0854c7645af187400ef2b2b8d778", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff4a41b271855634f3f31a1a95d0a1923e5663d3", "width": 320, "height": 239}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf08e010042ff3adc093c447f05b01d35617f4f8", "width": 640, "height": 479}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=64c428fb6bb53fc271e92487969781f20a0cf5c2", "width": 960, "height": 719}, {"url": "https://external-preview.redd.it/cPfhzsD3qR8AKrrpfJwab-Z2MrviATve6DlFGl8GDQc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b089381d255a531b9dfc063c300fc4d51d3ba692", "width": 1080, "height": 809}], "variants": {}, "id": "Sh9ywQbgHtUAXOpENNdmfZpxlwzzyYcC3r9bH8mkHRA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bf5csb", "is_robot_indexable": true, "report_reasons": null, "author": "Rollstack", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf5csb/how_figmas_databases_team_lived_to_tell_the_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/", "subreddit_subscribers": 168935, "created_utc": 1710476129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm primarily a frontend engineer. When I need to work on backend/infra, I often run into the issue of not knowing how to aggregate data in a scaleable way. E.g. pre-computing Reddit's upvote counts so you don't have to aggregate at read time. Aggregating for the top posts feed is easy because it's ok if it's stale by several minutes. However, if you upvote a post and refresh a second later, ideally the post count should include your upvote.\n\nIs there a standard way to do this? I don't know why it's so hard to find discussions around this problem. Afaik some ways of doing this are:\n\n1) When inserting the \"upvote\" record, also increment an \"upvoteCounts\" record\n\nIf you do this in a transaction, it would make the insertions slower. If you don't use a transaction, the upvote counts would go out of sync. I think some companies don't use transactions, but have a job to continuously recompute the counts. Another problem is if you need to mutate multiple aggregations per insertion, it'll be confusing to maintain.\n\n2) Use a trigger\n\nIt's basically the same as 1) with transactions, but handled by the DB instead of the application.\n\n3) Aggregate the event stream\n\nStream changes, then use something like Flink/ksqlDB to aggregate the stream, then stream the aggregation to somewhere for the application to read from. This has much higher latency than 1) with transactions. The benefit is that your aggregations are declarative, so they're easier to reason about.\n\n4) Have a batch job for old aggregations, then handle real-time aggregations in memory\n\nAssuming batch aggregation will take a long time, you can store real-time aggregations in memory. E.g. when someone upvotes, increment a count in memory. When fetching the count, sum the count from memory with the count from the batch job. However, it seems tricky to not double count or miss upvotes.\n\n5) Handling the aggregation at read-time with aggressive caching\n\nThis works when fetching the upvote count for a small set of posts. However, you wouldn't be able to sort posts by upvote count. Also, the max staleness is the cache timeout, which could be a long time.\n\n6) Real-time or incremental materialized views\n\nIdeally, we'd have materialized views that could quickly automatically respond to changes in the source tables. However, afaik the existing real-time materialized view systems are either in memory (e.g. Materialize) or places severe limitations on the source table (e.g. AnalyticDB). Materialize uses too much memory to be scaleable and I've never tried AnalyticDB, but it seems like it's too limiting on the source tables.\n\nAre there any resources to learn more about which approaches large companies use? Have you tried some of these and know first-hand about the pros and cons?", "author_fullname": "t2_4pdki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Different methods for real-time aggregation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf2jbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710467450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m primarily a frontend engineer. When I need to work on backend/infra, I often run into the issue of not knowing how to aggregate data in a scaleable way. E.g. pre-computing Reddit&amp;#39;s upvote counts so you don&amp;#39;t have to aggregate at read time. Aggregating for the top posts feed is easy because it&amp;#39;s ok if it&amp;#39;s stale by several minutes. However, if you upvote a post and refresh a second later, ideally the post count should include your upvote.&lt;/p&gt;\n\n&lt;p&gt;Is there a standard way to do this? I don&amp;#39;t know why it&amp;#39;s so hard to find discussions around this problem. Afaik some ways of doing this are:&lt;/p&gt;\n\n&lt;p&gt;1) When inserting the &amp;quot;upvote&amp;quot; record, also increment an &amp;quot;upvoteCounts&amp;quot; record&lt;/p&gt;\n\n&lt;p&gt;If you do this in a transaction, it would make the insertions slower. If you don&amp;#39;t use a transaction, the upvote counts would go out of sync. I think some companies don&amp;#39;t use transactions, but have a job to continuously recompute the counts. Another problem is if you need to mutate multiple aggregations per insertion, it&amp;#39;ll be confusing to maintain.&lt;/p&gt;\n\n&lt;p&gt;2) Use a trigger&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s basically the same as 1) with transactions, but handled by the DB instead of the application.&lt;/p&gt;\n\n&lt;p&gt;3) Aggregate the event stream&lt;/p&gt;\n\n&lt;p&gt;Stream changes, then use something like Flink/ksqlDB to aggregate the stream, then stream the aggregation to somewhere for the application to read from. This has much higher latency than 1) with transactions. The benefit is that your aggregations are declarative, so they&amp;#39;re easier to reason about.&lt;/p&gt;\n\n&lt;p&gt;4) Have a batch job for old aggregations, then handle real-time aggregations in memory&lt;/p&gt;\n\n&lt;p&gt;Assuming batch aggregation will take a long time, you can store real-time aggregations in memory. E.g. when someone upvotes, increment a count in memory. When fetching the count, sum the count from memory with the count from the batch job. However, it seems tricky to not double count or miss upvotes.&lt;/p&gt;\n\n&lt;p&gt;5) Handling the aggregation at read-time with aggressive caching&lt;/p&gt;\n\n&lt;p&gt;This works when fetching the upvote count for a small set of posts. However, you wouldn&amp;#39;t be able to sort posts by upvote count. Also, the max staleness is the cache timeout, which could be a long time.&lt;/p&gt;\n\n&lt;p&gt;6) Real-time or incremental materialized views&lt;/p&gt;\n\n&lt;p&gt;Ideally, we&amp;#39;d have materialized views that could quickly automatically respond to changes in the source tables. However, afaik the existing real-time materialized view systems are either in memory (e.g. Materialize) or places severe limitations on the source table (e.g. AnalyticDB). Materialize uses too much memory to be scaleable and I&amp;#39;ve never tried AnalyticDB, but it seems like it&amp;#39;s too limiting on the source tables.&lt;/p&gt;\n\n&lt;p&gt;Are there any resources to learn more about which approaches large companies use? Have you tried some of these and know first-hand about the pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf2jbk", "is_robot_indexable": true, "report_reasons": null, "author": "linksku", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf2jbk/different_methods_for_realtime_aggregation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf2jbk/different_methods_for_realtime_aggregation/", "subreddit_subscribers": 168935, "created_utc": 1710467450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello r/dataengineering,\n\nI've been deeply involved with Apache AGE, a graph database extension that's proving to be a game-changer for data engineering. As a core contributor, I've seen its potential to revolutionize the way we approach data relationships and analysis, especially in environments that traditionally rely on SQL databases.\n\nApache AGE enables the integration of graph database functionalities directly into PostgreSQL, allowing for complex data relationship queries without the need for a separate graph database system. This opens up new avenues for data modeling, querying, and analysis within the familiar PostgreSQL ecosystem.\n\nWhat's particularly exciting for data engineers is AGE's ability to handle complex, interconnected data scenarios more naturally and efficiently than traditional relational databases. This capability is invaluable for applications in social networking, fraud detection, recommendation systems, and more, where understanding the relationships between data points is key.\n\nI'm curious to hear from the community:\n\n* Have you integrated graph database features into your data engineering projects?\n* What challenges and opportunities do you see in adopting Apache AGE for your data workflows?\n\nLet's explore how Apache AGE can fit into our data engineering toolkit, share experiences, and discuss best practices for leveraging graph database capabilities in our projects.\n\nFor a deep dive into the technical workings, documentation, and to join our growing community, visit our [Apache AGE GitHub](https://github.com/apache/age) and [official website](https://age.apache.org/).", "author_fullname": "t2_ru7nh9tc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrating Apache AGE for Enhanced Data Engineering Workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bezk3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710459215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been deeply involved with Apache AGE, a graph database extension that&amp;#39;s proving to be a game-changer for data engineering. As a core contributor, I&amp;#39;ve seen its potential to revolutionize the way we approach data relationships and analysis, especially in environments that traditionally rely on SQL databases.&lt;/p&gt;\n\n&lt;p&gt;Apache AGE enables the integration of graph database functionalities directly into PostgreSQL, allowing for complex data relationship queries without the need for a separate graph database system. This opens up new avenues for data modeling, querying, and analysis within the familiar PostgreSQL ecosystem.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s particularly exciting for data engineers is AGE&amp;#39;s ability to handle complex, interconnected data scenarios more naturally and efficiently than traditional relational databases. This capability is invaluable for applications in social networking, fraud detection, recommendation systems, and more, where understanding the relationships between data points is key.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear from the community:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have you integrated graph database features into your data engineering projects?&lt;/li&gt;\n&lt;li&gt;What challenges and opportunities do you see in adopting Apache AGE for your data workflows?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Let&amp;#39;s explore how Apache AGE can fit into our data engineering toolkit, share experiences, and discuss best practices for leveraging graph database capabilities in our projects.&lt;/p&gt;\n\n&lt;p&gt;For a deep dive into the technical workings, documentation, and to join our growing community, visit our &lt;a href=\"https://github.com/apache/age\"&gt;Apache AGE GitHub&lt;/a&gt; and &lt;a href=\"https://age.apache.org/\"&gt;official website&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?auto=webp&amp;s=15f4c564dfb24842422fd91a488b20fa861c6ced", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82c39c26b2a4d0e656e8ed1368eef5e209fed31a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fb5dd8df8123d4cb3385c164770dc2f23d769", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0553dd3915d07598c17fea34ac7391746f060d6d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a85dd628f6980076c2a417ab108fd22292ef2789", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=426a3a01c2b2ad2624ca11ec382bfbc174a72df1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a50d70173c809efbe08ac98a2a0c511a63b56ae", "width": 1080, "height": 540}], "variants": {}, "id": "SGYMkpNq_z1nMJ_lmsYzVDtGYJK2yoPXMSe-sgg4NEQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bezk3y", "is_robot_indexable": true, "report_reasons": null, "author": "Eya_AGE", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bezk3y/integrating_apache_age_for_enhanced_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bezk3y/integrating_apache_age_for_enhanced_data/", "subreddit_subscribers": 168935, "created_utc": 1710459215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive Guide to Optimize Spark Data Workloads | Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1beza3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wSYuyjW9sHHw5_cAGIUIr_nm5ZCApMM3rIrUZvhd8S8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710458480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/discover/pages/optimize-data-workloads-guide#intro", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?auto=webp&amp;s=1888946a217a566b570f4afbb9c6cda7dae09653", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91375d0ca6cf9fd6186eb43a977e12bb7ac60794", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23b01c6ba19a97c4f454897dc3111177fc2598b1", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fcc73d3f10ecd1b06c22917d735a7eed72814572", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e0dac9968155d941a82fb9777c5cb692978475c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc43e0b2a699224e3c7e07d302187e9dbaed3f56", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0eb50fd84794eb10a4bcbffe0ec00c5545ad25f", "width": 1080, "height": 567}], "variants": {}, "id": "RS3lr2nqDF5sgLvhBJ8QfWyKVgI629watKirGMnYEBk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1beza3c", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beza3c/comprehensive_guide_to_optimize_spark_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/discover/pages/optimize-data-workloads-guide#intro", "subreddit_subscribers": 168935, "created_utc": 1710458480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have pyspark structured streaming code which writes to hive using in micro batch mode. I am using foreachbatch. I want to know if that particular batch failed then check point is commited or not. When exactly checkpoint is commited? Is it when function passed to foreachbatch runs sucessfully or just after passing df to that function .", "author_fullname": "t2_63ryq4q7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you help me understand checkpoints in structured streaming ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes500", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have pyspark structured streaming code which writes to hive using in micro batch mode. I am using foreachbatch. I want to know if that particular batch failed then check point is commited or not. When exactly checkpoint is commited? Is it when function passed to foreachbatch runs sucessfully or just after passing df to that function .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes500", "is_robot_indexable": true, "report_reasons": null, "author": "stuart_little_03", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes500/can_you_help_me_understand_checkpoints_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes500/can_you_help_me_understand_checkpoints_in/", "subreddit_subscribers": 168935, "created_utc": 1710440764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m curious to find out whether you use the BI tool or rely on SQL to get insights from data. In my experience, the BI tools can be complex and also quite limited in their utility for detailed analytics tasks. What do you currently use today for your analytics workflow, and what do you love and hate about it?", "author_fullname": "t2_ez4cm3m01", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BI tools!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bf5ij4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710476663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m curious to find out whether you use the BI tool or rely on SQL to get insights from data. In my experience, the BI tools can be complex and also quite limited in their utility for detailed analytics tasks. What do you currently use today for your analytics workflow, and what do you love and hate about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf5ij4", "is_robot_indexable": true, "report_reasons": null, "author": "glinter777", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf5ij4/bi_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf5ij4/bi_tools/", "subreddit_subscribers": 168935, "created_utc": 1710476663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a job where all I do is enter the $ amount, vendor name, and invoice # from a pdf invoice that is received in an outlook inbox into a Access database, There has to be a way to make it less manual.\n\nFor context I work for a government organization that receives 100s of invoices from many hundreds of different vendors every day.\n\nWhere should I start?", "author_fullname": "t2_g45v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I manually enter invoices in access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bf57d2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710475636.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a job where all I do is enter the $ amount, vendor name, and invoice # from a pdf invoice that is received in an outlook inbox into a Access database, There has to be a way to make it less manual.&lt;/p&gt;\n\n&lt;p&gt;For context I work for a government organization that receives 100s of invoices from many hundreds of different vendors every day.&lt;/p&gt;\n\n&lt;p&gt;Where should I start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bf57d2", "is_robot_indexable": true, "report_reasons": null, "author": "KingCharlemange", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf57d2/i_manually_enter_invoices_in_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf57d2/i_manually_enter_invoices_in_access/", "subreddit_subscribers": 168935, "created_utc": 1710475636.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been using talend open studio for our ETL needs but with that going out of support we are looking for a new tool. \n\nWe have about 8 ETLs that extract data from various sources: powershell scripts, python scripts, MySQL database, sql server database and do some pretty basic transformations into an on prem oracle database. After that we build dashboards in power bi. \n\nI think cloud tooling could be good as there is less maintenance (server patching, app upgrades) but it must be able to write to on prem data warehouse. I\u2019m not against on prem tool if it makes more sense cost wise. \n\nI think cloud tools are usually priced based on computation. \n\nOne big benefit for us would be amount of resources on the tool online. If you run into issues are there lots of forums that have similar issues with resolutions? \n\nAny good paid tools or open source tools you recommend or don\u2019t recommend? ", "author_fullname": "t2_p8gsv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL tool recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bf13x5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710463407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been using talend open studio for our ETL needs but with that going out of support we are looking for a new tool. &lt;/p&gt;\n\n&lt;p&gt;We have about 8 ETLs that extract data from various sources: powershell scripts, python scripts, MySQL database, sql server database and do some pretty basic transformations into an on prem oracle database. After that we build dashboards in power bi. &lt;/p&gt;\n\n&lt;p&gt;I think cloud tooling could be good as there is less maintenance (server patching, app upgrades) but it must be able to write to on prem data warehouse. I\u2019m not against on prem tool if it makes more sense cost wise. &lt;/p&gt;\n\n&lt;p&gt;I think cloud tools are usually priced based on computation. &lt;/p&gt;\n\n&lt;p&gt;One big benefit for us would be amount of resources on the tool online. If you run into issues are there lots of forums that have similar issues with resolutions? &lt;/p&gt;\n\n&lt;p&gt;Any good paid tools or open source tools you recommend or don\u2019t recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf13x5", "is_robot_indexable": true, "report_reasons": null, "author": "CyclingMonkey", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf13x5/etl_tool_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf13x5/etl_tool_recommendations/", "subreddit_subscribers": 168935, "created_utc": 1710463407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In [this demo](https://github.com/airbytehq/quickstarts/blob/main/pyairbyte_notebooks/PyAirbyte_GA4_Demo.ipynb), we use the Airbyte\u2019s Python library to ingest data from Google Analytics 4, followed by a series of transformations and analysis using pandas.\n\nThis demonstrates how you can do fast data ingestion prototyping of a complex data source, without having to start from scratch.\n\nhttps://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;format=png&amp;auto=webp&amp;s=8756a27a3ca8e790281300ea550e28a04406558e", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting data from GA4 with a few Python statements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "media_metadata": {"z9kyphw6hcoc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8ad7b80115aacdb1951caf26d714fd4d2ef58cc"}, {"y": 133, "x": 216, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40221bbc247a82140698c248eae9832b1753dd82"}, {"y": 198, "x": 320, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bd2331fbe54b67d31cdd95f6a77efe8e6b650ba"}, {"y": 396, "x": 640, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=03a915953cee724d87b1049572d1fa9d805ab48e"}, {"y": 594, "x": 960, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f77889758fc6a82e48015e7aa77519f8f198ff2"}], "s": {"y": 652, "x": 1052, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;format=png&amp;auto=webp&amp;s=8756a27a3ca8e790281300ea550e28a04406558e"}, "id": "z9kyphw6hcoc1"}}, "name": "t3_1besj42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zfbIWE_-nFwh8BNnpCmbiQSB3u8wW6btjPhDlYIJhRg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710441695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In &lt;a href=\"https://github.com/airbytehq/quickstarts/blob/main/pyairbyte_notebooks/PyAirbyte_GA4_Demo.ipynb\"&gt;this demo&lt;/a&gt;, we use the Airbyte\u2019s Python library to ingest data from Google Analytics 4, followed by a series of transformations and analysis using pandas.&lt;/p&gt;\n\n&lt;p&gt;This demonstrates how you can do fast data ingestion prototyping of a complex data source, without having to start from scratch.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8756a27a3ca8e790281300ea550e28a04406558e\"&gt;https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8756a27a3ca8e790281300ea550e28a04406558e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1besj42", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1besj42/ingesting_data_from_ga4_with_a_few_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1besj42/ingesting_data_from_ga4_with_a_few_python/", "subreddit_subscribers": 168935, "created_utc": 1710441695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a project that involves database schema migration,  and I'm looking for some guidance and suggestions from the community.  \nThe  goal of the project is to create a tool that can migrate the schema  from one database to another. Initially, we're targeting support for  MySQL, PostgreSQL, and MongoDB. The tool should be able to retrieve the  schema information from the source database, generate the necessary SQL  statements or equivalent commands to recreate the schema in the target  database, execute the schema migration, and perform validation to ensure  the migration was successful.  \nI've started implementing a package  structure for the schema migration process, which includes a retriever  package for retrieving schema information from different databases, a  generator package for generating the migration statements, an executor  package for executing the migration, and a validator package for  validating the migrated schema.  \nHere are my questions:  \nAre there  any existing open-source libraries in Go that already provide schema  migration functionality similar to what I'm trying to achieve? If so,  could you recommend some libraries worth checking out?     \n\n\nIf there aren't any suitable libraries available, I would appreciate  any suggestions or ideas on how to approach the implementation of the  schema migration tool. Are there any best practices, common pitfalls, or  important considerations I should keep in mind?     \n\n\nFor those who have worked on similar projects or have experience  with database schema migration, could you share any insights or lessons  learned that could help guide my implementation?  \nI'm open to any feedback, suggestions, or recommendations that could help me move forward with this project.   \n\nThanks   ", "author_fullname": "t2_s9sv2dxdt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for open-source libraries for database schema migration and suggestions on implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1berurq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project that involves database schema migration,  and I&amp;#39;m looking for some guidance and suggestions from the community.&lt;br/&gt;\nThe  goal of the project is to create a tool that can migrate the schema  from one database to another. Initially, we&amp;#39;re targeting support for  MySQL, PostgreSQL, and MongoDB. The tool should be able to retrieve the  schema information from the source database, generate the necessary SQL  statements or equivalent commands to recreate the schema in the target  database, execute the schema migration, and perform validation to ensure  the migration was successful.&lt;br/&gt;\nI&amp;#39;ve started implementing a package  structure for the schema migration process, which includes a retriever  package for retrieving schema information from different databases, a  generator package for generating the migration statements, an executor  package for executing the migration, and a validator package for  validating the migrated schema.&lt;br/&gt;\nHere are my questions:&lt;br/&gt;\nAre there  any existing open-source libraries in Go that already provide schema  migration functionality similar to what I&amp;#39;m trying to achieve? If so,  could you recommend some libraries worth checking out?     &lt;/p&gt;\n\n&lt;p&gt;If there aren&amp;#39;t any suitable libraries available, I would appreciate  any suggestions or ideas on how to approach the implementation of the  schema migration tool. Are there any best practices, common pitfalls, or  important considerations I should keep in mind?     &lt;/p&gt;\n\n&lt;p&gt;For those who have worked on similar projects or have experience  with database schema migration, could you share any insights or lessons  learned that could help guide my implementation?&lt;br/&gt;\nI&amp;#39;m open to any feedback, suggestions, or recommendations that could help me move forward with this project.   &lt;/p&gt;\n\n&lt;p&gt;Thanks   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1berurq", "is_robot_indexable": true, "report_reasons": null, "author": "Helpful-Natural6628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1berurq/looking_for_opensource_libraries_for_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1berurq/looking_for_opensource_libraries_for_database/", "subreddit_subscribers": 168935, "created_utc": 1710440063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You would think this would fall under data governance (internal and external). How ever it\u2019s really not, because obviously I wouldn\u2019t be asking this if there was proper governance, in the ethical form not business logic. More a question for  senior members. I\u2019ve been through this before at banks and parted ways. \nWhat would be the best route, if the data team is manipulating data (intentionally), albeit their objective is stockholders perception but I see this could have repercussions beyond the company.  How would u recommended absolving my involvement and not losing that contract because I don\u2019t know how far it climbs to the e-team. Sorry but I can\u2019t give details of the data. I was thinking just a registered letter to my lawyer (he agrees). I\u2019m also not into this whole \u201c I was being told what to do\u201d deal. The last time this happened it was pretty bad, I parted ways with the bank and their team tried to use me as a scape goat and the bank was considering pursuing charges against me (no joke) but the fact I left and gave reasons of \u201cteam dynamics\u201d and \u201cunethical data manipulation\u201d, which I guess they didn\u2019t read because it was on the second page of the voluntary contract termination I drafted up, my lawyer basically told them to take a hike but this back and forth a  few weeks costed me a few grand.\n", "author_fullname": "t2_6lfdg0it", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data policy / ethics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1berdf5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710438871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You would think this would fall under data governance (internal and external). How ever it\u2019s really not, because obviously I wouldn\u2019t be asking this if there was proper governance, in the ethical form not business logic. More a question for  senior members. I\u2019ve been through this before at banks and parted ways. \nWhat would be the best route, if the data team is manipulating data (intentionally), albeit their objective is stockholders perception but I see this could have repercussions beyond the company.  How would u recommended absolving my involvement and not losing that contract because I don\u2019t know how far it climbs to the e-team. Sorry but I can\u2019t give details of the data. I was thinking just a registered letter to my lawyer (he agrees). I\u2019m also not into this whole \u201c I was being told what to do\u201d deal. The last time this happened it was pretty bad, I parted ways with the bank and their team tried to use me as a scape goat and the bank was considering pursuing charges against me (no joke) but the fact I left and gave reasons of \u201cteam dynamics\u201d and \u201cunethical data manipulation\u201d, which I guess they didn\u2019t read because it was on the second page of the voluntary contract termination I drafted up, my lawyer basically told them to take a hike but this back and forth a  few weeks costed me a few grand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1berdf5", "is_robot_indexable": true, "report_reasons": null, "author": "soundboyselecta", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1berdf5/data_policy_ethics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1berdf5/data_policy_ethics/", "subreddit_subscribers": 168935, "created_utc": 1710438871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am reading about Snowflake and understand that it does Micro partition which is different from how partitioning is done in regular data warehouses.\n\nLet's say in a regular data warehouse I do\n\n    CREATE TABLE table_name \n    .... \n    ... \n    PARTITIONED BY datestr\n\nIs the equivalent of this in Snowflake\n\n    CREATE TABLE table_name \n    ....\n    ... \n    CLUSTER BY datestr\n\n\n&amp;#x200B;\n\nAs Snowflake does micro partitioning by itself?", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Partitioning in Snowflake vs Regular Data Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beqtw9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710437520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading about Snowflake and understand that it does Micro partition which is different from how partitioning is done in regular data warehouses.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say in a regular data warehouse I do&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE table_name \n.... \n... \nPARTITIONED BY datestr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Is the equivalent of this in Snowflake&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE table_name \n....\n... \nCLUSTER BY datestr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As Snowflake does micro partitioning by itself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1beqtw9", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1beqtw9/partitioning_in_snowflake_vs_regular_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beqtw9/partitioning_in_snowflake_vs_regular_data/", "subreddit_subscribers": 168935, "created_utc": 1710437520.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}