{"kind": "Listing", "data": {"after": "t3_1bew1z0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF's of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.\n\nI work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.", "author_fullname": "t2_t1bnfnc4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the hardest you have ever seen someone work manually?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beltlg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 134, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 134, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710425040.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710424759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF&amp;#39;s of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.&lt;/p&gt;\n\n&lt;p&gt;I work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1beltlg", "is_robot_indexable": true, "report_reasons": null, "author": "bjogc42069", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/", "subreddit_subscribers": 168894, "created_utc": 1710424759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the  project.\n\nThe dataset had over 5000 tables that were used across the board. \n\nMost of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don\u2019t help.\n\nIs there a way to restore it because we might lose the client at this rate? \n\nSample id of a table: \u2018project.Master.table1\u2019", "author_fullname": "t2_5hvalgp8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to recover a deleted dataset from BigQuery? (Urgent)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bekue2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710421925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the  project.&lt;/p&gt;\n\n&lt;p&gt;The dataset had over 5000 tables that were used across the board. &lt;/p&gt;\n\n&lt;p&gt;Most of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don\u2019t help.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to restore it because we might lose the client at this rate? &lt;/p&gt;\n\n&lt;p&gt;Sample id of a table: \u2018project.Master.table1\u2019&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bekue2", "is_robot_indexable": true, "report_reasons": null, "author": "honpra", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/", "subreddit_subscribers": 168894, "created_utc": 1710421925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4ymkgdql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python requests best practices for data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1begg9k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ugbrL_EtU52ai4ErXvB2sZD09kEziLG1QVKocyJn93c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710405308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/python-requests-best-practices-for-data-engineers", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?auto=webp&amp;s=fdae695fd5fe2cee64e1fb4a7ec4483c6443e584", "width": 2912, "height": 1632}, "resolutions": [{"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f31260549e9d62ab9efa9beedc3fb6de9b637b1", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=930ed37cb47a4999c9f98a4e2bdb99166c32c43e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0669238db69cae8dcc36a03fc19af9df9df8b54b", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db93ea8d2e717d1b325f1c01272dad51892ecf90", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b7dcf25cde15dce4e116d09646fd78936820785", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/FJk0f_J5UBiAK7cHHG5V83ONuOtI-OigwrzjNRbEY1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77bd0feb4c744910ad476681f1a10765dff24509", "width": 1080, "height": 605}], "variants": {}, "id": "dW0cXVdRG0AiMhGU9NGc_dayd_CcrOfYkH0KamIBEUQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1begg9k", "is_robot_indexable": true, "report_reasons": null, "author": "SnooBeans3890", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1begg9k/python_requests_best_practices_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/python-requests-best-practices-for-data-engineers", "subreddit_subscribers": 168894, "created_utc": 1710405308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, founder at Latitude here.\n\nWe spent the last 2 years building software for data teams. After many iterations, we've decided to rebuild everything from scratch and open-source it for the entire community.\n\nLatitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.\n\nYou can check out the repo here: [https://github.com/latitude-dev/latitude](https://github.com/latitude-dev/latitude)\n\nWe're actively looking for feedback and contributors. Let me know your thoughts!", "author_fullname": "t2_o4qnw2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Latitude: an open-source web framework to build data apps using SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bej704", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710416485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, founder at Latitude here.&lt;/p&gt;\n\n&lt;p&gt;We spent the last 2 years building software for data teams. After many iterations, we&amp;#39;ve decided to rebuild everything from scratch and open-source it for the entire community.&lt;/p&gt;\n\n&lt;p&gt;Latitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.&lt;/p&gt;\n\n&lt;p&gt;You can check out the repo here: &lt;a href=\"https://github.com/latitude-dev/latitude\"&gt;https://github.com/latitude-dev/latitude&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re actively looking for feedback and contributors. Let me know your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?auto=webp&amp;s=6eebca997ea4d4b55aeb0f760233c0415a0bdf63", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d827756cdde837af4ec07d99725da141bd435f68", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5612dbc3c955b0133b686b2df347ee7160426a8d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d57471adc017d17799740c2e311be154ac893f0c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbdd7b25a13222445010fda16e55f3b9df18a71b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ff1193415b2a1522056645b8cd2f1bfb564810b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/MLNGDePAtO1TbWGm-DIVjUNM2Hh0XLfb52qolN2ITsM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba65964d68ee85a55125b4e94d704ac87a36d2e3", "width": 1080, "height": 540}], "variants": {}, "id": "zRu4yaIX9sS06soWCMlaUqVBLjk_mkvd6UlhzEJIqNY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bej704", "is_robot_indexable": true, "report_reasons": null, "author": "EloquentPickle", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bej704/latitude_an_opensource_web_framework_to_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bej704/latitude_an_opensource_web_framework_to_build/", "subreddit_subscribers": 168894, "created_utc": 1710416485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm doing research on open source data quality tools, and I've found these so far:\n\n1. dbt core\n2. Apache Griffin\n3. Soda Core\n4. Deequ\n5. Tensorflow Data Validation\n6. Moby DQ\n7. Great Expectatons\n\nI've been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I'm missing here? \n\n(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).", "author_fullname": "t2_mc935wf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-Source Data Quality Tools Abound", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bemv7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710427511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing research on open source data quality tools, and I&amp;#39;ve found these so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;dbt core&lt;/li&gt;\n&lt;li&gt;Apache Griffin&lt;/li&gt;\n&lt;li&gt;Soda Core&lt;/li&gt;\n&lt;li&gt;Deequ&lt;/li&gt;\n&lt;li&gt;Tensorflow Data Validation&lt;/li&gt;\n&lt;li&gt;Moby DQ&lt;/li&gt;\n&lt;li&gt;Great Expectatons&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I&amp;#39;m missing here? &lt;/p&gt;\n\n&lt;p&gt;(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bemv7l", "is_robot_indexable": true, "report_reasons": null, "author": "ValidInternetCitizen", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bemv7l/opensource_data_quality_tools_abound/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bemv7l/opensource_data_quality_tools_abound/", "subreddit_subscribers": 168894, "created_utc": 1710427511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you showing visibility if you are part of a non tech management and org in general\nCurrent team structure is of an analyst and data scientist.\nDA and DS roles are front facing/stakeholder facing so they naturally get visibility. In a team structure like this, how do you make sure your work\u2019s visible and adds value", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solo DEs : How are you making sure your work is visible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes4bx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you showing visibility if you are part of a non tech management and org in general\nCurrent team structure is of an analyst and data scientist.\nDA and DS roles are front facing/stakeholder facing so they naturally get visibility. In a team structure like this, how do you make sure your work\u2019s visible and adds value&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes4bx", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes4bx/solo_des_how_are_you_making_sure_your_work_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes4bx/solo_des_how_are_you_making_sure_your_work_is/", "subreddit_subscribers": 168894, "created_utc": 1710440720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Link to blog post here](https://www.y42.com/blog/gitops-for-data-2) \\- feedback welcome!  \n\n\nDo you test all your changes in prod? \ud83e\udd26\u200d\u2642\ufe0f Let's borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.\n\nJust released a blog post explaining it and showing how to make sure you're:\n\n* Always working on production data in an isolated environment (dev/staging/prod environments)\n* Collaborating securely with custom approval flows (GitOps)\n* Preventing faulty builds from going into production (CI/CD)\n\nCheck it out and share your thoughts :)", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitOps for Data - the Write-Audit-Publish (WAP) pattern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1ben94r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710428522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.y42.com/blog/gitops-for-data-2\"&gt;Link to blog post here&lt;/a&gt; - feedback welcome!  &lt;/p&gt;\n\n&lt;p&gt;Do you test all your changes in prod? \ud83e\udd26\u200d\u2642\ufe0f Let&amp;#39;s borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.&lt;/p&gt;\n\n&lt;p&gt;Just released a blog post explaining it and showing how to make sure you&amp;#39;re:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Always working on production data in an isolated environment (dev/staging/prod environments)&lt;/li&gt;\n&lt;li&gt;Collaborating securely with custom approval flows (GitOps)&lt;/li&gt;\n&lt;li&gt;Preventing faulty builds from going into production (CI/CD)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Check it out and share your thoughts :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?auto=webp&amp;s=856e48bb6457d70b7d36615e87154df2ab80dd27", "width": 1456, "height": 816}, "resolutions": [{"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97a8117a8e7bd9d6e64bdfd71298da59725bee8a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c843ee7f0815e03799e7130789dd887ba7e04c3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71d797e7294077f0703e7de512fcdddfdbe6c3a4", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7947c9ce6d187c11827e95b5a17ee667fa4ea581", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d844f43d666e13ad989844094ff4c0fadb5f220", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/v5Gu0_pFZXTUX2HY2-M7_PZj1X_MJZ5sqz7KvRWFqPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e0246833a8cb6ab3a92145d268ba71b2d94d98d", "width": 1080, "height": 605}], "variants": {}, "id": "VPfZqKH060z4qwGQaqgebyoFVaPD4Rohnv811YzZ5WM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1ben94r", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1ben94r/gitops_for_data_the_writeauditpublish_wap_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1ben94r/gitops_for_data_the_writeauditpublish_wap_pattern/", "subreddit_subscribers": 168894, "created_utc": 1710428522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently using Windows Task Scheduler to schedule scripts on a Windows Server but it is too simply.\n\nI need software where I can easily manage and monitor 100+ jobs/scripts, including a good GUI.\n\nWhat would you recommend? It does not have to be free.", "author_fullname": "t2_b54vgte16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you recommend good enterprise job scheduling software for Windows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bevpcl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710449513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently using Windows Task Scheduler to schedule scripts on a Windows Server but it is too simply.&lt;/p&gt;\n\n&lt;p&gt;I need software where I can easily manage and monitor 100+ jobs/scripts, including a good GUI.&lt;/p&gt;\n\n&lt;p&gt;What would you recommend? It does not have to be free.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bevpcl", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Resource9267", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bevpcl/can_you_recommend_good_enterprise_job_scheduling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bevpcl/can_you_recommend_good_enterprise_job_scheduling/", "subreddit_subscribers": 168894, "created_utc": 1710449513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently joined a company that stores all Primary Keys for all tables as UUIDs instead of integers.\n\nAnd for some reason all these columns' dtypes are `VARCHAR(64)`. I know that storing them as `CHAR(36)` would be better.\n\nI've tried to run some benchmarking to backup my decision, but found almost no storage benefit, and minimal query performance improvement (for simple selects, and also for joins).\n\nWould it really make a difference or should I disregard this initiative?\n\nPS: we're using mariadb, and I've benchmarked by creating three identical 1M rows table with varchar64, varchar36, and char36.", "author_fullname": "t2_apdhbvv7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Var vs varchar for uuids", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bernd3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710439544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently joined a company that stores all Primary Keys for all tables as UUIDs instead of integers.&lt;/p&gt;\n\n&lt;p&gt;And for some reason all these columns&amp;#39; dtypes are &lt;code&gt;VARCHAR(64)&lt;/code&gt;. I know that storing them as &lt;code&gt;CHAR(36)&lt;/code&gt; would be better.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to run some benchmarking to backup my decision, but found almost no storage benefit, and minimal query performance improvement (for simple selects, and also for joins).&lt;/p&gt;\n\n&lt;p&gt;Would it really make a difference or should I disregard this initiative?&lt;/p&gt;\n\n&lt;p&gt;PS: we&amp;#39;re using mariadb, and I&amp;#39;ve benchmarked by creating three identical 1M rows table with varchar64, varchar36, and char36.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bernd3", "is_robot_indexable": true, "report_reasons": null, "author": "Agile-Scene-2465", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bernd3/var_vs_varchar_for_uuids/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bernd3/var_vs_varchar_for_uuids/", "subreddit_subscribers": 168894, "created_utc": 1710439544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nAs a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here's the concept:\n\nWe need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:\n\n* Static (meaning they update or add their rows every *n* minutes or hours).\n* Temporally ordered (i.e., Time Series).\n\nIt is highly likely that the observations, which are to be fetched every *n* seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by 'Real Time', I mean that it gets updated every *n* seconds).\n\nAlso, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.\n\nI am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.\n\nThank you in advance", "author_fullname": "t2_c0ghewdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What architecture would you suggest for a Real Time Streaming project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beo2yq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1710430878.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710430666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;As a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here&amp;#39;s the concept:&lt;/p&gt;\n\n&lt;p&gt;We need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Static (meaning they update or add their rows every &lt;em&gt;n&lt;/em&gt; minutes or hours).&lt;/li&gt;\n&lt;li&gt;Temporally ordered (i.e., Time Series).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It is highly likely that the observations, which are to be fetched every &lt;em&gt;n&lt;/em&gt; seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by &amp;#39;Real Time&amp;#39;, I mean that it gets updated every &lt;em&gt;n&lt;/em&gt; seconds).&lt;/p&gt;\n\n&lt;p&gt;Also, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.&lt;/p&gt;\n\n&lt;p&gt;I am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1beo2yq", "is_robot_indexable": true, "report_reasons": null, "author": "hasty-beaver", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beo2yq/what_architecture_would_you_suggest_for_a_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beo2yq/what_architecture_would_you_suggest_for_a_real/", "subreddit_subscribers": 168894, "created_utc": 1710430666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for nothing more than anecdotal data here. I\u2019m looking at these two options and am wondering what people end up paying at scale. If you could provide a bit about your configuration and use-cases, even better \ud83d\ude00", "author_fullname": "t2_5bf8p6qg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much do you pay for your Clickhouse or Apache Pinot implementations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes9nf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710441071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for nothing more than anecdotal data here. I\u2019m looking at these two options and am wondering what people end up paying at scale. If you could provide a bit about your configuration and use-cases, even better \ud83d\ude00&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes9nf", "is_robot_indexable": true, "report_reasons": null, "author": "gorba004", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes9nf/how_much_do_you_pay_for_your_clickhouse_or_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes9nf/how_much_do_you_pay_for_your_clickhouse_or_apache/", "subreddit_subscribers": 168894, "created_utc": 1710441071.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you guys are using DBT, I would like to know if my approach is good or not.\n\nI am using dbt with bigquery. In same gcp project, I have dataset\\_staging and dataset\\_production. In production target, I use dataset\\_production as source. And for local testing or staging, im using dataset\\_staging. The tables in dataset\\_staging is just the copy of tables from dataset\\_production with limited data from [table sampling](https://cloud.google.com/bigquery/docs/table-sampling).\n\nTo use them as source for a dbt model, this is what I am doing in my source yml:\n\n    sources: \n      - name: some_dataset\n        schema:  \"{{ 'dataset_production' if target.name == \"production\" else 'dataset_staging' }}\"\n        database: gcp_project_id\n        tables:\n         - name: table1\n         - name: table2\n\nI am not sure if this is the standard way. Or am I supposed to use dataset\\_production as source even for local testing and staging purpose? The actual goal is not to scan whole table partition during testing, as each partition is more than 5TB.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_dda8zr28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT source for production and testing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bedblt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710392440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you guys are using DBT, I would like to know if my approach is good or not.&lt;/p&gt;\n\n&lt;p&gt;I am using dbt with bigquery. In same gcp project, I have dataset_staging and dataset_production. In production target, I use dataset_production as source. And for local testing or staging, im using dataset_staging. The tables in dataset_staging is just the copy of tables from dataset_production with limited data from &lt;a href=\"https://cloud.google.com/bigquery/docs/table-sampling\"&gt;table sampling&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;To use them as source for a dbt model, this is what I am doing in my source yml:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sources: \n  - name: some_dataset\n    schema:  &amp;quot;{{ &amp;#39;dataset_production&amp;#39; if target.name == &amp;quot;production&amp;quot; else &amp;#39;dataset_staging&amp;#39; }}&amp;quot;\n    database: gcp_project_id\n    tables:\n     - name: table1\n     - name: table2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I am not sure if this is the standard way. Or am I supposed to use dataset_production as source even for local testing and staging purpose? The actual goal is not to scan whole table partition during testing, as each partition is more than 5TB.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?auto=webp&amp;s=b3c1793ddfb0595cba1bbb23fba79360953beb8d", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0329d4207ada0345185e70a97a0ef1f27aec034", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8722bf8052baa4647e96ebeb0d22f50bf529b6ac", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6562c4a330763746058f2250630ec6d3854b2e3d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fff0deae054d2476ac870508887dbbee06d9387c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6a32be275833b4d47802b79f3345f568bd43a4d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97e8d6d94b95697d64482f5fcda32d11814df7b8", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bedblt", "is_robot_indexable": true, "report_reasons": null, "author": "seeker114", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bedblt/dbt_source_for_production_and_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bedblt/dbt_source_for_production_and_testing/", "subreddit_subscribers": 168894, "created_utc": 1710392440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello r/dataengineering,\n\nI've been deeply involved with Apache AGE, a graph database extension that's proving to be a game-changer for data engineering. As a core contributor, I've seen its potential to revolutionize the way we approach data relationships and analysis, especially in environments that traditionally rely on SQL databases.\n\nApache AGE enables the integration of graph database functionalities directly into PostgreSQL, allowing for complex data relationship queries without the need for a separate graph database system. This opens up new avenues for data modeling, querying, and analysis within the familiar PostgreSQL ecosystem.\n\nWhat's particularly exciting for data engineers is AGE's ability to handle complex, interconnected data scenarios more naturally and efficiently than traditional relational databases. This capability is invaluable for applications in social networking, fraud detection, recommendation systems, and more, where understanding the relationships between data points is key.\n\nI'm curious to hear from the community:\n\n* Have you integrated graph database features into your data engineering projects?\n* What challenges and opportunities do you see in adopting Apache AGE for your data workflows?\n\nLet's explore how Apache AGE can fit into our data engineering toolkit, share experiences, and discuss best practices for leveraging graph database capabilities in our projects.\n\nFor a deep dive into the technical workings, documentation, and to join our growing community, visit our [Apache AGE GitHub](https://github.com/apache/age) and [official website](https://age.apache.org/).", "author_fullname": "t2_ru7nh9tc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrating Apache AGE for Enhanced Data Engineering Workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bezk3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710459215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been deeply involved with Apache AGE, a graph database extension that&amp;#39;s proving to be a game-changer for data engineering. As a core contributor, I&amp;#39;ve seen its potential to revolutionize the way we approach data relationships and analysis, especially in environments that traditionally rely on SQL databases.&lt;/p&gt;\n\n&lt;p&gt;Apache AGE enables the integration of graph database functionalities directly into PostgreSQL, allowing for complex data relationship queries without the need for a separate graph database system. This opens up new avenues for data modeling, querying, and analysis within the familiar PostgreSQL ecosystem.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s particularly exciting for data engineers is AGE&amp;#39;s ability to handle complex, interconnected data scenarios more naturally and efficiently than traditional relational databases. This capability is invaluable for applications in social networking, fraud detection, recommendation systems, and more, where understanding the relationships between data points is key.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear from the community:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have you integrated graph database features into your data engineering projects?&lt;/li&gt;\n&lt;li&gt;What challenges and opportunities do you see in adopting Apache AGE for your data workflows?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Let&amp;#39;s explore how Apache AGE can fit into our data engineering toolkit, share experiences, and discuss best practices for leveraging graph database capabilities in our projects.&lt;/p&gt;\n\n&lt;p&gt;For a deep dive into the technical workings, documentation, and to join our growing community, visit our &lt;a href=\"https://github.com/apache/age\"&gt;Apache AGE GitHub&lt;/a&gt; and &lt;a href=\"https://age.apache.org/\"&gt;official website&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?auto=webp&amp;s=15f4c564dfb24842422fd91a488b20fa861c6ced", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=82c39c26b2a4d0e656e8ed1368eef5e209fed31a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fb5dd8df8123d4cb3385c164770dc2f23d769", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0553dd3915d07598c17fea34ac7391746f060d6d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a85dd628f6980076c2a417ab108fd22292ef2789", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=426a3a01c2b2ad2624ca11ec382bfbc174a72df1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/n2lUMmLs9-na29cJEXQgAfXiYgPOvoM0gkzTHrCRS_k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a50d70173c809efbe08ac98a2a0c511a63b56ae", "width": 1080, "height": 540}], "variants": {}, "id": "SGYMkpNq_z1nMJ_lmsYzVDtGYJK2yoPXMSe-sgg4NEQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bezk3y", "is_robot_indexable": true, "report_reasons": null, "author": "Eya_AGE", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bezk3y/integrating_apache_age_for_enhanced_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bezk3y/integrating_apache_age_for_enhanced_data/", "subreddit_subscribers": 168894, "created_utc": 1710459215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive Guide to Optimize Spark Data Workloads | Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1beza3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wSYuyjW9sHHw5_cAGIUIr_nm5ZCApMM3rIrUZvhd8S8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1710458480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/discover/pages/optimize-data-workloads-guide#intro", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?auto=webp&amp;s=1888946a217a566b570f4afbb9c6cda7dae09653", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91375d0ca6cf9fd6186eb43a977e12bb7ac60794", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23b01c6ba19a97c4f454897dc3111177fc2598b1", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fcc73d3f10ecd1b06c22917d735a7eed72814572", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e0dac9968155d941a82fb9777c5cb692978475c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc43e0b2a699224e3c7e07d302187e9dbaed3f56", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/-OEqrGBGWPf9l2OIdvcx61-R1fAvNqGHvA4v_ep-BWc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0eb50fd84794eb10a4bcbffe0ec00c5545ad25f", "width": 1080, "height": 567}], "variants": {}, "id": "RS3lr2nqDF5sgLvhBJ8QfWyKVgI629watKirGMnYEBk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1beza3c", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beza3c/comprehensive_guide_to_optimize_spark_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/discover/pages/optimize-data-workloads-guide#intro", "subreddit_subscribers": 168894, "created_utc": 1710458480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In [this demo](https://github.com/airbytehq/quickstarts/blob/main/pyairbyte_notebooks/PyAirbyte_GA4_Demo.ipynb), we use the Airbyte\u2019s Python library to ingest data from Google Analytics 4, followed by a series of transformations and analysis using pandas.\n\nThis demonstrates how you can do fast data ingestion prototyping of a complex data source, without having to start from scratch.\n\nhttps://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;format=png&amp;auto=webp&amp;s=8756a27a3ca8e790281300ea550e28a04406558e", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting data from GA4 with a few Python statements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "media_metadata": {"z9kyphw6hcoc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8ad7b80115aacdb1951caf26d714fd4d2ef58cc"}, {"y": 133, "x": 216, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40221bbc247a82140698c248eae9832b1753dd82"}, {"y": 198, "x": 320, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bd2331fbe54b67d31cdd95f6a77efe8e6b650ba"}, {"y": 396, "x": 640, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=03a915953cee724d87b1049572d1fa9d805ab48e"}, {"y": 594, "x": 960, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f77889758fc6a82e48015e7aa77519f8f198ff2"}], "s": {"y": 652, "x": 1052, "u": "https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;format=png&amp;auto=webp&amp;s=8756a27a3ca8e790281300ea550e28a04406558e"}, "id": "z9kyphw6hcoc1"}}, "name": "t3_1besj42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zfbIWE_-nFwh8BNnpCmbiQSB3u8wW6btjPhDlYIJhRg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710441695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In &lt;a href=\"https://github.com/airbytehq/quickstarts/blob/main/pyairbyte_notebooks/PyAirbyte_GA4_Demo.ipynb\"&gt;this demo&lt;/a&gt;, we use the Airbyte\u2019s Python library to ingest data from Google Analytics 4, followed by a series of transformations and analysis using pandas.&lt;/p&gt;\n\n&lt;p&gt;This demonstrates how you can do fast data ingestion prototyping of a complex data source, without having to start from scratch.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8756a27a3ca8e790281300ea550e28a04406558e\"&gt;https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8756a27a3ca8e790281300ea550e28a04406558e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1besj42", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1besj42/ingesting_data_from_ga4_with_a_few_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1besj42/ingesting_data_from_ga4_with_a_few_python/", "subreddit_subscribers": 168894, "created_utc": 1710441695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have pyspark structured streaming code which writes to hive using in micro batch mode. I am using foreachbatch. I want to know if that particular batch failed then check point is commited or not. When exactly checkpoint is commited? Is it when function passed to foreachbatch runs sucessfully or just after passing df to that function .", "author_fullname": "t2_63ryq4q7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you help me understand checkpoints in structured streaming ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bes500", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have pyspark structured streaming code which writes to hive using in micro batch mode. I am using foreachbatch. I want to know if that particular batch failed then check point is commited or not. When exactly checkpoint is commited? Is it when function passed to foreachbatch runs sucessfully or just after passing df to that function .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bes500", "is_robot_indexable": true, "report_reasons": null, "author": "stuart_little_03", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bes500/can_you_help_me_understand_checkpoints_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bes500/can_you_help_me_understand_checkpoints_in/", "subreddit_subscribers": 168894, "created_utc": 1710440764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a project that involves database schema migration,  and I'm looking for some guidance and suggestions from the community.  \nThe  goal of the project is to create a tool that can migrate the schema  from one database to another. Initially, we're targeting support for  MySQL, PostgreSQL, and MongoDB. The tool should be able to retrieve the  schema information from the source database, generate the necessary SQL  statements or equivalent commands to recreate the schema in the target  database, execute the schema migration, and perform validation to ensure  the migration was successful.  \nI've started implementing a package  structure for the schema migration process, which includes a retriever  package for retrieving schema information from different databases, a  generator package for generating the migration statements, an executor  package for executing the migration, and a validator package for  validating the migrated schema.  \nHere are my questions:  \nAre there  any existing open-source libraries in Go that already provide schema  migration functionality similar to what I'm trying to achieve? If so,  could you recommend some libraries worth checking out?     \n\n\nIf there aren't any suitable libraries available, I would appreciate  any suggestions or ideas on how to approach the implementation of the  schema migration tool. Are there any best practices, common pitfalls, or  important considerations I should keep in mind?     \n\n\nFor those who have worked on similar projects or have experience  with database schema migration, could you share any insights or lessons  learned that could help guide my implementation?  \nI'm open to any feedback, suggestions, or recommendations that could help me move forward with this project.   \n\nThanks   ", "author_fullname": "t2_s9sv2dxdt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for open-source libraries for database schema migration and suggestions on implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1berurq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710440063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project that involves database schema migration,  and I&amp;#39;m looking for some guidance and suggestions from the community.&lt;br/&gt;\nThe  goal of the project is to create a tool that can migrate the schema  from one database to another. Initially, we&amp;#39;re targeting support for  MySQL, PostgreSQL, and MongoDB. The tool should be able to retrieve the  schema information from the source database, generate the necessary SQL  statements or equivalent commands to recreate the schema in the target  database, execute the schema migration, and perform validation to ensure  the migration was successful.&lt;br/&gt;\nI&amp;#39;ve started implementing a package  structure for the schema migration process, which includes a retriever  package for retrieving schema information from different databases, a  generator package for generating the migration statements, an executor  package for executing the migration, and a validator package for  validating the migrated schema.&lt;br/&gt;\nHere are my questions:&lt;br/&gt;\nAre there  any existing open-source libraries in Go that already provide schema  migration functionality similar to what I&amp;#39;m trying to achieve? If so,  could you recommend some libraries worth checking out?     &lt;/p&gt;\n\n&lt;p&gt;If there aren&amp;#39;t any suitable libraries available, I would appreciate  any suggestions or ideas on how to approach the implementation of the  schema migration tool. Are there any best practices, common pitfalls, or  important considerations I should keep in mind?     &lt;/p&gt;\n\n&lt;p&gt;For those who have worked on similar projects or have experience  with database schema migration, could you share any insights or lessons  learned that could help guide my implementation?&lt;br/&gt;\nI&amp;#39;m open to any feedback, suggestions, or recommendations that could help me move forward with this project.   &lt;/p&gt;\n\n&lt;p&gt;Thanks   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1berurq", "is_robot_indexable": true, "report_reasons": null, "author": "Helpful-Natural6628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1berurq/looking_for_opensource_libraries_for_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1berurq/looking_for_opensource_libraries_for_database/", "subreddit_subscribers": 168894, "created_utc": 1710440063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am reading about Snowflake and understand that it does Micro partition which is different from how partitioning is done in regular data warehouses.\n\nLet's say in a regular data warehouse I do\n\n    CREATE TABLE table_name \n    .... \n    ... \n    PARTITIONED BY datestr\n\nIs the equivalent of this in Snowflake\n\n    CREATE TABLE table_name \n    ....\n    ... \n    CLUSTER BY datestr\n\n\n&amp;#x200B;\n\nAs Snowflake does micro partitioning by itself?", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Partitioning in Snowflake vs Regular Data Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beqtw9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710437520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading about Snowflake and understand that it does Micro partition which is different from how partitioning is done in regular data warehouses.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say in a regular data warehouse I do&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE table_name \n.... \n... \nPARTITIONED BY datestr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Is the equivalent of this in Snowflake&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE table_name \n....\n... \nCLUSTER BY datestr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As Snowflake does micro partitioning by itself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1beqtw9", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1beqtw9/partitioning_in_snowflake_vs_regular_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beqtw9/partitioning_in_snowflake_vs_regular_data/", "subreddit_subscribers": 168894, "created_utc": 1710437520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://blog.twingdata.com/p/identify-unused-columns-in-snowflake](https://blog.twingdata.com/p/identify-unused-columns-in-snowflake)\n\n  \nTLDR: If you're on enterprise Snowflake you can do it via a query. Otherwise there's a simple Python script that uses the sqlglot library to extract the physical columns and compares them against the info schema.  \n", "author_fullname": "t2_bp7tn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Query + Script to identify unused columns in Snowflake and other data warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1belgv1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1710423768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://blog.twingdata.com/p/identify-unused-columns-in-snowflake\"&gt;https://blog.twingdata.com/p/identify-unused-columns-in-snowflake&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TLDR: If you&amp;#39;re on enterprise Snowflake you can do it via a query. Otherwise there&amp;#39;s a simple Python script that uses the sqlglot library to extract the physical columns and compares them against the info schema.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?auto=webp&amp;s=7a9fd4cbac369bdf44c0932caf089f3acf4f0183", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95df4917027026cfacaa016b5e3dba6ac427e5a1", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=798883132e928a3b65e3ac750e786fe2200c8ec5", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3e3d4248e553427fdc1d7972039b469fc247f84", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cfb65a334b8b1c97d00db9a89200396f1cad03f", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/kc11xEfZAOJu-BtArNsgWQ9LLjxJL2VhgTlA7RmWrw4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d219b01314c5718db4f8d8186a6272a0846572c", "width": 960, "height": 562}], "variants": {}, "id": "3CvJ-CSBlahwziE_SAyoY8Vwk75zbSq4MidCUoIDcOI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1belgv1", "is_robot_indexable": true, "report_reasons": null, "author": "dangoldin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1belgv1/query_script_to_identify_unused_columns_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1belgv1/query_script_to_identify_unused_columns_in/", "subreddit_subscribers": 168894, "created_utc": 1710423768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I am working as an Informatica developer with a  total of 7 yr of experience but now I think my career is not progressing as much. Please guide me what should i do next...TIA. ", "author_fullname": "t2_dwbp6nww0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informatica : career help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1beg9e1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710404461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I am working as an Informatica developer with a  total of 7 yr of experience but now I think my career is not progressing as much. Please guide me what should i do next...TIA. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1beg9e1", "is_robot_indexable": true, "report_reasons": null, "author": "amorcita_fishy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1beg9e1/informatica_career_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1beg9e1/informatica_career_help/", "subreddit_subscribers": 168894, "created_utc": 1710404461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I have been working on a personal project regarding data engineering. This project has to do with retrieving steam games prices for different games in different countries, and plotting the price difference in a world map.\n\nThis project is made up of 2 ETLs: One that retrieves price data and the other plots it using a world map.\n\nI would like some feedback on what I couldve done better. I tried using design pattern builder, using abstractions for different external resources and parametrization with Yaml.\n\nThis project uses 3 APIs and an S3 bucket for its internal processing.\n\n[here you have the project link](https://github.com/edseldim/steam_prices_data_engineering)\n\nThis is the final result\n\nhttps://preview.redd.it/139jgrlemeoc1.png?width=824&amp;format=png&amp;auto=webp&amp;s=204769ad503885eef0153d565f9243e5c5f56add", "author_fullname": "t2_kvl7dwa6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Steam Prices ETL (Personal Project)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "media_metadata": {"139jgrlemeoc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=39f8277d318238505cfb88cafb4380090c78efe2"}, {"y": 97, "x": 216, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0514ed0826dbe3dd540c824819241088fe1cb4c4"}, {"y": 144, "x": 320, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f84b4e44b907f687ada00592be8ea06d900ad8f"}, {"y": 289, "x": 640, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=42ecf2b7b21d10c5ef96b59d2218080c4023a1d9"}], "s": {"y": 373, "x": 824, "u": "https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;format=png&amp;auto=webp&amp;s=204769ad503885eef0153d565f9243e5c5f56add"}, "id": "139jgrlemeoc1"}}, "name": "t3_1bf2ntf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/-JOn9CR7ml0xwwpJP5LtTt8IOTeg6FNt_1CnSruPpM4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1710467816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I have been working on a personal project regarding data engineering. This project has to do with retrieving steam games prices for different games in different countries, and plotting the price difference in a world map.&lt;/p&gt;\n\n&lt;p&gt;This project is made up of 2 ETLs: One that retrieves price data and the other plots it using a world map.&lt;/p&gt;\n\n&lt;p&gt;I would like some feedback on what I couldve done better. I tried using design pattern builder, using abstractions for different external resources and parametrization with Yaml.&lt;/p&gt;\n\n&lt;p&gt;This project uses 3 APIs and an S3 bucket for its internal processing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/edseldim/steam_prices_data_engineering\"&gt;here you have the project link&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the final result&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=204769ad503885eef0153d565f9243e5c5f56add\"&gt;https://preview.redd.it/139jgrlemeoc1.png?width=824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=204769ad503885eef0153d565f9243e5c5f56add&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?auto=webp&amp;s=60d7f614a02fe266ad949dba81febe514ae1f75f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f474ec194a50612a3656981c46a5fd9df70dcee", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=214a43d5c075f07697fc5217ed7ec740397633f6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6f4e7b39d92036757989f827e590bd6c21fae2d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=59d55affc4a0264288094c3c6b4aa06c40899d13", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92a6161e090c6063dfa644e4dd0c3546ce422244", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_vGIe_gFUutf7JFnWQYHD4Zokb-jHSrFT5o3j1fMs1Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9ae6c30b86adcad5ef672246cc4997b99783452", "width": 1080, "height": 540}], "variants": {}, "id": "M9rR2_qFAuxZC-bF3f95cChjYc5baZxoQBUTgp2zRSg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bf2ntf", "is_robot_indexable": true, "report_reasons": null, "author": "Confident_Watch8207", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf2ntf/steam_prices_etl_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf2ntf/steam_prices_etl_personal_project/", "subreddit_subscribers": 168894, "created_utc": 1710467816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm primarily a frontend engineer. When I need to work on backend/infra, I often run into the issue of not knowing how to aggregate data in a scaleable way. E.g. pre-computing Reddit's upvote counts so you don't have to aggregate at read time. Aggregating for the top posts feed is easy because it's ok if it's stale by several minutes. However, if you upvote a post and refresh a second later, ideally the post count should include your upvote.\n\nIs there a standard way to do this? I don't know why it's so hard to find discussions around this problem. Afaik some ways of doing this are:\n\n1) When inserting the \"upvote\" record, also increment an \"upvoteCounts\" record\n\nIf you do this in a transaction, it would make the insertions slower. If you don't use a transaction, the upvote counts would go out of sync. I think some companies don't use transactions, but have a job to continuously recompute the counts. Another problem is if you need to mutate multiple aggregations per insertion, it'll be confusing to maintain.\n\n2) Use a trigger\n\nIt's basically the same as 1) with transactions, but handled by the DB instead of the application.\n\n3) Aggregate the event stream\n\nStream changes, then use something like Flink/ksqlDB to aggregate the stream, then stream the aggregation to somewhere for the application to read from. This has much higher latency than 1) with transactions. The benefit is that your aggregations are declarative, so they're easier to reason about.\n\n4) Have a batch job for old aggregations, then handle real-time aggregations in memory\n\nAssuming batch aggregation will take a long time, you can store real-time aggregations in memory. E.g. when someone upvotes, increment a count in memory. When fetching the count, sum the count from memory with the count from the batch job. However, it seems tricky to not double count or miss upvotes.\n\n5) Handling the aggregation at read-time with aggressive caching\n\nThis works when fetching the upvote count for a small set of posts. However, you wouldn't be able to sort posts by upvote count. Also, the max staleness is the cache timeout, which could be a long time.\n\n6) Real-time or incremental materialized views\n\nIdeally, we'd have materialized views that could quickly automatically respond to changes in the source tables. However, afaik the existing real-time materialized view systems are either in memory (e.g. Materialize) or places severe limitations on the source table (e.g. AnalyticDB). Materialize uses too much memory to be scaleable and I've never tried AnalyticDB, but it seems like it's too limiting on the source tables.\n\nAre there any resources to learn more about which approaches large companies use? Have you tried some of these and know first-hand about the pros and cons?", "author_fullname": "t2_4pdki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Different methods for real-time aggregation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bf2jbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710467450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m primarily a frontend engineer. When I need to work on backend/infra, I often run into the issue of not knowing how to aggregate data in a scaleable way. E.g. pre-computing Reddit&amp;#39;s upvote counts so you don&amp;#39;t have to aggregate at read time. Aggregating for the top posts feed is easy because it&amp;#39;s ok if it&amp;#39;s stale by several minutes. However, if you upvote a post and refresh a second later, ideally the post count should include your upvote.&lt;/p&gt;\n\n&lt;p&gt;Is there a standard way to do this? I don&amp;#39;t know why it&amp;#39;s so hard to find discussions around this problem. Afaik some ways of doing this are:&lt;/p&gt;\n\n&lt;p&gt;1) When inserting the &amp;quot;upvote&amp;quot; record, also increment an &amp;quot;upvoteCounts&amp;quot; record&lt;/p&gt;\n\n&lt;p&gt;If you do this in a transaction, it would make the insertions slower. If you don&amp;#39;t use a transaction, the upvote counts would go out of sync. I think some companies don&amp;#39;t use transactions, but have a job to continuously recompute the counts. Another problem is if you need to mutate multiple aggregations per insertion, it&amp;#39;ll be confusing to maintain.&lt;/p&gt;\n\n&lt;p&gt;2) Use a trigger&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s basically the same as 1) with transactions, but handled by the DB instead of the application.&lt;/p&gt;\n\n&lt;p&gt;3) Aggregate the event stream&lt;/p&gt;\n\n&lt;p&gt;Stream changes, then use something like Flink/ksqlDB to aggregate the stream, then stream the aggregation to somewhere for the application to read from. This has much higher latency than 1) with transactions. The benefit is that your aggregations are declarative, so they&amp;#39;re easier to reason about.&lt;/p&gt;\n\n&lt;p&gt;4) Have a batch job for old aggregations, then handle real-time aggregations in memory&lt;/p&gt;\n\n&lt;p&gt;Assuming batch aggregation will take a long time, you can store real-time aggregations in memory. E.g. when someone upvotes, increment a count in memory. When fetching the count, sum the count from memory with the count from the batch job. However, it seems tricky to not double count or miss upvotes.&lt;/p&gt;\n\n&lt;p&gt;5) Handling the aggregation at read-time with aggressive caching&lt;/p&gt;\n\n&lt;p&gt;This works when fetching the upvote count for a small set of posts. However, you wouldn&amp;#39;t be able to sort posts by upvote count. Also, the max staleness is the cache timeout, which could be a long time.&lt;/p&gt;\n\n&lt;p&gt;6) Real-time or incremental materialized views&lt;/p&gt;\n\n&lt;p&gt;Ideally, we&amp;#39;d have materialized views that could quickly automatically respond to changes in the source tables. However, afaik the existing real-time materialized view systems are either in memory (e.g. Materialize) or places severe limitations on the source table (e.g. AnalyticDB). Materialize uses too much memory to be scaleable and I&amp;#39;ve never tried AnalyticDB, but it seems like it&amp;#39;s too limiting on the source tables.&lt;/p&gt;\n\n&lt;p&gt;Are there any resources to learn more about which approaches large companies use? Have you tried some of these and know first-hand about the pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf2jbk", "is_robot_indexable": true, "report_reasons": null, "author": "linksku", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf2jbk/different_methods_for_realtime_aggregation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf2jbk/different_methods_for_realtime_aggregation/", "subreddit_subscribers": 168894, "created_utc": 1710467450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the best data conference you\u2019ve ever participated in?\n\nAlso, check out and register for Free Virtual Subsurface Data Lakehouse Conference May 2nd/3rd at Dremio.com/subsurface.", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Your Favorite Data Conference?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bf15ft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710463523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the best data conference you\u2019ve ever participated in?&lt;/p&gt;\n\n&lt;p&gt;Also, check out and register for Free Virtual Subsurface Data Lakehouse Conference May 2nd/3rd at Dremio.com/subsurface.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf15ft", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf15ft/what_is_your_favorite_data_conference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf15ft/what_is_your_favorite_data_conference/", "subreddit_subscribers": 168894, "created_utc": 1710463523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been using talend open studio for our ETL needs but with that going out of support we are looking for a new tool. \n\nWe have about 8 ETLs that extract data from various sources: powershell scripts, python scripts, MySQL database, sql server database and do some pretty basic transformations into an on prem oracle database. After that we build dashboards in power bi. \n\nI think cloud tooling could be good as there is less maintenance (server patching, app upgrades) but it must be able to write to on prem data warehouse. I\u2019m not against on prem tool if it makes more sense cost wise. \n\nI think cloud tools are usually priced based on computation. \n\nOne big benefit for us would be amount of resources on the tool online. If you run into issues are there lots of forums that have similar issues with resolutions? \n\nAny good paid tools or open source tools you recommend or don\u2019t recommend? ", "author_fullname": "t2_p8gsv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL tool recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bf13x5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710463407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been using talend open studio for our ETL needs but with that going out of support we are looking for a new tool. &lt;/p&gt;\n\n&lt;p&gt;We have about 8 ETLs that extract data from various sources: powershell scripts, python scripts, MySQL database, sql server database and do some pretty basic transformations into an on prem oracle database. After that we build dashboards in power bi. &lt;/p&gt;\n\n&lt;p&gt;I think cloud tooling could be good as there is less maintenance (server patching, app upgrades) but it must be able to write to on prem data warehouse. I\u2019m not against on prem tool if it makes more sense cost wise. &lt;/p&gt;\n\n&lt;p&gt;I think cloud tools are usually priced based on computation. &lt;/p&gt;\n\n&lt;p&gt;One big benefit for us would be amount of resources on the tool online. If you run into issues are there lots of forums that have similar issues with resolutions? &lt;/p&gt;\n\n&lt;p&gt;Any good paid tools or open source tools you recommend or don\u2019t recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bf13x5", "is_robot_indexable": true, "report_reasons": null, "author": "CyclingMonkey", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bf13x5/etl_tool_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bf13x5/etl_tool_recommendations/", "subreddit_subscribers": 168894, "created_utc": 1710463407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At [Multiwoven](https://github.com/Multiwoven/multiwoven), When building our [open-source reverse ETL platform](https://github.com/Multiwoven/multiwoven) on Ruby on Rails &amp; Postgres, as part of the ETL process running SQL queries in background and extracting data from data warehouses was critical and we needed a way to handle long running workloads.\n\nWe wanted something that was durable &amp; that could handle retries, our initial thoughts were to use Rails Sidekiq, but the lack of durability and the need to handle retries made us look for other options, also we didn't want to create a dependency on Redis.\n\nWhen we first did a POC with [Temporal IO](https://github.com/temporalio/temporal), we were amazed by the performance of running long tasks using Postgres as data store, we were able to benchmark by running a workload that tool **3-4 minutes** to complete and processed **100K records** from a data warehouse. Of course, we had to make some changes to our code to make it work with Temporal IO, but it was worth it.it also provided a nice looking UI to monitor the Syncs and the ability to retry failed tasks, which was a big plus for us.\n\nhttps://preview.redd.it/1qemhu4h7doc1.png?width=3824&amp;format=png&amp;auto=webp&amp;s=6deb4d423b3f8b6a7761d6d6e98570f968fafa33\n\n**If anyone looking to build a long running workloads, I would highly recommend Temporal IO, it just works ;)**", "author_fullname": "t2_rtrd3q97v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We used Temporal IO to build data pipelines &amp; it just works ;)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1qemhu4h7doc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/1qemhu4h7doc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e45907839f883b1aab491bf5b757722d769740fd"}, {"y": 130, "x": 216, "u": "https://preview.redd.it/1qemhu4h7doc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=57403bb04a5ddfe6a3ce9fe45f00c6654b1fd5b8"}, {"y": 193, "x": 320, "u": "https://preview.redd.it/1qemhu4h7doc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce9be1c38cf5a9211af7af0d254f9f5a2e330766"}, {"y": 387, "x": 640, "u": "https://preview.redd.it/1qemhu4h7doc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f6f187dee7b9f59d6a49bd4989c8f72a031f08d"}, {"y": 580, "x": 960, "u": "https://preview.redd.it/1qemhu4h7doc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=efe9b510f4b7403664d08f80fc73b0f885e8f7df"}, {"y": 653, "x": 1080, "u": "https://preview.redd.it/1qemhu4h7doc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2025284d9c91e8918b91cbbff70503f8b84d31c"}], "s": {"y": 2314, "x": 3824, "u": "https://preview.redd.it/1qemhu4h7doc1.png?width=3824&amp;format=png&amp;auto=webp&amp;s=6deb4d423b3f8b6a7761d6d6e98570f968fafa33"}, "id": "1qemhu4h7doc1"}}, "name": "t3_1bew1z0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GPam4HiElaD_VZ1y1Ou4cKpaxPgYU68z9bQL4az526o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1710450381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At &lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;Multiwoven&lt;/a&gt;, When building our &lt;a href=\"https://github.com/Multiwoven/multiwoven\"&gt;open-source reverse ETL platform&lt;/a&gt; on Ruby on Rails &amp;amp; Postgres, as part of the ETL process running SQL queries in background and extracting data from data warehouses was critical and we needed a way to handle long running workloads.&lt;/p&gt;\n\n&lt;p&gt;We wanted something that was durable &amp;amp; that could handle retries, our initial thoughts were to use Rails Sidekiq, but the lack of durability and the need to handle retries made us look for other options, also we didn&amp;#39;t want to create a dependency on Redis.&lt;/p&gt;\n\n&lt;p&gt;When we first did a POC with &lt;a href=\"https://github.com/temporalio/temporal\"&gt;Temporal IO&lt;/a&gt;, we were amazed by the performance of running long tasks using Postgres as data store, we were able to benchmark by running a workload that tool &lt;strong&gt;3-4 minutes&lt;/strong&gt; to complete and processed &lt;strong&gt;100K records&lt;/strong&gt; from a data warehouse. Of course, we had to make some changes to our code to make it work with Temporal IO, but it was worth it.it also provided a nice looking UI to monitor the Syncs and the ability to retry failed tasks, which was a big plus for us.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1qemhu4h7doc1.png?width=3824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6deb4d423b3f8b6a7761d6d6e98570f968fafa33\"&gt;https://preview.redd.it/1qemhu4h7doc1.png?width=3824&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6deb4d423b3f8b6a7761d6d6e98570f968fafa33&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;If anyone looking to build a long running workloads, I would highly recommend Temporal IO, it just works ;)&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bew1z0", "is_robot_indexable": true, "report_reasons": null, "author": "nagstler", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bew1z0/we_used_temporal_io_to_build_data_pipelines_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bew1z0/we_used_temporal_io_to_build_data_pipelines_it/", "subreddit_subscribers": 168894, "created_utc": 1710450381.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}