{"kind": "Listing", "data": {"after": null, "dist": 3, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Use the Display API to replace complex Matplotlib code \n\n[ Scikit-learn Visualization Guide: Making Models Speak. ](https://preview.redd.it/pimy1i38a1qc1.png?width=896&amp;format=png&amp;auto=webp&amp;s=f31492e48a5d39171c60d8e96ee8698c670327e7)\n\n# Introduction\n\n In the journey of machine learning, explaining models with visualization is as important as training them. \n\n A good chart can show us what a model is doing in an easy-to-understand way. Here's an example: \n\n[ Decision boundaries of two different generalization performances. ](https://preview.redd.it/3rrwu8rfa1qc1.png?width=863&amp;format=png&amp;auto=webp&amp;s=9425bbe57deb98a854a40fdc0979149637078047)\n\n This graph makes it clear that for the same dataset, the model on the right is better at generalizing. \n\n Most machine learning books prefer to use raw Matplotlib code for visualization, which leads to issues: \n\n1. You have to learn a lot about drawing with Matplotlib.\n2. Plotting code fills up your notebook, making it hard to read.\n3. Sometimes you need third-party libraries, which isn't ideal in business settings.\n\n Good news! Scikit-learn now offers Display classes that let us use methods like from\\_estimator and from\\_predictions to make drawing graphs for different situations much easier. \n\n Curious? Let me show you these cool APIs. \n\n \n\n# Scikit-learn Display API Introduction\n\n### Use utils.discovery.all_displays to find available APIs\n\n Scikit-learn (sklearn) always adds Display APIs in new releases, so it's key to know what's available in your version. \n\n Sklearn's [utils.discovery.all\\_displays](https://scikit-learn.org/stable/modules/generated/sklearn.utils.discovery.all_displays.html?ref=dataleadsfuture.com#sklearn.utils.discovery.all_displays) lets you see which classes you can use. \n\n    from sklearn.utils.discovery import all_displays\n    \n    displays = all_displays()\n    displays\n\n For example, in my Scikit-learn 1.4.0, these classes are available: \n\n    [('CalibrationDisplay', sklearn.calibration.CalibrationDisplay),\n     ('ConfusionMatrixDisplay',\n      sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay),\n     ('DecisionBoundaryDisplay',\n      sklearn.inspection._plot.decision_boundary.DecisionBoundaryDisplay),\n     ('DetCurveDisplay', sklearn.metrics._plot.det_curve.DetCurveDisplay),\n     ('LearningCurveDisplay', sklearn.model_selection._plot.LearningCurveDisplay),\n     ('PartialDependenceDisplay',\n      sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay),\n     ('PrecisionRecallDisplay',\n      sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay),\n     ('PredictionErrorDisplay',\n      sklearn.metrics._plot.regression.PredictionErrorDisplay),\n     ('RocCurveDisplay', sklearn.metrics._plot.roc_curve.RocCurveDisplay),\n     ('ValidationCurveDisplay',\n      sklearn.model_selection._plot.ValidationCurveDisplay)]\n\n### Using inspection.DecisionBoundaryDisplay for decision boundaries\n\n Since we mentioned it, let's start with decision boundaries. \n\n If you use Matplotlib to draw them, it's a hassle: \n\n* Use np.linspace to set coordinate ranges;\n* Use plt.meshgrid to calculate the grid;\n* Use plt.contourf to draw the decision boundary fill;\n* Then use plt.scatter to plot data points.\n\n Now, with  [inspection.DecisionBoundaryDispla](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html?ref=dataleadsfuture.com#sklearn-inspection-decisionboundarydisplay), you can simplify this process: \n\n    from sklearn.inspection import DecisionBoundaryDisplay\n    from sklearn.datasets import load_iris\n    from sklearn.svm import SVC\n    from sklearn.pipeline import make_pipeline\n    from sklearn.preprocessing import StandardScaler\n    import matplotlib.pyplot as plt\n    \n    iris = load_iris(as_frame=True)\n    X = iris.data[['petal length (cm)', 'petal width (cm)']]\n    y = iris.target\n    \n    \n    svc_clf = make_pipeline(StandardScaler(), \n                            SVC(kernel='linear', C=1))\n    svc_clf.fit(X, y)\n    \n    display = DecisionBoundaryDisplay.from_estimator(svc_clf, X, \n                                                     grid_resolution=1000,\n                                                     xlabel=\"Petal length (cm)\",\n                                                     ylabel=\"Petal width (cm)\")\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='w')\n    plt.title(\"Decision Boundary\")\n    plt.show()\n\n See the final effect in the figure: \n\n[ Use DecisionBoundaryDisplay to draw a triple classification model. ](https://preview.redd.it/501h2r27b1qc1.png?width=665&amp;format=png&amp;auto=webp&amp;s=f3614719f48e74c6aabd0d0754c6b08b80f87d02)\n\n Remember, Display can only draw 2D, so make sure your data has only two features or reduced dimensions. \n\n### Using calibration.CalibrationDisplay for probability calibration\n\n To compare classification models, probability calibration curves show how confident models are in their predictions. \n\n Note that  [CalibrationDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibrationDisplay.html?ref=dataleadsfuture.com#sklearn.calibration.CalibrationDisplay) uses the model's  predict\\_proba. If you use a support vector machine, set probability to True: \n\n    from sklearn.calibration import CalibrationDisplay\n    from sklearn.model_selection import train_test_split\n    from sklearn.datasets import make_classification\n    from sklearn.ensemble import HistGradientBoostingClassifier\n    \n    X, y = make_classification(n_samples=1000,\n                               n_classes=2, n_features=5,\n                               random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                        test_size=0.3, random_state=42)\n    proba_clf = make_pipeline(StandardScaler(), \n                              SVC(kernel=\"rbf\", gamma=\"auto\", \n                                  C=10, probability=True))\n    proba_clf.fit(X_train, y_train)\n    \n    CalibrationDisplay.from_estimator(proba_clf, \n                                                X_test, y_test)\n    \n    hist_clf = HistGradientBoostingClassifier()\n    hist_clf.fit(X_train, y_train)\n    \n    ax = plt.gca()\n    CalibrationDisplay.from_estimator(hist_clf,\n                                      X_test, y_test,\n                                      ax=ax)\n    plt.show()\n\n[ Charts drawn by CalibrationDisplay. ](https://preview.redd.it/ovcp8hcgb1qc1.png?width=599&amp;format=png&amp;auto=webp&amp;s=93515b81145abbb70424dcc69bf0cf8569ac4e3a)\n\n### Using metrics.ConfusionMatrixDisplay for confusion matrices\n\n When assessing classification models and dealing with imbalanced data, we look at precision and recall. \n\n These break down into TP, FP, TN, and FN \u2013 a confusion matrix. \n\n To draw one, use  [metrics.ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-confusionmatrixdisplay). It's well-known, so I'll skip the details. \n\n    from sklearn.datasets import fetch_openml\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import ConfusionMatrixDisplay\n    \n    digits = fetch_openml('mnist_784', version=1)\n    X, y = digits.data, digits.target\n    rf_clf = RandomForestClassifier(max_depth=5, random_state=42)\n    rf_clf.fit(X, y)\n    \n    ConfusionMatrixDisplay.from_estimator(rf_clf, X, y)\n    plt.show()\n\n[ Charts drawn with ConfusionMatrixDisplay. ](https://preview.redd.it/tfbcdin8f1qc1.png?width=572&amp;format=png&amp;auto=webp&amp;s=b1eaaf8e085e1cff8bb9c4bd971dade508010390)\n\n### metrics.RocCurveDisplay and metrics.DetCurveDisplay\n\n These two are together because they're often used to evaluate side by side. \n\n [RocCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html?ref=dataleadsfuture.com#sklearn.metrics.RocCurveDisplay) compares TPR and FPR for the model. \n\n For binary classification, you want low FPR and high TPR, so the upper left corner is best. The Roc curve bends towards this corner. \n\n Because the Roc curve stays near the upper left, leaving the lower right empty, it's hard to see model differences. \n\n So, we also use [DetCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DetCurveDisplay.html?ref=dataleadsfuture.com#sklearn.metrics.DetCurveDisplay) to draw a Det curve with FNR and FPR. It uses more space, making it clearer than the Roc curve. \n\n The perfect point for a Det curve is the lower left corner. \n\n    from sklearn.metrics import RocCurveDisplay\n    from sklearn.metrics import DetCurveDisplay\n    \n    X, y = make_classification(n_samples=10_000, n_features=5,\n                               n_classes=2, n_informative=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                        test_size=0.3, random_state=42,\n                                                        stratify=y)\n    \n    \n    classifiers = {\n        \"SVC\": make_pipeline(StandardScaler(), SVC(kernel=\"linear\", C=0.1, random_state=42)),\n        \"Random Forest\": RandomForestClassifier(max_depth=5, random_state=42)\n    }\n    \n    fig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(10, 4))\n    for name, clf in classifiers.items():\n        clf.fit(X_train, y_train)\n        \n        RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=name)\n        DetCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_det, name=name)\n\n[ Comparison Chart of RocCurveDisplay and DetCurveDisplay. ](https://preview.redd.it/odavl54yh1qc1.png?width=884&amp;format=png&amp;auto=webp&amp;s=6408e319b099ce12c3eff70e788ed0224558e099)\n\n### Using metrics.PrecisionRecallDisplay to adjust thresholds\n\n With imbalanced data, you might want to shift recall and precision. \n\n* For email fraud, you want high precision.\n* For disease screening, you want high recall to catch more cases.\n\n You can adjust the threshold, but what's the right amount? \n\n Here, [metrics.PrecisionRecallDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-precisionrecalldisplay) can help. \n\n    from xgboost import XGBClassifier\n    from sklearn.datasets import load_wine\n    from sklearn.metrics import PrecisionRecallDisplay\n    \n    wine = load_wine()\n    X, y = wine.data[wine.target&lt;=1], wine.target[wine.target&lt;=1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n                                                        stratify=y, random_state=42)\n    \n    xgb_clf = XGBClassifier()\n    xgb_clf.fit(X_train, y_train)\n    \n    PrecisionRecallDisplay.from_estimator(xgb_clf, X_test, y_test)\n    plt.show()\n\n[ Charting xgboost model evaluation using PrecisionRecallDisplay.  ](https://preview.redd.it/8giy5tr8i1qc1.png?width=489&amp;format=png&amp;auto=webp&amp;s=06b96373c182f515a655d6f4029f44982c87b05d)\n\n This shows that models following Scikit-learn's design can be drawn, like xgboost here. Handy, right? \n\n### Using metrics.PredictionErrorDisplay for regression models\n\n We've talked about classification, now let's talk about regression. \n\n Scikit-learn's [metrics.PredictionErrorDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PredictionErrorDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-predictionerrordisplay) helps assess regression models. \n\n    from sklearn.svm import SVR\n    from sklearn.metrics import PredictionErrorDisplay\n    \n    rng = np.random.default_rng(42)\n    X = rng.random(size=(200, 2)) * 10\n    y = X[:, 0]**2 + 5 * X[:, 1] + 10 + rng.normal(loc=0.0, scale=0.1, size=(200,))\n    \n    reg = make_pipeline(StandardScaler(), SVR(kernel='linear', C=10))\n    reg.fit(X, y)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n    PredictionErrorDisplay.from_estimator(reg, X, y, ax=axes[0], kind=\"actual_vs_predicted\")\n    PredictionErrorDisplay.from_estimator(reg, X, y, ax=axes[1], kind=\"residual_vs_predicted\")\n    plt.show()\n\n[Two charts were drawn by PredictionErrorDisplay.](https://preview.redd.it/9uxuobuti1qc1.png?width=764&amp;format=png&amp;auto=webp&amp;s=7db276b2a7d637cf53887e4cebe87b9a23593b9b)\n\n As shown, it can draw two kinds of graphs. The left shows predicted vs. actual values \u2013 good for linear regression. \n\n However, not all data is perfectly linear. For that, use the right graph. \n\n It compares real vs. predicted differences, a residuals plot. \n\n This plot's banana shape suggests our data might not fit linear regression. \n\n Switching from a linear to an rbf kernel can help. \n\n    reg = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=10))\n\n[ A visual demonstration of the improved model performance.  ](https://preview.redd.it/fsu1yqg0j1qc1.png?width=763&amp;format=png&amp;auto=webp&amp;s=860447d0c8a1c0c38539f5fac16998ed928c8714)\n\n See, with rbf, the residual plot looks better. \n\n### Using model_selection.LearningCurveDisplay for learning curves\n\n After assessing performance, let's look at optimization with [LearningCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LearningCurveDisplay.html?ref=dataleadsfuture.com#sklearn.model_selection.LearningCurveDisplay).\n\n  First up, learning curves \u2013 how well the model generalizes with different training and testing data, and if it suffers from variance or bias. \n\n As shown below, we compare a DecisionTreeClassifier and a GradientBoostingClassifier to see how they do as training data changes. \n\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import GradientBoostingClassifier\n    from sklearn.model_selection import LearningCurveDisplay\n    \n    X, y = make_classification(n_samples=1000, n_classes=2, n_features=10,\n                               n_informative=2, n_redundant=0, n_repeated=0)\n    \n    tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n    gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=3, tol=1e-3)\n    \n    train_sizes = np.linspace(0.4, 1.0, 10)\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    LearningCurveDisplay.from_estimator(tree_clf, X, y,\n                                        train_sizes=train_sizes,\n                                        ax=axes[0],\n                                        scoring='accuracy')\n    axes[0].set_title('DecisionTreeClassifier')\n    LearningCurveDisplay.from_estimator(gb_clf, X, y,\n                                        train_sizes=train_sizes,\n                                        ax=axes[1],\n                                        scoring='accuracy')\n    axes[1].set_title('GradientBoostingClassifier')\n    plt.show()\n\n[ Comparison of the learning curve of two different models. ](https://preview.redd.it/2ljyfl9aj1qc1.png?width=896&amp;format=png&amp;auto=webp&amp;s=7d7af8bd8a4295c54f6f5c3ce41caf2fd2ddb0c1)\n\n The graph shows that although the tree-based GradientBoostingClassifier maintains good accuracy on the training data, its generalization capability on test data does not have a significant advantage over the DecisionTreeClassifier. \n\n### Using model_selection.ValidationCurveDisplay for visualizing parameter tuning\n\n So, for models that don't generalize well, you might try adjusting the model's regularization parameters to tweak its performance. \n\n The traditional approach is to use tools like GridSearchCV or Optuna to tune the model, but these methods only give you the overall best-performing model and the tuning process is not very intuitive. \n\n For scenarios where you want to adjust a specific parameter to test its effect on the model, I recommend using [model\\_selection.ValidationCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ValidationCurveDisplay.html?ref=dataleadsfuture.com#sklearn.model_selection.ValidationCurveDisplay) to visualize how the model performs as the parameter changes. \n\n    from sklearn.model_selection import ValidationCurveDisplay\n    from sklearn.linear_model import LogisticRegression\n    \n    param_name, param_range = \"C\", np.logspace(-8, 3, 10)\n    lr_clf = LogisticRegression()\n    \n    ValidationCurveDisplay.from_estimator(lr_clf, X, y,\n                                          param_name=param_name,\n                                          param_range=param_range,\n                                          scoring='f1_weighted',\n                                          cv=5, n_jobs=-1)\n    plt.show()\n\n[ Fine-tuning of model parameters plotted with ValidationCurveDisplay. ](https://preview.redd.it/5dcgo35jj1qc1.png?width=634&amp;format=png&amp;auto=webp&amp;s=ed3d587e59d31902951ba0c9869ccd2bc39476d4)\n\n&amp;#x200B;\n\n# Some regrets\n\n After trying out all these Displays, I must admit some regrets: \n\n \n\n* The biggest one is that most of these APIs lack detailed tutorials, which is probably why they're not well-known compared to Scikit-learn's thorough documentation.\n* These APIs are scattered across various packages, making it hard to reference them from a single place.\n* The code is still pretty basic. You often need to pair it with Matplotlib's APIs to get the job done. A typical example is DecisionBoundaryDisplay  \n, where after plotting the decision boundary, you still need Matplotlib to plot the data distribution.\n* They're hard to extend. Besides a few methods validating parameters, it's tough to simplify my model visualization process with tools or methods; I end up rewriting a lot.\n\n I hope these APIs get more attention, and as versions upgrade, visualization APIs become even easier to use. \n\n \n\n# Conclusion\n\n In the journey of machine learning, explaining models with visualization is as important as training them. \n\n This article introduced various plotting APIs in the current version of scikit-learn. \n\n With these APIs, you can simplify some Matplotlib code, ease your learning curve, and streamline your model evaluation process. \n\n Due to length, I didn't expand on each API. If interested, you can check the [official documentation](https://scikit-learn.org/stable/visualizations.html?ref=dataleadsfuture.com) for more details. \n\n Now it's your turn. What are your expectations for visualizing machine learning methods? Feel free to leave a comment and discuss. \n\n This article was originally published on my personal blog [Data Leads Future](https://www.dataleadsfuture.com/scikit-learn-visualization-guide-making-models-speak/). ", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scikit-learn Visualization Guide: Making Models Speak", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ovcp8hcgb1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 78, "x": 108, "u": "https://preview.redd.it/ovcp8hcgb1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=17e5bbedd8449c6872603b7205956e8ca9ef2341"}, {"y": 156, "x": 216, "u": "https://preview.redd.it/ovcp8hcgb1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a90a81269c30887fc270dd93e3877986a81d318"}, {"y": 232, "x": 320, "u": "https://preview.redd.it/ovcp8hcgb1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3a1b015ff1f3724e4322f9609ec6290e966e188"}], "s": {"y": 435, "x": 599, "u": "https://preview.redd.it/ovcp8hcgb1qc1.png?width=599&amp;format=png&amp;auto=webp&amp;s=93515b81145abbb70424dcc69bf0cf8569ac4e3a"}, "id": "ovcp8hcgb1qc1"}, "8giy5tr8i1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 95, "x": 108, "u": "https://preview.redd.it/8giy5tr8i1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbe30a8457b499c8972f4680d16990b0bf63a3ac"}, {"y": 190, "x": 216, "u": "https://preview.redd.it/8giy5tr8i1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6ba970198a4e5d73665786efc9c98e25bd35164"}, {"y": 282, "x": 320, "u": "https://preview.redd.it/8giy5tr8i1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=edf4bd493a0846971df6083bbb590235b176b07c"}], "s": {"y": 431, "x": 489, "u": "https://preview.redd.it/8giy5tr8i1qc1.png?width=489&amp;format=png&amp;auto=webp&amp;s=06b96373c182f515a655d6f4029f44982c87b05d"}, "id": "8giy5tr8i1qc1"}, "9uxuobuti1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/9uxuobuti1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e5d555cb3aac1b1a22ea3cdcd571efa85da7986"}, {"y": 106, "x": 216, "u": "https://preview.redd.it/9uxuobuti1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=91fd5dc6646d73e07c8750fedb19e8daa78ea5bc"}, {"y": 157, "x": 320, "u": "https://preview.redd.it/9uxuobuti1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0500d709190faa5edb0ae95210cb70b1be5c79c5"}, {"y": 315, "x": 640, "u": "https://preview.redd.it/9uxuobuti1qc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=911a9aad120b45310cd9fd0e93ef109b81a5a893"}], "s": {"y": 377, "x": 764, "u": "https://preview.redd.it/9uxuobuti1qc1.png?width=764&amp;format=png&amp;auto=webp&amp;s=7db276b2a7d637cf53887e4cebe87b9a23593b9b"}, "id": "9uxuobuti1qc1"}, "pimy1i38a1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 88, "x": 108, "u": "https://preview.redd.it/pimy1i38a1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a6ac4feaa64b4dd5e2859513075d6c5ab054f8c"}, {"y": 177, "x": 216, "u": "https://preview.redd.it/pimy1i38a1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d3d05150a0439b3ea38338623daef7e343a0a8c"}, {"y": 262, "x": 320, "u": "https://preview.redd.it/pimy1i38a1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef14bb8001993085686cf2a95b4e994bd29bfcdc"}, {"y": 525, "x": 640, "u": "https://preview.redd.it/pimy1i38a1qc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac69b45e96e6717501d27ec0aca893310501979b"}], "s": {"y": 735, "x": 896, "u": "https://preview.redd.it/pimy1i38a1qc1.png?width=896&amp;format=png&amp;auto=webp&amp;s=f31492e48a5d39171c60d8e96ee8698c670327e7"}, "id": "pimy1i38a1qc1"}, "odavl54yh1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/odavl54yh1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3845dbc6d8bbf8287e0132d6b50d8d48b945aafc"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/odavl54yh1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1af8d49b75fdd6eaf7ffd4622dbced319f8a5c17"}, {"y": 140, "x": 320, "u": "https://preview.redd.it/odavl54yh1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4a3f38e1b29cb99fad388c0f7b01e96d57566d0"}, {"y": 280, "x": 640, "u": "https://preview.redd.it/odavl54yh1qc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cbcceaa2aefe675ddbfaea04f36643909e22c5d"}], "s": {"y": 388, "x": 884, "u": "https://preview.redd.it/odavl54yh1qc1.png?width=884&amp;format=png&amp;auto=webp&amp;s=6408e319b099ce12c3eff70e788ed0224558e099"}, "id": "odavl54yh1qc1"}, "3rrwu8rfa1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 44, "x": 108, "u": "https://preview.redd.it/3rrwu8rfa1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e76e8b456f86b6f6b45c4af40849a91f3ea17c29"}, {"y": 88, "x": 216, "u": "https://preview.redd.it/3rrwu8rfa1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=242ebdeec06a6537144aef46f51d9cfda151989d"}, {"y": 130, "x": 320, "u": "https://preview.redd.it/3rrwu8rfa1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=161ca83b6c755ff4771c6c5e44989bba051d6c53"}, {"y": 261, "x": 640, "u": "https://preview.redd.it/3rrwu8rfa1qc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad9710895464e41ad67dd1f5dc1cf8018ccd9090"}], "s": {"y": 353, "x": 863, "u": "https://preview.redd.it/3rrwu8rfa1qc1.png?width=863&amp;format=png&amp;auto=webp&amp;s=9425bbe57deb98a854a40fdc0979149637078047"}, "id": "3rrwu8rfa1qc1"}, "fsu1yqg0j1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/fsu1yqg0j1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=408a87cfcaffd94052dc858ba4b4ad30bf45cf41"}, {"y": 105, "x": 216, "u": "https://preview.redd.it/fsu1yqg0j1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e575c17eec01b8c7e4b820505b46e9b7641f4fc"}, {"y": 156, "x": 320, "u": "https://preview.redd.it/fsu1yqg0j1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=15adb2b52941846249030871739fa223b6e8ca73"}, {"y": 312, "x": 640, "u": "https://preview.redd.it/fsu1yqg0j1qc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e322e8dd9b2a2e669e99b7a4b8faec7b2c10c0e"}], "s": {"y": 372, "x": 763, "u": "https://preview.redd.it/fsu1yqg0j1qc1.png?width=763&amp;format=png&amp;auto=webp&amp;s=860447d0c8a1c0c38539f5fac16998ed928c8714"}, "id": "fsu1yqg0j1qc1"}, "2ljyfl9aj1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/2ljyfl9aj1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d09fcea131f1bc249cb07aac28ff658fc7c8610f"}, {"y": 101, "x": 216, "u": "https://preview.redd.it/2ljyfl9aj1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df0e0e78d1e2fea99f75ed225fb33d5675e20647"}, {"y": 149, "x": 320, "u": "https://preview.redd.it/2ljyfl9aj1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cddd5154eba0db85971c96f2369226cc0abfa03"}, {"y": 299, "x": 640, "u": "https://preview.redd.it/2ljyfl9aj1qc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=55cd533b10a15b2576fbd1ebcf5ec79b59c4a847"}], "s": {"y": 419, "x": 896, "u": "https://preview.redd.it/2ljyfl9aj1qc1.png?width=896&amp;format=png&amp;auto=webp&amp;s=7d7af8bd8a4295c54f6f5c3ce41caf2fd2ddb0c1"}, "id": "2ljyfl9aj1qc1"}, "5dcgo35jj1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/5dcgo35jj1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbc2b8444b5dc29efae71665816ee2c5edf74b91"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/5dcgo35jj1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f543309af4ab203d49174903f8204b85275160c8"}, {"y": 225, "x": 320, "u": "https://preview.redd.it/5dcgo35jj1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4afce6dec21ea1ba1b9b645b7ec0529eb573da29"}], "s": {"y": 447, "x": 634, "u": "https://preview.redd.it/5dcgo35jj1qc1.png?width=634&amp;format=png&amp;auto=webp&amp;s=ed3d587e59d31902951ba0c9869ccd2bc39476d4"}, "id": "5dcgo35jj1qc1"}, "501h2r27b1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/501h2r27b1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0a07f25711e39776d73bf8cecaf346eb507185f"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/501h2r27b1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=836593f116f7f3fd772d1a9d9e6bb715eeaffc40"}, {"y": 225, "x": 320, "u": "https://preview.redd.it/501h2r27b1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3004ad8c7a9b97e43dc222d0a87beefde83005b9"}, {"y": 451, "x": 640, "u": "https://preview.redd.it/501h2r27b1qc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7c0c66d5e2de4682010fa4cf907e3402f040e7e6"}], "s": {"y": 469, "x": 665, "u": "https://preview.redd.it/501h2r27b1qc1.png?width=665&amp;format=png&amp;auto=webp&amp;s=f3614719f48e74c6aabd0d0754c6b08b80f87d02"}, "id": "501h2r27b1qc1"}, "tfbcdin8f1qc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/tfbcdin8f1qc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a17aa5e717bea0006aae8c185be771783c73e2c"}, {"y": 163, "x": 216, "u": "https://preview.redd.it/tfbcdin8f1qc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3398c424460230ee278eb93092c67a8d6f21476"}, {"y": 242, "x": 320, "u": "https://preview.redd.it/tfbcdin8f1qc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=54dff025a646b2c23da010b04100c58527ab3a2a"}], "s": {"y": 433, "x": 572, "u": "https://preview.redd.it/tfbcdin8f1qc1.png?width=572&amp;format=png&amp;auto=webp&amp;s=b1eaaf8e085e1cff8bb9c4bd971dade508010390"}, "id": "tfbcdin8f1qc1"}}, "name": "t3_1bln0ix", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 170, "domain": "self.datascience", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_9r8ft2a0", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 170, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ja6aneliTPCrCRkhI6ycj6HxvoVRMj6m23TVshfLXAQ.jpg", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711181199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Use the Display API to replace complex Matplotlib code &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pimy1i38a1qc1.png?width=896&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f31492e48a5d39171c60d8e96ee8698c670327e7\"&gt; Scikit-learn Visualization Guide: Making Models Speak. &lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Introduction&lt;/h1&gt;\n\n&lt;p&gt;In the journey of machine learning, explaining models with visualization is as important as training them. &lt;/p&gt;\n\n&lt;p&gt;A good chart can show us what a model is doing in an easy-to-understand way. Here&amp;#39;s an example: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3rrwu8rfa1qc1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9425bbe57deb98a854a40fdc0979149637078047\"&gt; Decision boundaries of two different generalization performances. &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This graph makes it clear that for the same dataset, the model on the right is better at generalizing. &lt;/p&gt;\n\n&lt;p&gt;Most machine learning books prefer to use raw Matplotlib code for visualization, which leads to issues: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You have to learn a lot about drawing with Matplotlib.&lt;/li&gt;\n&lt;li&gt;Plotting code fills up your notebook, making it hard to read.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Sometimes you need third-party libraries, which isn&amp;#39;t ideal in business settings.&lt;/p&gt;\n\n&lt;p&gt;Good news! Scikit-learn now offers Display classes that let us use methods like from_estimator and from_predictions to make drawing graphs for different situations much easier. &lt;/p&gt;\n\n&lt;p&gt;Curious? Let me show you these cool APIs. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Scikit-learn Display API Introduction&lt;/h1&gt;\n\n&lt;h3&gt;Use utils.discovery.all_displays to find available APIs&lt;/h3&gt;\n\n&lt;p&gt;Scikit-learn (sklearn) always adds Display APIs in new releases, so it&amp;#39;s key to know what&amp;#39;s available in your version. &lt;/p&gt;\n\n&lt;p&gt;Sklearn&amp;#39;s &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.utils.discovery.all_displays.html?ref=dataleadsfuture.com#sklearn.utils.discovery.all_displays\"&gt;utils.discovery.all_displays&lt;/a&gt; lets you see which classes you can use. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.utils.discovery import all_displays\n\ndisplays = all_displays()\ndisplays\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;For example, in my Scikit-learn 1.4.0, these classes are available: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[(&amp;#39;CalibrationDisplay&amp;#39;, sklearn.calibration.CalibrationDisplay),\n (&amp;#39;ConfusionMatrixDisplay&amp;#39;,\n  sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay),\n (&amp;#39;DecisionBoundaryDisplay&amp;#39;,\n  sklearn.inspection._plot.decision_boundary.DecisionBoundaryDisplay),\n (&amp;#39;DetCurveDisplay&amp;#39;, sklearn.metrics._plot.det_curve.DetCurveDisplay),\n (&amp;#39;LearningCurveDisplay&amp;#39;, sklearn.model_selection._plot.LearningCurveDisplay),\n (&amp;#39;PartialDependenceDisplay&amp;#39;,\n  sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay),\n (&amp;#39;PrecisionRecallDisplay&amp;#39;,\n  sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay),\n (&amp;#39;PredictionErrorDisplay&amp;#39;,\n  sklearn.metrics._plot.regression.PredictionErrorDisplay),\n (&amp;#39;RocCurveDisplay&amp;#39;, sklearn.metrics._plot.roc_curve.RocCurveDisplay),\n (&amp;#39;ValidationCurveDisplay&amp;#39;,\n  sklearn.model_selection._plot.ValidationCurveDisplay)]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Using inspection.DecisionBoundaryDisplay for decision boundaries&lt;/h3&gt;\n\n&lt;p&gt;Since we mentioned it, let&amp;#39;s start with decision boundaries. &lt;/p&gt;\n\n&lt;p&gt;If you use Matplotlib to draw them, it&amp;#39;s a hassle: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use np.linspace to set coordinate ranges;&lt;/li&gt;\n&lt;li&gt;Use plt.meshgrid to calculate the grid;&lt;/li&gt;\n&lt;li&gt;Use plt.contourf to draw the decision boundary fill;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Then use plt.scatter to plot data points.&lt;/p&gt;\n\n&lt;p&gt;Now, with  &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html?ref=dataleadsfuture.com#sklearn-inspection-decisionboundarydisplay\"&gt;inspection.DecisionBoundaryDispla&lt;/a&gt;, you can simplify this process: &lt;/p&gt;\n\n&lt;p&gt;from sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.datasets import load_iris\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt&lt;/p&gt;\n\n&lt;p&gt;iris = load_iris(as_frame=True)\nX = iris.data[[&amp;#39;petal length (cm)&amp;#39;, &amp;#39;petal width (cm)&amp;#39;]]\ny = iris.target&lt;/p&gt;\n\n&lt;p&gt;svc_clf = make_pipeline(StandardScaler(), \n                        SVC(kernel=&amp;#39;linear&amp;#39;, C=1))\nsvc_clf.fit(X, y)&lt;/p&gt;\n\n&lt;p&gt;display = DecisionBoundaryDisplay.from_estimator(svc_clf, X, \n                                                 grid_resolution=1000,\n                                                 xlabel=&amp;quot;Petal length (cm)&amp;quot;,\n                                                 ylabel=&amp;quot;Petal width (cm)&amp;quot;)\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors=&amp;#39;w&amp;#39;)\nplt.title(&amp;quot;Decision Boundary&amp;quot;)\nplt.show()&lt;/p&gt;\n\n&lt;p&gt;See the final effect in the figure: &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/501h2r27b1qc1.png?width=665&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f3614719f48e74c6aabd0d0754c6b08b80f87d02\"&gt; Use DecisionBoundaryDisplay to draw a triple classification model. &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Remember, Display can only draw 2D, so make sure your data has only two features or reduced dimensions. &lt;/p&gt;\n\n&lt;h3&gt;Using calibration.CalibrationDisplay for probability calibration&lt;/h3&gt;\n\n&lt;p&gt;To compare classification models, probability calibration curves show how confident models are in their predictions. &lt;/p&gt;\n\n&lt;p&gt;Note that  &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibrationDisplay.html?ref=dataleadsfuture.com#sklearn.calibration.CalibrationDisplay\"&gt;CalibrationDisplay&lt;/a&gt; uses the model&amp;#39;s  predict_proba. If you use a support vector machine, set probability to True: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.calibration import CalibrationDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nX, y = make_classification(n_samples=1000,\n                           n_classes=2, n_features=5,\n                           random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, random_state=42)\nproba_clf = make_pipeline(StandardScaler(), \n                          SVC(kernel=&amp;quot;rbf&amp;quot;, gamma=&amp;quot;auto&amp;quot;, \n                              C=10, probability=True))\nproba_clf.fit(X_train, y_train)\n\nCalibrationDisplay.from_estimator(proba_clf, \n                                            X_test, y_test)\n\nhist_clf = HistGradientBoostingClassifier()\nhist_clf.fit(X_train, y_train)\n\nax = plt.gca()\nCalibrationDisplay.from_estimator(hist_clf,\n                                  X_test, y_test,\n                                  ax=ax)\nplt.show()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ovcp8hcgb1qc1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93515b81145abbb70424dcc69bf0cf8569ac4e3a\"&gt; Charts drawn by CalibrationDisplay. &lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Using metrics.ConfusionMatrixDisplay for confusion matrices&lt;/h3&gt;\n\n&lt;p&gt;When assessing classification models and dealing with imbalanced data, we look at precision and recall. &lt;/p&gt;\n\n&lt;p&gt;These break down into TP, FP, TN, and FN \u2013 a confusion matrix. &lt;/p&gt;\n\n&lt;p&gt;To draw one, use  &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-confusionmatrixdisplay\"&gt;metrics.ConfusionMatrixDisplay&lt;/a&gt;. It&amp;#39;s well-known, so I&amp;#39;ll skip the details. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndigits = fetch_openml(&amp;#39;mnist_784&amp;#39;, version=1)\nX, y = digits.data, digits.target\nrf_clf = RandomForestClassifier(max_depth=5, random_state=42)\nrf_clf.fit(X, y)\n\nConfusionMatrixDisplay.from_estimator(rf_clf, X, y)\nplt.show()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tfbcdin8f1qc1.png?width=572&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1eaaf8e085e1cff8bb9c4bd971dade508010390\"&gt; Charts drawn with ConfusionMatrixDisplay. &lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;metrics.RocCurveDisplay and metrics.DetCurveDisplay&lt;/h3&gt;\n\n&lt;p&gt;These two are together because they&amp;#39;re often used to evaluate side by side. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html?ref=dataleadsfuture.com#sklearn.metrics.RocCurveDisplay\"&gt;RocCurveDisplay&lt;/a&gt; compares TPR and FPR for the model. &lt;/p&gt;\n\n&lt;p&gt;For binary classification, you want low FPR and high TPR, so the upper left corner is best. The Roc curve bends towards this corner. &lt;/p&gt;\n\n&lt;p&gt;Because the Roc curve stays near the upper left, leaving the lower right empty, it&amp;#39;s hard to see model differences. &lt;/p&gt;\n\n&lt;p&gt;So, we also use &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DetCurveDisplay.html?ref=dataleadsfuture.com#sklearn.metrics.DetCurveDisplay\"&gt;DetCurveDisplay&lt;/a&gt; to draw a Det curve with FNR and FPR. It uses more space, making it clearer than the Roc curve. &lt;/p&gt;\n\n&lt;p&gt;The perfect point for a Det curve is the lower left corner. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.metrics import RocCurveDisplay\nfrom sklearn.metrics import DetCurveDisplay\n\nX, y = make_classification(n_samples=10_000, n_features=5,\n                           n_classes=2, n_informative=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, random_state=42,\n                                                    stratify=y)\n\n\nclassifiers = {\n    &amp;quot;SVC&amp;quot;: make_pipeline(StandardScaler(), SVC(kernel=&amp;quot;linear&amp;quot;, C=0.1, random_state=42)),\n    &amp;quot;Random Forest&amp;quot;: RandomForestClassifier(max_depth=5, random_state=42)\n}\n\nfig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(10, 4))\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n\n    RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=name)\n    DetCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_det, name=name)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/odavl54yh1qc1.png?width=884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6408e319b099ce12c3eff70e788ed0224558e099\"&gt; Comparison Chart of RocCurveDisplay and DetCurveDisplay. &lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Using metrics.PrecisionRecallDisplay to adjust thresholds&lt;/h3&gt;\n\n&lt;p&gt;With imbalanced data, you might want to shift recall and precision. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;For email fraud, you want high precision.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For disease screening, you want high recall to catch more cases.&lt;/p&gt;\n\n&lt;p&gt;You can adjust the threshold, but what&amp;#39;s the right amount? &lt;/p&gt;\n\n&lt;p&gt;Here, &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-precisionrecalldisplay\"&gt;metrics.PrecisionRecallDisplay&lt;/a&gt; can help. &lt;/p&gt;\n\n&lt;p&gt;from xgboost import XGBClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.metrics import PrecisionRecallDisplay&lt;/p&gt;\n\n&lt;p&gt;wine = load_wine()\nX, y = wine.data[wine.target&amp;lt;=1], wine.target[wine.target&amp;lt;=1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n                                                    stratify=y, random_state=42)&lt;/p&gt;\n\n&lt;p&gt;xgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)&lt;/p&gt;\n\n&lt;p&gt;PrecisionRecallDisplay.from_estimator(xgb_clf, X_test, y_test)\nplt.show()&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8giy5tr8i1qc1.png?width=489&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06b96373c182f515a655d6f4029f44982c87b05d\"&gt; Charting xgboost model evaluation using PrecisionRecallDisplay.  &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This shows that models following Scikit-learn&amp;#39;s design can be drawn, like xgboost here. Handy, right? &lt;/p&gt;\n\n&lt;h3&gt;Using metrics.PredictionErrorDisplay for regression models&lt;/h3&gt;\n\n&lt;p&gt;We&amp;#39;ve talked about classification, now let&amp;#39;s talk about regression. &lt;/p&gt;\n\n&lt;p&gt;Scikit-learn&amp;#39;s &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PredictionErrorDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-predictionerrordisplay\"&gt;metrics.PredictionErrorDisplay&lt;/a&gt; helps assess regression models. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.svm import SVR\nfrom sklearn.metrics import PredictionErrorDisplay\n\nrng = np.random.default_rng(42)\nX = rng.random(size=(200, 2)) * 10\ny = X[:, 0]**2 + 5 * X[:, 1] + 10 + rng.normal(loc=0.0, scale=0.1, size=(200,))\n\nreg = make_pipeline(StandardScaler(), SVR(kernel=&amp;#39;linear&amp;#39;, C=10))\nreg.fit(X, y)\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nPredictionErrorDisplay.from_estimator(reg, X, y, ax=axes[0], kind=&amp;quot;actual_vs_predicted&amp;quot;)\nPredictionErrorDisplay.from_estimator(reg, X, y, ax=axes[1], kind=&amp;quot;residual_vs_predicted&amp;quot;)\nplt.show()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9uxuobuti1qc1.png?width=764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7db276b2a7d637cf53887e4cebe87b9a23593b9b\"&gt;Two charts were drawn by PredictionErrorDisplay.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As shown, it can draw two kinds of graphs. The left shows predicted vs. actual values \u2013 good for linear regression. &lt;/p&gt;\n\n&lt;p&gt;However, not all data is perfectly linear. For that, use the right graph. &lt;/p&gt;\n\n&lt;p&gt;It compares real vs. predicted differences, a residuals plot. &lt;/p&gt;\n\n&lt;p&gt;This plot&amp;#39;s banana shape suggests our data might not fit linear regression. &lt;/p&gt;\n\n&lt;p&gt;Switching from a linear to an rbf kernel can help. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;reg = make_pipeline(StandardScaler(), SVR(kernel=&amp;#39;rbf&amp;#39;, C=10))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fsu1yqg0j1qc1.png?width=763&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=860447d0c8a1c0c38539f5fac16998ed928c8714\"&gt; A visual demonstration of the improved model performance.  &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;See, with rbf, the residual plot looks better. &lt;/p&gt;\n\n&lt;h3&gt;Using model_selection.LearningCurveDisplay for learning curves&lt;/h3&gt;\n\n&lt;p&gt;After assessing performance, let&amp;#39;s look at optimization with &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LearningCurveDisplay.html?ref=dataleadsfuture.com#sklearn.model_selection.LearningCurveDisplay\"&gt;LearningCurveDisplay&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;First up, learning curves \u2013 how well the model generalizes with different training and testing data, and if it suffers from variance or bias. &lt;/p&gt;\n\n&lt;p&gt;As shown below, we compare a DecisionTreeClassifier and a GradientBoostingClassifier to see how they do as training data changes. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import LearningCurveDisplay\n\nX, y = make_classification(n_samples=1000, n_classes=2, n_features=10,\n                           n_informative=2, n_redundant=0, n_repeated=0)\n\ntree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\ngb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=3, tol=1e-3)\n\ntrain_sizes = np.linspace(0.4, 1.0, 10)\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nLearningCurveDisplay.from_estimator(tree_clf, X, y,\n                                    train_sizes=train_sizes,\n                                    ax=axes[0],\n                                    scoring=&amp;#39;accuracy&amp;#39;)\naxes[0].set_title(&amp;#39;DecisionTreeClassifier&amp;#39;)\nLearningCurveDisplay.from_estimator(gb_clf, X, y,\n                                    train_sizes=train_sizes,\n                                    ax=axes[1],\n                                    scoring=&amp;#39;accuracy&amp;#39;)\naxes[1].set_title(&amp;#39;GradientBoostingClassifier&amp;#39;)\nplt.show()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2ljyfl9aj1qc1.png?width=896&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d7af8bd8a4295c54f6f5c3ce41caf2fd2ddb0c1\"&gt; Comparison of the learning curve of two different models. &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The graph shows that although the tree-based GradientBoostingClassifier maintains good accuracy on the training data, its generalization capability on test data does not have a significant advantage over the DecisionTreeClassifier. &lt;/p&gt;\n\n&lt;h3&gt;Using model_selection.ValidationCurveDisplay for visualizing parameter tuning&lt;/h3&gt;\n\n&lt;p&gt;So, for models that don&amp;#39;t generalize well, you might try adjusting the model&amp;#39;s regularization parameters to tweak its performance. &lt;/p&gt;\n\n&lt;p&gt;The traditional approach is to use tools like GridSearchCV or Optuna to tune the model, but these methods only give you the overall best-performing model and the tuning process is not very intuitive. &lt;/p&gt;\n\n&lt;p&gt;For scenarios where you want to adjust a specific parameter to test its effect on the model, I recommend using &lt;a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ValidationCurveDisplay.html?ref=dataleadsfuture.com#sklearn.model_selection.ValidationCurveDisplay\"&gt;model_selection.ValidationCurveDisplay&lt;/a&gt; to visualize how the model performs as the parameter changes. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import ValidationCurveDisplay\nfrom sklearn.linear_model import LogisticRegression\n\nparam_name, param_range = &amp;quot;C&amp;quot;, np.logspace(-8, 3, 10)\nlr_clf = LogisticRegression()\n\nValidationCurveDisplay.from_estimator(lr_clf, X, y,\n                                      param_name=param_name,\n                                      param_range=param_range,\n                                      scoring=&amp;#39;f1_weighted&amp;#39;,\n                                      cv=5, n_jobs=-1)\nplt.show()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5dcgo35jj1qc1.png?width=634&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed3d587e59d31902951ba0c9869ccd2bc39476d4\"&gt; Fine-tuning of model parameters plotted with ValidationCurveDisplay. &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;Some regrets&lt;/h1&gt;\n\n&lt;p&gt;After trying out all these Displays, I must admit some regrets: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The biggest one is that most of these APIs lack detailed tutorials, which is probably why they&amp;#39;re not well-known compared to Scikit-learn&amp;#39;s thorough documentation.&lt;/li&gt;\n&lt;li&gt;These APIs are scattered across various packages, making it hard to reference them from a single place.&lt;/li&gt;\n&lt;li&gt;The code is still pretty basic. You often need to pair it with Matplotlib&amp;#39;s APIs to get the job done. A typical example is DecisionBoundaryDisplay&lt;br/&gt;\n, where after plotting the decision boundary, you still need Matplotlib to plot the data distribution.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;They&amp;#39;re hard to extend. Besides a few methods validating parameters, it&amp;#39;s tough to simplify my model visualization process with tools or methods; I end up rewriting a lot.&lt;/p&gt;\n\n&lt;p&gt;I hope these APIs get more attention, and as versions upgrade, visualization APIs become even easier to use. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Conclusion&lt;/h1&gt;\n\n&lt;p&gt;In the journey of machine learning, explaining models with visualization is as important as training them. &lt;/p&gt;\n\n&lt;p&gt;This article introduced various plotting APIs in the current version of scikit-learn. &lt;/p&gt;\n\n&lt;p&gt;With these APIs, you can simplify some Matplotlib code, ease your learning curve, and streamline your model evaluation process. &lt;/p&gt;\n\n&lt;p&gt;Due to length, I didn&amp;#39;t expand on each API. If interested, you can check the &lt;a href=\"https://scikit-learn.org/stable/visualizations.html?ref=dataleadsfuture.com\"&gt;official documentation&lt;/a&gt; for more details. &lt;/p&gt;\n\n&lt;p&gt;Now it&amp;#39;s your turn. What are your expectations for visualizing machine learning methods? Feel free to leave a comment and discuss. &lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/scikit-learn-visualization-guide-making-models-speak/\"&gt;Data Leads Future&lt;/a&gt;. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3QeLrvmrkTlith0rWod1RvJb84uIyQ_ooxhR4GAxxyk.jpg?auto=webp&amp;s=0898caa546f8aacdf897f436bb227a3544bf2c83", "width": 160, "height": 58}, "resolutions": [{"url": "https://external-preview.redd.it/3QeLrvmrkTlith0rWod1RvJb84uIyQ_ooxhR4GAxxyk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54fa7103ac1fa7c5fa4f7488aed2f8f183533aa7", "width": 108, "height": 39}], "variants": {}, "id": "aRDTtkw0_GBCGb9E7JLBzDXeEp2pWa53pBC0AILtbPw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "1bln0ix", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1bln0ix/scikitlearn_visualization_guide_making_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1bln0ix/scikitlearn_visualization_guide_making_models/", "subreddit_subscribers": 1453770, "created_utc": 1711181199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently working as a Data Science intern at an MNC. However, I am facing a dilemma regarding whether to continue at my present company or move to another one.\n\nOn the positive side:\n\n1. We are given unrestricted access to real company data to do our projects.\n2. The workplace environment is very nice and provides us with a lot of flexibility and independence. We can choose our own projects, never have to work overtime and work mode is hybrid, even for senior employees. Everyone in the team is technically sound and given equal opportunities and responsibilities.\n3. Our (original) manager is great, very experienced and genuinely teaches us a lot of valuable things, including life skills.\n4. We are taught everything from data analysis to software engineering, project management and presenting to non-technical audiences, which are increasingly necessary skills.\n5. The projects we work on are fantastic. We do everything from coming up with project ideas and giving proposals for those ideas and getting them approved, to making dashboards for storytelling of our findings and deploying them on a private company channel.\n6. We are encouraged to understand the business process and supply chain as well as how our work affects the company's financial metrics instead of blindly saying on our resume that we contributed x% to y metric. We are allowed to participate in professional meetings and even contribute in them!\n\nHowever, there are some negative aspects:\n\n1. Often, it feels like we aren't doing data science at all. For many projects, instead of building our own models, we simply make an API call to a pre-trained HuggingFace model or similar. It sometimes feels like we are doing a software engineering internship rather than a data science internship.\n2. Our manager recently got promoted and hired our senior intern to become our new manager. Our new manager was confused and made frequent mistakes in management. It was a weird experience to go from someone extremely experienced to someone extremely inexperienced, though he has improved in the last few days.\n3. Me and another intern were given the task of interviewing new internship candidates, of course under the supervision of our original manager. This was a good experience, but our manager told us to \"try\" hiring female interns for the sake of \"diversity\". As a result, two extremely good male candidates were rejected and one barely decent female intern was hired (along with a genuinely good female candidate).\n4. Our new manager and the aforementioned female candidate have a lot of office politics going on. He has abysmal joking skills, saying inappropriate jokes like \"Why was this fellow hired? He should be fired!\" and she is very sensitive, taking offense very easily, leading to a very tense atmosphere at the office.\n\nSo, I would like to ask my seniors - should I continue here or apply to another company? ", "author_fullname": "t2_vwjrbow8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting advice for an intern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bljc6v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711167073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working as a Data Science intern at an MNC. However, I am facing a dilemma regarding whether to continue at my present company or move to another one.&lt;/p&gt;\n\n&lt;p&gt;On the positive side:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We are given unrestricted access to real company data to do our projects.&lt;/li&gt;\n&lt;li&gt;The workplace environment is very nice and provides us with a lot of flexibility and independence. We can choose our own projects, never have to work overtime and work mode is hybrid, even for senior employees. Everyone in the team is technically sound and given equal opportunities and responsibilities.&lt;/li&gt;\n&lt;li&gt;Our (original) manager is great, very experienced and genuinely teaches us a lot of valuable things, including life skills.&lt;/li&gt;\n&lt;li&gt;We are taught everything from data analysis to software engineering, project management and presenting to non-technical audiences, which are increasingly necessary skills.&lt;/li&gt;\n&lt;li&gt;The projects we work on are fantastic. We do everything from coming up with project ideas and giving proposals for those ideas and getting them approved, to making dashboards for storytelling of our findings and deploying them on a private company channel.&lt;/li&gt;\n&lt;li&gt;We are encouraged to understand the business process and supply chain as well as how our work affects the company&amp;#39;s financial metrics instead of blindly saying on our resume that we contributed x% to y metric. We are allowed to participate in professional meetings and even contribute in them!&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;However, there are some negative aspects:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Often, it feels like we aren&amp;#39;t doing data science at all. For many projects, instead of building our own models, we simply make an API call to a pre-trained HuggingFace model or similar. It sometimes feels like we are doing a software engineering internship rather than a data science internship.&lt;/li&gt;\n&lt;li&gt;Our manager recently got promoted and hired our senior intern to become our new manager. Our new manager was confused and made frequent mistakes in management. It was a weird experience to go from someone extremely experienced to someone extremely inexperienced, though he has improved in the last few days.&lt;/li&gt;\n&lt;li&gt;Me and another intern were given the task of interviewing new internship candidates, of course under the supervision of our original manager. This was a good experience, but our manager told us to &amp;quot;try&amp;quot; hiring female interns for the sake of &amp;quot;diversity&amp;quot;. As a result, two extremely good male candidates were rejected and one barely decent female intern was hired (along with a genuinely good female candidate).&lt;/li&gt;\n&lt;li&gt;Our new manager and the aforementioned female candidate have a lot of office politics going on. He has abysmal joking skills, saying inappropriate jokes like &amp;quot;Why was this fellow hired? He should be fired!&amp;quot; and she is very sensitive, taking offense very easily, leading to a very tense atmosphere at the office.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So, I would like to ask my seniors - should I continue here or apply to another company? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bljc6v", "is_robot_indexable": true, "report_reasons": null, "author": "Economy_Feeling_3661", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1bljc6v/requesting_advice_for_an_intern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1bljc6v/requesting_advice_for_an_intern/", "subreddit_subscribers": 1453770, "created_utc": 1711167073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hi there! In short, how do you guys deal with adjusting forecasts manually?\n\nA bit of background, a while back, I got into web development as a hobby and also been working on DS and forecasting for my job. We deal with a lot of one-off promotions that are almost impossible to forecast well (or at least that's the opinion of the marketing team, who always consider model outputs pessimistic and its very frustrating), so I got kind of tired of manually adjusting forecast data points in Excel and thought, why not try to create something to make it easier. After messing around for a bit, I created this ([**www.visualizzee.com**](http://www.visualizzee.com/)) and now I'm trying to figure out how it matches how people in the field deal with these things. \n\nHow do you deal when you have to adjust manual data points? How common is it for you to have to manually adjust model outputs? \n\n&amp;#x200B;", "author_fullname": "t2_gs7jf360", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you dealing with manually fine-tuning time series data poitns/prediction outcomes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1blqyst", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711196802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there! In short, how do you guys deal with adjusting forecasts manually?&lt;/p&gt;\n\n&lt;p&gt;A bit of background, a while back, I got into web development as a hobby and also been working on DS and forecasting for my job. We deal with a lot of one-off promotions that are almost impossible to forecast well (or at least that&amp;#39;s the opinion of the marketing team, who always consider model outputs pessimistic and its very frustrating), so I got kind of tired of manually adjusting forecast data points in Excel and thought, why not try to create something to make it easier. After messing around for a bit, I created this (&lt;a href=\"http://www.visualizzee.com/\"&gt;&lt;strong&gt;www.visualizzee.com&lt;/strong&gt;&lt;/a&gt;) and now I&amp;#39;m trying to figure out how it matches how people in the field deal with these things. &lt;/p&gt;\n\n&lt;p&gt;How do you deal when you have to adjust manual data points? How common is it for you to have to manually adjust model outputs? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "1blqyst", "is_robot_indexable": true, "report_reasons": null, "author": "AccomplishedPace6024", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1blqyst/how_are_you_dealing_with_manually_finetuning_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1blqyst/how_are_you_dealing_with_manually_finetuning_time/", "subreddit_subscribers": 1453770, "created_utc": 1711196802.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}