{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have always struggled with keeping my data needs to a minimum, typical case of a Data Hoarder I guess! One after another, and before you know it, you have a lot of drives that are hard to manage, we all have been there. I was trying to find a good solution for consolidating some of my most used drives into a single enclosure, but I couldn't find a solution small enough that worked well for me. In this video, I build a custom 2.5 inch HDD/SSD drive enclosure that you can recreate very easily if you are in the same boat.\n\n[https://youtu.be/RFL1oH1VZOk?si=g\\_jfRnjDr9nP\\_-gu](https://youtu.be/RFL1oH1VZOk?si=g_jfRnjDr9nP_-gu)", "author_fullname": "t2_dlw9jrc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You can't buy this! Affordable Custom 2.5 inch SATA SSD/HDD 4 Bay Direct Attached Drive Enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkkws8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 105, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 105, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711063749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have always struggled with keeping my data needs to a minimum, typical case of a Data Hoarder I guess! One after another, and before you know it, you have a lot of drives that are hard to manage, we all have been there. I was trying to find a good solution for consolidating some of my most used drives into a single enclosure, but I couldn&amp;#39;t find a solution small enough that worked well for me. In this video, I build a custom 2.5 inch HDD/SSD drive enclosure that you can recreate very easily if you are in the same boat.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/RFL1oH1VZOk?si=g_jfRnjDr9nP_-gu\"&gt;https://youtu.be/RFL1oH1VZOk?si=g_jfRnjDr9nP_-gu&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OyIx1nGbwdu7p7ig0h6FB9ATIHD10lkw8HBvab8NFcA.jpg?auto=webp&amp;s=add45e984c0d8d554ed203c4ddb7fd3eb207fd2b", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/OyIx1nGbwdu7p7ig0h6FB9ATIHD10lkw8HBvab8NFcA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b4526ef66d1341a495e7bbeb59c4d1ba0e732b7", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/OyIx1nGbwdu7p7ig0h6FB9ATIHD10lkw8HBvab8NFcA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e29f22b7ea506023c643be5c89d91a504bb18916", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/OyIx1nGbwdu7p7ig0h6FB9ATIHD10lkw8HBvab8NFcA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c28a96bab337555a4f84235940216fafb7277f16", "width": 320, "height": 240}], "variants": {}, "id": "ORf0WvzOH53xMdCuZbeUdDvf0gimtdVaGTD1ExIpgM0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkkws8", "is_robot_indexable": true, "report_reasons": null, "author": "himangshunits", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkkws8/you_cant_buy_this_affordable_custom_25_inch_sata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkkws8/you_cant_buy_this_affordable_custom_25_inch_sata/", "subreddit_subscribers": 740232, "created_utc": 1711063749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_d5s9e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a tool to bulk-download Steam screenshots", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl5s7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZKLV62E0ThGsili5BkyNoZSRPARR-zb16SZnRoWrXZc.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711130772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/ScienceDiscoverer/steamscrd", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GYZTSacLaAOtA-oUgniGHOQ_XYEdXjWsjiEotYx-ygU.jpg?auto=webp&amp;s=6c4b422763965efd7c49d4c8e58851b11097eba6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/GYZTSacLaAOtA-oUgniGHOQ_XYEdXjWsjiEotYx-ygU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=85cd14e6355c5949edf85f5c4f379ec014c46b3d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/GYZTSacLaAOtA-oUgniGHOQ_XYEdXjWsjiEotYx-ygU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92db79e1857aaae1d894f6d46b1de113d76f81a0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/GYZTSacLaAOtA-oUgniGHOQ_XYEdXjWsjiEotYx-ygU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae9a6f95f102aa2d40528560a9b0d24c8162e769", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/GYZTSacLaAOtA-oUgniGHOQ_XYEdXjWsjiEotYx-ygU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60b78bce9588d14fdefecef2ca5b964c75d163c1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/GYZTSacLaAOtA-oUgniGHOQ_XYEdXjWsjiEotYx-ygU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3a9e5029eb0a2721400d7f0424f979cbd2b21c9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/GYZTSacLaAOtA-oUgniGHOQ_XYEdXjWsjiEotYx-ygU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f52f3de63ece9f42cf45134272c2ee69471514aa", "width": 1080, "height": 540}], "variants": {}, "id": "xCykBZUtzazFIYFh3mKXjmRf1eInVyrPpcG7460NRgQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "7TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bl5s7s", "is_robot_indexable": true, "report_reasons": null, "author": "ScienceDiscoverer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bl5s7s/i_made_a_tool_to_bulkdownload_steam_screenshots/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/ScienceDiscoverer/steamscrd", "subreddit_subscribers": 740232, "created_utc": 1711130772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ofej473u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Here is the entirety of \"Ham Radio Magazine\" from 1968 to 1990", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1blanqe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Magazines", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1711142865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ia903006.us.archive.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ia903006.us.archive.org/12/items/hamradiomag/ham_radio_magazine/index.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1blanqe", "is_robot_indexable": true, "report_reasons": null, "author": "Run_the_Line", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1blanqe/here_is_the_entirety_of_ham_radio_magazine_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ia903006.us.archive.org/12/items/hamradiomag/ham_radio_magazine/index.html", "subreddit_subscribers": 740232, "created_utc": 1711142865.0, "num_crossposts": 3, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Very weird experience this week....\n\nI was running out of space so I took advantage of some BF deals last year and got some EasyStore 18TB (WD180EDGZ) drives. Slowly migrated my array to the 18TB drives. Everything ran fine for over 60 days. Suddenly my NAS faulted and started to beep. I logged in and saw that one of the drives was not showing up and the volume was degraded. Ran out bought an extra 18TB drive before I did anything.  I thought something like the drive controller died so swap and rebuild was the optimal first step before I RMA the drive. \n\nThen, during the rebuild I got an alert that the array completely crashed. Looking at the log, there was checksum mismatch on a file and the array crashed. I also don't see drive 1 in the system anymore. \n\nThe interesting thing is that once I took the first failed drive that vanished and put it back into an enclosure, it showed up fine and CrystalDisk shows the SMART as fine. \n\nThe question I have for everyone is what might be the cause of the initial drive vanishing from the NAS. And would you trust it again? \n\nIf my array is truly crashed and the data is not recoverable, I will likely just start from scratch but having 2 disks \"vanish\" without prior errors and showing up fine in an enclosure is a bit worrying to me. I had no issues with 8 &amp; 14 TB drives for many years. ", "author_fullname": "t2_13y130", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Almost new EasyStores vanished from Synology DS918+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl20lz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711121413.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Very weird experience this week....&lt;/p&gt;\n\n&lt;p&gt;I was running out of space so I took advantage of some BF deals last year and got some EasyStore 18TB (WD180EDGZ) drives. Slowly migrated my array to the 18TB drives. Everything ran fine for over 60 days. Suddenly my NAS faulted and started to beep. I logged in and saw that one of the drives was not showing up and the volume was degraded. Ran out bought an extra 18TB drive before I did anything.  I thought something like the drive controller died so swap and rebuild was the optimal first step before I RMA the drive. &lt;/p&gt;\n\n&lt;p&gt;Then, during the rebuild I got an alert that the array completely crashed. Looking at the log, there was checksum mismatch on a file and the array crashed. I also don&amp;#39;t see drive 1 in the system anymore. &lt;/p&gt;\n\n&lt;p&gt;The interesting thing is that once I took the first failed drive that vanished and put it back into an enclosure, it showed up fine and CrystalDisk shows the SMART as fine. &lt;/p&gt;\n\n&lt;p&gt;The question I have for everyone is what might be the cause of the initial drive vanishing from the NAS. And would you trust it again? &lt;/p&gt;\n\n&lt;p&gt;If my array is truly crashed and the data is not recoverable, I will likely just start from scratch but having 2 disks &amp;quot;vanish&amp;quot; without prior errors and showing up fine in an enclosure is a bit worrying to me. I had no issues with 8 &amp;amp; 14 TB drives for many years. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bl20lz", "is_robot_indexable": true, "report_reasons": null, "author": "psychephylax", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bl20lz/almost_new_easystores_vanished_from_synology_ds918/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bl20lz/almost_new_easystores_vanished_from_synology_ds918/", "subreddit_subscribers": 740232, "created_utc": 1711121413.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 1 12TB WD MyBook external drive that currently i use to store all alot of my media. I have no backup as of yet. i want one drive to be in use daily and the other just to store them obviously as a backup. my plan is to get chronosync so it can just mirror this current drive and at a click of a button manually do a weekly backup so i dont have to figure out whats new or what ive deleted on my drive etc. is this an efficient way to go about it? I know drives can fail at any point so im going to buy an identical second drive and just mirror it weekly to keep updated. ive used chronosync before but the trial ran out so i need to pay for the application which is fine with me. also I have several files 50GB and up, would it be easier to just drag and drop those or just use the program to mirror them? there is an option \"mirror left to right\" which is what i did last time and it just copies them over. but i know the old school drag and drop works just as good when dealing with large files.", "author_fullname": "t2_2o8t3n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would like advice/suggestions on a novice setup im getting into", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl2p0n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711123141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 1 12TB WD MyBook external drive that currently i use to store all alot of my media. I have no backup as of yet. i want one drive to be in use daily and the other just to store them obviously as a backup. my plan is to get chronosync so it can just mirror this current drive and at a click of a button manually do a weekly backup so i dont have to figure out whats new or what ive deleted on my drive etc. is this an efficient way to go about it? I know drives can fail at any point so im going to buy an identical second drive and just mirror it weekly to keep updated. ive used chronosync before but the trial ran out so i need to pay for the application which is fine with me. also I have several files 50GB and up, would it be easier to just drag and drop those or just use the program to mirror them? there is an option &amp;quot;mirror left to right&amp;quot; which is what i did last time and it just copies them over. but i know the old school drag and drop works just as good when dealing with large files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bl2p0n", "is_robot_indexable": true, "report_reasons": null, "author": "QualitySound96", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bl2p0n/would_like_advicesuggestions_on_a_novice_setup_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bl2p0n/would_like_advicesuggestions_on_a_novice_setup_im/", "subreddit_subscribers": 740232, "created_utc": 1711123141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI have a few VHS tapes that I'd like to digitalize.\n\nI found a VHS player that on the back has these ports: [https://imgur.com/a/GIJmohH](https://imgur.com/a/GIJmohH)\n\nWhat's the best way to go now? Cheap composite to HDMI adapter and HDMI capture card? Something else? I'm open to ideas. Thanks for the help!", "author_fullname": "t2_1yhc7ph9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitalize VHS tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkwgrm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711104981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I have a few VHS tapes that I&amp;#39;d like to digitalize.&lt;/p&gt;\n\n&lt;p&gt;I found a VHS player that on the back has these ports: &lt;a href=\"https://imgur.com/a/GIJmohH\"&gt;https://imgur.com/a/GIJmohH&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to go now? Cheap composite to HDMI adapter and HDMI capture card? Something else? I&amp;#39;m open to ideas. Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?auto=webp&amp;s=5254eb65a2b2c240b9638e5294c998c2b4473500", "width": 1871, "height": 1408}, "resolutions": [{"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c1c9ee4c5548481a8320b88b41a31c2306a1716", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53441216a37efe3f837057e646bf795cfdaf52a9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee41d77e2719947298113bedd6ba8f622caac12c", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c451ac33882e660135efc569c11bf41984637f7", "width": 640, "height": 481}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37a35810501127a1ddad46a88b8f3b62bc361e99", "width": 960, "height": 722}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=919ca169dbf5f3fdbd6c40cc74ea436b8cb47fbd", "width": 1080, "height": 812}], "variants": {}, "id": "TVY7oGcueiCIcXqnmJjJbxHz7VyHoCi7OXabDBR9SlA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkwgrm", "is_robot_indexable": true, "report_reasons": null, "author": "DFalconD", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkwgrm/digitalize_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkwgrm/digitalize_vhs_tapes/", "subreddit_subscribers": 740232, "created_utc": 1711104981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m reaching out for advice on an issue that\u2019s been a bottleneck for my video production team. We generate a lot of 4K video footage, primarily in h.264 MP4 format, and store it all on our NAS, with new footage being added daily.\n\nThe challenge we\u2019re facing is enabling myself and a few employees to log in remotely to the NAS to review this footage. The review process doesn\u2019t require high quality; we just need to be able to watch the footage and make notes. Despite having fast internet both at the studio and at our homes, the large file sizes and the quantity of files make this process unworkable.\n\nI\u2019ve been toying with a couple of ideas to solve this:\n\n\t1.\tAutomatic Compression: One idea was to find a way to automatically compress every video file as it\u2019s uploaded, so we\u2019d have two copies of every video: one high-quality (for final use) and one low-quality (for reviewing purposes). I\u2019ve heard suggestions like using ffmpeg, Jellyfin, or Tdarr for this, but I\u2019m unsure how complex it would be to set up and maintain such a system.\n\t2.\tStreaming Solution: Another thought was whether there might be a software or hardware solution that allows for streaming the files directly from the NAS in a more manageable quality for review purposes. This way, we could bypass the need to store two versions of every file.\n\nI\u2019m curious if anyone here has tackled similar challenges or knows of existing solutions that could fit our needs. Whether it\u2019s specific software, a hardware setup, or scripting our way through with ffmpeg or similar tools, I\u2019m all ears. Our main goal is to streamline the review process without sacrificing too much on video quality for reviewing purposes and without requiring significant manual effort for file conversion.\n\nAny insights, recommendations, or advice would be greatly appreciated. ", "author_fullname": "t2_8738b0rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Solutions for Remote Review of 4K Video Footage on NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkp4mq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711075909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m reaching out for advice on an issue that\u2019s been a bottleneck for my video production team. We generate a lot of 4K video footage, primarily in h.264 MP4 format, and store it all on our NAS, with new footage being added daily.&lt;/p&gt;\n\n&lt;p&gt;The challenge we\u2019re facing is enabling myself and a few employees to log in remotely to the NAS to review this footage. The review process doesn\u2019t require high quality; we just need to be able to watch the footage and make notes. Despite having fast internet both at the studio and at our homes, the large file sizes and the quantity of files make this process unworkable.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been toying with a couple of ideas to solve this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;1.  Automatic Compression: One idea was to find a way to automatically compress every video file as it\u2019s uploaded, so we\u2019d have two copies of every video: one high-quality (for final use) and one low-quality (for reviewing purposes). I\u2019ve heard suggestions like using ffmpeg, Jellyfin, or Tdarr for this, but I\u2019m unsure how complex it would be to set up and maintain such a system.\n2.  Streaming Solution: Another thought was whether there might be a software or hardware solution that allows for streaming the files directly from the NAS in a more manageable quality for review purposes. This way, we could bypass the need to store two versions of every file.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I\u2019m curious if anyone here has tackled similar challenges or knows of existing solutions that could fit our needs. Whether it\u2019s specific software, a hardware setup, or scripting our way through with ffmpeg or similar tools, I\u2019m all ears. Our main goal is to streamline the review process without sacrificing too much on video quality for reviewing purposes and without requiring significant manual effort for file conversion.&lt;/p&gt;\n\n&lt;p&gt;Any insights, recommendations, or advice would be greatly appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkp4mq", "is_robot_indexable": true, "report_reasons": null, "author": "TheBigBrezinski", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkp4mq/seeking_solutions_for_remote_review_of_4k_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkp4mq/seeking_solutions_for_remote_review_of_4k_video/", "subreddit_subscribers": 740232, "created_utc": 1711075909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My Western Digital 2TB internal 2,5\" HDD that I was using with an external enclosure died 1 month ago, I need a new HDD, I avoided Seagate because I know they aren't the most reliable, but I need something cheaper, so I can consider it. I live in Italy, i need at least a 2TB(The old drive was nearly full so I can consider a 4TB). It will be used only for backup(Photo, some TV Series, Programs installer ecc...) so I don't need anything fast. I'm considering a M.2 SSD since I need reliability(This drive lasted only 2 years with a Power on count of 50 days). Can someone suggest something good? ", "author_fullname": "t2_2j2v7pw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need recommendation on external HDD/SSD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1blbbj9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711144518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Western Digital 2TB internal 2,5&amp;quot; HDD that I was using with an external enclosure died 1 month ago, I need a new HDD, I avoided Seagate because I know they aren&amp;#39;t the most reliable, but I need something cheaper, so I can consider it. I live in Italy, i need at least a 2TB(The old drive was nearly full so I can consider a 4TB). It will be used only for backup(Photo, some TV Series, Programs installer ecc...) so I don&amp;#39;t need anything fast. I&amp;#39;m considering a M.2 SSD since I need reliability(This drive lasted only 2 years with a Power on count of 50 days). Can someone suggest something good? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1blbbj9", "is_robot_indexable": true, "report_reasons": null, "author": "bigfabrizio2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1blbbj9/need_recommendation_on_external_hddssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1blbbj9/need_recommendation_on_external_hddssd/", "subreddit_subscribers": 740232, "created_utc": 1711144518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Here\u2019s the situation \n\n\nI currently have about 70TB of Linux ISO\u2019s \n\n\nMy collection grew organically and started before I really knew what I was doing. I never intended it to get this big. Therefore, the drives were never set up in RAID. The drives are all now 5+ years old. This keeps me awake at night. \n\n\nThe ultimate goal is to eventually buy a new NAS and a few new drives, set up RAID 5 or 6 and transfer the data over. However I\u2019m saving to buy a house at the moment and have told myself that I\u2019m not allowed to make any big purchases. \n\n\nIn the meantime, I\u2019d like to back up as much data as possible to cloud storage. Just so I can get some sleep at night without worrying that I might lose data. \n\n\nMy requirements would be:\n\n- Cheap \n\n- Reliable \n\n- 70TB+ Storage\n\n- Transfer speed uncapped\n\n\nAnyone got any recommendations for a cloud provider? \n\n\nTLDR: I have lots of data and want to back it up online so I can get some sleep. Hit me with your suggestions. \n\nCheers \n\nP.S - I know that RAID is not a backup solution. \n", "author_fullname": "t2_6za10xlpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to back up my data to cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1blb4vd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711144059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here\u2019s the situation &lt;/p&gt;\n\n&lt;p&gt;I currently have about 70TB of Linux ISO\u2019s &lt;/p&gt;\n\n&lt;p&gt;My collection grew organically and started before I really knew what I was doing. I never intended it to get this big. Therefore, the drives were never set up in RAID. The drives are all now 5+ years old. This keeps me awake at night. &lt;/p&gt;\n\n&lt;p&gt;The ultimate goal is to eventually buy a new NAS and a few new drives, set up RAID 5 or 6 and transfer the data over. However I\u2019m saving to buy a house at the moment and have told myself that I\u2019m not allowed to make any big purchases. &lt;/p&gt;\n\n&lt;p&gt;In the meantime, I\u2019d like to back up as much data as possible to cloud storage. Just so I can get some sleep at night without worrying that I might lose data. &lt;/p&gt;\n\n&lt;p&gt;My requirements would be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Cheap &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Reliable &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;70TB+ Storage&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Transfer speed uncapped&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone got any recommendations for a cloud provider? &lt;/p&gt;\n\n&lt;p&gt;TLDR: I have lots of data and want to back it up online so I can get some sleep. Hit me with your suggestions. &lt;/p&gt;\n\n&lt;p&gt;Cheers &lt;/p&gt;\n\n&lt;p&gt;P.S - I know that RAID is not a backup solution. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1blb4vd", "is_robot_indexable": true, "report_reasons": null, "author": "Neither-Engine-5852", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1blb4vd/looking_to_back_up_my_data_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1blb4vd/looking_to_back_up_my_data_to_cloud/", "subreddit_subscribers": 740232, "created_utc": 1711144059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to ditch WhatsApp and move onto a more privacy-friendly app that would let me easily export all my data, kinda like Telegram does with its JSON export. I have lots of long chats in WhatsApp (how to count messages?), some of them including messages that have Emoji reactions and stickers and I want to export them as well. There are also some group chats.\n\nMy phone runs Android 9 and isn't rooted, and I don't know if it's possible to root it without data loss (probably not), since the bootloader is locked. It's my main phone and I have zero experience with rooting.\n\nSo as of March 2024 is there any relatively reliable method to export my WhatsApp data without the silly limits imposed by Meta? I've searched and found a post about a method that involves a simulation of running WhatsApp on another phone (virtual machine) and restoring from backups, which the author wrote it can be a hit or miss. My WhatsApp backup is 10 GB and I live in a country with slow and expensive Internet, so I need a more reliable method... Also, not sure if it's able to export those stickers and reactions too, or if any 'hiccups' would cause loosing some messages or other data during the export.\n\nAny input would be MUCH appreciated.", "author_fullname": "t2_254e4x5n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Full WhatsApp data export?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl88a9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711136782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to ditch WhatsApp and move onto a more privacy-friendly app that would let me easily export all my data, kinda like Telegram does with its JSON export. I have lots of long chats in WhatsApp (how to count messages?), some of them including messages that have Emoji reactions and stickers and I want to export them as well. There are also some group chats.&lt;/p&gt;\n\n&lt;p&gt;My phone runs Android 9 and isn&amp;#39;t rooted, and I don&amp;#39;t know if it&amp;#39;s possible to root it without data loss (probably not), since the bootloader is locked. It&amp;#39;s my main phone and I have zero experience with rooting.&lt;/p&gt;\n\n&lt;p&gt;So as of March 2024 is there any relatively reliable method to export my WhatsApp data without the silly limits imposed by Meta? I&amp;#39;ve searched and found a post about a method that involves a simulation of running WhatsApp on another phone (virtual machine) and restoring from backups, which the author wrote it can be a hit or miss. My WhatsApp backup is 10 GB and I live in a country with slow and expensive Internet, so I need a more reliable method... Also, not sure if it&amp;#39;s able to export those stickers and reactions too, or if any &amp;#39;hiccups&amp;#39; would cause loosing some messages or other data during the export.&lt;/p&gt;\n\n&lt;p&gt;Any input would be MUCH appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bl88a9", "is_robot_indexable": true, "report_reasons": null, "author": "imsosappy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bl88a9/full_whatsapp_data_export/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bl88a9/full_whatsapp_data_export/", "subreddit_subscribers": 740232, "created_utc": 1711136782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am attempting to save a documentary from Truestory, I can access and play the video in the browser. It appears to use Vimeo's OTT system.\n\nThere's a piece of javascript on the page which reveals a lot of information, such as the auth user token and video ID but Jdownloader can't work with the video url, as it prompts me for a password presumably to the Vimeo account which doesn't belong to me.\n\nNavigating to the video (https://player.vimeo.com/video/XXXXXXX) says it can't be played due to privacy settings.\n\nHere's a redacted version (omitted the IDs and my email) to give you an idea of what's returned on the page. https://pastebin.com/jyhejdFS\n\nWould yt-dlp be able to handle something like this? if so, an example would be very much appreciated.\n\nI could record my screen but ideally, I'd like to find a way to save the video. Does anybody have any suggestions?\n\nThank you!", "author_fullname": "t2_rxt22yfca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling with saving Vimeo OTT External video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkvoc9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711101904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to save a documentary from Truestory, I can access and play the video in the browser. It appears to use Vimeo&amp;#39;s OTT system.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a piece of javascript on the page which reveals a lot of information, such as the auth user token and video ID but Jdownloader can&amp;#39;t work with the video url, as it prompts me for a password presumably to the Vimeo account which doesn&amp;#39;t belong to me.&lt;/p&gt;\n\n&lt;p&gt;Navigating to the video (&lt;a href=\"https://player.vimeo.com/video/XXXXXXX\"&gt;https://player.vimeo.com/video/XXXXXXX&lt;/a&gt;) says it can&amp;#39;t be played due to privacy settings.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a redacted version (omitted the IDs and my email) to give you an idea of what&amp;#39;s returned on the page. &lt;a href=\"https://pastebin.com/jyhejdFS\"&gt;https://pastebin.com/jyhejdFS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would yt-dlp be able to handle something like this? if so, an example would be very much appreciated.&lt;/p&gt;\n\n&lt;p&gt;I could record my screen but ideally, I&amp;#39;d like to find a way to save the video. Does anybody have any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkvoc9", "is_robot_indexable": true, "report_reasons": null, "author": "TertiaryOrbit", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkvoc9/struggling_with_saving_vimeo_ott_external_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkvoc9/struggling_with_saving_vimeo_ott_external_video/", "subreddit_subscribers": 740232, "created_utc": 1711101904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have lots of YouTube videos archived, I have channels and playlists, like this \n\nChannels \n-&gt;channel 1\n    -&gt;video 1, 2 etc \n-&gt;channel 2\n    -&gt;video 1, 2 etc\n\nPlaylists\n-&gt;playlist name \n    -&gt;video 1, 2 etc\n\nThat works for just general browsing, and I have a script that automates downloading and stuff with metadata, however I'm looking for a nice way to display the archive. I've tried Tube archivist on GitHub, and that's pretty much what I'm looking for but that stores videos based off of IDs, I understand why that is the way it is but I'm looking for something I can just point at my archive right now and have it display like YouTube, I hope that makes sense, if anyone can help that would be appreciated :) ", "author_fullname": "t2_3jbd8ec6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to organise/display YouTube archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkmfoe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711067922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have lots of YouTube videos archived, I have channels and playlists, like this &lt;/p&gt;\n\n&lt;p&gt;Channels \n-&amp;gt;channel 1\n    -&amp;gt;video 1, 2 etc \n-&amp;gt;channel 2\n    -&amp;gt;video 1, 2 etc&lt;/p&gt;\n\n&lt;p&gt;Playlists\n-&amp;gt;playlist name \n    -&amp;gt;video 1, 2 etc&lt;/p&gt;\n\n&lt;p&gt;That works for just general browsing, and I have a script that automates downloading and stuff with metadata, however I&amp;#39;m looking for a nice way to display the archive. I&amp;#39;ve tried Tube archivist on GitHub, and that&amp;#39;s pretty much what I&amp;#39;m looking for but that stores videos based off of IDs, I understand why that is the way it is but I&amp;#39;m looking for something I can just point at my archive right now and have it display like YouTube, I hope that makes sense, if anyone can help that would be appreciated :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkmfoe", "is_robot_indexable": true, "report_reasons": null, "author": "Tomma365", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkmfoe/best_way_to_organisedisplay_youtube_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkmfoe/best_way_to_organisedisplay_youtube_archive/", "subreddit_subscribers": 740232, "created_utc": 1711067922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to figure out the best way to optimize and sync my hoarded files across multiple hard drives in two different locations. I searched around the community and saw a lot of syncing apps suggestions but I'm not really sure which would be the best solution. I have 2 desktop pcs:\n\n  \n**1) My main PC is in my apartment, with one 4tb drive and now a brand new 8tb drive** \n\n**2) My secondary PC is in my \"office\" (actually the back of my mother's house) with one 4tb drive**  \n\n\n**I also have a hot swappable 4tb and a 2tb drive, as well as a 6tb usb Seagate backup drive.**  \n**I access both my files from those two pcs via ZeroTier and Windows networking.**  \nI also have 3tb of ssds in both those pcs as well as my laptop as well as some 1tb temporary/download drives not actually used for data storage that I don' t really bother to back up.\n\nThe thing is, between the two pc locations, I wanted them to mostly have access to the same files while having them synced/backed up. I thought about having a sync software to keep folders and media files easily accessible and synced in those two locations, and every time I organize or delete stuf,f I want those deleted on both locations. I DO REALIZE syncing is not really a proper backup, but I can't really afford MORE hard drives to keep full disk-image clones.\n\n  \nI want to sync between 15days\\~1 month or manually when I do some binge organizations and get new important stuff, **My biggest concern is how to make this safer and more efficient as possible:** I'm willing to check a long list of deleted and modified files to see if any disk corruption or malware infections affected files that shouldn't have been affected. **If the software has CRC/MD5 integration** would also help with that. \n\n  \nI don't feel like paying 60 bucks for SyncBackpro, I was looking to get FreeFileSync to see If I can organize them, but if there are any other options better than those two, or if only syncbackpro has CRC integration and safety checks before syncing (I honestly don't really know) I'm willing to pay at most 60 bucks for its license. \n\nI had a RAID5 array with cold backup years ago, tried syncback software, full disc backups and lately I have been manually syncing folders but I have just too much stuff, my disks got full and I have been running a lot of stuff without backup out of poor time and space in the last years. I know this was a time bomb situation, luckily I decided to purchase an extra 8tb drive before anything went wrong. \n\n&amp;#x200B;", "author_fullname": "t2_71ovwhpi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup-Syncing hard drives from two different locations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1blalvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711142734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out the best way to optimize and sync my hoarded files across multiple hard drives in two different locations. I searched around the community and saw a lot of syncing apps suggestions but I&amp;#39;m not really sure which would be the best solution. I have 2 desktop pcs:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1) My main PC is in my apartment, with one 4tb drive and now a brand new 8tb drive&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2) My secondary PC is in my &amp;quot;office&amp;quot; (actually the back of my mother&amp;#39;s house) with one 4tb drive&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I also have a hot swappable 4tb and a 2tb drive, as well as a 6tb usb Seagate backup drive.&lt;/strong&gt;&lt;br/&gt;\n&lt;strong&gt;I access both my files from those two pcs via ZeroTier and Windows networking.&lt;/strong&gt;&lt;br/&gt;\nI also have 3tb of ssds in both those pcs as well as my laptop as well as some 1tb temporary/download drives not actually used for data storage that I don&amp;#39; t really bother to back up.&lt;/p&gt;\n\n&lt;p&gt;The thing is, between the two pc locations, I wanted them to mostly have access to the same files while having them synced/backed up. I thought about having a sync software to keep folders and media files easily accessible and synced in those two locations, and every time I organize or delete stuf,f I want those deleted on both locations. I DO REALIZE syncing is not really a proper backup, but I can&amp;#39;t really afford MORE hard drives to keep full disk-image clones.&lt;/p&gt;\n\n&lt;p&gt;I want to sync between 15days~1 month or manually when I do some binge organizations and get new important stuff, &lt;strong&gt;My biggest concern is how to make this safer and more efficient as possible:&lt;/strong&gt; I&amp;#39;m willing to check a long list of deleted and modified files to see if any disk corruption or malware infections affected files that shouldn&amp;#39;t have been affected. &lt;strong&gt;If the software has CRC/MD5 integration&lt;/strong&gt; would also help with that. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t feel like paying 60 bucks for SyncBackpro, I was looking to get FreeFileSync to see If I can organize them, but if there are any other options better than those two, or if only syncbackpro has CRC integration and safety checks before syncing (I honestly don&amp;#39;t really know) I&amp;#39;m willing to pay at most 60 bucks for its license. &lt;/p&gt;\n\n&lt;p&gt;I had a RAID5 array with cold backup years ago, tried syncback software, full disc backups and lately I have been manually syncing folders but I have just too much stuff, my disks got full and I have been running a lot of stuff without backup out of poor time and space in the last years. I know this was a time bomb situation, luckily I decided to purchase an extra 8tb drive before anything went wrong. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1blalvv", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo_3546", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1blalvv/backupsyncing_hard_drives_from_two_different/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1blalvv/backupsyncing_hard_drives_from_two_different/", "subreddit_subscribers": 740232, "created_utc": 1711142734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there guys,\n\nI'm running a Plex and an Emby server simultaneously and I've set up mergerfs for that. Currently I have \\~310TiB of disk space, where \\~255 TiB are in use. They all separate on 21 HDDs I've attached in various 4- and 8-bays.\n\nMy question now is: Are there any known limits to mergerfs? Or even any hard caps? The only thing I've found were some setups on the mergerfs wiki with 155TB, which I've apparently surpassed already.\n\nAny help or insights with your experience would help me a lot :)", "author_fullname": "t2_aumdbdvt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mergerfs maximum capacity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl8k8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711137609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running a Plex and an Emby server simultaneously and I&amp;#39;ve set up mergerfs for that. Currently I have ~310TiB of disk space, where ~255 TiB are in use. They all separate on 21 HDDs I&amp;#39;ve attached in various 4- and 8-bays.&lt;/p&gt;\n\n&lt;p&gt;My question now is: Are there any known limits to mergerfs? Or even any hard caps? The only thing I&amp;#39;ve found were some setups on the mergerfs wiki with 155TB, which I&amp;#39;ve apparently surpassed already.&lt;/p&gt;\n\n&lt;p&gt;Any help or insights with your experience would help me a lot :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bl8k8m", "is_robot_indexable": true, "report_reasons": null, "author": "Icy_Ear_4931", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bl8k8m/mergerfs_maximum_capacity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bl8k8m/mergerfs_maximum_capacity/", "subreddit_subscribers": 740232, "created_utc": 1711137609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Everything on my imgur account just gives 404 errors though all the data is there on the site, i can see my data on there and look at it in the web ui, i want to save images and descriptions i have typed, if i can save stuff albums to separate folders that would be ideal\n\ni have all my images, but they are not in a sensible order and my only copy of what i typed is on imgur\n\nit seems like imgur wants to keep user data and make it look like they deleted it when they only removed access to it", "author_fullname": "t2_1a0p33l3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have a imgur script to export data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl0300", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711116405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everything on my imgur account just gives 404 errors though all the data is there on the site, i can see my data on there and look at it in the web ui, i want to save images and descriptions i have typed, if i can save stuff albums to separate folders that would be ideal&lt;/p&gt;\n\n&lt;p&gt;i have all my images, but they are not in a sensible order and my only copy of what i typed is on imgur&lt;/p&gt;\n\n&lt;p&gt;it seems like imgur wants to keep user data and make it look like they deleted it when they only removed access to it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bl0300", "is_robot_indexable": true, "report_reasons": null, "author": "Evil_Kittie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bl0300/anyone_have_a_imgur_script_to_export_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bl0300/anyone_have_a_imgur_script_to_export_data/", "subreddit_subscribers": 740232, "created_utc": 1711116405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got a business account but can't stream anything due to a pickle internet, is there any working way to download from udemy business? ", "author_fullname": "t2_dpczq8ix", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "working way to download from Udemy Business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkvke5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711101470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got a business account but can&amp;#39;t stream anything due to a pickle internet, is there any working way to download from udemy business? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkvke5", "is_robot_indexable": true, "report_reasons": null, "author": "xeroxBD", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkvke5/working_way_to_download_from_udemy_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkvke5/working_way_to_download_from_udemy_business/", "subreddit_subscribers": 740232, "created_utc": 1711101470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used to manually rsync a set of files from my Linux desktop to an external usb drive and every time i did this i had to be extra careful with the source and destination parameters since i use --delete-during.\n\nNow I've moved that folder to a NAS and i was hoping to sorta do the same but actually it just occurred to me that it would be cool to just do this every time i plug the drive into USB of the nas... I'm sure of y'all have figured it out already... Any ideas?\n\nMy NAS it's a terramaster that i installed (Arch) Linux on so the implementation could be anything that runs on Linux and it doesn't need to be specific to me model .", "author_fullname": "t2_ly1a1ad1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cold storage on USB connect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bksfx3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711087784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to manually rsync a set of files from my Linux desktop to an external usb drive and every time i did this i had to be extra careful with the source and destination parameters since i use --delete-during.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;ve moved that folder to a NAS and i was hoping to sorta do the same but actually it just occurred to me that it would be cool to just do this every time i plug the drive into USB of the nas... I&amp;#39;m sure of y&amp;#39;all have figured it out already... Any ideas?&lt;/p&gt;\n\n&lt;p&gt;My NAS it&amp;#39;s a terramaster that i installed (Arch) Linux on so the implementation could be anything that runs on Linux and it doesn&amp;#39;t need to be specific to me model .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bksfx3", "is_robot_indexable": true, "report_reasons": null, "author": "GloriousHousehold", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bksfx3/cold_storage_on_usb_connect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bksfx3/cold_storage_on_usb_connect/", "subreddit_subscribers": 740232, "created_utc": 1711087784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The motherboard of my NAS failed today (15 years old, so no big loss) and it got me thinking about backup software. I backup my PC on my NAS using Areca, but to backup my NAS to another drive, should I use a regular backup software (that permits complete and incremental backups) or should I just use rsync to sync the content? I find the backup software rather heavy to backup ~12TB of data. Thanks!", "author_fullname": "t2_8n0m0c8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS backup: backup software or rsync?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkm5n2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": "", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711067136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The motherboard of my NAS failed today (15 years old, so no big loss) and it got me thinking about backup software. I backup my PC on my NAS using Areca, but to backup my NAS to another drive, should I use a regular backup software (that permits complete and incremental backups) or should I just use rsync to sync the content? I find the backup software rather heavy to backup ~12TB of data. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkm5n2", "is_robot_indexable": true, "report_reasons": null, "author": "SomeoneHereIsMissing", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bkm5n2/nas_backup_backup_software_or_rsync/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkm5n2/nas_backup_backup_software_or_rsync/", "subreddit_subscribers": 740232, "created_utc": 1711067136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need storage. For my music production, video editing, etc. Also for my personal data as well. \nI need faster access to my data, I am not going to use it outside of my local network, which I might not want NAS, and I am running out of storage in my pc right now. I would prefer to have bunch of disks and its enclosure which can provide me enough read and write speed. I would need another power supply for that since each drive consumes power anyways,\n\nWhat do you call these kinds of setup? Locally attached bunch of disks.\n\nHow would you set them up? Just connecting bunch of USB cables to each disks are not so neat and it will overflow my USB ports.\n\nIs NAS still be enough to handle those data?\n\n", "author_fullname": "t2_14m87z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External storage setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkleza", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711065108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need storage. For my music production, video editing, etc. Also for my personal data as well. \nI need faster access to my data, I am not going to use it outside of my local network, which I might not want NAS, and I am running out of storage in my pc right now. I would prefer to have bunch of disks and its enclosure which can provide me enough read and write speed. I would need another power supply for that since each drive consumes power anyways,&lt;/p&gt;\n\n&lt;p&gt;What do you call these kinds of setup? Locally attached bunch of disks.&lt;/p&gt;\n\n&lt;p&gt;How would you set them up? Just connecting bunch of USB cables to each disks are not so neat and it will overflow my USB ports.&lt;/p&gt;\n\n&lt;p&gt;Is NAS still be enough to handle those data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkleza", "is_robot_indexable": true, "report_reasons": null, "author": "trapoad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkleza/external_storage_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkleza/external_storage_setup/", "subreddit_subscribers": 740232, "created_utc": 1711065108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a ds1621+, which is a great machine for NAS, but there are just some things I wish I had a more powerful processor and GPU to do. I have a 12th gen i5 and a nice tower case going unused, but I have grown accustomed to the Synology ecosystem and I don't have much experience with OS other than Windows and DSM. Would anyone recommend I make the switch and if so, how would you recommend I go about it?", "author_fullname": "t2_aeywfrknk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone migrate from Synology after growing accustomed to their apps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl975w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711139194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a ds1621+, which is a great machine for NAS, but there are just some things I wish I had a more powerful processor and GPU to do. I have a 12th gen i5 and a nice tower case going unused, but I have grown accustomed to the Synology ecosystem and I don&amp;#39;t have much experience with OS other than Windows and DSM. Would anyone recommend I make the switch and if so, how would you recommend I go about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bl975w", "is_robot_indexable": true, "report_reasons": null, "author": "SidneyHuffman316", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bl975w/anyone_migrate_from_synology_after_growing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bl975w/anyone_migrate_from_synology_after_growing/", "subreddit_subscribers": 740232, "created_utc": 1711139194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nFor the last 7-8 years, I've been content with my old HTPC case and slapping a 4TB disk in whenever I  neared my capacity. I'm now at about 16TB of filled storage, and ready to slap again - but I'm getting to the point where I need something more robust.\n\nMy plan was to eventually create a JBOD and a dedicated storage server to manage it, but I'm not quite there yet; so, I'd like to work with what I have (plus a few more disks) to create a better solution, and I'd love your help reviewing my plan to do so.\n\nI can't spend much on this project right now, but I have bought some higher-capacity drives and have the ability to reuse some of my old Home Theater PC hardware to take baby steps toward that goal. With that, these are my goals with this plan:\n\n1. Move everything (except the Operating System) onto ZFS.\n2. Implement single-drive parity for my non-critical data (about 14TB which would be annoying to replace, but not critical) and setup dual-drive parity for my critical media with an off-site backup solution (3-2-1).\n3. Keep everything relatively performant and extensible, as I'll eventually fill up both of these storage spaces.\n\nMy use case is 99% write-once read-often and very large files (e.g., home theater stuff); however, I do have some applications with very light database-style usage (think grandma's recipe website).\n\nSo here's my plan - please poke holes and help me avoid hurdles, as I'm entirely new to ZFS, especially, and there's always room for improvement. And another thanks toeEveryone who helped with my [last post](https://www.reddit.com/r/DataHoarder/comments/1bgxank/best_current_way_to_rebuild_storage/) to get here:\n\nWithout further ado...\n\n## Where I'm at today....\n\nI have available to me a pretty decent HTPC case that can easily support 11+ SATA drives (9x 3.5\" and 2x SSDs); it currently contains:\n\n* An **Intel(R) Core(TM) i5-4590 CPU @ 3.30GHz** quad-core processor\n* **8GB RAM** (non-ECC) taking up 2 of 4 slots *(what can I say? it's an old box)*\n* A very underutilized **PSU**\n* The following **storage disks** *(all connected via 6.0GB/s SATA)*:\n\n&amp;#x200B;\n\n|***Device***|***Type***|***Capacity***|***Model***|***Purpose***|***Power-On Hours / Years***|\n|:-|:-|:-|:-|:-|:-|\n|`/dev/sda`|SSD|120 GiB|Samsung SSD 850|Operating System|58,451 hours / 6.67 years|\n|`/dev/sdb`|SSD|1 TiB|Samsung SSD 850|HTPC Transcoding + Cache|45,648 hours / 5.21 years|\n|`/dev/sdc`|HDD|4 TiB|WD Red (`WDC WD40EFRX-68N`)|Non-Critical Media; Artifacts (e.g., Docker Images)|24,505 hours / 2.80 years|\n|`/dev/sdd`|HDD|4 TiB|WD Red (`WDC WD40EFRX-68W`)|Non-Critical Media|58,499 hours / 6.68 years|\n|`/dev/sde`|HDD|4 TiB|WD Red (`WDC WD40EFRX-68W`)|Non-Critical Media|56,113 hours / 6.41 years|\n|`/dev/sdf`|HDD|4 TiB|WD Red (`WDC WD40EFRX-68N`)|Non-Critical Media and **important 2nd backups from home computers**|30,906 hours / 3.53 years|\n\nNow with the above... not a RAID or 3-2-1 backup in sight, mind you, like a good agent of chaos. All of those drives are nearly at-capacity (save the Operating System disk) with no recoverability. I do have an off-site backup solution for some folders containing critical information (so, yep, I'm aware drive resilience via RAID isn't backup).\n\n**My data falls into three (3) categories:**\n\n1. My operating system and configurations;\n2. Non-critical data that's annoying, but not impossible, to replace *(14TB and growing)*; and,\n3. Critical data that I cherish and can't lose *(about 3TB)*.\n\nInstead of adding another yet-another chaos drive, I plan on taking the following course of action, and this is what I'd like your advice on:\n\n## The Plan\n\nThe only thing I can afford to put into this project is more drive space, so that's what I'm doing. So, as much as I'd like to increase the RAM on the machine, as I've heard you should \"ideally\" have 1GB per 1TB of ZFS storage (but, tell me if that's B.S.), that's not an option at the moment. So, what I'm going to do, is:\n\n## Rebuild the storage on new, high-capacity drives\n\n1. I purchased and today received 3x [Seagate Exos X18 ST16000NM000J 16TB Drives](https://serverpartdeals.com/products/seagate-exos-x18-st16000nm000j-16tb-7-2k-rpm-sata-6gb-s-3-5-recertified-hard-drive) from ServerPartDeals.com;\n2. I checked the Seagate warranty for the drives (all the serial numbers say out of warranty and to contact the manufacturer), so I have an outstanding request out to SPD to hopefully provide a supplier warranty; I'll RMA them, I guess, if there's no warranty;\n3. In the meantime, I'm running [`bht`](https://github.com/ezonakiusagi/bht) on the drives overnight to check for any immediate problems; and,\n4. I'm  currently running a self-made script creating a `sha256sum` of every file on my existing drives for later verification.\n\nOnce the checksums are built, I'll simply unmount and remov the 4TB drives from the server and put the applications dependent on that data in maintenance mode (see: `SIGTERM`).\n\nI'm not planning on touching my 120GB Operating System SSD at all, - so `/dev/sda` isn't being touched. Ideally, I'd like to have this disk regularly backed up into my ZFS pool, or at least the configurations and applications; so, if anyone has any recommendations on how best to do this regularly, I'd very much appreciate it. I'm running Ubuntu Server.\n\n&amp;#x200B;\n\n&gt;***Note*** *- from here on, as I dive into ZFS , my terminology or understanding of best-practices may be incorrect.* ***Please correct me!***\n\n&amp;#x200B;\n\nWith the 48TB of additional capacity I just purchased, I plan on creating a 3-drive `raidz` **vdev** in a **zpool** called `standard` resulting in 32TB of usable, additional storage.\n\nWhy is it called `standard`? Because it's going to follow a very-standard 3-drive, single-drive parity structure that I'll repeat any time I need to expand the pool's size. My understanding is you can increase a vpool later by adding vdevs (right?).\n\nThis will give me 16TB of space to transfer from my existing drives and 16TB of free space for expansion, and finally with some redundancy. I'm not backing up any data on this pool, as everything here falls into the \"annoying, but not impossible to replace\" category.\n\nMy understanding is it's best-practice to use the UUID of these drives when creating the pools; so, my construction will look something like this on Ubuntu:\n\n    sudo apt install -y zfsutils-linux\n    \n    sudo zpool create standard raidz /dev/disk/by-uuid/XX01... /dev/disk/by-uuid/XX02... /dev/disk/by-uuid/XX03...\n\nWith any expansion later simply requiring another vdev (right?), like this:\n\n    sudo zpool add standard raidz /dev/disk/by-uuid/XX04... /dev/disk/by-uuid/XX05... /dev/disk/by-uuid/XX06...\n\nWith this done, I simply need to create a filesystem and mount it for access. I actually intend to create two filesystems, in this case - one for my HTPC media and one for my artifacts like Docker images, but please let me know if there are drawbacks or alternative considerations to this pattern:\n\n    sudo create standard/media\n    sudo zfs set compression=on standard/media\n    sudo zfs set mountpoint=/media/media standard/media\n    \n    sudo create standard/artifacts\n    sudo zfs set quota=2T standard/artifacts\n    sudo zfs set mountpoint=/media/artifacts standard/artifacts\n\nThe media directory, especially, is write-once, read-many and mostly large files. Our pattern of usage tends to have the same 1-2TB of data read regularly, with most stuff being read from my cache drive, anyway... hence why I thought `lz4` compression might be beneficial. There's only a couple users (my immediate family) using this; but, if this is a terrible idea, please let me know (read my note about **Caching**, below, before you comment, though!).\n\n## Transfer\n\nFrom here, I'd simply transfer the existing files from my 4TB drives with `rsync` to the `standard/media` mount point and reconfigure my applications, which is easy enough. I can re-generate the `sha256sum` hashes and verify them - although I believe `rsync` actually has a built-in archive function that will run its own verifications... maybe I can save some time there, as building sums for 16TB takes quite some time and is unnecessary wear on the drives.\n\n## About the critical data\n\nThis leaves me with 4x 4TB drives of differing ages and batches that seem like a great fit for my critical data. I'll do the whole thing again, wiping these drives, and using a `mirror` strategy to create 8TB of usable storage from the 16TB of space (note, I was originally going to use a `raidz2` strategy, but I realized that if I lose 2 drives with `raidz2`, I lose 100% of the vdev, meanwhile, if I lose 2 drives with a `mirror` strategy, my worst case scenario is a 50% data loss, and best case I have 0% data loss):\n\n    sudo zpool add critical mirror /dev/disk/by-uuid/XX01... /dev/disk/by-uuid/XX02... /dev/disk/by-uuid/XX03... /dev/disk/by-uuid/XX04...\n\nI want this data following a 3-2-1 strategy, too, so I need offsite backup. Right now, I simply copy files to Dropbox - but I'm outgrowing that solution. I really don't want to pay a regular fee for Backblaze or similar.\n\nInterestingly, I have a friend who is undergoing this same project right now (*waves hi, friend!*), so we've decided to be each other's offsite backup solution. We're each going to setup a quota'd filesystem in each others' `critical` vpools to, using client-side encryption to \"trade\" storage:\n\n    sudo create critical/backup\n    sudo zfs set mountpoint=/media/backup critical/backup\n    \n    sudo create critical/friend_name\n    sudo zfs set quota=2T critical/friend_name\n    sudo zfs set compression=on critical/friend_name\n    sudo zfs set mountpoint=/media/storage-trades/friend-name critical/friend_name\n\n## Caching\n\nSo now I have two pools, a few file systems, and configurations that suit my needs.\n\nThis leaves me with the 1TB Samsung 850 SSD that is currently caching, but all of that caching is managed by my applications. I'd much prefer to hand all that caching over to ZFS to manage, so all of my applications can benefit from the speeds (especially with my 8GB of RAM limitation).\n\nMost of my cache need will definitely come from the `standard` pool, but it would be useful to have some for the `critical` pool as well, as I run a few application databases in there with infrequent, but small and database-like, write patterns.\n\nAnyway, I think a 75/25 split of the drive capacity into two partitions makes sense. I'll give one to each of the vpools to use as a cache drive. That said, if there's a better way to let ZFS use this drive and figure out how best to cache across multiple pools (instead of me having to guess that 75/25 is the right split), please definitely let me know.\n\nSo I format and partition the drive:\n\n    sudo parted /dev/sdb\n    (parted) mklabel gpt\n    (parted) mkpart\n    Partition name?  []? cache_standard\n    File system type?  [ext2]? ext4\n    Start? 0%\n    End? 75%\n    (parted) mkpart\n    Partition name?  []? cache_critical\n    File system type?  [ext2]? ext4\n    Start? 75%\n    End? 100%\n    (parted) quit\n\nThen use each partition as a cache disk in the vpools:\n\n    sudo zpool add standard cache /dev/sdb1 -f\n    sudo zpool add critical cache /dev/sdb2 -f\n\nI'll be honest, I don't know what would be a more beneficial use for the SSD, a cache drive or a log drive. I settled on cache because this isn't a write-heavy server, but there's limited RAM, so maybe a log drive is the right choice? Let me know your thoughts.\n\n## Maintenance\n\nMy expectation is that ZFS out of the box is pretty well suited for my needs - but I definitely want to know if that's a dangerous assumption.\n\nMy understanding is that scrubbing occurs once a month - I'll likely bump that up to weekly, at least on the critical space. I also have SMART short-tests running every 4-hours on the drives and long-tests occurring weekly.\n\nAny other recommendations for best-practices would be great to hear, or where I should look into to advance my ZFS knowledge and experience, from here.\n\nFinally, I'd love to hear what utilities folks use for monitoring and alerts on their drives and with ZFS... eventually the Homelab will have a proper, centralized logging and alert platform for the entire lab, including storage, but something like \"YOUR DRIVE DONE BROKE\" in an email would more than suffice, for now, so that's likely my interim.\n\nIf you've read all the way to the bottom, here, I thank you - and I look forward to your thoughts, corrections, advice, critique, and collaboration! Cheers!", "author_fullname": "t2_43498", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rate My Plan - Giving Up my Chaotic Ways", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkoh9b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711073927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;For the last 7-8 years, I&amp;#39;ve been content with my old HTPC case and slapping a 4TB disk in whenever I  neared my capacity. I&amp;#39;m now at about 16TB of filled storage, and ready to slap again - but I&amp;#39;m getting to the point where I need something more robust.&lt;/p&gt;\n\n&lt;p&gt;My plan was to eventually create a JBOD and a dedicated storage server to manage it, but I&amp;#39;m not quite there yet; so, I&amp;#39;d like to work with what I have (plus a few more disks) to create a better solution, and I&amp;#39;d love your help reviewing my plan to do so.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t spend much on this project right now, but I have bought some higher-capacity drives and have the ability to reuse some of my old Home Theater PC hardware to take baby steps toward that goal. With that, these are my goals with this plan:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Move everything (except the Operating System) onto ZFS.&lt;/li&gt;\n&lt;li&gt;Implement single-drive parity for my non-critical data (about 14TB which would be annoying to replace, but not critical) and setup dual-drive parity for my critical media with an off-site backup solution (3-2-1).&lt;/li&gt;\n&lt;li&gt;Keep everything relatively performant and extensible, as I&amp;#39;ll eventually fill up both of these storage spaces.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My use case is 99% write-once read-often and very large files (e.g., home theater stuff); however, I do have some applications with very light database-style usage (think grandma&amp;#39;s recipe website).&lt;/p&gt;\n\n&lt;p&gt;So here&amp;#39;s my plan - please poke holes and help me avoid hurdles, as I&amp;#39;m entirely new to ZFS, especially, and there&amp;#39;s always room for improvement. And another thanks toeEveryone who helped with my &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1bgxank/best_current_way_to_rebuild_storage/\"&gt;last post&lt;/a&gt; to get here:&lt;/p&gt;\n\n&lt;p&gt;Without further ado...&lt;/p&gt;\n\n&lt;h2&gt;Where I&amp;#39;m at today....&lt;/h2&gt;\n\n&lt;p&gt;I have available to me a pretty decent HTPC case that can easily support 11+ SATA drives (9x 3.5&amp;quot; and 2x SSDs); it currently contains:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An &lt;strong&gt;Intel(R) Core(TM) i5-4590 CPU @ 3.30GHz&lt;/strong&gt; quad-core processor&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;8GB RAM&lt;/strong&gt; (non-ECC) taking up 2 of 4 slots &lt;em&gt;(what can I say? it&amp;#39;s an old box)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;A very underutilized &lt;strong&gt;PSU&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;The following &lt;strong&gt;storage disks&lt;/strong&gt; &lt;em&gt;(all connected via 6.0GB/s SATA)&lt;/em&gt;:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Device&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Type&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Capacity&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Model&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Purpose&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Power-On Hours / Years&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sda&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;SSD&lt;/td&gt;\n&lt;td align=\"left\"&gt;120 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;Samsung SSD 850&lt;/td&gt;\n&lt;td align=\"left\"&gt;Operating System&lt;/td&gt;\n&lt;td align=\"left\"&gt;58,451 hours / 6.67 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sdb&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;SSD&lt;/td&gt;\n&lt;td align=\"left\"&gt;1 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;Samsung SSD 850&lt;/td&gt;\n&lt;td align=\"left\"&gt;HTPC Transcoding + Cache&lt;/td&gt;\n&lt;td align=\"left\"&gt;45,648 hours / 5.21 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sdc&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;4 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;WD Red (&lt;code&gt;WDC WD40EFRX-68N&lt;/code&gt;)&lt;/td&gt;\n&lt;td align=\"left\"&gt;Non-Critical Media; Artifacts (e.g., Docker Images)&lt;/td&gt;\n&lt;td align=\"left\"&gt;24,505 hours / 2.80 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sdd&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;4 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;WD Red (&lt;code&gt;WDC WD40EFRX-68W&lt;/code&gt;)&lt;/td&gt;\n&lt;td align=\"left\"&gt;Non-Critical Media&lt;/td&gt;\n&lt;td align=\"left\"&gt;58,499 hours / 6.68 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sde&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;4 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;WD Red (&lt;code&gt;WDC WD40EFRX-68W&lt;/code&gt;)&lt;/td&gt;\n&lt;td align=\"left\"&gt;Non-Critical Media&lt;/td&gt;\n&lt;td align=\"left\"&gt;56,113 hours / 6.41 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sdf&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;4 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;WD Red (&lt;code&gt;WDC WD40EFRX-68N&lt;/code&gt;)&lt;/td&gt;\n&lt;td align=\"left\"&gt;Non-Critical Media and &lt;strong&gt;important 2nd backups from home computers&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30,906 hours / 3.53 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Now with the above... not a RAID or 3-2-1 backup in sight, mind you, like a good agent of chaos. All of those drives are nearly at-capacity (save the Operating System disk) with no recoverability. I do have an off-site backup solution for some folders containing critical information (so, yep, I&amp;#39;m aware drive resilience via RAID isn&amp;#39;t backup).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My data falls into three (3) categories:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;My operating system and configurations;&lt;/li&gt;\n&lt;li&gt;Non-critical data that&amp;#39;s annoying, but not impossible, to replace &lt;em&gt;(14TB and growing)&lt;/em&gt;; and,&lt;/li&gt;\n&lt;li&gt;Critical data that I cherish and can&amp;#39;t lose &lt;em&gt;(about 3TB)&lt;/em&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Instead of adding another yet-another chaos drive, I plan on taking the following course of action, and this is what I&amp;#39;d like your advice on:&lt;/p&gt;\n\n&lt;h2&gt;The Plan&lt;/h2&gt;\n\n&lt;p&gt;The only thing I can afford to put into this project is more drive space, so that&amp;#39;s what I&amp;#39;m doing. So, as much as I&amp;#39;d like to increase the RAM on the machine, as I&amp;#39;ve heard you should &amp;quot;ideally&amp;quot; have 1GB per 1TB of ZFS storage (but, tell me if that&amp;#39;s B.S.), that&amp;#39;s not an option at the moment. So, what I&amp;#39;m going to do, is:&lt;/p&gt;\n\n&lt;h2&gt;Rebuild the storage on new, high-capacity drives&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I purchased and today received 3x &lt;a href=\"https://serverpartdeals.com/products/seagate-exos-x18-st16000nm000j-16tb-7-2k-rpm-sata-6gb-s-3-5-recertified-hard-drive\"&gt;Seagate Exos X18 ST16000NM000J 16TB Drives&lt;/a&gt; from ServerPartDeals.com;&lt;/li&gt;\n&lt;li&gt;I checked the Seagate warranty for the drives (all the serial numbers say out of warranty and to contact the manufacturer), so I have an outstanding request out to SPD to hopefully provide a supplier warranty; I&amp;#39;ll RMA them, I guess, if there&amp;#39;s no warranty;&lt;/li&gt;\n&lt;li&gt;In the meantime, I&amp;#39;m running &lt;a href=\"https://github.com/ezonakiusagi/bht\"&gt;&lt;code&gt;bht&lt;/code&gt;&lt;/a&gt; on the drives overnight to check for any immediate problems; and,&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m  currently running a self-made script creating a &lt;code&gt;sha256sum&lt;/code&gt; of every file on my existing drives for later verification.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once the checksums are built, I&amp;#39;ll simply unmount and remov the 4TB drives from the server and put the applications dependent on that data in maintenance mode (see: &lt;code&gt;SIGTERM&lt;/code&gt;).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not planning on touching my 120GB Operating System SSD at all, - so &lt;code&gt;/dev/sda&lt;/code&gt; isn&amp;#39;t being touched. Ideally, I&amp;#39;d like to have this disk regularly backed up into my ZFS pool, or at least the configurations and applications; so, if anyone has any recommendations on how best to do this regularly, I&amp;#39;d very much appreciate it. I&amp;#39;m running Ubuntu Server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;- from here on, as I dive into ZFS , my terminology or understanding of best-practices may be incorrect.&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Please correct me!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;With the 48TB of additional capacity I just purchased, I plan on creating a 3-drive &lt;code&gt;raidz&lt;/code&gt; &lt;strong&gt;vdev&lt;/strong&gt; in a &lt;strong&gt;zpool&lt;/strong&gt; called &lt;code&gt;standard&lt;/code&gt; resulting in 32TB of usable, additional storage.&lt;/p&gt;\n\n&lt;p&gt;Why is it called &lt;code&gt;standard&lt;/code&gt;? Because it&amp;#39;s going to follow a very-standard 3-drive, single-drive parity structure that I&amp;#39;ll repeat any time I need to expand the pool&amp;#39;s size. My understanding is you can increase a vpool later by adding vdevs (right?).&lt;/p&gt;\n\n&lt;p&gt;This will give me 16TB of space to transfer from my existing drives and 16TB of free space for expansion, and finally with some redundancy. I&amp;#39;m not backing up any data on this pool, as everything here falls into the &amp;quot;annoying, but not impossible to replace&amp;quot; category.&lt;/p&gt;\n\n&lt;p&gt;My understanding is it&amp;#39;s best-practice to use the UUID of these drives when creating the pools; so, my construction will look something like this on Ubuntu:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo apt install -y zfsutils-linux\n\nsudo zpool create standard raidz /dev/disk/by-uuid/XX01... /dev/disk/by-uuid/XX02... /dev/disk/by-uuid/XX03...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;With any expansion later simply requiring another vdev (right?), like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo zpool add standard raidz /dev/disk/by-uuid/XX04... /dev/disk/by-uuid/XX05... /dev/disk/by-uuid/XX06...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;With this done, I simply need to create a filesystem and mount it for access. I actually intend to create two filesystems, in this case - one for my HTPC media and one for my artifacts like Docker images, but please let me know if there are drawbacks or alternative considerations to this pattern:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo create standard/media\nsudo zfs set compression=on standard/media\nsudo zfs set mountpoint=/media/media standard/media\n\nsudo create standard/artifacts\nsudo zfs set quota=2T standard/artifacts\nsudo zfs set mountpoint=/media/artifacts standard/artifacts\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The media directory, especially, is write-once, read-many and mostly large files. Our pattern of usage tends to have the same 1-2TB of data read regularly, with most stuff being read from my cache drive, anyway... hence why I thought &lt;code&gt;lz4&lt;/code&gt; compression might be beneficial. There&amp;#39;s only a couple users (my immediate family) using this; but, if this is a terrible idea, please let me know (read my note about &lt;strong&gt;Caching&lt;/strong&gt;, below, before you comment, though!).&lt;/p&gt;\n\n&lt;h2&gt;Transfer&lt;/h2&gt;\n\n&lt;p&gt;From here, I&amp;#39;d simply transfer the existing files from my 4TB drives with &lt;code&gt;rsync&lt;/code&gt; to the &lt;code&gt;standard/media&lt;/code&gt; mount point and reconfigure my applications, which is easy enough. I can re-generate the &lt;code&gt;sha256sum&lt;/code&gt; hashes and verify them - although I believe &lt;code&gt;rsync&lt;/code&gt; actually has a built-in archive function that will run its own verifications... maybe I can save some time there, as building sums for 16TB takes quite some time and is unnecessary wear on the drives.&lt;/p&gt;\n\n&lt;h2&gt;About the critical data&lt;/h2&gt;\n\n&lt;p&gt;This leaves me with 4x 4TB drives of differing ages and batches that seem like a great fit for my critical data. I&amp;#39;ll do the whole thing again, wiping these drives, and using a &lt;code&gt;mirror&lt;/code&gt; strategy to create 8TB of usable storage from the 16TB of space (note, I was originally going to use a &lt;code&gt;raidz2&lt;/code&gt; strategy, but I realized that if I lose 2 drives with &lt;code&gt;raidz2&lt;/code&gt;, I lose 100% of the vdev, meanwhile, if I lose 2 drives with a &lt;code&gt;mirror&lt;/code&gt; strategy, my worst case scenario is a 50% data loss, and best case I have 0% data loss):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo zpool add critical mirror /dev/disk/by-uuid/XX01... /dev/disk/by-uuid/XX02... /dev/disk/by-uuid/XX03... /dev/disk/by-uuid/XX04...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I want this data following a 3-2-1 strategy, too, so I need offsite backup. Right now, I simply copy files to Dropbox - but I&amp;#39;m outgrowing that solution. I really don&amp;#39;t want to pay a regular fee for Backblaze or similar.&lt;/p&gt;\n\n&lt;p&gt;Interestingly, I have a friend who is undergoing this same project right now (&lt;em&gt;waves hi, friend!&lt;/em&gt;), so we&amp;#39;ve decided to be each other&amp;#39;s offsite backup solution. We&amp;#39;re each going to setup a quota&amp;#39;d filesystem in each others&amp;#39; &lt;code&gt;critical&lt;/code&gt; vpools to, using client-side encryption to &amp;quot;trade&amp;quot; storage:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo create critical/backup\nsudo zfs set mountpoint=/media/backup critical/backup\n\nsudo create critical/friend_name\nsudo zfs set quota=2T critical/friend_name\nsudo zfs set compression=on critical/friend_name\nsudo zfs set mountpoint=/media/storage-trades/friend-name critical/friend_name\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;Caching&lt;/h2&gt;\n\n&lt;p&gt;So now I have two pools, a few file systems, and configurations that suit my needs.&lt;/p&gt;\n\n&lt;p&gt;This leaves me with the 1TB Samsung 850 SSD that is currently caching, but all of that caching is managed by my applications. I&amp;#39;d much prefer to hand all that caching over to ZFS to manage, so all of my applications can benefit from the speeds (especially with my 8GB of RAM limitation).&lt;/p&gt;\n\n&lt;p&gt;Most of my cache need will definitely come from the &lt;code&gt;standard&lt;/code&gt; pool, but it would be useful to have some for the &lt;code&gt;critical&lt;/code&gt; pool as well, as I run a few application databases in there with infrequent, but small and database-like, write patterns.&lt;/p&gt;\n\n&lt;p&gt;Anyway, I think a 75/25 split of the drive capacity into two partitions makes sense. I&amp;#39;ll give one to each of the vpools to use as a cache drive. That said, if there&amp;#39;s a better way to let ZFS use this drive and figure out how best to cache across multiple pools (instead of me having to guess that 75/25 is the right split), please definitely let me know.&lt;/p&gt;\n\n&lt;p&gt;So I format and partition the drive:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo parted /dev/sdb\n(parted) mklabel gpt\n(parted) mkpart\nPartition name?  []? cache_standard\nFile system type?  [ext2]? ext4\nStart? 0%\nEnd? 75%\n(parted) mkpart\nPartition name?  []? cache_critical\nFile system type?  [ext2]? ext4\nStart? 75%\nEnd? 100%\n(parted) quit\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then use each partition as a cache disk in the vpools:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo zpool add standard cache /dev/sdb1 -f\nsudo zpool add critical cache /dev/sdb2 -f\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ll be honest, I don&amp;#39;t know what would be a more beneficial use for the SSD, a cache drive or a log drive. I settled on cache because this isn&amp;#39;t a write-heavy server, but there&amp;#39;s limited RAM, so maybe a log drive is the right choice? Let me know your thoughts.&lt;/p&gt;\n\n&lt;h2&gt;Maintenance&lt;/h2&gt;\n\n&lt;p&gt;My expectation is that ZFS out of the box is pretty well suited for my needs - but I definitely want to know if that&amp;#39;s a dangerous assumption.&lt;/p&gt;\n\n&lt;p&gt;My understanding is that scrubbing occurs once a month - I&amp;#39;ll likely bump that up to weekly, at least on the critical space. I also have SMART short-tests running every 4-hours on the drives and long-tests occurring weekly.&lt;/p&gt;\n\n&lt;p&gt;Any other recommendations for best-practices would be great to hear, or where I should look into to advance my ZFS knowledge and experience, from here.&lt;/p&gt;\n\n&lt;p&gt;Finally, I&amp;#39;d love to hear what utilities folks use for monitoring and alerts on their drives and with ZFS... eventually the Homelab will have a proper, centralized logging and alert platform for the entire lab, including storage, but something like &amp;quot;YOUR DRIVE DONE BROKE&amp;quot; in an email would more than suffice, for now, so that&amp;#39;s likely my interim.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve read all the way to the bottom, here, I thank you - and I look forward to your thoughts, corrections, advice, critique, and collaboration! Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?auto=webp&amp;s=89c4ffa73482cef13b51593b4245c7c8bc91357d", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3faf7a527dba156060b6c5387da87bc4c217f961", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc6b141dd4231615ed2bcb3dbc3180e8e312693f", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cabb4b8cc62554c5dfd6599257358cc5642a50c5", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df067833aebe9d9cf9f5b3a2c542f2ab1b595b30", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f70fcc21f59c9cd9b557a11ca2db9af59a6f25f0", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f637eb1c0e535637f052b3a74f50986e977c1c2b", "width": 1080, "height": 1080}], "variants": {}, "id": "Oi9IobXaT6dZqAg3ezTwRUUH6Cow8RYcOx4AM3ZxO0Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkoh9b", "is_robot_indexable": true, "report_reasons": null, "author": "wspnut", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkoh9b/rate_my_plan_giving_up_my_chaotic_ways/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkoh9b/rate_my_plan_giving_up_my_chaotic_ways/", "subreddit_subscribers": 740232, "created_utc": 1711073927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nLooking to download a video of a funeral service that was live streamed. The funeral home is asking for $200 for this file. Any help would be appreciated. \n\nThank you so much Foydianslip88. \n\nI was able to download the video with his suggestion. ", "author_fullname": "t2_s1p2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Funeral Service Video Download ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkojch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711124304.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711074095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to download a video of a funeral service that was live streamed. The funeral home is asking for $200 for this file. Any help would be appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thank you so much Foydianslip88. &lt;/p&gt;\n\n&lt;p&gt;I was able to download the video with his suggestion. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkojch", "is_robot_indexable": true, "report_reasons": null, "author": "Ironicallyi", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkojch/funeral_service_video_download/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkojch/funeral_service_video_download/", "subreddit_subscribers": 740232, "created_utc": 1711074095.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}