{"kind": "Listing", "data": {"after": "t3_1bktn9d", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm on my way to learn DE, and this sub has helped me a lot to understand better many concepts, but one thing I see very often is that Postgres seems to be the best DWH in most cases, so my question is, which are those cases where Postgres is NOT recommended? What alternatives are better suited for that situation and Why? \n\nThank you! ", "author_fullname": "t2_4iz2pipg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When NOT to use PostgreSQL? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bklthc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 72, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 72, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711066181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m on my way to learn DE, and this sub has helped me a lot to understand better many concepts, but one thing I see very often is that Postgres seems to be the best DWH in most cases, so my question is, which are those cases where Postgres is NOT recommended? What alternatives are better suited for that situation and Why? &lt;/p&gt;\n\n&lt;p&gt;Thank you! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bklthc", "is_robot_indexable": true, "report_reasons": null, "author": "_Lavender_Brown_", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bklthc/when_not_to_use_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bklthc/when_not_to_use_postgresql/", "subreddit_subscribers": 170974, "created_utc": 1711066181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, r/dataengineering!   \n\nOver the last ten years, I've written tons of SQL and learned a few lessons. I [summarize them in a blog post.](https://ploomber.io/blog/sql/)\n\nA few things I discuss:   \n\n* When should I use Python/R over SQL? (and vice versa)   \n* How to write clean SQL queries   \n* How to document queries   \n* Auto-formatting   \n* Debugging   \n* Templating   \n* Testing   \n\nI hope you enjoy it!   ", "author_fullname": "t2_40t1gisl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing effective SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl3n3r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711125480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;!   &lt;/p&gt;\n\n&lt;p&gt;Over the last ten years, I&amp;#39;ve written tons of SQL and learned a few lessons. I &lt;a href=\"https://ploomber.io/blog/sql/\"&gt;summarize them in a blog post.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A few things I discuss:   &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When should I use Python/R over SQL? (and vice versa)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;How to write clean SQL queries&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;How to document queries&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Auto-formatting&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Debugging&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Templating&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Testing&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I hope you enjoy it!   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bl3n3r", "is_robot_indexable": true, "report_reasons": null, "author": "databot_", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl3n3r/writing_effective_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl3n3r/writing_effective_sql/", "subreddit_subscribers": 170974, "created_utc": 1711125480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently using a Kafka cluster primarily for receiving monitoring data from our other systems. The overall throughput of our cluster is around 20GB/s. As reading is usually lagging production by several minutes, most reads require disk access.  \nLong-term usage of Kafka has exposed some serious issues that we hope to address through some technical upgrades. These problems include:\n\n  \n**Cost**: We are using a large number of high-spec SSDs to handle the reading of historical data, which are quite expensive.\n\n  \n**Elasticity**: Scaling Kafka clusters up and down is a risky operation. We have not been able to make adjustments to the cluster's capacity without affecting the business, which is quite bothersome. Moreover, each scaling operation is accompanied by a large amount of partition replication, which affects read and write operations.  \nSince many of our applications are already running based on Kafka, we do not want to start from scratch. Therefore, we have researched some solutions compatible with the Kafka protocol. Could you recommend which one to use in our case, or suggest a better solution?  \n[WarpStream](https://www.warpstream.com/): It seems like a decent solution. However, some of our Kafka clusters are quite latency-sensitive, and we have concerns that using S3 as the only storage for reading and writing might not meet our requirements in some scenarios. Also, we initially want to try an open-source solution, and WarpStream is not open-source.  \n[Redpanda](https://github.com/redpanda-data/redpanda): It seems to match our needs quite well overall. It supports S3-based tiered storage to reduce costs and offer great performance. However, we are not sure whether it has solved the elasticity issues of Kafka.  \n[AutoMQ](https://github.com/AutoMQ/automq-for-kafka):  This open-source project seems to match our needs the best at the moment. It extensively reuses Kafka's original code, only modifying Kafka's underlying storage to be S3-based. Its claimed features of replicating partitions in seconds and rebalancing network traffic automatically are the key challenges we have encountered while using Kafka. Another major reason we are currently leaning towards this solution is its high compatibility with Apache Kafka, as we do not want to require the entire business system to upgrade while we upgrade the messaging system's technical architecture.  \n\n\nThe above represents my personal learning and research results, which may not be entirely accurate. Can you help me validate the accuracy of my technical considerations? If there are better Kafka solutions, I would also appreciate your suggestions.  \n", "author_fullname": "t2_w5zv981t7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Kafka solution best match my scenario?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bktex1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711091866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently using a Kafka cluster primarily for receiving monitoring data from our other systems. The overall throughput of our cluster is around 20GB/s. As reading is usually lagging production by several minutes, most reads require disk access.&lt;br/&gt;\nLong-term usage of Kafka has exposed some serious issues that we hope to address through some technical upgrades. These problems include:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;: We are using a large number of high-spec SSDs to handle the reading of historical data, which are quite expensive.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Elasticity&lt;/strong&gt;: Scaling Kafka clusters up and down is a risky operation. We have not been able to make adjustments to the cluster&amp;#39;s capacity without affecting the business, which is quite bothersome. Moreover, each scaling operation is accompanied by a large amount of partition replication, which affects read and write operations.&lt;br/&gt;\nSince many of our applications are already running based on Kafka, we do not want to start from scratch. Therefore, we have researched some solutions compatible with the Kafka protocol. Could you recommend which one to use in our case, or suggest a better solution?&lt;br/&gt;\n&lt;a href=\"https://www.warpstream.com/\"&gt;WarpStream&lt;/a&gt;: It seems like a decent solution. However, some of our Kafka clusters are quite latency-sensitive, and we have concerns that using S3 as the only storage for reading and writing might not meet our requirements in some scenarios. Also, we initially want to try an open-source solution, and WarpStream is not open-source.&lt;br/&gt;\n&lt;a href=\"https://github.com/redpanda-data/redpanda\"&gt;Redpanda&lt;/a&gt;: It seems to match our needs quite well overall. It supports S3-based tiered storage to reduce costs and offer great performance. However, we are not sure whether it has solved the elasticity issues of Kafka.&lt;br/&gt;\n&lt;a href=\"https://github.com/AutoMQ/automq-for-kafka\"&gt;AutoMQ&lt;/a&gt;:  This open-source project seems to match our needs the best at the moment. It extensively reuses Kafka&amp;#39;s original code, only modifying Kafka&amp;#39;s underlying storage to be S3-based. Its claimed features of replicating partitions in seconds and rebalancing network traffic automatically are the key challenges we have encountered while using Kafka. Another major reason we are currently leaning towards this solution is its high compatibility with Apache Kafka, as we do not want to require the entire business system to upgrade while we upgrade the messaging system&amp;#39;s technical architecture.  &lt;/p&gt;\n\n&lt;p&gt;The above represents my personal learning and research results, which may not be entirely accurate. Can you help me validate the accuracy of my technical considerations? If there are better Kafka solutions, I would also appreciate your suggestions.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?auto=webp&amp;s=ff284f11ac8f0f90e8caa94666bd750b03c6b066", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0ecf3998e7ec77914eb73b6f0c5ee32620845a7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=346932ecfac8795af83daa4a2099968155375bd3", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b55a836ac226b24511cd0522059fa77db1acab9b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fcf5f0d9d5be599de11a47aa560d1159c26638d4", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=83e0183ecb82586470f1b3b9506f109e416b8333", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=38861cbd662ddc1423985a2ea052b024b95c5423", "width": 1080, "height": 567}], "variants": {}, "id": "Gap-kV8Fnj_BYQDMqW_-RQJyqwJpXaay3pZZTkcPjvE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bktex1", "is_robot_indexable": true, "report_reasons": null, "author": "tommy_19882024", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bktex1/which_kafka_solution_best_match_my_scenario/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bktex1/which_kafka_solution_best_match_my_scenario/", "subreddit_subscribers": 170974, "created_utc": 1711091866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa5dw92do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Not Run a $12,000 Query on Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl1j5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1GdweO6OB9ixjhp8vNoJ6q3DSS5mwdjQRrMEannJoi8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711120159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "baselit.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://baselit.ai/blog/how-to-not-run-a-12-000-query-on-snowflake", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7SBt48cgQXAL30En_lDW9_joyuoEunbKAdVESu6zQJg.jpg?auto=webp&amp;s=a1c6f18447dde18e1ebf0c54186ad305832ea3f2", "width": 4096, "height": 2731}, "resolutions": [{"url": "https://external-preview.redd.it/7SBt48cgQXAL30En_lDW9_joyuoEunbKAdVESu6zQJg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2858a576acfd1407493a054435451b9025b7cdd2", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/7SBt48cgQXAL30En_lDW9_joyuoEunbKAdVESu6zQJg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3743c9c263eb0397467d04d255c76ea2958ee919", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/7SBt48cgQXAL30En_lDW9_joyuoEunbKAdVESu6zQJg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=303361fad79d3a329688add4d0e7caf55c8aed4e", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/7SBt48cgQXAL30En_lDW9_joyuoEunbKAdVESu6zQJg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f0c149c09a357a63edb8173e0729a334d0f6d69", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/7SBt48cgQXAL30En_lDW9_joyuoEunbKAdVESu6zQJg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ffb74d66c8ea2f7d508ac778344d8092031e26b1", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/7SBt48cgQXAL30En_lDW9_joyuoEunbKAdVESu6zQJg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=17c53c9a6403761ce98c71b3802e595205ede32f", "width": 1080, "height": 720}], "variants": {}, "id": "BzltQecOGowQaXLG7pSLGEO_CMpQbsWKE1efbZI8up4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bl1j5k", "is_robot_indexable": true, "report_reasons": null, "author": "sahil_singla", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl1j5k/how_to_not_run_a_12000_query_on_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://baselit.ai/blog/how-to-not-run-a-12-000-query-on-snowflake", "subreddit_subscribers": 170974, "created_utc": 1711120159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'd like to hear about the Databricks projects that pushed the limits. Enough with the medallion architecture and simple ETL/ELT demos...", "author_fullname": "t2_3tzpeuhd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Share your Databricks war stories: What were your toughest use cases/projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bky3mv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711110665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to hear about the Databricks projects that pushed the limits. Enough with the medallion architecture and simple ETL/ELT demos...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bky3mv", "is_robot_indexable": true, "report_reasons": null, "author": "randomusicjunkie", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bky3mv/share_your_databricks_war_stories_what_were_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bky3mv/share_your_databricks_war_stories_what_were_your/", "subreddit_subscribers": 170974, "created_utc": 1711110665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ttr4u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafbat UI for Apache Kafka v1.0 is out!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkzmhm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/05kD2p_pWfJr-IigG2Fge1Vmk5GjbuF7hUBBMk0Imaw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711115180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/kafbat/kafka-ui/releases/tag/v1.0.0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bUp5P3_PUyhrpNYB1Si_-78Y1dwhKOgTjipd6HmzH5I.jpg?auto=webp&amp;s=bc937b62e21690f6720517607ea4c0a209743e21", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/bUp5P3_PUyhrpNYB1Si_-78Y1dwhKOgTjipd6HmzH5I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ea26a819ae59cc24c216fb7791b7c9649d85dde", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bUp5P3_PUyhrpNYB1Si_-78Y1dwhKOgTjipd6HmzH5I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fc472d2d1ea3dd1d589a053d0ee27e236915545d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/bUp5P3_PUyhrpNYB1Si_-78Y1dwhKOgTjipd6HmzH5I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2841154d2c8eb11266025cdcab9c992447e6fe5b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/bUp5P3_PUyhrpNYB1Si_-78Y1dwhKOgTjipd6HmzH5I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4376484e5ea4012782073d754caae178a069693f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/bUp5P3_PUyhrpNYB1Si_-78Y1dwhKOgTjipd6HmzH5I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=62bf5dc0e3422533120ae230bb5af5f8e36b5eab", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/bUp5P3_PUyhrpNYB1Si_-78Y1dwhKOgTjipd6HmzH5I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0669f1e58cf3b62e2e35f01202d76e947bded301", "width": 1080, "height": 540}], "variants": {}, "id": "DhM3cmUsbInNpFXOIBOdyOHnwT20L_SpF0eRBJFiok8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1bkzmhm", "is_robot_indexable": true, "report_reasons": null, "author": "Haarolean", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkzmhm/kafbat_ui_for_apache_kafka_v10_is_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/kafbat/kafka-ui/releases/tag/v1.0.0", "subreddit_subscribers": 170974, "created_utc": 1711115180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI\u2019m starting a job where I have to extract json files from API and then put the data in azure SQL DB.\n\nI was thinking on running the scripts on VM and setting the API key as environment variable, but would like best practices when it comes to similar scenarios?\n\n\n", "author_fullname": "t2_8b259b71", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extract data from API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl0iho", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711117519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m starting a job where I have to extract json files from API and then put the data in azure SQL DB.&lt;/p&gt;\n\n&lt;p&gt;I was thinking on running the scripts on VM and setting the API key as environment variable, but would like best practices when it comes to similar scenarios?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bl0iho", "is_robot_indexable": true, "report_reasons": null, "author": "andreeva_2", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl0iho/extract_data_from_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl0iho/extract_data_from_api/", "subreddit_subscribers": 170974, "created_utc": 1711117519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nI have been struggling for hours to get this simple real-time streaming feature to work and desperately need some help.  \n\n\nI am trying to achieve a real-time streaming application with Spark Structured Streaming to calculate the average value of power aggregated over the latest minute. So say if the time now is 18:15:00, I only want the application to write the average value calculated between 18:14:00 - 18:15:00 to the console. The frequency to write to console is set to 10 seconds and it should only output the MOST UPDATED averaged value.  \n\n\nThis is the simplified code: \n\n    aggregated_df = input_df \\\n        .groupBy(\n            F.window(\"engine_time\", \"60 seconds\", \"10 seconds\"),\n        ).agg(\n            F.format_number(F.avg(\"power\"), 2)\n    \u00a0 \u00a0 )\n    \n    # Some more transformation\n    \n    output = res_df.writeStream \\\n    \u00a0   .outputMode(\"update\") \\\n    \u00a0   .format(\"console\") \\\n    \u00a0 \u00a0 .option(\"truncate\", False) \\\n    \u00a0   .trigger(processingTime='10 seconds') \\\n    \u00a0 \u00a0 .start()\n\nThe problem with this is that the averaging window depends on the event time (i.e. engine\\_time) instead of the actual time now. The engine\\_time can sometimes be delayed. If it's 15 seconds delayed, the whole averaging window will be shifted by the same amount.  \n\n\nIf the outputMode is set to \"complete\", all past rows are shown which is not helpful.\n\nIf the outputMode is set to \"update\", it will show the 10-second-window rows that are updated which is not helpful either.\n\nIf the outputMode is set to \"append\", it will show 1 row at a time which is good but there is a 10 seconds delay.  \n\n\nHOW DO I GET IT TO FREAKING WORK!!?!?\n\nAny help much appreciated!\n\n&amp;#x200B;\n\nNOTE: Late data can be completely ignored which is why I am not using watermark.", "author_fullname": "t2_e6ol6ewl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cannot realtime streaming with Spark Structured Streaming!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl73gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711134017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;I have been struggling for hours to get this simple real-time streaming feature to work and desperately need some help.  &lt;/p&gt;\n\n&lt;p&gt;I am trying to achieve a real-time streaming application with Spark Structured Streaming to calculate the average value of power aggregated over the latest minute. So say if the time now is 18:15:00, I only want the application to write the average value calculated between 18:14:00 - 18:15:00 to the console. The frequency to write to console is set to 10 seconds and it should only output the MOST UPDATED averaged value.  &lt;/p&gt;\n\n&lt;p&gt;This is the simplified code: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;aggregated_df = input_df \\\n    .groupBy(\n        F.window(&amp;quot;engine_time&amp;quot;, &amp;quot;60 seconds&amp;quot;, &amp;quot;10 seconds&amp;quot;),\n    ).agg(\n        F.format_number(F.avg(&amp;quot;power&amp;quot;), 2)\n\u00a0 \u00a0 )\n\n# Some more transformation\n\noutput = res_df.writeStream \\\n\u00a0   .outputMode(&amp;quot;update&amp;quot;) \\\n\u00a0   .format(&amp;quot;console&amp;quot;) \\\n\u00a0 \u00a0 .option(&amp;quot;truncate&amp;quot;, False) \\\n\u00a0   .trigger(processingTime=&amp;#39;10 seconds&amp;#39;) \\\n\u00a0 \u00a0 .start()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The problem with this is that the averaging window depends on the event time (i.e. engine_time) instead of the actual time now. The engine_time can sometimes be delayed. If it&amp;#39;s 15 seconds delayed, the whole averaging window will be shifted by the same amount.  &lt;/p&gt;\n\n&lt;p&gt;If the outputMode is set to &amp;quot;complete&amp;quot;, all past rows are shown which is not helpful.&lt;/p&gt;\n\n&lt;p&gt;If the outputMode is set to &amp;quot;update&amp;quot;, it will show the 10-second-window rows that are updated which is not helpful either.&lt;/p&gt;\n\n&lt;p&gt;If the outputMode is set to &amp;quot;append&amp;quot;, it will show 1 row at a time which is good but there is a 10 seconds delay.  &lt;/p&gt;\n\n&lt;p&gt;HOW DO I GET IT TO FREAKING WORK!!?!?&lt;/p&gt;\n\n&lt;p&gt;Any help much appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;NOTE: Late data can be completely ignored which is why I am not using watermark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bl73gj", "is_robot_indexable": true, "report_reasons": null, "author": "rpi_hy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl73gj/cannot_realtime_streaming_with_spark_structured/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl73gj/cannot_realtime_streaming_with_spark_structured/", "subreddit_subscribers": 170974, "created_utc": 1711134017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently I found myself having to try find the causes of slowness in an AWS Glue Spark job. Eventually I landed on a stage that had 99% skew (see screenshot).\n\nhttps://preview.redd.it/pb4bgm4zywpc1.png?width=1884&amp;format=png&amp;auto=webp&amp;s=8aa8e7c3c8da88bcf751f3278b6e9b73432c852c\n\nIt's my first time having to dig this deep into Spark so I was a bit out of my depth. The job itself also didn't help, because it's basically a single step like this:\n\n    spark.sql(a_700_lines_sql_query)\n\nThe query isn't even that esoteric, but it joins many wide (and relatively long) tables, all at once at the end of it.\n\nBy looking at the DAG, I found out that this specific stage (which is also the longest one), happens in correspondence of one of those joins. Knowing that skews happen (also?) because of uneven distribution of partition keys, I've checked the join keys of the 2 involved tables, but found nothing dodgy.\n\nI'm a bit at a loss here so my question to you is: in general, how do you address situations of this kind?\n\nAssume that I can't touch the SQL query itself because it was written by another team we have no control over.", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you generally debug a heavy skew in a spark job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 24, "top_awarded_type": null, "hide_score": false, "media_metadata": {"pb4bgm4zywpc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 18, "x": 108, "u": "https://preview.redd.it/pb4bgm4zywpc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=470e9af5e2738244d53301f07f4c96121c4680d2"}, {"y": 37, "x": 216, "u": "https://preview.redd.it/pb4bgm4zywpc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=143007123498fd7566145a6ab5955790fe287def"}, {"y": 55, "x": 320, "u": "https://preview.redd.it/pb4bgm4zywpc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=240601d32ebdba4a55b4d5fbda38f898541a3aa1"}, {"y": 110, "x": 640, "u": "https://preview.redd.it/pb4bgm4zywpc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9255adac2be8776555fb92c3b807164f607597d"}, {"y": 165, "x": 960, "u": "https://preview.redd.it/pb4bgm4zywpc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7f2ab7c648add162b32ae6095257f54b85f6c90a"}, {"y": 186, "x": 1080, "u": "https://preview.redd.it/pb4bgm4zywpc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=93d098d50d8fe61de8e24e17453ef0a388a7babf"}], "s": {"y": 325, "x": 1884, "u": "https://preview.redd.it/pb4bgm4zywpc1.png?width=1884&amp;format=png&amp;auto=webp&amp;s=8aa8e7c3c8da88bcf751f3278b6e9b73432c852c"}, "id": "pb4bgm4zywpc1"}}, "name": "t3_1bl3z33", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tOprnMASS_NcqtfZkpRrjd_hKBoRPgG3py1YphXdlnw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711126302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I found myself having to try find the causes of slowness in an AWS Glue Spark job. Eventually I landed on a stage that had 99% skew (see screenshot).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pb4bgm4zywpc1.png?width=1884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8aa8e7c3c8da88bcf751f3278b6e9b73432c852c\"&gt;https://preview.redd.it/pb4bgm4zywpc1.png?width=1884&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8aa8e7c3c8da88bcf751f3278b6e9b73432c852c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s my first time having to dig this deep into Spark so I was a bit out of my depth. The job itself also didn&amp;#39;t help, because it&amp;#39;s basically a single step like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;spark.sql(a_700_lines_sql_query)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The query isn&amp;#39;t even that esoteric, but it joins many wide (and relatively long) tables, all at once at the end of it.&lt;/p&gt;\n\n&lt;p&gt;By looking at the DAG, I found out that this specific stage (which is also the longest one), happens in correspondence of one of those joins. Knowing that skews happen (also?) because of uneven distribution of partition keys, I&amp;#39;ve checked the join keys of the 2 involved tables, but found nothing dodgy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a bit at a loss here so my question to you is: in general, how do you address situations of this kind?&lt;/p&gt;\n\n&lt;p&gt;Assume that I can&amp;#39;t touch the SQL query itself because it was written by another team we have no control over.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bl3z33", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl3z33/how_would_you_generally_debug_a_heavy_skew_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl3z33/how_would_you_generally_debug_a_heavy_skew_in_a/", "subreddit_subscribers": 170974, "created_utc": 1711126302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company, we restrict certain tables to certain people in the company, depending on their clearance to see the tables.  \n\n\nWe have implemented IAM permissions at the table level. Say data engineer X should not see finance records, so he is not on the list of people who can view the tables.\n\nThe dilemma comes in when using service accounts. Airflow needs access to the tables for extraction and DBT for modelling. What's stopping engineer X from using such to gain access to the private tables?\n", "author_fullname": "t2_2gfm4wqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery IAM dilemma", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl0kxw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711118975.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711117703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company, we restrict certain tables to certain people in the company, depending on their clearance to see the tables.  &lt;/p&gt;\n\n&lt;p&gt;We have implemented IAM permissions at the table level. Say data engineer X should not see finance records, so he is not on the list of people who can view the tables.&lt;/p&gt;\n\n&lt;p&gt;The dilemma comes in when using service accounts. Airflow needs access to the tables for extraction and DBT for modelling. What&amp;#39;s stopping engineer X from using such to gain access to the private tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bl0kxw", "is_robot_indexable": true, "report_reasons": null, "author": "5pitt4", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl0kxw/bigquery_iam_dilemma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl0kxw/bigquery_iam_dilemma/", "subreddit_subscribers": 170974, "created_utc": 1711117703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nI am currently a Junior in college and am doing an end-to-end analytics project that requires data extraction (web scraping), data cleaning, EDA, etc... Right now I was wondering if there's any way to schedule the [extraction.py](https://extraction.py) file to run every 2 weeks, then trigger the data\\_cleaning.py file to run after the [extraction.py](https://extraction.py) file. Also, I am open to any feedback regarding my project. Since I am an MIS major instead of CS, my code might not be as clean as it is supposed to be, but I am trying my best to work on it daily. Truly appreciate the feedback and the help.\n\n[Project Link](https://github.com/MarkPhamm/British-Airway)", "author_fullname": "t2_7vqsib1l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking for help with side Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkq5yk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711079233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;I am currently a Junior in college and am doing an end-to-end analytics project that requires data extraction (web scraping), data cleaning, EDA, etc... Right now I was wondering if there&amp;#39;s any way to schedule the &lt;a href=\"https://extraction.py\"&gt;extraction.py&lt;/a&gt; file to run every 2 weeks, then trigger the data_cleaning.py file to run after the &lt;a href=\"https://extraction.py\"&gt;extraction.py&lt;/a&gt; file. Also, I am open to any feedback regarding my project. Since I am an MIS major instead of CS, my code might not be as clean as it is supposed to be, but I am trying my best to work on it daily. Truly appreciate the feedback and the help.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/MarkPhamm/British-Airway\"&gt;Project Link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkq5yk", "is_robot_indexable": true, "report_reasons": null, "author": "SmartPersonality1862", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkq5yk/asking_for_help_with_side_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkq5yk/asking_for_help_with_side_project/", "subreddit_subscribers": 170974, "created_utc": 1711079233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I work in Data and mostly have employees working on google sheets where data is processing step by step by diff people before its been locked and then we move to next sheet for another day. \nWe are looking for a database which automatically stores this data from time to time and where I can connect this database to a visualization tool say PpwerBi. This way if I need to look at 2023 data I can code and extract all data from this database. \n\nSorry for lame tech lang but Im not a tech person and work for a disorganized and small healthcsre company. \n\nQuestion is what show we really get? We only have sheeets and powerbi. No place where we store our data. If we need to do analayses on yearly basis, need to copy paste values from several sheets into excel and import to powerBI. Ofcourse can connect bi to sheets but thats going to be 500+ connections on yearly basis which powerbi cannot handle\nThanks", "author_fullname": "t2_83gq3oxts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkrjby", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711084072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I work in Data and mostly have employees working on google sheets where data is processing step by step by diff people before its been locked and then we move to next sheet for another day. \nWe are looking for a database which automatically stores this data from time to time and where I can connect this database to a visualization tool say PpwerBi. This way if I need to look at 2023 data I can code and extract all data from this database. &lt;/p&gt;\n\n&lt;p&gt;Sorry for lame tech lang but Im not a tech person and work for a disorganized and small healthcsre company. &lt;/p&gt;\n\n&lt;p&gt;Question is what show we really get? We only have sheeets and powerbi. No place where we store our data. If we need to do analayses on yearly basis, need to copy paste values from several sheets into excel and import to powerBI. Ofcourse can connect bi to sheets but thats going to be 500+ connections on yearly basis which powerbi cannot handle\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkrjby", "is_robot_indexable": true, "report_reasons": null, "author": "FigTraditional1201", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkrjby/database_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkrjby/database_solution/", "subreddit_subscribers": 170974, "created_utc": 1711084072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am developing a medium data warehouse with size about 100 GB, using Postgres as database. At the moment I  running a generic postgresql server for dev environment, but I wonder if that version will hold up on production.\n\nSo, what is your setup? Please share!", "author_fullname": "t2_125mdi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your Postgres setup for data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkpfx3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711076883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am developing a medium data warehouse with size about 100 GB, using Postgres as database. At the moment I  running a generic postgresql server for dev environment, but I wonder if that version will hold up on production.&lt;/p&gt;\n\n&lt;p&gt;So, what is your setup? Please share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkpfx3", "is_robot_indexable": true, "report_reasons": null, "author": "dreamingfighter", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkpfx3/what_is_your_postgres_setup_for_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkpfx3/what_is_your_postgres_setup_for_data_warehouse/", "subreddit_subscribers": 170974, "created_utc": 1711076883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,  \nI'm working on a project  to analyze security vulnerabilities data, and IT incidents data.  \nThe data sources are mainly excel files.\n\nThe list of features present in the IT incidents data: \\[Imgur\\]([https://i.imgur.com/jkno6wW.png](https://i.imgur.com/jkno6wW.png))   \n\nSecurity vulnerabilities data comes in 3 different excel files, since the data provider uses 3 different scan solutions.   \n\nThe list of features present in excel file 1  : \\[Imgur\\]([https://i.imgur.com/Dxe8Ju2.png](https://i.imgur.com/Dxe8Ju2.png))   \n\nThe list of features present in  excel file 2  : \\[Imgur\\]([https://i.imgur.com/70fZCb4.png](https://i.imgur.com/70fZCb4.png))   \n\nThe list of features present in excel file 3 : \\[Imgur\\]([https://i.imgur.com/egAZTaG.png](https://i.imgur.com/egAZTaG.png))   \n\nAnd this is my Star Schema data model: \\[Imgur\\]([https://i.imgur.com/ulsggyB.png](https://i.imgur.com/ulsggyB.png))   \n\n I kindly ask you to criticize my Star Schema data model . I'm open to any advice or modification. Thanks.   ", "author_fullname": "t2_54yh79tez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Can you criticize my Star Schema Data Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl4il3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711127615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;br/&gt;\nI&amp;#39;m working on a project  to analyze security vulnerabilities data, and IT incidents data.&lt;br/&gt;\nThe data sources are mainly excel files.&lt;/p&gt;\n\n&lt;p&gt;The list of features present in the IT incidents data: [Imgur](&lt;a href=\"https://i.imgur.com/jkno6wW.png\"&gt;https://i.imgur.com/jkno6wW.png&lt;/a&gt;)   &lt;/p&gt;\n\n&lt;p&gt;Security vulnerabilities data comes in 3 different excel files, since the data provider uses 3 different scan solutions.   &lt;/p&gt;\n\n&lt;p&gt;The list of features present in excel file 1  : [Imgur](&lt;a href=\"https://i.imgur.com/Dxe8Ju2.png\"&gt;https://i.imgur.com/Dxe8Ju2.png&lt;/a&gt;)   &lt;/p&gt;\n\n&lt;p&gt;The list of features present in  excel file 2  : [Imgur](&lt;a href=\"https://i.imgur.com/70fZCb4.png\"&gt;https://i.imgur.com/70fZCb4.png&lt;/a&gt;)   &lt;/p&gt;\n\n&lt;p&gt;The list of features present in excel file 3 : [Imgur](&lt;a href=\"https://i.imgur.com/egAZTaG.png\"&gt;https://i.imgur.com/egAZTaG.png&lt;/a&gt;)   &lt;/p&gt;\n\n&lt;p&gt;And this is my Star Schema data model: [Imgur](&lt;a href=\"https://i.imgur.com/ulsggyB.png\"&gt;https://i.imgur.com/ulsggyB.png&lt;/a&gt;)   &lt;/p&gt;\n\n&lt;p&gt;I kindly ask you to criticize my Star Schema data model . I&amp;#39;m open to any advice or modification. Thanks.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OkprCV4wqFQYKCs6ZKnBtuNP-LRtl7Hc3JDG1jN-ts0.png?auto=webp&amp;s=7cd19bec20bd4c3f6a2a851a3bd9d2d77ee019f9", "width": 705, "height": 796}, "resolutions": [{"url": "https://external-preview.redd.it/OkprCV4wqFQYKCs6ZKnBtuNP-LRtl7Hc3JDG1jN-ts0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=225e0fb1657715fa8ac3f1c6dd5c1cf5726477ac", "width": 108, "height": 121}, {"url": "https://external-preview.redd.it/OkprCV4wqFQYKCs6ZKnBtuNP-LRtl7Hc3JDG1jN-ts0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=50b848eaf9fde9d52199cbc7da26b7e9314b12fd", "width": 216, "height": 243}, {"url": "https://external-preview.redd.it/OkprCV4wqFQYKCs6ZKnBtuNP-LRtl7Hc3JDG1jN-ts0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd105d229c4aaf1e26a6e4af698f8be1f1ae0524", "width": 320, "height": 361}, {"url": "https://external-preview.redd.it/OkprCV4wqFQYKCs6ZKnBtuNP-LRtl7Hc3JDG1jN-ts0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=359aa0b7827fc9d9dbcb20882145c4af684b8c5e", "width": 640, "height": 722}], "variants": {}, "id": "A5ZqfUX_dhp34jwcHHIOt9QC0RLXySAd95P-RNUE7jE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bl4il3", "is_robot_indexable": true, "report_reasons": null, "author": "3Ammar404", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl4il3/d_can_you_criticize_my_star_schema_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl4il3/d_can_you_criticize_my_star_schema_data_model/", "subreddit_subscribers": 170974, "created_utc": 1711127615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the pros and cons of using tools like Starburst and Dremio for data virtualization when the data sources are on prem or cloud relational databases? Can MPP help with such sources in any way or will it just act as performance bottleneck? Anyone has any experience with such setup?", "author_fullname": "t2_k1sl1lu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data mesh/ virtualization using tools like Starburst and Dremio for data virtualization when the data sources are on prem or cloud relational databases? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl39nc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711124549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the pros and cons of using tools like Starburst and Dremio for data virtualization when the data sources are on prem or cloud relational databases? Can MPP help with such sources in any way or will it just act as performance bottleneck? Anyone has any experience with such setup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bl39nc", "is_robot_indexable": true, "report_reasons": null, "author": "SeriousShoppr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl39nc/data_mesh_virtualization_using_tools_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl39nc/data_mesh_virtualization_using_tools_like/", "subreddit_subscribers": 170974, "created_utc": 1711124549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have the following doubts related to Spark.\n\n1) Is it really needed to install a distribute file system (HDFS) on top of the regular file system the nodes are using?\n\nI have only seen examples where HDFS is included as part of the solution and if this is true, wouldn't the file be present in the local file system and then distributed across the nodes taking more space? \n\n2) Is it true that spark does the processing distributing part of that load across the nodes and persist in memory? How does the distributed file system and this memory processing relate to each other?\n\n3) Is it true that the main difference between haddop and spark is that the first one does not rad and write to disk for the dataset processing? \n\n4) When we say that spark can distribute sql queries, does that literally mean that you make a query to an external database and the nodes do the calculation? Wouldn't you have to bring the entire sql database locally first? I'm little lost there and an example would be great to illustrate better. \n\nI appreciate in advance your help as I am really new to all of this. \n", "author_fullname": "t2_7al5p0w0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Spark Distributed File System and DB queries ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkt04n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711090131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have the following doubts related to Spark.&lt;/p&gt;\n\n&lt;p&gt;1) Is it really needed to install a distribute file system (HDFS) on top of the regular file system the nodes are using?&lt;/p&gt;\n\n&lt;p&gt;I have only seen examples where HDFS is included as part of the solution and if this is true, wouldn&amp;#39;t the file be present in the local file system and then distributed across the nodes taking more space? &lt;/p&gt;\n\n&lt;p&gt;2) Is it true that spark does the processing distributing part of that load across the nodes and persist in memory? How does the distributed file system and this memory processing relate to each other?&lt;/p&gt;\n\n&lt;p&gt;3) Is it true that the main difference between haddop and spark is that the first one does not rad and write to disk for the dataset processing? &lt;/p&gt;\n\n&lt;p&gt;4) When we say that spark can distribute sql queries, does that literally mean that you make a query to an external database and the nodes do the calculation? Wouldn&amp;#39;t you have to bring the entire sql database locally first? I&amp;#39;m little lost there and an example would be great to illustrate better. &lt;/p&gt;\n\n&lt;p&gt;I appreciate in advance your help as I am really new to all of this. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bkt04n", "is_robot_indexable": true, "report_reasons": null, "author": "erudes91", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkt04n/apache_spark_distributed_file_system_and_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkt04n/apache_spark_distributed_file_system_and_db/", "subreddit_subscribers": 170974, "created_utc": 1711090131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have a unique case where I cannot find much information on how to architect or provide a good enough solution (which may be due to **how** I am looking for a solution). Most if not all of my background comes from the standard BI/analytics/internal use case DE, and I'm presently working in an environment where DE supports a product itself.\n\nEssentially, we have the following requirements:\n\n1.) Dynamic DAG generation, taking the data and either aggregating it further into a very denormalized structure and then also into OLTP to be consumed by an RDS instance. By dynamic, I mean that the tables created in the database follow a DAG generated by variables passed plus a configuration template. So if customer XYZ signed up, we would create table \"agg\\_XYZ\\_stuff\".\n\n2.) Multi-tenant (customer) DAG execution, where the DAG executions need to be separated by the specific customer, So from the first requirement, customer XYZ's DAG would execute on an interval set by their timezone (likely daily batch, but again, by their timezone).\n\n3.) As little human involvement beyond the config files or templates. Trying to stay away from manually having to create customer specific files/tables/models in order for \"agg\\_XYZ\\_stuff\" to exist. \n\nMost of my recent experience is using dbt-core, Meltano+Dagster, GBQ/Snowflake, but this is a whole new beast to begin with. I know that dbt can support pre-post hooks, but I don't know if it could handle the complexity of generating new models at runtime without invoking a PR to manually create the models. It seems to me that this kind of work falls under \"software engineer, data\" type work.\n\nAny advice is very much appreciated.  ", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-tenant dynamic DAG options for a series of requirements?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1blao2n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711142888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a unique case where I cannot find much information on how to architect or provide a good enough solution (which may be due to &lt;strong&gt;how&lt;/strong&gt; I am looking for a solution). Most if not all of my background comes from the standard BI/analytics/internal use case DE, and I&amp;#39;m presently working in an environment where DE supports a product itself.&lt;/p&gt;\n\n&lt;p&gt;Essentially, we have the following requirements:&lt;/p&gt;\n\n&lt;p&gt;1.) Dynamic DAG generation, taking the data and either aggregating it further into a very denormalized structure and then also into OLTP to be consumed by an RDS instance. By dynamic, I mean that the tables created in the database follow a DAG generated by variables passed plus a configuration template. So if customer XYZ signed up, we would create table &amp;quot;agg_XYZ_stuff&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;2.) Multi-tenant (customer) DAG execution, where the DAG executions need to be separated by the specific customer, So from the first requirement, customer XYZ&amp;#39;s DAG would execute on an interval set by their timezone (likely daily batch, but again, by their timezone).&lt;/p&gt;\n\n&lt;p&gt;3.) As little human involvement beyond the config files or templates. Trying to stay away from manually having to create customer specific files/tables/models in order for &amp;quot;agg_XYZ_stuff&amp;quot; to exist. &lt;/p&gt;\n\n&lt;p&gt;Most of my recent experience is using dbt-core, Meltano+Dagster, GBQ/Snowflake, but this is a whole new beast to begin with. I know that dbt can support pre-post hooks, but I don&amp;#39;t know if it could handle the complexity of generating new models at runtime without invoking a PR to manually create the models. It seems to me that this kind of work falls under &amp;quot;software engineer, data&amp;quot; type work.&lt;/p&gt;\n\n&lt;p&gt;Any advice is very much appreciated.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1blao2n", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1blao2n/multitenant_dynamic_dag_options_for_a_series_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1blao2n/multitenant_dynamic_dag_options_for_a_series_of/", "subreddit_subscribers": 170974, "created_utc": 1711142888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nI got the simple requirement, in raw schema already tables are populated by legacy tool and on top required to create the multiple views for that dbt i want to use.\n\nIn first run the view creation is successful and then next run onwards keep failing on same view with rename error.\n\n20:20:30  1 of 1 ERROR creating sql view model dbo.view\\_a ................................ \\[ERROR in\n\n20:20:30  Finished running 1 view model in 0 hours 0 minutes and 2.40 seconds (2.40s).\n\n20:20:30  Completed with 1 error and 0 warnings:\n\n&amp;#x200B;\n\n20:20:30    Database Error in model view\\_a (models\\\\example\\\\view\\_a.sql)\n\n('42000', \"\\[42000\\] \\[Microsoft\\]\\[ODBC Driver 17 for SQL Server\\]\\[SQL Server\\]Incorrect syntax near 'rename'. (102) (SQLExecDirectW)\")\n\n&amp;#x200B;\n\nCopy below my code and error message. Please advise how to resolve, if the view already exist just has to overwrite or ignore.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kiokhy7a5ypc1.png?width=1926&amp;format=png&amp;auto=webp&amp;s=578bfac11652f77f310b2c8aae94540e40bb2d71\n\nhttps://preview.redd.it/lg6jytph3ypc1.png?width=2210&amp;format=png&amp;auto=webp&amp;s=8f191d95c71a454103f61b6330f51a51e378b9ee\n\nhttps://preview.redd.it/skp9n6aa3ypc1.png?width=2813&amp;format=png&amp;auto=webp&amp;s=735bfbc08950eed6a5d40045b1ec9ec1be679b9e", "author_fullname": "t2_gw1qtave", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt view creation run failing on second run onwards.. please help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 49, "top_awarded_type": null, "hide_score": false, "media_metadata": {"kiokhy7a5ypc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/kiokhy7a5ypc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fcfcdeb1d1095d4aeb6cf125738ab67dcac75341"}, {"y": 76, "x": 216, "u": "https://preview.redd.it/kiokhy7a5ypc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f9503f4f191c65a838de817cf361737187bf461"}, {"y": 113, "x": 320, "u": "https://preview.redd.it/kiokhy7a5ypc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=071dd98f8b1b0e926897261a05358e39e5f275ee"}, {"y": 227, "x": 640, "u": "https://preview.redd.it/kiokhy7a5ypc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bfeb0490451c02244f3a0f7a642d15c98dc8de8"}, {"y": 341, "x": 960, "u": "https://preview.redd.it/kiokhy7a5ypc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fea267b8570019705c14e118fc65eea26079b69"}, {"y": 384, "x": 1080, "u": "https://preview.redd.it/kiokhy7a5ypc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ae5aa0e4931a70ab0940e0a949e616bc3971addf"}], "s": {"y": 686, "x": 1926, "u": "https://preview.redd.it/kiokhy7a5ypc1.png?width=1926&amp;format=png&amp;auto=webp&amp;s=578bfac11652f77f310b2c8aae94540e40bb2d71"}, "id": "kiokhy7a5ypc1"}, "lg6jytph3ypc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/lg6jytph3ypc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c31afc7bc3b5a3bb1547928c51581d0c6f892ce"}, {"y": 158, "x": 216, "u": "https://preview.redd.it/lg6jytph3ypc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ce5c439a99f0eaf3677eccde60a01a29db4fb9c"}, {"y": 235, "x": 320, "u": "https://preview.redd.it/lg6jytph3ypc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de3e1e1b561e11a7d575cebbb03793cc5ee41bab"}, {"y": 470, "x": 640, "u": "https://preview.redd.it/lg6jytph3ypc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f6796483158d784c9d3795b9ca84b9cceaeccdb"}, {"y": 706, "x": 960, "u": "https://preview.redd.it/lg6jytph3ypc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b9365c516b5cca7881f2f7c61d62332c41185f6e"}, {"y": 794, "x": 1080, "u": "https://preview.redd.it/lg6jytph3ypc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37eab2fd06ead0c4d9ce2a5df20683e4a07a02c9"}], "s": {"y": 1626, "x": 2210, "u": "https://preview.redd.it/lg6jytph3ypc1.png?width=2210&amp;format=png&amp;auto=webp&amp;s=8f191d95c71a454103f61b6330f51a51e378b9ee"}, "id": "lg6jytph3ypc1"}, "skp9n6aa3ypc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/skp9n6aa3ypc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=17417f0701688cdd795cc0f7d517395a08e587dd"}, {"y": 118, "x": 216, "u": "https://preview.redd.it/skp9n6aa3ypc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eac967b95edda8090f8cb5e632a868f4a5bb6c4e"}, {"y": 175, "x": 320, "u": "https://preview.redd.it/skp9n6aa3ypc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d02d12ca2f0151fde8ade4ac59ce28ffd7a328ea"}, {"y": 350, "x": 640, "u": "https://preview.redd.it/skp9n6aa3ypc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7fad6c7c0509c9927351bda841b3253b12531606"}, {"y": 526, "x": 960, "u": "https://preview.redd.it/skp9n6aa3ypc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0bde6af28d8b09a4a8b16087715e239aa44bfd3d"}, {"y": 592, "x": 1080, "u": "https://preview.redd.it/skp9n6aa3ypc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf87c495299ff6b764a37b200aa65edde56bcdf2"}], "s": {"y": 1542, "x": 2813, "u": "https://preview.redd.it/skp9n6aa3ypc1.png?width=2813&amp;format=png&amp;auto=webp&amp;s=735bfbc08950eed6a5d40045b1ec9ec1be679b9e"}, "id": "skp9n6aa3ypc1"}}, "name": "t3_1bl9f3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/436Cl3363g2SNIpdZr9DiUXCgDW1m2Ko7BDsZS5wpVA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711139725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I got the simple requirement, in raw schema already tables are populated by legacy tool and on top required to create the multiple views for that dbt i want to use.&lt;/p&gt;\n\n&lt;p&gt;In first run the view creation is successful and then next run onwards keep failing on same view with rename error.&lt;/p&gt;\n\n&lt;p&gt;20:20:30  1 of 1 ERROR creating sql view model dbo.view_a ................................ [ERROR in&lt;/p&gt;\n\n&lt;p&gt;20:20:30  Finished running 1 view model in 0 hours 0 minutes and 2.40 seconds (2.40s).&lt;/p&gt;\n\n&lt;p&gt;20:20:30  Completed with 1 error and 0 warnings:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;20:20:30    Database Error in model view_a (models\\example\\view_a.sql)&lt;/p&gt;\n\n&lt;p&gt;(&amp;#39;42000&amp;#39;, &amp;quot;[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Incorrect syntax near &amp;#39;rename&amp;#39;. (102) (SQLExecDirectW)&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Copy below my code and error message. Please advise how to resolve, if the view already exist just has to overwrite or ignore.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kiokhy7a5ypc1.png?width=1926&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=578bfac11652f77f310b2c8aae94540e40bb2d71\"&gt;https://preview.redd.it/kiokhy7a5ypc1.png?width=1926&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=578bfac11652f77f310b2c8aae94540e40bb2d71&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lg6jytph3ypc1.png?width=2210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f191d95c71a454103f61b6330f51a51e378b9ee\"&gt;https://preview.redd.it/lg6jytph3ypc1.png?width=2210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f191d95c71a454103f61b6330f51a51e378b9ee&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/skp9n6aa3ypc1.png?width=2813&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=735bfbc08950eed6a5d40045b1ec9ec1be679b9e\"&gt;https://preview.redd.it/skp9n6aa3ypc1.png?width=2813&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=735bfbc08950eed6a5d40045b1ec9ec1be679b9e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bl9f3c", "is_robot_indexable": true, "report_reasons": null, "author": "efor007", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl9f3c/dbt_view_creation_run_failing_on_second_run/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl9f3c/dbt_view_creation_run_failing_on_second_run/", "subreddit_subscribers": 170974, "created_utc": 1711139725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Atscale is not pulling any punches here about the limitations of Fabric when it comes to Power BI + big data.\n\n  \nBased on their report, their CTO and founder is saying that its a half-baked solution for large workloads. It might work with smaller data sets, but it falls flat (query performance &amp; timeouts) after 100+ GBs of data if you are using the Direct Lake interface.\n\n  \nIs anyone else running into these types of scalability challenges?\n\n  \n[https://www.atscale.com/blog/power-bi-face-off-databricks-vs-microsoft-fabric/](https://www.atscale.com/blog/power-bi-face-off-databricks-vs-microsoft-fabric/)", "author_fullname": "t2_tm7h4ttv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using PowerBI + Fabric at an Enterprise Scale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bl8vbz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711138359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Atscale is not pulling any punches here about the limitations of Fabric when it comes to Power BI + big data.&lt;/p&gt;\n\n&lt;p&gt;Based on their report, their CTO and founder is saying that its a half-baked solution for large workloads. It might work with smaller data sets, but it falls flat (query performance &amp;amp; timeouts) after 100+ GBs of data if you are using the Direct Lake interface.&lt;/p&gt;\n\n&lt;p&gt;Is anyone else running into these types of scalability challenges?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.atscale.com/blog/power-bi-face-off-databricks-vs-microsoft-fabric/\"&gt;https://www.atscale.com/blog/power-bi-face-off-databricks-vs-microsoft-fabric/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EhGRy1JXOPFa3ospqKYinDjctxvl5W_DXL-ih6sMbPI.jpg?auto=webp&amp;s=ef0167fbc5df2624b364f3d35bc4868f14fac173", "width": 2048, "height": 1072}, "resolutions": [{"url": "https://external-preview.redd.it/EhGRy1JXOPFa3ospqKYinDjctxvl5W_DXL-ih6sMbPI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=584825f0568b23db2c7f4a7f7aa761f044b8e671", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EhGRy1JXOPFa3ospqKYinDjctxvl5W_DXL-ih6sMbPI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e92d06480e6e2d7c0cb05c1213908d59bbb80dbe", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EhGRy1JXOPFa3ospqKYinDjctxvl5W_DXL-ih6sMbPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34433847663248873a7618e96a8c37c00c9184e1", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/EhGRy1JXOPFa3ospqKYinDjctxvl5W_DXL-ih6sMbPI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=10dc1855235265aca1ee21174ea3ce5522330112", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/EhGRy1JXOPFa3ospqKYinDjctxvl5W_DXL-ih6sMbPI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a2de35d1299a6a3849b4b0a88932ba631a7302bb", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/EhGRy1JXOPFa3ospqKYinDjctxvl5W_DXL-ih6sMbPI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81a76fab2c2e71a702f63cad6157ee0b26d55dde", "width": 1080, "height": 565}], "variants": {}, "id": "dsus145I8c2mn6MwIGR4yyru8X7lKi3WqnbhPCocRUg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bl8vbz", "is_robot_indexable": true, "report_reasons": null, "author": "ProgramFriendly6608", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bl8vbz/is_anyone_using_powerbi_fabric_at_an_enterprise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bl8vbz/is_anyone_using_powerbi_fabric_at_an_enterprise/", "subreddit_subscribers": 170974, "created_utc": 1711138359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, im a mineral processing engineer and i want to learn where can i use data and ai engineering in my field.\n\n\u0130n mineral processing, we are crushing, grinding,  hydrometallurgically and pyrometallurgically enchanting the raw ore. \n\nWe are currently using USIM PAC and minitab for predicting plant feed and concentrate.\n\nThank you.", "author_fullname": "t2_4j6plq2q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The usage of data eng. in mineral processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkxj2n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711108781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im a mineral processing engineer and i want to learn where can i use data and ai engineering in my field.&lt;/p&gt;\n\n&lt;p&gt;\u0130n mineral processing, we are crushing, grinding,  hydrometallurgically and pyrometallurgically enchanting the raw ore. &lt;/p&gt;\n\n&lt;p&gt;We are currently using USIM PAC and minitab for predicting plant feed and concentrate.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bkxj2n", "is_robot_indexable": true, "report_reasons": null, "author": "Designer_Nebula", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkxj2n/the_usage_of_data_eng_in_mineral_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkxj2n/the_usage_of_data_eng_in_mineral_processing/", "subreddit_subscribers": 170974, "created_utc": 1711108781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those whose archives contain mixed data types, e.g. video, images, texts, even bookmarks, which tools would you suggest using to tag the archive?  \n  \nIs there much from AI that can help with the tagging these days?  \n  \nIn a use case where you wish to self-host and keep part of your archive private at home, but share the rest to the world, ideally without sharing your home ip address, what tools are there to help you with this?\n\nWhat is there to provide a landing page, somewhere to browse the categories, and highlight new additions to the archive?  \n  \nHere are some cool tools that might help others:  \n  \nTropy:\nhttps://tropy.org  \n  \nTagspaces:\nhttps://www.tagspaces.org/", "author_fullname": "t2_jv3994r4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best open source tools for tagging a digital archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkvww9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711102836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those whose archives contain mixed data types, e.g. video, images, texts, even bookmarks, which tools would you suggest using to tag the archive?  &lt;/p&gt;\n\n&lt;p&gt;Is there much from AI that can help with the tagging these days?  &lt;/p&gt;\n\n&lt;p&gt;In a use case where you wish to self-host and keep part of your archive private at home, but share the rest to the world, ideally without sharing your home ip address, what tools are there to help you with this?&lt;/p&gt;\n\n&lt;p&gt;What is there to provide a landing page, somewhere to browse the categories, and highlight new additions to the archive?  &lt;/p&gt;\n\n&lt;p&gt;Here are some cool tools that might help others:  &lt;/p&gt;\n\n&lt;p&gt;Tropy:\n&lt;a href=\"https://tropy.org\"&gt;https://tropy.org&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Tagspaces:\n&lt;a href=\"https://www.tagspaces.org/\"&gt;https://www.tagspaces.org/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/op6V6MCaaNF8aaMwl0s-mdHI795nt74gIKtRBPOezQ0.jpg?auto=webp&amp;s=6628e1290974af88bbd379f8171f5e957ae11d0f", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/op6V6MCaaNF8aaMwl0s-mdHI795nt74gIKtRBPOezQ0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12e6ce40a490d0d458162a42be692831b57f2957", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/op6V6MCaaNF8aaMwl0s-mdHI795nt74gIKtRBPOezQ0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25908a60a1bf75103343c3f43b0b444bc888bbac", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/op6V6MCaaNF8aaMwl0s-mdHI795nt74gIKtRBPOezQ0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c4f141ded1dba40666676d1051effc1fa4c3b49c", "width": 320, "height": 320}], "variants": {}, "id": "x30P3lc1n_9CECGu0Rq5SgUbJGpakv2SXZZBmTN9Ne0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkvww9", "is_robot_indexable": true, "report_reasons": null, "author": "innocuousAzureus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkvww9/best_open_source_tools_for_tagging_a_digital/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkvww9/best_open_source_tools_for_tagging_a_digital/", "subreddit_subscribers": 170974, "created_utc": 1711102836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! My name is Peter and I've been working on a data generation and validation tool called [Data Caterer](https://data.catering/). The latest version comes with a UI and you can now run it on Windows, Mac or Linux (also via Docker). I hope this tool makes generating and validating data across any data source simple and easy.\n\nKey features:\n- Batch and event data generation\n- Maintain relationships across any dataset\n- Create custom data generation scenarios\n- Clean up generated data\n- Advanced validation options\n- Suggest data validations\n\n[Quick start to run it yourself.](https://data.catering/get-started/docker)\n\n[Demo of the UI.](https://data.catering/sample/ui/index.html)\n\n[Github repo.](https://github.com/data-catering/data-caterer)\n\nHappy to receive any feedback.", "author_fullname": "t2_h209j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Generation and Validation Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkuszf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711098169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! My name is Peter and I&amp;#39;ve been working on a data generation and validation tool called &lt;a href=\"https://data.catering/\"&gt;Data Caterer&lt;/a&gt;. The latest version comes with a UI and you can now run it on Windows, Mac or Linux (also via Docker). I hope this tool makes generating and validating data across any data source simple and easy.&lt;/p&gt;\n\n&lt;p&gt;Key features:\n- Batch and event data generation\n- Maintain relationships across any dataset\n- Create custom data generation scenarios\n- Clean up generated data\n- Advanced validation options\n- Suggest data validations&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://data.catering/get-started/docker\"&gt;Quick start to run it yourself.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://data.catering/sample/ui/index.html\"&gt;Demo of the UI.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/data-catering/data-caterer\"&gt;Github repo.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to receive any feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?auto=webp&amp;s=b6dd71e86b80372c7b40d0dd1cbe8eb15713d211", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f27dad83d552b4340e4e2fba146314a746d5b6c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d63cf74d31d8fdb9c4e5e11f409c1e48920d936", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5aa706541a35748bdb248dff3f22c5b49db11f17", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=717125de4265264970e61b6f3bfce9306b99c3b2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=363a4f4536ecc43b58fff3e01f8be492e2fcdcd7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff1d200561f21b331379bc1b2305b2921d385f5e", "width": 1080, "height": 567}], "variants": {}, "id": "Us7ZEZRSIqU-6GfX_0BHZ2rb6V_vyVy2N3JCeqc72S0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1bkuszf", "is_robot_indexable": true, "report_reasons": null, "author": "Pitah7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkuszf/data_generation_and_validation_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkuszf/data_generation_and_validation_tool/", "subreddit_subscribers": 170974, "created_utc": 1711098169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI have a quick question about command id. I need to pull cdc logs to Storage account from on prem sql server and table is badly designed so I do not have any watermark column I can grab onto. Is command id good choice for know ing what I ingested with copy activity (ADF) since I do not have any other options. Also would date without timestamp be somewhat acceptable choice? \n\nThanks! ", "author_fullname": "t2_4c7udteq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC logs - __$command_id", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bku4jp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711095103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I have a quick question about command id. I need to pull cdc logs to Storage account from on prem sql server and table is badly designed so I do not have any watermark column I can grab onto. Is command id good choice for know ing what I ingested with copy activity (ADF) since I do not have any other options. Also would date without timestamp be somewhat acceptable choice? &lt;/p&gt;\n\n&lt;p&gt;Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bku4jp", "is_robot_indexable": true, "report_reasons": null, "author": "MahoYami", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bku4jp/cdc_logs_command_id/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bku4jp/cdc_logs_command_id/", "subreddit_subscribers": 170974, "created_utc": 1711095103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working for a firm that has to pull in new financial records each night from a series of about three dozen SQL databases at semi-independent branches, and I\u2019m looking for the smartest way to do this.\n\nCurrently we\u2019re pulling data nightly from each branch\u2019s database as linked servers into our on-prem MSSQL data warehouse, doing a little cleaning/mapping, and combining all of those branch tables into one huge reporting table. We\u2019ve got the go-ahead to migrate to Snowflake/BigQuery/some cloud solution this year, and I\u2019m trying to figure out what the best solution for getting data up to those cloud architectures would be.\n\nIs it better to pull all of the branches\u2019 data into our server and then move those staging tables up to S3 and then feed them into Snowflake, or is it more effective to just pull each branch\u2019s data (10k~15k rows per day) into a Pandas/Polars dataframe in turn and then push that up to Snowflake? Is it possible to pull the data up to Snowflake from each branch\u2019s database like a SELECT INTO query against a linked server, or does data need to be pushed up to Snowflake from the branch\u2019s server side?\n\nHonestly, I\u2019d appreciate any help or advice you folks think would be relevant/useful.", "author_fullname": "t2_ua3q1tqh6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consolidating Data nightly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bktsyz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711093625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working for a firm that has to pull in new financial records each night from a series of about three dozen SQL databases at semi-independent branches, and I\u2019m looking for the smartest way to do this.&lt;/p&gt;\n\n&lt;p&gt;Currently we\u2019re pulling data nightly from each branch\u2019s database as linked servers into our on-prem MSSQL data warehouse, doing a little cleaning/mapping, and combining all of those branch tables into one huge reporting table. We\u2019ve got the go-ahead to migrate to Snowflake/BigQuery/some cloud solution this year, and I\u2019m trying to figure out what the best solution for getting data up to those cloud architectures would be.&lt;/p&gt;\n\n&lt;p&gt;Is it better to pull all of the branches\u2019 data into our server and then move those staging tables up to S3 and then feed them into Snowflake, or is it more effective to just pull each branch\u2019s data (10k~15k rows per day) into a Pandas/Polars dataframe in turn and then push that up to Snowflake? Is it possible to pull the data up to Snowflake from each branch\u2019s database like a SELECT INTO query against a linked server, or does data need to be pushed up to Snowflake from the branch\u2019s server side?&lt;/p&gt;\n\n&lt;p&gt;Honestly, I\u2019d appreciate any help or advice you folks think would be relevant/useful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bktsyz", "is_robot_indexable": true, "report_reasons": null, "author": "JohnPaulDavyJones", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bktsyz/consolidating_data_nightly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bktsyz/consolidating_data_nightly/", "subreddit_subscribers": 170974, "created_utc": 1711093625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working in an asset management company and we are looking at some of the data management tool. These vendors often talk about Operational Data Store and I still can't figure out what's the purpose of these system. Looking at it i just feel like it provides real-time analytics and perform some mastering on the data. I just wondering that can't these features be implements on Data Warehouse such as Snowflake?", "author_fullname": "t2_dgqi4197", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Operational Data Store(ODS)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bktn9d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711092899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working in an asset management company and we are looking at some of the data management tool. These vendors often talk about Operational Data Store and I still can&amp;#39;t figure out what&amp;#39;s the purpose of these system. Looking at it i just feel like it provides real-time analytics and perform some mastering on the data. I just wondering that can&amp;#39;t these features be implements on Data Warehouse such as Snowflake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bktn9d", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Criticism-8127", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bktn9d/operational_data_storeods/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bktn9d/operational_data_storeods/", "subreddit_subscribers": 170974, "created_utc": 1711092899.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}