{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve wiped it, reinitialized as GPT, checked on both Mac &amp; Windows, tried different cables &amp; sleds\u2014nothing seems to change the reported capacity.  \nI\u2019ll reach out to Seagate since it\u2019s still covered under warranty\u2026but curious if anyone here has seen this before.", "author_fullname": "t2_3t9lh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Having trouble with this 16tb drive showing up as 566gb. Any suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"pj355kjupppc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/pj355kjupppc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0fee02c55d4b93e77917ac96811b9f646b43757"}, {"y": 107, "x": 216, "u": "https://preview.redd.it/pj355kjupppc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2a77a7e8a1cbc41fab18aabcf8aa6175108a1a96"}], "s": {"y": 111, "x": 224, "u": "https://preview.redd.it/pj355kjupppc1.jpg?width=224&amp;format=pjpg&amp;auto=webp&amp;s=43d16bd20b6361f2d83e467873ad9576fe8020f8"}, "id": "pj355kjupppc1"}, "20d5hjjupppc1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 143, "x": 108, "u": "https://preview.redd.it/20d5hjjupppc1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a7a91edc485d15a37597f4db343868615e5aac6"}, {"y": 287, "x": 216, "u": "https://preview.redd.it/20d5hjjupppc1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=42e87937cefab296cd20c02caf9028593c37fc68"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/20d5hjjupppc1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=32a3320f42363236a3373eb5ccbe087bbf72968d"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/20d5hjjupppc1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=12a2ed9b612ddeeeb29029b42e410e0b7a69ae13"}, {"y": 1279, "x": 960, "u": "https://preview.redd.it/20d5hjjupppc1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=de92cd806e8eb4381b132e7e9bfdc1d1a8aaed6e"}, {"y": 1439, "x": 1080, "u": "https://preview.redd.it/20d5hjjupppc1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ad3065a98a12325ac39c580bb92e56acbb1cf72e"}], "s": {"y": 2193, "x": 1645, "u": "https://preview.redd.it/20d5hjjupppc1.jpg?width=1645&amp;format=pjpg&amp;auto=webp&amp;s=521e060a60d0f2c434a637509fe94b8dc5435510"}, "id": "20d5hjjupppc1"}}, "name": "t3_1bkadgq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 492, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "20d5hjjupppc1", "id": 424175266}, {"media_id": "pj355kjupppc1", "id": 424175267}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 492, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3jN2FFcdxuoVRdBP2HpazdeRqEWe8XS6hXNyUoUEMuU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1711037797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve wiped it, reinitialized as GPT, checked on both Mac &amp;amp; Windows, tried different cables &amp;amp; sleds\u2014nothing seems to change the reported capacity.&lt;br/&gt;\nI\u2019ll reach out to Seagate since it\u2019s still covered under warranty\u2026but curious if anyone here has seen this before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/1bkadgq", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkadgq", "is_robot_indexable": true, "report_reasons": null, "author": "andytagonist", "discussion_type": null, "num_comments": 143, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkadgq/having_trouble_with_this_16tb_drive_showing_up_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/1bkadgq", "subreddit_subscribers": 740148, "created_utc": 1711037797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have always struggled with keeping my data needs to a minimum, typical case of a Data Hoarder I guess! One after another, and before you know it, you have a lot of drives that are hard to manage, we all have been there. I was trying to find a good solution for consolidating some of my most used drives into a single enclosure, but I couldn't find a solution small enough that worked well for me. In this video, I build a custom 2.5 inch HDD/SSD drive enclosure that you can recreate very easily if you are in the same boat.\n\n[https://youtu.be/RFL1oH1VZOk?si=g\\_jfRnjDr9nP\\_-gu](https://youtu.be/RFL1oH1VZOk?si=g_jfRnjDr9nP_-gu)", "author_fullname": "t2_dlw9jrc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You can't buy this! Affordable Custom 2.5 inch SATA SSD/HDD 4 Bay Direct Attached Drive Enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkkws8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 81, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 81, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711063749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have always struggled with keeping my data needs to a minimum, typical case of a Data Hoarder I guess! One after another, and before you know it, you have a lot of drives that are hard to manage, we all have been there. I was trying to find a good solution for consolidating some of my most used drives into a single enclosure, but I couldn&amp;#39;t find a solution small enough that worked well for me. In this video, I build a custom 2.5 inch HDD/SSD drive enclosure that you can recreate very easily if you are in the same boat.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/RFL1oH1VZOk?si=g_jfRnjDr9nP_-gu\"&gt;https://youtu.be/RFL1oH1VZOk?si=g_jfRnjDr9nP_-gu&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OyIx1nGbwdu7p7ig0h6FB9ATIHD10lkw8HBvab8NFcA.jpg?auto=webp&amp;s=add45e984c0d8d554ed203c4ddb7fd3eb207fd2b", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/OyIx1nGbwdu7p7ig0h6FB9ATIHD10lkw8HBvab8NFcA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b4526ef66d1341a495e7bbeb59c4d1ba0e732b7", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/OyIx1nGbwdu7p7ig0h6FB9ATIHD10lkw8HBvab8NFcA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e29f22b7ea506023c643be5c89d91a504bb18916", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/OyIx1nGbwdu7p7ig0h6FB9ATIHD10lkw8HBvab8NFcA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c28a96bab337555a4f84235940216fafb7277f16", "width": 320, "height": 240}], "variants": {}, "id": "ORf0WvzOH53xMdCuZbeUdDvf0gimtdVaGTD1ExIpgM0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkkws8", "is_robot_indexable": true, "report_reasons": null, "author": "himangshunits", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkkws8/you_cant_buy_this_affordable_custom_25_inch_sata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkkws8/you_cant_buy_this_affordable_custom_25_inch_sata/", "subreddit_subscribers": 740148, "created_utc": 1711063749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just heard it somewhere that flash memory type storages might lose their data if you do not connect them to a computer for a long time or something.\n\nGoogling gave me mixing results (some saying only the write/erase cycles matter, some saying other things) so I am not sure", "author_fullname": "t2_11llz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For storing back ups on portable SSDs and usb flash drives; Do you have to connect them once in a while to a computer to make sure they keep the data or its unnecessary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkw3ci", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711104574.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711103524.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just heard it somewhere that flash memory type storages might lose their data if you do not connect them to a computer for a long time or something.&lt;/p&gt;\n\n&lt;p&gt;Googling gave me mixing results (some saying only the write/erase cycles matter, some saying other things) so I am not sure&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkw3ci", "is_robot_indexable": true, "report_reasons": null, "author": "laci4225", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkw3ci/for_storing_back_ups_on_portable_ssds_and_usb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkw3ci/for_storing_back_ups_on_portable_ssds_and_usb/", "subreddit_subscribers": 740148, "created_utc": 1711103524.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m reaching out for advice on an issue that\u2019s been a bottleneck for my video production team. We generate a lot of 4K video footage, primarily in h.264 MP4 format, and store it all on our NAS, with new footage being added daily.\n\nThe challenge we\u2019re facing is enabling myself and a few employees to log in remotely to the NAS to review this footage. The review process doesn\u2019t require high quality; we just need to be able to watch the footage and make notes. Despite having fast internet both at the studio and at our homes, the large file sizes and the quantity of files make this process unworkable.\n\nI\u2019ve been toying with a couple of ideas to solve this:\n\n\t1.\tAutomatic Compression: One idea was to find a way to automatically compress every video file as it\u2019s uploaded, so we\u2019d have two copies of every video: one high-quality (for final use) and one low-quality (for reviewing purposes). I\u2019ve heard suggestions like using ffmpeg, Jellyfin, or Tdarr for this, but I\u2019m unsure how complex it would be to set up and maintain such a system.\n\t2.\tStreaming Solution: Another thought was whether there might be a software or hardware solution that allows for streaming the files directly from the NAS in a more manageable quality for review purposes. This way, we could bypass the need to store two versions of every file.\n\nI\u2019m curious if anyone here has tackled similar challenges or knows of existing solutions that could fit our needs. Whether it\u2019s specific software, a hardware setup, or scripting our way through with ffmpeg or similar tools, I\u2019m all ears. Our main goal is to streamline the review process without sacrificing too much on video quality for reviewing purposes and without requiring significant manual effort for file conversion.\n\nAny insights, recommendations, or advice would be greatly appreciated. ", "author_fullname": "t2_8738b0rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Solutions for Remote Review of 4K Video Footage on NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkp4mq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711075909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m reaching out for advice on an issue that\u2019s been a bottleneck for my video production team. We generate a lot of 4K video footage, primarily in h.264 MP4 format, and store it all on our NAS, with new footage being added daily.&lt;/p&gt;\n\n&lt;p&gt;The challenge we\u2019re facing is enabling myself and a few employees to log in remotely to the NAS to review this footage. The review process doesn\u2019t require high quality; we just need to be able to watch the footage and make notes. Despite having fast internet both at the studio and at our homes, the large file sizes and the quantity of files make this process unworkable.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been toying with a couple of ideas to solve this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;1.  Automatic Compression: One idea was to find a way to automatically compress every video file as it\u2019s uploaded, so we\u2019d have two copies of every video: one high-quality (for final use) and one low-quality (for reviewing purposes). I\u2019ve heard suggestions like using ffmpeg, Jellyfin, or Tdarr for this, but I\u2019m unsure how complex it would be to set up and maintain such a system.\n2.  Streaming Solution: Another thought was whether there might be a software or hardware solution that allows for streaming the files directly from the NAS in a more manageable quality for review purposes. This way, we could bypass the need to store two versions of every file.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I\u2019m curious if anyone here has tackled similar challenges or knows of existing solutions that could fit our needs. Whether it\u2019s specific software, a hardware setup, or scripting our way through with ffmpeg or similar tools, I\u2019m all ears. Our main goal is to streamline the review process without sacrificing too much on video quality for reviewing purposes and without requiring significant manual effort for file conversion.&lt;/p&gt;\n\n&lt;p&gt;Any insights, recommendations, or advice would be greatly appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkp4mq", "is_robot_indexable": true, "report_reasons": null, "author": "TheBigBrezinski", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkp4mq/seeking_solutions_for_remote_review_of_4k_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkp4mq/seeking_solutions_for_remote_review_of_4k_video/", "subreddit_subscribers": 740148, "created_utc": 1711075909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've just set up my first software RAID (1)  using mdadm on Ubuntu and I'm getting slower transfer speeds than I was expecting. I'm getting in the 20 MB/s range for both read and write.\n\nThe motherboard and processor are only a year or two old, and there is a USB 3.1 Gen 2 type C port on it, as well as on the hard drive enclosure. \n\nIs there a way I can identify where the bottleneck is? I'm not sure where to begin.\n\nThese are the hard drives: [https://www.amazon.com/MDD-MD20TS25672NAS-Internal-Network-Storage/dp/B0C6BZ41TW/ref=cm\\_cr\\_arp\\_d\\_product\\_top?ie=UTF8](https://www.amazon.com/MDD-MD20TS25672NAS-Internal-Network-Storage/dp/B0C6BZ41TW/ref=cm_cr_arp_d_product_top?ie=UTF8)\n\nThis is the enclosure:   \n[https://www.amazon.com/dp/B07Y3WDHLD?ref=ppx\\_yo2ov\\_dt\\_b\\_product\\_details&amp;th=1](https://www.amazon.com/dp/B07Y3WDHLD?ref=ppx_yo2ov_dt_b_product_details&amp;th=1)", "author_fullname": "t2_3hsif", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slow transfer speeds on RAID 1 over USB C 3.1 Gen 2 DAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkeif5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711047900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just set up my first software RAID (1)  using mdadm on Ubuntu and I&amp;#39;m getting slower transfer speeds than I was expecting. I&amp;#39;m getting in the 20 MB/s range for both read and write.&lt;/p&gt;\n\n&lt;p&gt;The motherboard and processor are only a year or two old, and there is a USB 3.1 Gen 2 type C port on it, as well as on the hard drive enclosure. &lt;/p&gt;\n\n&lt;p&gt;Is there a way I can identify where the bottleneck is? I&amp;#39;m not sure where to begin.&lt;/p&gt;\n\n&lt;p&gt;These are the hard drives: &lt;a href=\"https://www.amazon.com/MDD-MD20TS25672NAS-Internal-Network-Storage/dp/B0C6BZ41TW/ref=cm_cr_arp_d_product_top?ie=UTF8\"&gt;https://www.amazon.com/MDD-MD20TS25672NAS-Internal-Network-Storage/dp/B0C6BZ41TW/ref=cm_cr_arp_d_product_top?ie=UTF8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the enclosure:&lt;br/&gt;\n&lt;a href=\"https://www.amazon.com/dp/B07Y3WDHLD?ref=ppx_yo2ov_dt_b_product_details&amp;amp;th=1\"&gt;https://www.amazon.com/dp/B07Y3WDHLD?ref=ppx_yo2ov_dt_b_product_details&amp;amp;th=1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkeif5", "is_robot_indexable": true, "report_reasons": null, "author": "tektite", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkeif5/slow_transfer_speeds_on_raid_1_over_usb_c_31_gen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkeif5/slow_transfer_speeds_on_raid_1_over_usb_c_31_gen/", "subreddit_subscribers": 740148, "created_utc": 1711047900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI have a few VHS tapes that I'd like to digitalize.\n\nI found a VHS player that on the back has these ports: [https://imgur.com/a/GIJmohH](https://imgur.com/a/GIJmohH)\n\nWhat's the best way to go now? Cheap composite to HDMI adapter and HDMI capture card? Something else? I'm open to ideas. Thanks for the help!", "author_fullname": "t2_1yhc7ph9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitalize VHS tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkwgrm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711104981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I have a few VHS tapes that I&amp;#39;d like to digitalize.&lt;/p&gt;\n\n&lt;p&gt;I found a VHS player that on the back has these ports: &lt;a href=\"https://imgur.com/a/GIJmohH\"&gt;https://imgur.com/a/GIJmohH&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to go now? Cheap composite to HDMI adapter and HDMI capture card? Something else? I&amp;#39;m open to ideas. Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?auto=webp&amp;s=5254eb65a2b2c240b9638e5294c998c2b4473500", "width": 1871, "height": 1408}, "resolutions": [{"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c1c9ee4c5548481a8320b88b41a31c2306a1716", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53441216a37efe3f837057e646bf795cfdaf52a9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee41d77e2719947298113bedd6ba8f622caac12c", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c451ac33882e660135efc569c11bf41984637f7", "width": 640, "height": 481}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37a35810501127a1ddad46a88b8f3b62bc361e99", "width": 960, "height": 722}, {"url": "https://external-preview.redd.it/gHK2S31UOw5kXh_Z5iDQdKF7R02GKBbtV-yBVOMMseM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=919ca169dbf5f3fdbd6c40cc74ea436b8cb47fbd", "width": 1080, "height": 812}], "variants": {}, "id": "TVY7oGcueiCIcXqnmJjJbxHz7VyHoCi7OXabDBR9SlA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkwgrm", "is_robot_indexable": true, "report_reasons": null, "author": "DFalconD", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkwgrm/digitalize_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkwgrm/digitalize_vhs_tapes/", "subreddit_subscribers": 740148, "created_utc": 1711104981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am attempting to save a documentary from Truestory, I can access and play the video in the browser. It appears to use Vimeo's OTT system.\n\nThere's a piece of javascript on the page which reveals a lot of information, such as the auth user token and video ID but Jdownloader can't work with the video url, as it prompts me for a password presumably to the Vimeo account which doesn't belong to me.\n\nNavigating to the video (https://player.vimeo.com/video/XXXXXXX) says it can't be played due to privacy settings.\n\nHere's a redacted version (omitted the IDs and my email) to give you an idea of what's returned on the page. https://pastebin.com/jyhejdFS\n\nWould yt-dlp be able to handle something like this? if so, an example would be very much appreciated.\n\nI could record my screen but ideally, I'd like to find a way to save the video. Does anybody have any suggestions?\n\nThank you!", "author_fullname": "t2_rxt22yfca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling with saving Vimeo OTT External video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkvoc9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711101904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to save a documentary from Truestory, I can access and play the video in the browser. It appears to use Vimeo&amp;#39;s OTT system.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a piece of javascript on the page which reveals a lot of information, such as the auth user token and video ID but Jdownloader can&amp;#39;t work with the video url, as it prompts me for a password presumably to the Vimeo account which doesn&amp;#39;t belong to me.&lt;/p&gt;\n\n&lt;p&gt;Navigating to the video (&lt;a href=\"https://player.vimeo.com/video/XXXXXXX\"&gt;https://player.vimeo.com/video/XXXXXXX&lt;/a&gt;) says it can&amp;#39;t be played due to privacy settings.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a redacted version (omitted the IDs and my email) to give you an idea of what&amp;#39;s returned on the page. &lt;a href=\"https://pastebin.com/jyhejdFS\"&gt;https://pastebin.com/jyhejdFS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would yt-dlp be able to handle something like this? if so, an example would be very much appreciated.&lt;/p&gt;\n\n&lt;p&gt;I could record my screen but ideally, I&amp;#39;d like to find a way to save the video. Does anybody have any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkvoc9", "is_robot_indexable": true, "report_reasons": null, "author": "TertiaryOrbit", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkvoc9/struggling_with_saving_vimeo_ott_external_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkvoc9/struggling_with_saving_vimeo_ott_external_video/", "subreddit_subscribers": 740148, "created_utc": 1711101904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nFor the last 7-8 years, I've been content with my old HTPC case and slapping a 4TB disk in whenever I  neared my capacity. I'm now at about 16TB of filled storage, and ready to slap again - but I'm getting to the point where I need something more robust.\n\nMy plan was to eventually create a JBOD and a dedicated storage server to manage it, but I'm not quite there yet; so, I'd like to work with what I have (plus a few more disks) to create a better solution, and I'd love your help reviewing my plan to do so.\n\nI can't spend much on this project right now, but I have bought some higher-capacity drives and have the ability to reuse some of my old Home Theater PC hardware to take baby steps toward that goal. With that, these are my goals with this plan:\n\n1. Move everything (except the Operating System) onto ZFS.\n2. Implement single-drive parity for my non-critical data (about 14TB which would be annoying to replace, but not critical) and setup dual-drive parity for my critical media with an off-site backup solution (3-2-1).\n3. Keep everything relatively performant and extensible, as I'll eventually fill up both of these storage spaces.\n\nMy use case is 99% write-once read-often and very large files (e.g., home theater stuff); however, I do have some applications with very light database-style usage (think grandma's recipe website).\n\nSo here's my plan - please poke holes and help me avoid hurdles, as I'm entirely new to ZFS, especially, and there's always room for improvement. And another thanks toeEveryone who helped with my [last post](https://www.reddit.com/r/DataHoarder/comments/1bgxank/best_current_way_to_rebuild_storage/) to get here:\n\nWithout further ado...\n\n## Where I'm at today....\n\nI have available to me a pretty decent HTPC case that can easily support 11+ SATA drives (9x 3.5\" and 2x SSDs); it currently contains:\n\n* An **Intel(R) Core(TM) i5-4590 CPU @ 3.30GHz** quad-core processor\n* **8GB RAM** (non-ECC) taking up 2 of 4 slots *(what can I say? it's an old box)*\n* A very underutilized **PSU**\n* The following **storage disks** *(all connected via 6.0GB/s SATA)*:\n\n&amp;#x200B;\n\n|***Device***|***Type***|***Capacity***|***Model***|***Purpose***|***Power-On Hours / Years***|\n|:-|:-|:-|:-|:-|:-|\n|`/dev/sda`|SSD|120 GiB|Samsung SSD 850|Operating System|58,451 hours / 6.67 years|\n|`/dev/sdb`|SSD|1 TiB|Samsung SSD 850|HTPC Transcoding + Cache|45,648 hours / 5.21 years|\n|`/dev/sdc`|HDD|4 TiB|WD Red (`WDC WD40EFRX-68N`)|Non-Critical Media; Artifacts (e.g., Docker Images)|24,505 hours / 2.80 years|\n|`/dev/sdd`|HDD|4 TiB|WD Red (`WDC WD40EFRX-68W`)|Non-Critical Media|58,499 hours / 6.68 years|\n|`/dev/sde`|HDD|4 TiB|WD Red (`WDC WD40EFRX-68W`)|Non-Critical Media|56,113 hours / 6.41 years|\n|`/dev/sdf`|HDD|4 TiB|WD Red (`WDC WD40EFRX-68N`)|Non-Critical Media and **important 2nd backups from home computers**|30,906 hours / 3.53 years|\n\nNow with the above... not a RAID or 3-2-1 backup in sight, mind you, like a good agent of chaos. All of those drives are nearly at-capacity (save the Operating System disk) with no recoverability. I do have an off-site backup solution for some folders containing critical information (so, yep, I'm aware drive resilience via RAID isn't backup).\n\n**My data falls into three (3) categories:**\n\n1. My operating system and configurations;\n2. Non-critical data that's annoying, but not impossible, to replace *(14TB and growing)*; and,\n3. Critical data that I cherish and can't lose *(about 3TB)*.\n\nInstead of adding another yet-another chaos drive, I plan on taking the following course of action, and this is what I'd like your advice on:\n\n## The Plan\n\nThe only thing I can afford to put into this project is more drive space, so that's what I'm doing. So, as much as I'd like to increase the RAM on the machine, as I've heard you should \"ideally\" have 1GB per 1TB of ZFS storage (but, tell me if that's B.S.), that's not an option at the moment. So, what I'm going to do, is:\n\n## Rebuild the storage on new, high-capacity drives\n\n1. I purchased and today received 3x [Seagate Exos X18 ST16000NM000J 16TB Drives](https://serverpartdeals.com/products/seagate-exos-x18-st16000nm000j-16tb-7-2k-rpm-sata-6gb-s-3-5-recertified-hard-drive) from ServerPartDeals.com;\n2. I checked the Seagate warranty for the drives (all the serial numbers say out of warranty and to contact the manufacturer), so I have an outstanding request out to SPD to hopefully provide a supplier warranty; I'll RMA them, I guess, if there's no warranty;\n3. In the meantime, I'm running [`bht`](https://github.com/ezonakiusagi/bht) on the drives overnight to check for any immediate problems; and,\n4. I'm  currently running a self-made script creating a `sha256sum` of every file on my existing drives for later verification.\n\nOnce the checksums are built, I'll simply unmount and remov the 4TB drives from the server and put the applications dependent on that data in maintenance mode (see: `SIGTERM`).\n\nI'm not planning on touching my 120GB Operating System SSD at all, - so `/dev/sda` isn't being touched. Ideally, I'd like to have this disk regularly backed up into my ZFS pool, or at least the configurations and applications; so, if anyone has any recommendations on how best to do this regularly, I'd very much appreciate it. I'm running Ubuntu Server.\n\n&amp;#x200B;\n\n&gt;***Note*** *- from here on, as I dive into ZFS , my terminology or understanding of best-practices may be incorrect.* ***Please correct me!***\n\n&amp;#x200B;\n\nWith the 48TB of additional capacity I just purchased, I plan on creating a 3-drive `raidz` **vdev** in a **zpool** called `standard` resulting in 32TB of usable, additional storage.\n\nWhy is it called `standard`? Because it's going to follow a very-standard 3-drive, single-drive parity structure that I'll repeat any time I need to expand the pool's size. My understanding is you can increase a vpool later by adding vdevs (right?).\n\nThis will give me 16TB of space to transfer from my existing drives and 16TB of free space for expansion, and finally with some redundancy. I'm not backing up any data on this pool, as everything here falls into the \"annoying, but not impossible to replace\" category.\n\nMy understanding is it's best-practice to use the UUID of these drives when creating the pools; so, my construction will look something like this on Ubuntu:\n\n    sudo apt install -y zfsutils-linux\n    \n    sudo zpool create standard raidz /dev/disk/by-uuid/XX01... /dev/disk/by-uuid/XX02... /dev/disk/by-uuid/XX03...\n\nWith any expansion later simply requiring another vdev (right?), like this:\n\n    sudo zpool add standard raidz /dev/disk/by-uuid/XX04... /dev/disk/by-uuid/XX05... /dev/disk/by-uuid/XX06...\n\nWith this done, I simply need to create a filesystem and mount it for access. I actually intend to create two filesystems, in this case - one for my HTPC media and one for my artifacts like Docker images, but please let me know if there are drawbacks or alternative considerations to this pattern:\n\n    sudo create standard/media\n    sudo zfs set compression=on standard/media\n    sudo zfs set mountpoint=/media/media standard/media\n    \n    sudo create standard/artifacts\n    sudo zfs set quota=2T standard/artifacts\n    sudo zfs set mountpoint=/media/artifacts standard/artifacts\n\nThe media directory, especially, is write-once, read-many and mostly large files. Our pattern of usage tends to have the same 1-2TB of data read regularly, with most stuff being read from my cache drive, anyway... hence why I thought `lz4` compression might be beneficial. There's only a couple users (my immediate family) using this; but, if this is a terrible idea, please let me know (read my note about **Caching**, below, before you comment, though!).\n\n## Transfer\n\nFrom here, I'd simply transfer the existing files from my 4TB drives with `rsync` to the `standard/media` mount point and reconfigure my applications, which is easy enough. I can re-generate the `sha256sum` hashes and verify them - although I believe `rsync` actually has a built-in archive function that will run its own verifications... maybe I can save some time there, as building sums for 16TB takes quite some time and is unnecessary wear on the drives.\n\n## About the critical data\n\nThis leaves me with 4x 4TB drives of differing ages and batches that seem like a great fit for my critical data. I'll do the whole thing again, wiping these drives, and using a `mirror` strategy to create 8TB of usable storage from the 16TB of space (note, I was originally going to use a `raidz2` strategy, but I realized that if I lose 2 drives with `raidz2`, I lose 100% of the vdev, meanwhile, if I lose 2 drives with a `mirror` strategy, my worst case scenario is a 50% data loss, and best case I have 0% data loss):\n\n    sudo zpool add critical mirror /dev/disk/by-uuid/XX01... /dev/disk/by-uuid/XX02... /dev/disk/by-uuid/XX03... /dev/disk/by-uuid/XX04...\n\nI want this data following a 3-2-1 strategy, too, so I need offsite backup. Right now, I simply copy files to Dropbox - but I'm outgrowing that solution. I really don't want to pay a regular fee for Backblaze or similar.\n\nInterestingly, I have a friend who is undergoing this same project right now (*waves hi, friend!*), so we've decided to be each other's offsite backup solution. We're each going to setup a quota'd filesystem in each others' `critical` vpools to, using client-side encryption to \"trade\" storage:\n\n    sudo create critical/backup\n    sudo zfs set mountpoint=/media/backup critical/backup\n    \n    sudo create critical/friend_name\n    sudo zfs set quota=2T critical/friend_name\n    sudo zfs set compression=on critical/friend_name\n    sudo zfs set mountpoint=/media/storage-trades/friend-name critical/friend_name\n\n## Caching\n\nSo now I have two pools, a few file systems, and configurations that suit my needs.\n\nThis leaves me with the 1TB Samsung 850 SSD that is currently caching, but all of that caching is managed by my applications. I'd much prefer to hand all that caching over to ZFS to manage, so all of my applications can benefit from the speeds (especially with my 8GB of RAM limitation).\n\nMost of my cache need will definitely come from the `standard` pool, but it would be useful to have some for the `critical` pool as well, as I run a few application databases in there with infrequent, but small and database-like, write patterns.\n\nAnyway, I think a 75/25 split of the drive capacity into two partitions makes sense. I'll give one to each of the vpools to use as a cache drive. That said, if there's a better way to let ZFS use this drive and figure out how best to cache across multiple pools (instead of me having to guess that 75/25 is the right split), please definitely let me know.\n\nSo I format and partition the drive:\n\n    sudo parted /dev/sdb\n    (parted) mklabel gpt\n    (parted) mkpart\n    Partition name?  []? cache_standard\n    File system type?  [ext2]? ext4\n    Start? 0%\n    End? 75%\n    (parted) mkpart\n    Partition name?  []? cache_critical\n    File system type?  [ext2]? ext4\n    Start? 75%\n    End? 100%\n    (parted) quit\n\nThen use each partition as a cache disk in the vpools:\n\n    sudo zpool add standard cache /dev/sdb1 -f\n    sudo zpool add critical cache /dev/sdb2 -f\n\nI'll be honest, I don't know what would be a more beneficial use for the SSD, a cache drive or a log drive. I settled on cache because this isn't a write-heavy server, but there's limited RAM, so maybe a log drive is the right choice? Let me know your thoughts.\n\n## Maintenance\n\nMy expectation is that ZFS out of the box is pretty well suited for my needs - but I definitely want to know if that's a dangerous assumption.\n\nMy understanding is that scrubbing occurs once a month - I'll likely bump that up to weekly, at least on the critical space. I also have SMART short-tests running every 4-hours on the drives and long-tests occurring weekly.\n\nAny other recommendations for best-practices would be great to hear, or where I should look into to advance my ZFS knowledge and experience, from here.\n\nFinally, I'd love to hear what utilities folks use for monitoring and alerts on their drives and with ZFS... eventually the Homelab will have a proper, centralized logging and alert platform for the entire lab, including storage, but something like \"YOUR DRIVE DONE BROKE\" in an email would more than suffice, for now, so that's likely my interim.\n\nIf you've read all the way to the bottom, here, I thank you - and I look forward to your thoughts, corrections, advice, critique, and collaboration! Cheers!", "author_fullname": "t2_43498", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rate My Plan - Giving Up my Chaotic Ways", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkoh9b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711073927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;For the last 7-8 years, I&amp;#39;ve been content with my old HTPC case and slapping a 4TB disk in whenever I  neared my capacity. I&amp;#39;m now at about 16TB of filled storage, and ready to slap again - but I&amp;#39;m getting to the point where I need something more robust.&lt;/p&gt;\n\n&lt;p&gt;My plan was to eventually create a JBOD and a dedicated storage server to manage it, but I&amp;#39;m not quite there yet; so, I&amp;#39;d like to work with what I have (plus a few more disks) to create a better solution, and I&amp;#39;d love your help reviewing my plan to do so.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t spend much on this project right now, but I have bought some higher-capacity drives and have the ability to reuse some of my old Home Theater PC hardware to take baby steps toward that goal. With that, these are my goals with this plan:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Move everything (except the Operating System) onto ZFS.&lt;/li&gt;\n&lt;li&gt;Implement single-drive parity for my non-critical data (about 14TB which would be annoying to replace, but not critical) and setup dual-drive parity for my critical media with an off-site backup solution (3-2-1).&lt;/li&gt;\n&lt;li&gt;Keep everything relatively performant and extensible, as I&amp;#39;ll eventually fill up both of these storage spaces.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My use case is 99% write-once read-often and very large files (e.g., home theater stuff); however, I do have some applications with very light database-style usage (think grandma&amp;#39;s recipe website).&lt;/p&gt;\n\n&lt;p&gt;So here&amp;#39;s my plan - please poke holes and help me avoid hurdles, as I&amp;#39;m entirely new to ZFS, especially, and there&amp;#39;s always room for improvement. And another thanks toeEveryone who helped with my &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1bgxank/best_current_way_to_rebuild_storage/\"&gt;last post&lt;/a&gt; to get here:&lt;/p&gt;\n\n&lt;p&gt;Without further ado...&lt;/p&gt;\n\n&lt;h2&gt;Where I&amp;#39;m at today....&lt;/h2&gt;\n\n&lt;p&gt;I have available to me a pretty decent HTPC case that can easily support 11+ SATA drives (9x 3.5&amp;quot; and 2x SSDs); it currently contains:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An &lt;strong&gt;Intel(R) Core(TM) i5-4590 CPU @ 3.30GHz&lt;/strong&gt; quad-core processor&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;8GB RAM&lt;/strong&gt; (non-ECC) taking up 2 of 4 slots &lt;em&gt;(what can I say? it&amp;#39;s an old box)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;A very underutilized &lt;strong&gt;PSU&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;The following &lt;strong&gt;storage disks&lt;/strong&gt; &lt;em&gt;(all connected via 6.0GB/s SATA)&lt;/em&gt;:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Device&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Type&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Capacity&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Model&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Purpose&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;&lt;strong&gt;&lt;em&gt;Power-On Hours / Years&lt;/em&gt;&lt;/strong&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sda&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;SSD&lt;/td&gt;\n&lt;td align=\"left\"&gt;120 GiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;Samsung SSD 850&lt;/td&gt;\n&lt;td align=\"left\"&gt;Operating System&lt;/td&gt;\n&lt;td align=\"left\"&gt;58,451 hours / 6.67 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sdb&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;SSD&lt;/td&gt;\n&lt;td align=\"left\"&gt;1 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;Samsung SSD 850&lt;/td&gt;\n&lt;td align=\"left\"&gt;HTPC Transcoding + Cache&lt;/td&gt;\n&lt;td align=\"left\"&gt;45,648 hours / 5.21 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sdc&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;4 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;WD Red (&lt;code&gt;WDC WD40EFRX-68N&lt;/code&gt;)&lt;/td&gt;\n&lt;td align=\"left\"&gt;Non-Critical Media; Artifacts (e.g., Docker Images)&lt;/td&gt;\n&lt;td align=\"left\"&gt;24,505 hours / 2.80 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sdd&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;4 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;WD Red (&lt;code&gt;WDC WD40EFRX-68W&lt;/code&gt;)&lt;/td&gt;\n&lt;td align=\"left\"&gt;Non-Critical Media&lt;/td&gt;\n&lt;td align=\"left\"&gt;58,499 hours / 6.68 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sde&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;4 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;WD Red (&lt;code&gt;WDC WD40EFRX-68W&lt;/code&gt;)&lt;/td&gt;\n&lt;td align=\"left\"&gt;Non-Critical Media&lt;/td&gt;\n&lt;td align=\"left\"&gt;56,113 hours / 6.41 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;code&gt;/dev/sdf&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;4 TiB&lt;/td&gt;\n&lt;td align=\"left\"&gt;WD Red (&lt;code&gt;WDC WD40EFRX-68N&lt;/code&gt;)&lt;/td&gt;\n&lt;td align=\"left\"&gt;Non-Critical Media and &lt;strong&gt;important 2nd backups from home computers&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30,906 hours / 3.53 years&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Now with the above... not a RAID or 3-2-1 backup in sight, mind you, like a good agent of chaos. All of those drives are nearly at-capacity (save the Operating System disk) with no recoverability. I do have an off-site backup solution for some folders containing critical information (so, yep, I&amp;#39;m aware drive resilience via RAID isn&amp;#39;t backup).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My data falls into three (3) categories:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;My operating system and configurations;&lt;/li&gt;\n&lt;li&gt;Non-critical data that&amp;#39;s annoying, but not impossible, to replace &lt;em&gt;(14TB and growing)&lt;/em&gt;; and,&lt;/li&gt;\n&lt;li&gt;Critical data that I cherish and can&amp;#39;t lose &lt;em&gt;(about 3TB)&lt;/em&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Instead of adding another yet-another chaos drive, I plan on taking the following course of action, and this is what I&amp;#39;d like your advice on:&lt;/p&gt;\n\n&lt;h2&gt;The Plan&lt;/h2&gt;\n\n&lt;p&gt;The only thing I can afford to put into this project is more drive space, so that&amp;#39;s what I&amp;#39;m doing. So, as much as I&amp;#39;d like to increase the RAM on the machine, as I&amp;#39;ve heard you should &amp;quot;ideally&amp;quot; have 1GB per 1TB of ZFS storage (but, tell me if that&amp;#39;s B.S.), that&amp;#39;s not an option at the moment. So, what I&amp;#39;m going to do, is:&lt;/p&gt;\n\n&lt;h2&gt;Rebuild the storage on new, high-capacity drives&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I purchased and today received 3x &lt;a href=\"https://serverpartdeals.com/products/seagate-exos-x18-st16000nm000j-16tb-7-2k-rpm-sata-6gb-s-3-5-recertified-hard-drive\"&gt;Seagate Exos X18 ST16000NM000J 16TB Drives&lt;/a&gt; from ServerPartDeals.com;&lt;/li&gt;\n&lt;li&gt;I checked the Seagate warranty for the drives (all the serial numbers say out of warranty and to contact the manufacturer), so I have an outstanding request out to SPD to hopefully provide a supplier warranty; I&amp;#39;ll RMA them, I guess, if there&amp;#39;s no warranty;&lt;/li&gt;\n&lt;li&gt;In the meantime, I&amp;#39;m running &lt;a href=\"https://github.com/ezonakiusagi/bht\"&gt;&lt;code&gt;bht&lt;/code&gt;&lt;/a&gt; on the drives overnight to check for any immediate problems; and,&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m  currently running a self-made script creating a &lt;code&gt;sha256sum&lt;/code&gt; of every file on my existing drives for later verification.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once the checksums are built, I&amp;#39;ll simply unmount and remov the 4TB drives from the server and put the applications dependent on that data in maintenance mode (see: &lt;code&gt;SIGTERM&lt;/code&gt;).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not planning on touching my 120GB Operating System SSD at all, - so &lt;code&gt;/dev/sda&lt;/code&gt; isn&amp;#39;t being touched. Ideally, I&amp;#39;d like to have this disk regularly backed up into my ZFS pool, or at least the configurations and applications; so, if anyone has any recommendations on how best to do this regularly, I&amp;#39;d very much appreciate it. I&amp;#39;m running Ubuntu Server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;- from here on, as I dive into ZFS , my terminology or understanding of best-practices may be incorrect.&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Please correct me!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;With the 48TB of additional capacity I just purchased, I plan on creating a 3-drive &lt;code&gt;raidz&lt;/code&gt; &lt;strong&gt;vdev&lt;/strong&gt; in a &lt;strong&gt;zpool&lt;/strong&gt; called &lt;code&gt;standard&lt;/code&gt; resulting in 32TB of usable, additional storage.&lt;/p&gt;\n\n&lt;p&gt;Why is it called &lt;code&gt;standard&lt;/code&gt;? Because it&amp;#39;s going to follow a very-standard 3-drive, single-drive parity structure that I&amp;#39;ll repeat any time I need to expand the pool&amp;#39;s size. My understanding is you can increase a vpool later by adding vdevs (right?).&lt;/p&gt;\n\n&lt;p&gt;This will give me 16TB of space to transfer from my existing drives and 16TB of free space for expansion, and finally with some redundancy. I&amp;#39;m not backing up any data on this pool, as everything here falls into the &amp;quot;annoying, but not impossible to replace&amp;quot; category.&lt;/p&gt;\n\n&lt;p&gt;My understanding is it&amp;#39;s best-practice to use the UUID of these drives when creating the pools; so, my construction will look something like this on Ubuntu:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo apt install -y zfsutils-linux\n\nsudo zpool create standard raidz /dev/disk/by-uuid/XX01... /dev/disk/by-uuid/XX02... /dev/disk/by-uuid/XX03...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;With any expansion later simply requiring another vdev (right?), like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo zpool add standard raidz /dev/disk/by-uuid/XX04... /dev/disk/by-uuid/XX05... /dev/disk/by-uuid/XX06...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;With this done, I simply need to create a filesystem and mount it for access. I actually intend to create two filesystems, in this case - one for my HTPC media and one for my artifacts like Docker images, but please let me know if there are drawbacks or alternative considerations to this pattern:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo create standard/media\nsudo zfs set compression=on standard/media\nsudo zfs set mountpoint=/media/media standard/media\n\nsudo create standard/artifacts\nsudo zfs set quota=2T standard/artifacts\nsudo zfs set mountpoint=/media/artifacts standard/artifacts\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The media directory, especially, is write-once, read-many and mostly large files. Our pattern of usage tends to have the same 1-2TB of data read regularly, with most stuff being read from my cache drive, anyway... hence why I thought &lt;code&gt;lz4&lt;/code&gt; compression might be beneficial. There&amp;#39;s only a couple users (my immediate family) using this; but, if this is a terrible idea, please let me know (read my note about &lt;strong&gt;Caching&lt;/strong&gt;, below, before you comment, though!).&lt;/p&gt;\n\n&lt;h2&gt;Transfer&lt;/h2&gt;\n\n&lt;p&gt;From here, I&amp;#39;d simply transfer the existing files from my 4TB drives with &lt;code&gt;rsync&lt;/code&gt; to the &lt;code&gt;standard/media&lt;/code&gt; mount point and reconfigure my applications, which is easy enough. I can re-generate the &lt;code&gt;sha256sum&lt;/code&gt; hashes and verify them - although I believe &lt;code&gt;rsync&lt;/code&gt; actually has a built-in archive function that will run its own verifications... maybe I can save some time there, as building sums for 16TB takes quite some time and is unnecessary wear on the drives.&lt;/p&gt;\n\n&lt;h2&gt;About the critical data&lt;/h2&gt;\n\n&lt;p&gt;This leaves me with 4x 4TB drives of differing ages and batches that seem like a great fit for my critical data. I&amp;#39;ll do the whole thing again, wiping these drives, and using a &lt;code&gt;mirror&lt;/code&gt; strategy to create 8TB of usable storage from the 16TB of space (note, I was originally going to use a &lt;code&gt;raidz2&lt;/code&gt; strategy, but I realized that if I lose 2 drives with &lt;code&gt;raidz2&lt;/code&gt;, I lose 100% of the vdev, meanwhile, if I lose 2 drives with a &lt;code&gt;mirror&lt;/code&gt; strategy, my worst case scenario is a 50% data loss, and best case I have 0% data loss):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo zpool add critical mirror /dev/disk/by-uuid/XX01... /dev/disk/by-uuid/XX02... /dev/disk/by-uuid/XX03... /dev/disk/by-uuid/XX04...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I want this data following a 3-2-1 strategy, too, so I need offsite backup. Right now, I simply copy files to Dropbox - but I&amp;#39;m outgrowing that solution. I really don&amp;#39;t want to pay a regular fee for Backblaze or similar.&lt;/p&gt;\n\n&lt;p&gt;Interestingly, I have a friend who is undergoing this same project right now (&lt;em&gt;waves hi, friend!&lt;/em&gt;), so we&amp;#39;ve decided to be each other&amp;#39;s offsite backup solution. We&amp;#39;re each going to setup a quota&amp;#39;d filesystem in each others&amp;#39; &lt;code&gt;critical&lt;/code&gt; vpools to, using client-side encryption to &amp;quot;trade&amp;quot; storage:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo create critical/backup\nsudo zfs set mountpoint=/media/backup critical/backup\n\nsudo create critical/friend_name\nsudo zfs set quota=2T critical/friend_name\nsudo zfs set compression=on critical/friend_name\nsudo zfs set mountpoint=/media/storage-trades/friend-name critical/friend_name\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;Caching&lt;/h2&gt;\n\n&lt;p&gt;So now I have two pools, a few file systems, and configurations that suit my needs.&lt;/p&gt;\n\n&lt;p&gt;This leaves me with the 1TB Samsung 850 SSD that is currently caching, but all of that caching is managed by my applications. I&amp;#39;d much prefer to hand all that caching over to ZFS to manage, so all of my applications can benefit from the speeds (especially with my 8GB of RAM limitation).&lt;/p&gt;\n\n&lt;p&gt;Most of my cache need will definitely come from the &lt;code&gt;standard&lt;/code&gt; pool, but it would be useful to have some for the &lt;code&gt;critical&lt;/code&gt; pool as well, as I run a few application databases in there with infrequent, but small and database-like, write patterns.&lt;/p&gt;\n\n&lt;p&gt;Anyway, I think a 75/25 split of the drive capacity into two partitions makes sense. I&amp;#39;ll give one to each of the vpools to use as a cache drive. That said, if there&amp;#39;s a better way to let ZFS use this drive and figure out how best to cache across multiple pools (instead of me having to guess that 75/25 is the right split), please definitely let me know.&lt;/p&gt;\n\n&lt;p&gt;So I format and partition the drive:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo parted /dev/sdb\n(parted) mklabel gpt\n(parted) mkpart\nPartition name?  []? cache_standard\nFile system type?  [ext2]? ext4\nStart? 0%\nEnd? 75%\n(parted) mkpart\nPartition name?  []? cache_critical\nFile system type?  [ext2]? ext4\nStart? 75%\nEnd? 100%\n(parted) quit\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then use each partition as a cache disk in the vpools:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo zpool add standard cache /dev/sdb1 -f\nsudo zpool add critical cache /dev/sdb2 -f\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ll be honest, I don&amp;#39;t know what would be a more beneficial use for the SSD, a cache drive or a log drive. I settled on cache because this isn&amp;#39;t a write-heavy server, but there&amp;#39;s limited RAM, so maybe a log drive is the right choice? Let me know your thoughts.&lt;/p&gt;\n\n&lt;h2&gt;Maintenance&lt;/h2&gt;\n\n&lt;p&gt;My expectation is that ZFS out of the box is pretty well suited for my needs - but I definitely want to know if that&amp;#39;s a dangerous assumption.&lt;/p&gt;\n\n&lt;p&gt;My understanding is that scrubbing occurs once a month - I&amp;#39;ll likely bump that up to weekly, at least on the critical space. I also have SMART short-tests running every 4-hours on the drives and long-tests occurring weekly.&lt;/p&gt;\n\n&lt;p&gt;Any other recommendations for best-practices would be great to hear, or where I should look into to advance my ZFS knowledge and experience, from here.&lt;/p&gt;\n\n&lt;p&gt;Finally, I&amp;#39;d love to hear what utilities folks use for monitoring and alerts on their drives and with ZFS... eventually the Homelab will have a proper, centralized logging and alert platform for the entire lab, including storage, but something like &amp;quot;YOUR DRIVE DONE BROKE&amp;quot; in an email would more than suffice, for now, so that&amp;#39;s likely my interim.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve read all the way to the bottom, here, I thank you - and I look forward to your thoughts, corrections, advice, critique, and collaboration! Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?auto=webp&amp;s=89c4ffa73482cef13b51593b4245c7c8bc91357d", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3faf7a527dba156060b6c5387da87bc4c217f961", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc6b141dd4231615ed2bcb3dbc3180e8e312693f", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cabb4b8cc62554c5dfd6599257358cc5642a50c5", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df067833aebe9d9cf9f5b3a2c542f2ab1b595b30", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f70fcc21f59c9cd9b557a11ca2db9af59a6f25f0", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/GaYlcZKKE619JekqvVWQNHoIKXQrWFwJq6OxVzdElcc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f637eb1c0e535637f052b3a74f50986e977c1c2b", "width": 1080, "height": 1080}], "variants": {}, "id": "Oi9IobXaT6dZqAg3ezTwRUUH6Cow8RYcOx4AM3ZxO0Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkoh9b", "is_robot_indexable": true, "report_reasons": null, "author": "wspnut", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkoh9b/rate_my_plan_giving_up_my_chaotic_ways/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkoh9b/rate_my_plan_giving_up_my_chaotic_ways/", "subreddit_subscribers": 740148, "created_utc": 1711073927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have 1 14TB drive I'm syncing with Backblaze's Personal Backup and am looking to expand my pool. I'm on macos and am looking for a good DAS with a possible 1 drive parity (likely RAID5)", "author_fullname": "t2_93rkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for 4 Bay DAS Solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkcjau", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711043087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have 1 14TB drive I&amp;#39;m syncing with Backblaze&amp;#39;s Personal Backup and am looking to expand my pool. I&amp;#39;m on macos and am looking for a good DAS with a possible 1 drive parity (likely RAID5)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkcjau", "is_robot_indexable": true, "report_reasons": null, "author": "klnadler", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkcjau/looking_for_4_bay_das_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkcjau/looking_for_4_bay_das_solution/", "subreddit_subscribers": 740148, "created_utc": 1711043087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nLooking to download a video of a funeral service that was live streamed. The funeral home is asking for $200 for this file. Any help would be appreciated. \n\nThank you so much Foydianslip88", "author_fullname": "t2_s1p2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Funeral Service Video Download ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkojch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711075583.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711074095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to download a video of a funeral service that was live streamed. The funeral home is asking for $200 for this file. Any help would be appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thank you so much Foydianslip88&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkojch", "is_robot_indexable": true, "report_reasons": null, "author": "Ironicallyi", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkojch/funeral_service_video_download/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkojch/funeral_service_video_download/", "subreddit_subscribers": 740148, "created_utc": 1711074095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have lots of YouTube videos archived, I have channels and playlists, like this \n\nChannels \n-&gt;channel 1\n    -&gt;video 1, 2 etc \n-&gt;channel 2\n    -&gt;video 1, 2 etc\n\nPlaylists\n-&gt;playlist name \n    -&gt;video 1, 2 etc\n\nThat works for just general browsing, and I have a script that automates downloading and stuff with metadata, however I'm looking for a nice way to display the archive. I've tried Tube archivist on GitHub, and that's pretty much what I'm looking for but that stores videos based off of IDs, I understand why that is the way it is but I'm looking for something I can just point at my archive right now and have it display like YouTube, I hope that makes sense, if anyone can help that would be appreciated :) ", "author_fullname": "t2_3jbd8ec6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to organise/display YouTube archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkmfoe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711067922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have lots of YouTube videos archived, I have channels and playlists, like this &lt;/p&gt;\n\n&lt;p&gt;Channels \n-&amp;gt;channel 1\n    -&amp;gt;video 1, 2 etc \n-&amp;gt;channel 2\n    -&amp;gt;video 1, 2 etc&lt;/p&gt;\n\n&lt;p&gt;Playlists\n-&amp;gt;playlist name \n    -&amp;gt;video 1, 2 etc&lt;/p&gt;\n\n&lt;p&gt;That works for just general browsing, and I have a script that automates downloading and stuff with metadata, however I&amp;#39;m looking for a nice way to display the archive. I&amp;#39;ve tried Tube archivist on GitHub, and that&amp;#39;s pretty much what I&amp;#39;m looking for but that stores videos based off of IDs, I understand why that is the way it is but I&amp;#39;m looking for something I can just point at my archive right now and have it display like YouTube, I hope that makes sense, if anyone can help that would be appreciated :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkmfoe", "is_robot_indexable": true, "report_reasons": null, "author": "Tomma365", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkmfoe/best_way_to_organisedisplay_youtube_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkmfoe/best_way_to_organisedisplay_youtube_archive/", "subreddit_subscribers": 740148, "created_utc": 1711067922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The motherboard of my NAS failed today (15 years old, so no big loss) and it got me thinking about backup software. I backup my PC on my NAS using Areca, but to backup my NAS to another drive, should I use a regular backup software (that permits complete and incremental backups) or should I just use rsync to sync the content? I find the backup software rather heavy to backup ~12TB of data. Thanks!", "author_fullname": "t2_8n0m0c8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS backup: backup software or rsync?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkm5n2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": "", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711067136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The motherboard of my NAS failed today (15 years old, so no big loss) and it got me thinking about backup software. I backup my PC on my NAS using Areca, but to backup my NAS to another drive, should I use a regular backup software (that permits complete and incremental backups) or should I just use rsync to sync the content? I find the backup software rather heavy to backup ~12TB of data. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkm5n2", "is_robot_indexable": true, "report_reasons": null, "author": "SomeoneHereIsMissing", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1bkm5n2/nas_backup_backup_software_or_rsync/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkm5n2/nas_backup_backup_software_or_rsync/", "subreddit_subscribers": 740148, "created_utc": 1711067136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used to manually rsync a set of files from my Linux desktop to an external usb drive and every time i did this i had to be extra careful with the source and destination parameters since i use --delete-during.\n\nNow I've moved that folder to a NAS and i was hoping to sorta do the same but actually it just occurred to me that it would be cool to just do this every time i plug the drive into USB of the nas... I'm sure of y'all have figured it out already... Any ideas?\n\nMy NAS it's a terramaster that i installed (Arch) Linux on so the implementation could be anything that runs on Linux and it doesn't need to be specific to me model .", "author_fullname": "t2_ly1a1ad1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cold storage on USB connect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bksfx3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711087784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to manually rsync a set of files from my Linux desktop to an external usb drive and every time i did this i had to be extra careful with the source and destination parameters since i use --delete-during.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;ve moved that folder to a NAS and i was hoping to sorta do the same but actually it just occurred to me that it would be cool to just do this every time i plug the drive into USB of the nas... I&amp;#39;m sure of y&amp;#39;all have figured it out already... Any ideas?&lt;/p&gt;\n\n&lt;p&gt;My NAS it&amp;#39;s a terramaster that i installed (Arch) Linux on so the implementation could be anything that runs on Linux and it doesn&amp;#39;t need to be specific to me model .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bksfx3", "is_robot_indexable": true, "report_reasons": null, "author": "GloriousHousehold", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bksfx3/cold_storage_on_usb_connect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bksfx3/cold_storage_on_usb_connect/", "subreddit_subscribers": 740148, "created_utc": 1711087784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have about 30 links from Newgrounds all videos, I need to be able to download the videos on all 30 links simultaneously but I have not figured out the best and most simple way to do this. I first tried using \"J-Downloader 2\" and it did not work for me for some reason. Then I tried using \"gallery-dl\" but it refused to install. Does anyone have any ideas to do this?", "author_fullname": "t2_txlltni1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mass Video Downloading", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkee1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711047626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 30 links from Newgrounds all videos, I need to be able to download the videos on all 30 links simultaneously but I have not figured out the best and most simple way to do this. I first tried using &amp;quot;J-Downloader 2&amp;quot; and it did not work for me for some reason. Then I tried using &amp;quot;gallery-dl&amp;quot; but it refused to install. Does anyone have any ideas to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkee1q", "is_robot_indexable": true, "report_reasons": null, "author": "RyphReturns2024", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkee1q/mass_video_downloading/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkee1q/mass_video_downloading/", "subreddit_subscribers": 740148, "created_utc": 1711047626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need storage. For my music production, video editing, etc. Also for my personal data as well. \nI need faster access to my data, I am not going to use it outside of my local network, which I might not want NAS, and I am running out of storage in my pc right now. I would prefer to have bunch of disks and its enclosure which can provide me enough read and write speed. I would need another power supply for that since each drive consumes power anyways,\n\nWhat do you call these kinds of setup? Locally attached bunch of disks.\n\nHow would you set them up? Just connecting bunch of USB cables to each disks are not so neat and it will overflow my USB ports.\n\nIs NAS still be enough to handle those data?\n\n", "author_fullname": "t2_14m87z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External storage setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkleza", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711065108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need storage. For my music production, video editing, etc. Also for my personal data as well. \nI need faster access to my data, I am not going to use it outside of my local network, which I might not want NAS, and I am running out of storage in my pc right now. I would prefer to have bunch of disks and its enclosure which can provide me enough read and write speed. I would need another power supply for that since each drive consumes power anyways,&lt;/p&gt;\n\n&lt;p&gt;What do you call these kinds of setup? Locally attached bunch of disks.&lt;/p&gt;\n\n&lt;p&gt;How would you set them up? Just connecting bunch of USB cables to each disks are not so neat and it will overflow my USB ports.&lt;/p&gt;\n\n&lt;p&gt;Is NAS still be enough to handle those data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkleza", "is_robot_indexable": true, "report_reasons": null, "author": "trapoad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkleza/external_storage_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkleza/external_storage_setup/", "subreddit_subscribers": 740148, "created_utc": 1711065108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I recently bought six 12TB used enterprise hard drives from a reputable seller on eBay. I expect these drives to be reliable. Along with the drives, I acquired a 6-bay Direct Attached Storage (DAS) to expand the storage for my Network Attached Storage (NAS). I'm planning to organize the drives using StableBit DrivePool or perhaps Windows Storage Spaces, which act like software RAID systems.\n\nThe primary use for this storage will be a Plex server. The data, mainly media files, isn't critical and can be easily replaced. However, I have a few terabytes of personal data that I also want to secure (I'm aware RAID isn't a substitute for data backup) and am trying to decide on the most appropriate RAID configuration for my needs.\n\nInitially, I was leaning towards RAID 5. This setup would offer a good balance between storage capacity and redundancy, providing some protection in case one of the drives fails. However, I've seen recommendations against using RAID 5 with large-capacity drives, such as 12TB.\n\nAn alternative I'm considering is setting up the first four drives in JBOD (Just a Bunch Of Disks) mode for storing Plex media, and configuring the remaining two drives in a RAID 0 setup for redundancy of the critical data. This approach seems to balance capacity and safety for my more important files.\n\nI'm curious if there are factors I might be overlooking. Also, I'd love to hear from anyone who has successfully used RAID 5 with 12TB drives. What has your experience been?", "author_fullname": "t2_gseuof0i2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice: Optimizing 12TB HDDs for Plex + Critical Data Storage \u2013 RAID 5, JBOD, or Other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkh4in", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711054339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought six 12TB used enterprise hard drives from a reputable seller on eBay. I expect these drives to be reliable. Along with the drives, I acquired a 6-bay Direct Attached Storage (DAS) to expand the storage for my Network Attached Storage (NAS). I&amp;#39;m planning to organize the drives using StableBit DrivePool or perhaps Windows Storage Spaces, which act like software RAID systems.&lt;/p&gt;\n\n&lt;p&gt;The primary use for this storage will be a Plex server. The data, mainly media files, isn&amp;#39;t critical and can be easily replaced. However, I have a few terabytes of personal data that I also want to secure (I&amp;#39;m aware RAID isn&amp;#39;t a substitute for data backup) and am trying to decide on the most appropriate RAID configuration for my needs.&lt;/p&gt;\n\n&lt;p&gt;Initially, I was leaning towards RAID 5. This setup would offer a good balance between storage capacity and redundancy, providing some protection in case one of the drives fails. However, I&amp;#39;ve seen recommendations against using RAID 5 with large-capacity drives, such as 12TB.&lt;/p&gt;\n\n&lt;p&gt;An alternative I&amp;#39;m considering is setting up the first four drives in JBOD (Just a Bunch Of Disks) mode for storing Plex media, and configuring the remaining two drives in a RAID 0 setup for redundancy of the critical data. This approach seems to balance capacity and safety for my more important files.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if there are factors I might be overlooking. Also, I&amp;#39;d love to hear from anyone who has successfully used RAID 5 with 12TB drives. What has your experience been?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkh4in", "is_robot_indexable": true, "report_reasons": null, "author": "y0llow", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkh4in/seeking_advice_optimizing_12tb_hdds_for_plex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkh4in/seeking_advice_optimizing_12tb_hdds_for_plex/", "subreddit_subscribers": 740148, "created_utc": 1711054339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Should be easy peasy here but I'm not coming up with an easy fix after searching.  \n\nI'm using 7-Zip to extract some moderately large HTML directories (nothing crazy, just a bunch of folders with offline html documents).  7-Zip produces %20 in place of every space in every file name.  \n\nI understand there are ways to batch rename files after the fact, but I feel like this is unnecessary effort I shouldn't have to take. Is there simply something in 7-Zip I can alter to unpack these file names correctly? Or should I avoid using 7-Zip altogether for this reason? Windows is not able to extract these folders, and I would rather not use WinRar unless I just have to.  \n\nMaybe it's unrelated and is a problem at the source?  \n\n&amp;#x200B;", "author_fullname": "t2_2ozmwt52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uncompressed file names loose all spaces", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkapz1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711038663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Should be easy peasy here but I&amp;#39;m not coming up with an easy fix after searching.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using 7-Zip to extract some moderately large HTML directories (nothing crazy, just a bunch of folders with offline html documents).  7-Zip produces %20 in place of every space in every file name.  &lt;/p&gt;\n\n&lt;p&gt;I understand there are ways to batch rename files after the fact, but I feel like this is unnecessary effort I shouldn&amp;#39;t have to take. Is there simply something in 7-Zip I can alter to unpack these file names correctly? Or should I avoid using 7-Zip altogether for this reason? Windows is not able to extract these folders, and I would rather not use WinRar unless I just have to.  &lt;/p&gt;\n\n&lt;p&gt;Maybe it&amp;#39;s unrelated and is a problem at the source?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bkapz1", "is_robot_indexable": true, "report_reasons": null, "author": "fdrowell", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bkapz1/uncompressed_file_names_loose_all_spaces/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bkapz1/uncompressed_file_names_loose_all_spaces/", "subreddit_subscribers": 740148, "created_utc": 1711038663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im looking on WD website, it shows 256mg cache, but on amazon, that apparently same drive has 512mb cache. Which one is correct, and how important is cache for performance?", "author_fullname": "t2_t1n26nlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Red Plus 12 TB - does it have 256 or 512mb cache?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bk89eb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.18, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711032456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking on WD website, it shows 256mg cache, but on amazon, that apparently same drive has 512mb cache. Which one is correct, and how important is cache for performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1bk89eb", "is_robot_indexable": true, "report_reasons": null, "author": "CountingStars29", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1bk89eb/wd_red_plus_12_tb_does_it_have_256_or_512mb_cache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1bk89eb/wd_red_plus_12_tb_does_it_have_256_or_512mb_cache/", "subreddit_subscribers": 740148, "created_utc": 1711032456.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}