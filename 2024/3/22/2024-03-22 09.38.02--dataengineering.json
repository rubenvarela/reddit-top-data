{"kind": "Listing", "data": {"after": "t3_1bku2m7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi /r/dataengineering,\n\nI'm Pete, the CEO at Dagster Labs. We're launching our new product, Dagster+, in a few weeks, and are having launch parties in SF and NYC to preview the release with the community and enjoy some food, drinks, and good company.\n\nThis subreddit has been very supportive of us over the past few years, and we'd love to see you there! They're happening the evening of April 12th, and you can sign up [here](https://share.hsforms.com/10i2u9WzCRyK77-D0MCn89gq55vz).\n\nSpots are limited, so we\u2019ll reach out to confirm attendance as available closer to the date of the parties. Thanks in advance for expressing your interest!", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We (Dagster) are throwing a party", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkebv5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 176, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 176, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1711050842.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711047477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Pete, the CEO at Dagster Labs. We&amp;#39;re launching our new product, Dagster+, in a few weeks, and are having launch parties in SF and NYC to preview the release with the community and enjoy some food, drinks, and good company.&lt;/p&gt;\n\n&lt;p&gt;This subreddit has been very supportive of us over the past few years, and we&amp;#39;d love to see you there! They&amp;#39;re happening the evening of April 12th, and you can sign up &lt;a href=\"https://share.hsforms.com/10i2u9WzCRyK77-D0MCn89gq55vz\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Spots are limited, so we\u2019ll reach out to confirm attendance as available closer to the date of the parties. Thanks in advance for expressing your interest!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bkebv5", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkebv5/we_dagster_are_throwing_a_party/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkebv5/we_dagster_are_throwing_a_party/", "subreddit_subscribers": 170830, "created_utc": 1711047477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm 27(F) and I've been working in data engineering for a good 5 years now. I recently started working for a Canadian company where I already knew my boss from another place we worked together. When we were discussing the opportunity he had told me that the salary would be around 7500 dollars a month, but after a conversation with the founders of the company I received the offer with a salary of 5800, but I managed to cry for 6500. From what my boss said, it was because Canadians think I'm too young, but I'm sure part of it was because I'm a woman too.\n\nNow that I've been with the company for a year, I've made some great deliveries and I give a lot of support to other people whose technical level is well below mine, but who earn more, I'd like to ask for a decent raise. I've already talked to my boss, but he tells me to be patient because HR is working on a career plan, but that should take another 6 months at least.\n\nI'd like some help in approaching my boss more firmly, but without sounding like an ultimatum. I'd like him to talk directly to the founders, as I think this would make it easier to get a raise (the founders are aware of the work I've been doing).\n\nI wouldn't want to change jobs now, because I like the company, I'd just like a fair salary on a par with that of my colleagues.", "author_fullname": "t2_mjzy4r5fx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ask for a salary increase that I deserve", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bk5tkj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711025592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m 27(F) and I&amp;#39;ve been working in data engineering for a good 5 years now. I recently started working for a Canadian company where I already knew my boss from another place we worked together. When we were discussing the opportunity he had told me that the salary would be around 7500 dollars a month, but after a conversation with the founders of the company I received the offer with a salary of 5800, but I managed to cry for 6500. From what my boss said, it was because Canadians think I&amp;#39;m too young, but I&amp;#39;m sure part of it was because I&amp;#39;m a woman too.&lt;/p&gt;\n\n&lt;p&gt;Now that I&amp;#39;ve been with the company for a year, I&amp;#39;ve made some great deliveries and I give a lot of support to other people whose technical level is well below mine, but who earn more, I&amp;#39;d like to ask for a decent raise. I&amp;#39;ve already talked to my boss, but he tells me to be patient because HR is working on a career plan, but that should take another 6 months at least.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like some help in approaching my boss more firmly, but without sounding like an ultimatum. I&amp;#39;d like him to talk directly to the founders, as I think this would make it easier to get a raise (the founders are aware of the work I&amp;#39;ve been doing).&lt;/p&gt;\n\n&lt;p&gt;I wouldn&amp;#39;t want to change jobs now, because I like the company, I&amp;#39;d just like a fair salary on a par with that of my colleagues.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bk5tkj", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Engineering-5752", "discussion_type": null, "num_comments": 109, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bk5tkj/how_to_ask_for_a_salary_increase_that_i_deserve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bk5tkj/how_to_ask_for_a_salary_increase_that_i_deserve/", "subreddit_subscribers": 170830, "created_utc": 1711025592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI was wondering if anyone started out with a new DE job with a MacBook when they\u2019ve used windows most of their life? If so, was it easy to adjust and did it impact your work at all as a DE?\n\nI\u2019ve always wanted to code and work on a MacBook Pro but I\u2019m worried I might hit roadblocks I\u2019m not used to vs working in windows. \n\nOne thing that worries me is I know MacBook isn\u2019t compatible with SSMS which is one of my bread and butter. But I read online that you can access a container with windows installed and use SSMS that way? Something to do with docker?\n\nDid anyone else face these concerns or issues converting to MacBook? And if so, did you overcome it and adjust quickly?", "author_fullname": "t2_pz85y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you have the option to switch to MacBook for work would you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkcret", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711043627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone started out with a new DE job with a MacBook when they\u2019ve used windows most of their life? If so, was it easy to adjust and did it impact your work at all as a DE?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve always wanted to code and work on a MacBook Pro but I\u2019m worried I might hit roadblocks I\u2019m not used to vs working in windows. &lt;/p&gt;\n\n&lt;p&gt;One thing that worries me is I know MacBook isn\u2019t compatible with SSMS which is one of my bread and butter. But I read online that you can access a container with windows installed and use SSMS that way? Something to do with docker?&lt;/p&gt;\n\n&lt;p&gt;Did anyone else face these concerns or issues converting to MacBook? And if so, did you overcome it and adjust quickly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bkcret", "is_robot_indexable": true, "report_reasons": null, "author": "imperialka", "discussion_type": null, "num_comments": 80, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1bkcret/if_you_have_the_option_to_switch_to_macbook_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkcret/if_you_have_the_option_to_switch_to_macbook_for/", "subreddit_subscribers": 170830, "created_utc": 1711043627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm on my way to learn DE, and this sub has helped me a lot to understand better many concepts, but one thing I see very often is that Postgres seems to be the best DWH in most cases, so my question is, which are those cases where Postgres is NOT recommended? What alternatives are better suited for that situation and Why? \n\nThank you! ", "author_fullname": "t2_4iz2pipg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When NOT to use PostgreSQL? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bklthc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711066181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m on my way to learn DE, and this sub has helped me a lot to understand better many concepts, but one thing I see very often is that Postgres seems to be the best DWH in most cases, so my question is, which are those cases where Postgres is NOT recommended? What alternatives are better suited for that situation and Why? &lt;/p&gt;\n\n&lt;p&gt;Thank you! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bklthc", "is_robot_indexable": true, "report_reasons": null, "author": "_Lavender_Brown_", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bklthc/when_not_to_use_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bklthc/when_not_to_use_postgresql/", "subreddit_subscribers": 170830, "created_utc": 1711066181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there, this question might be a bit out of context for this sub but I'm really confused and don't know what to do :\n\nSo im a 4th Year software engineering student(out of 5 years in total) and up until this point I've been focusing only on software development and all the projects internships I did consisted mainly of web, mobile development and recently I started to get introduced to data engineering and data analysis as they added some new subjects to the curriculum and I sort of liked them but I don't know if it would be a better career path If I shift my focus to data engineering instead of focusing solely on software develoment, \nPlease help me I need advices and opinions.\n", "author_fullname": "t2_kjvbhqyx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SWE vs Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bk5yyi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711026058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, this question might be a bit out of context for this sub but I&amp;#39;m really confused and don&amp;#39;t know what to do :&lt;/p&gt;\n\n&lt;p&gt;So im a 4th Year software engineering student(out of 5 years in total) and up until this point I&amp;#39;ve been focusing only on software development and all the projects internships I did consisted mainly of web, mobile development and recently I started to get introduced to data engineering and data analysis as they added some new subjects to the curriculum and I sort of liked them but I don&amp;#39;t know if it would be a better career path If I shift my focus to data engineering instead of focusing solely on software develoment, \nPlease help me I need advices and opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bk5yyi", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic_Battle876", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bk5yyi/swe_vs_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bk5yyi/swe_vs_data_engineer/", "subreddit_subscribers": 170830, "created_utc": 1711026058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Saw a post that required a Minimum of 10 Years of experience in Azure to warehouse snowflake Data. This technology itself is not that old! What is your suggestion for someone with little experience to be a DE?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/tlz3ihwt3spc1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=663c7b400835533e57fb459459619bde04984c7d", "author_fullname": "t2_65gcymee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to break into a Data Engineering Job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "media_metadata": {"tlz3ihwt3spc1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/tlz3ihwt3spc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=79493bde33c3ba3fe41292490888a129232dca9a"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/tlz3ihwt3spc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebea4e82e057ce8133d05fd93bd7b2be5b583d31"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/tlz3ihwt3spc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4902b81a61bc6dd0cf45360af3e27b57ec1ddfc0"}, {"y": 401, "x": 640, "u": "https://preview.redd.it/tlz3ihwt3spc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=85f2b45b588cf989e642d28a186aeec85b8b83e8"}, {"y": 602, "x": 960, "u": "https://preview.redd.it/tlz3ihwt3spc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ebe07f5f5522491daf3c9d660e79e5dbf0d78da8"}, {"y": 678, "x": 1080, "u": "https://preview.redd.it/tlz3ihwt3spc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e54ff27b24e19738cdafbd3a34bfe1f0c7c2b1a4"}], "s": {"y": 770, "x": 1226, "u": "https://preview.redd.it/tlz3ihwt3spc1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=663c7b400835533e57fb459459619bde04984c7d"}, "id": "tlz3ihwt3spc1"}}, "name": "t3_1bkm10r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UvSIJshNf6bBVZc-HLmbAfwKmmg20vUZJTGr5J3bDnA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711066775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw a post that required a Minimum of 10 Years of experience in Azure to warehouse snowflake Data. This technology itself is not that old! What is your suggestion for someone with little experience to be a DE?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tlz3ihwt3spc1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=663c7b400835533e57fb459459619bde04984c7d\"&gt;https://preview.redd.it/tlz3ihwt3spc1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=663c7b400835533e57fb459459619bde04984c7d&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bkm10r", "is_robot_indexable": true, "report_reasons": null, "author": "BrownChubbyDoggy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkm10r/how_to_break_into_a_data_engineering_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkm10r/how_to_break_into_a_data_engineering_job/", "subreddit_subscribers": 170830, "created_utc": 1711066775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently using a Kafka cluster primarily for receiving monitoring data from our other systems. The overall throughput of our cluster is around 20GB/s. As reading is usually lagging production by several minutes, most reads require disk access.  \nLong-term usage of Kafka has exposed some serious issues that we hope to address through some technical upgrades. These problems include:\n\n  \n**Cost**: We are using a large number of high-spec SSDs to handle the reading of historical data, which are quite expensive.\n\n  \n**Elasticity**: Scaling Kafka clusters up and down is a risky operation. We have not been able to make adjustments to the cluster's capacity without affecting the business, which is quite bothersome. Moreover, each scaling operation is accompanied by a large amount of partition replication, which affects read and write operations.  \nSince many of our applications are already running based on Kafka, we do not want to start from scratch. Therefore, we have researched some solutions compatible with the Kafka protocol. Could you recommend which one to use in our case, or suggest a better solution?  \n[WarpStream](https://www.warpstream.com/): It seems like a decent solution. However, some of our Kafka clusters are quite latency-sensitive, and we have concerns that using S3 as the only storage for reading and writing might not meet our requirements in some scenarios. Also, we initially want to try an open-source solution, and WarpStream is not open-source.  \n[Redpanda](https://github.com/redpanda-data/redpanda): It seems to match our needs quite well overall. It supports S3-based tiered storage to reduce costs and offer great performance. However, we are not sure whether it has solved the elasticity issues of Kafka.  \n[AutoMQ](https://github.com/AutoMQ/automq-for-kafka):  This open-source project seems to match our needs the best at the moment. It extensively reuses Kafka's original code, only modifying Kafka's underlying storage to be S3-based. Its claimed features of replicating partitions in seconds and rebalancing network traffic automatically are the key challenges we have encountered while using Kafka. Another major reason we are currently leaning towards this solution is its high compatibility with Apache Kafka, as we do not want to require the entire business system to upgrade while we upgrade the messaging system's technical architecture.  \n\n\nThe above represents my personal learning and research results, which may not be entirely accurate. Can you help me validate the accuracy of my technical considerations? If there are better Kafka solutions, I would also appreciate your suggestions.  \n", "author_fullname": "t2_w5zv981t7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Kafka solution best match my scenario?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bktex1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711091866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently using a Kafka cluster primarily for receiving monitoring data from our other systems. The overall throughput of our cluster is around 20GB/s. As reading is usually lagging production by several minutes, most reads require disk access.&lt;br/&gt;\nLong-term usage of Kafka has exposed some serious issues that we hope to address through some technical upgrades. These problems include:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;: We are using a large number of high-spec SSDs to handle the reading of historical data, which are quite expensive.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Elasticity&lt;/strong&gt;: Scaling Kafka clusters up and down is a risky operation. We have not been able to make adjustments to the cluster&amp;#39;s capacity without affecting the business, which is quite bothersome. Moreover, each scaling operation is accompanied by a large amount of partition replication, which affects read and write operations.&lt;br/&gt;\nSince many of our applications are already running based on Kafka, we do not want to start from scratch. Therefore, we have researched some solutions compatible with the Kafka protocol. Could you recommend which one to use in our case, or suggest a better solution?&lt;br/&gt;\n&lt;a href=\"https://www.warpstream.com/\"&gt;WarpStream&lt;/a&gt;: It seems like a decent solution. However, some of our Kafka clusters are quite latency-sensitive, and we have concerns that using S3 as the only storage for reading and writing might not meet our requirements in some scenarios. Also, we initially want to try an open-source solution, and WarpStream is not open-source.&lt;br/&gt;\n&lt;a href=\"https://github.com/redpanda-data/redpanda\"&gt;Redpanda&lt;/a&gt;: It seems to match our needs quite well overall. It supports S3-based tiered storage to reduce costs and offer great performance. However, we are not sure whether it has solved the elasticity issues of Kafka.&lt;br/&gt;\n&lt;a href=\"https://github.com/AutoMQ/automq-for-kafka\"&gt;AutoMQ&lt;/a&gt;:  This open-source project seems to match our needs the best at the moment. It extensively reuses Kafka&amp;#39;s original code, only modifying Kafka&amp;#39;s underlying storage to be S3-based. Its claimed features of replicating partitions in seconds and rebalancing network traffic automatically are the key challenges we have encountered while using Kafka. Another major reason we are currently leaning towards this solution is its high compatibility with Apache Kafka, as we do not want to require the entire business system to upgrade while we upgrade the messaging system&amp;#39;s technical architecture.  &lt;/p&gt;\n\n&lt;p&gt;The above represents my personal learning and research results, which may not be entirely accurate. Can you help me validate the accuracy of my technical considerations? If there are better Kafka solutions, I would also appreciate your suggestions.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?auto=webp&amp;s=ff284f11ac8f0f90e8caa94666bd750b03c6b066", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0ecf3998e7ec77914eb73b6f0c5ee32620845a7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=346932ecfac8795af83daa4a2099968155375bd3", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b55a836ac226b24511cd0522059fa77db1acab9b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fcf5f0d9d5be599de11a47aa560d1159c26638d4", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=83e0183ecb82586470f1b3b9506f109e416b8333", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/q6_golmQX5tVDbUMKFHTkokyL-XNHIGTzQOKXSg1kBk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=38861cbd662ddc1423985a2ea052b024b95c5423", "width": 1080, "height": 567}], "variants": {}, "id": "Gap-kV8Fnj_BYQDMqW_-RQJyqwJpXaay3pZZTkcPjvE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bktex1", "is_robot_indexable": true, "report_reasons": null, "author": "tommy_19882024", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bktex1/which_kafka_solution_best_match_my_scenario/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bktex1/which_kafka_solution_best_match_my_scenario/", "subreddit_subscribers": 170830, "created_utc": 1711091866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a company that's heavily relying on databricks for ETL and modeling. I am expected to become an expert using databricks, but I do not have permissions to create new clusters, workspaces, etc. And, when I attempt to create a \"training profile\", the system will still only log me in to these limited profiles.  \n  \nDoes anyone know how to access/create a student-tier workspace that capable of handling the demands of Databrick's own learning modules? Does Databricks provide access to something like this for people training to use their systems?  \n  \nAny help would be appreciated.  \nI did check online and everything points toward having admin access, which leads me to believe I will need to set up a personal azure account and pay for databricks itself. I'm trying to avoid that.", "author_fullname": "t2_8ehj9bag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I learn databricks without admin rights / or at a low cost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkeepi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711047663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a company that&amp;#39;s heavily relying on databricks for ETL and modeling. I am expected to become an expert using databricks, but I do not have permissions to create new clusters, workspaces, etc. And, when I attempt to create a &amp;quot;training profile&amp;quot;, the system will still only log me in to these limited profiles.  &lt;/p&gt;\n\n&lt;p&gt;Does anyone know how to access/create a student-tier workspace that capable of handling the demands of Databrick&amp;#39;s own learning modules? Does Databricks provide access to something like this for people training to use their systems?  &lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated.&lt;br/&gt;\nI did check online and everything points toward having admin access, which leads me to believe I will need to set up a personal azure account and pay for databricks itself. I&amp;#39;m trying to avoid that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkeepi", "is_robot_indexable": true, "report_reasons": null, "author": "ExpertlyAmateur", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkeepi/how_can_i_learn_databricks_without_admin_rights/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkeepi/how_can_i_learn_databricks_without_admin_rights/", "subreddit_subscribers": 170830, "created_utc": 1711047663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Was recently hired as a data analyst in a small company and I want to help out in restructuring our data systems (we don't have data engineers or the like). We're pulling CSV's regularly (about 4-5 times a day) and we use it to update our BigQuery tables. These CSV's however, are highly overlapping and duplicates across multiple CSV's are common. Our approach right now is just outright deleting a portion of our main table corresponding to the recent previous pulls and then repopulating them with the latest one. This main table is also what we use for regular reporting and visualizations. There are a lot of joins across other tables as well but all tables don't have keys and indices. The joins are also usually on derived columns.\n\nJust started learning about data warehousing and dimensional modelling and I wonder if this is applicable in our case. Should I: create a more efficient fully-functioning database (not necessarily conforming to dimensional modeling frameworks), or design a data warehouse pulling the CSV's that conforms to dimensional modeling, or both, ie. having a database and a separate data warehouse pulling from that database?\n\nOur team is mostly using Google Cloud and I was wondering if BigQuery or Cloud SQL or maybe a combination of both would be the ideal tool to use. Thanks!", "author_fullname": "t2_98q4cvf0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on restructuring of data infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bka37v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711037093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was recently hired as a data analyst in a small company and I want to help out in restructuring our data systems (we don&amp;#39;t have data engineers or the like). We&amp;#39;re pulling CSV&amp;#39;s regularly (about 4-5 times a day) and we use it to update our BigQuery tables. These CSV&amp;#39;s however, are highly overlapping and duplicates across multiple CSV&amp;#39;s are common. Our approach right now is just outright deleting a portion of our main table corresponding to the recent previous pulls and then repopulating them with the latest one. This main table is also what we use for regular reporting and visualizations. There are a lot of joins across other tables as well but all tables don&amp;#39;t have keys and indices. The joins are also usually on derived columns.&lt;/p&gt;\n\n&lt;p&gt;Just started learning about data warehousing and dimensional modelling and I wonder if this is applicable in our case. Should I: create a more efficient fully-functioning database (not necessarily conforming to dimensional modeling frameworks), or design a data warehouse pulling the CSV&amp;#39;s that conforms to dimensional modeling, or both, ie. having a database and a separate data warehouse pulling from that database?&lt;/p&gt;\n\n&lt;p&gt;Our team is mostly using Google Cloud and I was wondering if BigQuery or Cloud SQL or maybe a combination of both would be the ideal tool to use. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bka37v", "is_robot_indexable": true, "report_reasons": null, "author": "MushroomDegenerate", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bka37v/advice_on_restructuring_of_data_infrastructure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bka37v/advice_on_restructuring_of_data_infrastructure/", "subreddit_subscribers": 170830, "created_utc": 1711037093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nI am currently a Junior in college and am doing an end-to-end analytics project that requires data extraction (web scraping), data cleaning, EDA, etc... Right now I was wondering if there's any way to schedule the [extraction.py](https://extraction.py) file to run every 2 weeks, then trigger the data\\_cleaning.py file to run after the [extraction.py](https://extraction.py) file. Also, I am open to any feedback regarding my project. Since I am an MIS major instead of CS, my code might not be as clean as it is supposed to be, but I am trying my best to work on it daily. Truly appreciate the feedback and the help.\n\n[Project Link](https://github.com/MarkPhamm/British-Airway)", "author_fullname": "t2_7vqsib1l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking for help with side Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkq5yk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711079233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;I am currently a Junior in college and am doing an end-to-end analytics project that requires data extraction (web scraping), data cleaning, EDA, etc... Right now I was wondering if there&amp;#39;s any way to schedule the &lt;a href=\"https://extraction.py\"&gt;extraction.py&lt;/a&gt; file to run every 2 weeks, then trigger the data_cleaning.py file to run after the &lt;a href=\"https://extraction.py\"&gt;extraction.py&lt;/a&gt; file. Also, I am open to any feedback regarding my project. Since I am an MIS major instead of CS, my code might not be as clean as it is supposed to be, but I am trying my best to work on it daily. Truly appreciate the feedback and the help.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/MarkPhamm/British-Airway\"&gt;Project Link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkq5yk", "is_robot_indexable": true, "report_reasons": null, "author": "SmartPersonality1862", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkq5yk/asking_for_help_with_side_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkq5yk/asking_for_help_with_side_project/", "subreddit_subscribers": 170830, "created_utc": 1711079233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I recently started playing with airflow since the project I work on is migrating to GCP where we will be using Composer.\n\nTill now I was playing on my personal device with Pop OS. Recently I tried to create some dags from my organisation's windows machine and found out airflow tests won't run on Windows, it just supports POSIX systems.\n\nI saw people running airflow on windows with Docker or WSL, but since I am using a company provided laptop, due to restrictions I cannot enable WSL, and not sure how tedious it will be to setup docker here.\n\nJust being able to run unit tests on DAGs and tasks should do the job for me. Anyone have faced similar issue ? Any suggestions here are welcome. \n\nIf nothing, I will have to maybe setup some scripts that will sync my code with a compute engine instance which will run those tests for me. Not sure how feasible this is though ..", "author_fullname": "t2_aaa2s820a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to run airflow tests on local windows machine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkaqt4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711038719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I recently started playing with airflow since the project I work on is migrating to GCP where we will be using Composer.&lt;/p&gt;\n\n&lt;p&gt;Till now I was playing on my personal device with Pop OS. Recently I tried to create some dags from my organisation&amp;#39;s windows machine and found out airflow tests won&amp;#39;t run on Windows, it just supports POSIX systems.&lt;/p&gt;\n\n&lt;p&gt;I saw people running airflow on windows with Docker or WSL, but since I am using a company provided laptop, due to restrictions I cannot enable WSL, and not sure how tedious it will be to setup docker here.&lt;/p&gt;\n\n&lt;p&gt;Just being able to run unit tests on DAGs and tasks should do the job for me. Anyone have faced similar issue ? Any suggestions here are welcome. &lt;/p&gt;\n\n&lt;p&gt;If nothing, I will have to maybe setup some scripts that will sync my code with a compute engine instance which will run those tests for me. Not sure how feasible this is though ..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkaqt4", "is_robot_indexable": true, "report_reasons": null, "author": "Suitable-Side-4133", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkaqt4/what_is_the_best_way_to_run_airflow_tests_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkaqt4/what_is_the_best_way_to_run_airflow_tests_on/", "subreddit_subscribers": 170830, "created_utc": 1711038719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Note: This isn't about Extract/Load data in delta mode but rather the Transform part.\n\nLet's say I'm loading orders and I'm joining the header and detail together.\n\nI'm trying to figure out how to handle scenarios where either table gets a change.\n\nLet's say the Order header changes, I'd want to reload the entire Order by joining the header/detail but, only the order currently has delta data.\n\nThere's also the scenario where a new order line appears and I need to reload the entire order because, let's say I'm performing window functions or any similar workload.\n\n&amp;#x200B;\n\nI also have scenarios where more than 2 tables get joined together and all of them are deltas.\n\n&amp;#x200B;\n\nI'm currently using SSIS with most of the workload being pure SQL or Stored Procedures.\n\nI'm aware the tool is old and we are looking into new tools but I still need to develop new stuff right now, plus I assume the issue would remain in other tools.\n\n&amp;#x200B;\n\nThanks!\n\n&amp;#x200B;", "author_fullname": "t2_6293r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performing delta/incremental loads with header/detail relationships", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bk8637", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711032214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Note: This isn&amp;#39;t about Extract/Load data in delta mode but rather the Transform part.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I&amp;#39;m loading orders and I&amp;#39;m joining the header and detail together.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to figure out how to handle scenarios where either table gets a change.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say the Order header changes, I&amp;#39;d want to reload the entire Order by joining the header/detail but, only the order currently has delta data.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also the scenario where a new order line appears and I need to reload the entire order because, let&amp;#39;s say I&amp;#39;m performing window functions or any similar workload.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I also have scenarios where more than 2 tables get joined together and all of them are deltas.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using SSIS with most of the workload being pure SQL or Stored Procedures.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware the tool is old and we are looking into new tools but I still need to develop new stuff right now, plus I assume the issue would remain in other tools.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bk8637", "is_robot_indexable": true, "report_reasons": null, "author": "meatmick", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bk8637/performing_deltaincremental_loads_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bk8637/performing_deltaincremental_loads_with/", "subreddit_subscribers": 170830, "created_utc": 1711032214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am the only data engineer at a small company with not too much experience, and I have to figure out a system to supply the data for our BI platform and other reports, from various sources (APIs, csv files, SQL databases, etc.).\n\nI looked around and I found Apache NiFi (which has already been recommended by other people on this sub). I've already played around with it a little bit and it seems to be what I'm looking for. It was also important that I can run python scripts with any dependencies, and it seems to be allowing it through the ExecuteStreamCommand processor, but what I don't know is how I should organize these scripts and where to store them.\n\nWe will run NiFi in Kubernetes, and according to our DevOps guy, it would be very bad practice if we just kept adding these python scripts / virtual environments (in case we really need that complexity) to the container of NiFi. \n\nWhat would be the best practice here? We want to keep the python scripts version controlled, so it would be pushed to our own gitlab. Of course we don't want to deploy each python script individually and create a bunch of pods either, but we're struggling to find our way.\n\nThank you in advance!", "author_fullname": "t2_bcsbesiv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing scripts in Apache NiFi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bk45ct", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711019860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am the only data engineer at a small company with not too much experience, and I have to figure out a system to supply the data for our BI platform and other reports, from various sources (APIs, csv files, SQL databases, etc.).&lt;/p&gt;\n\n&lt;p&gt;I looked around and I found Apache NiFi (which has already been recommended by other people on this sub). I&amp;#39;ve already played around with it a little bit and it seems to be what I&amp;#39;m looking for. It was also important that I can run python scripts with any dependencies, and it seems to be allowing it through the ExecuteStreamCommand processor, but what I don&amp;#39;t know is how I should organize these scripts and where to store them.&lt;/p&gt;\n\n&lt;p&gt;We will run NiFi in Kubernetes, and according to our DevOps guy, it would be very bad practice if we just kept adding these python scripts / virtual environments (in case we really need that complexity) to the container of NiFi. &lt;/p&gt;\n\n&lt;p&gt;What would be the best practice here? We want to keep the python scripts version controlled, so it would be pushed to our own gitlab. Of course we don&amp;#39;t want to deploy each python script individually and create a bunch of pods either, but we&amp;#39;re struggling to find our way.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bk45ct", "is_robot_indexable": true, "report_reasons": null, "author": "csicskagyasz00", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bk45ct/organizing_scripts_in_apache_nifi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bk45ct/organizing_scripts_in_apache_nifi/", "subreddit_subscribers": 170830, "created_utc": 1711019860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I work in Data and mostly have employees working on google sheets where data is processing step by step by diff people before its been locked and then we move to next sheet for another day. \nWe are looking for a database which automatically stores this data from time to time and where I can connect this database to a visualization tool say PpwerBi. This way if I need to look at 2023 data I can code and extract all data from this database. \n\nSorry for lame tech lang but Im not a tech person and work for a disorganized and small healthcsre company. \n\nQuestion is what show we really get? We only have sheeets and powerbi. No place where we store our data. If we need to do analayses on yearly basis, need to copy paste values from several sheets into excel and import to powerBI. Ofcourse can connect bi to sheets but thats going to be 500+ connections on yearly basis which powerbi cannot handle\nThanks", "author_fullname": "t2_83gq3oxts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkrjby", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711084072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I work in Data and mostly have employees working on google sheets where data is processing step by step by diff people before its been locked and then we move to next sheet for another day. \nWe are looking for a database which automatically stores this data from time to time and where I can connect this database to a visualization tool say PpwerBi. This way if I need to look at 2023 data I can code and extract all data from this database. &lt;/p&gt;\n\n&lt;p&gt;Sorry for lame tech lang but Im not a tech person and work for a disorganized and small healthcsre company. &lt;/p&gt;\n\n&lt;p&gt;Question is what show we really get? We only have sheeets and powerbi. No place where we store our data. If we need to do analayses on yearly basis, need to copy paste values from several sheets into excel and import to powerBI. Ofcourse can connect bi to sheets but thats going to be 500+ connections on yearly basis which powerbi cannot handle\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkrjby", "is_robot_indexable": true, "report_reasons": null, "author": "FigTraditional1201", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkrjby/database_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkrjby/database_solution/", "subreddit_subscribers": 170830, "created_utc": 1711084072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am developing a medium data warehouse with size about 100 GB, using Postgres as database. At the moment I  running a generic postgresql server for dev environment, but I wonder if that version will hold up on production.\n\nSo, what is your setup? Please share!", "author_fullname": "t2_125mdi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your Postgres setup for data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkpfx3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711076883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am developing a medium data warehouse with size about 100 GB, using Postgres as database. At the moment I  running a generic postgresql server for dev environment, but I wonder if that version will hold up on production.&lt;/p&gt;\n\n&lt;p&gt;So, what is your setup? Please share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkpfx3", "is_robot_indexable": true, "report_reasons": null, "author": "dreamingfighter", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkpfx3/what_is_your_postgres_setup_for_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkpfx3/what_is_your_postgres_setup_for_data_warehouse/", "subreddit_subscribers": 170830, "created_utc": 1711076883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, we're recently undergoing a bit of a project to extract data from a SAAS platform. Just a few API endpoints. It's pretty niche, so no managed EL services (Fivetran, Airbyte), so we gotta do it in-house. Fine.\n\nBecause the dataset is relatively small, for 9/10 endpoints, we can just do a full extract (~300 pages) every 6 hours or so using a Google Cloud Function and overwrite our BQ tables. \n\nFor one of the endpoints, however, we're talking 3000+ API calls for a full data dump. Not great. Cloud function execution time is 10 minutes so it times out. We could move the script to Cloud Run, but I feel we're going to come up against the same issues.\n\nNow, there is a /changes endpoint, which only returns records that have changed since the provided date, but in terms of EL strategy, we still need to somehow consolidate the data into a source of truth that mirrors the source.\n\nI'm for a bit of a sense-check. I propose that \n\n* Every 6 hours (or any arbitrary cycle time), we ask for any data that's changed at source for the last four* cycle periods.\n* We take those changed records and dump them into Google Cloud Storage in JSON/Avro, whatever.\n* A BQ data transfer job appends all records into a table\n* We create a Dataform/DBT SQL pipeline that picks out all of the most recent records for every record ID into a staging table.\n* Downstream BI can use that staging table as a 'mirrored' source of truth from the SAAS platform.\n\nWhy four*? Contingency. If an EL fails, it can fail for 4 cycles before we need manual intervention.\n\nAm I overcomplicating this?", "author_fullname": "t2_hpuhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting from API strategy - what to do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkiai1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711057173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, we&amp;#39;re recently undergoing a bit of a project to extract data from a SAAS platform. Just a few API endpoints. It&amp;#39;s pretty niche, so no managed EL services (Fivetran, Airbyte), so we gotta do it in-house. Fine.&lt;/p&gt;\n\n&lt;p&gt;Because the dataset is relatively small, for 9/10 endpoints, we can just do a full extract (~300 pages) every 6 hours or so using a Google Cloud Function and overwrite our BQ tables. &lt;/p&gt;\n\n&lt;p&gt;For one of the endpoints, however, we&amp;#39;re talking 3000+ API calls for a full data dump. Not great. Cloud function execution time is 10 minutes so it times out. We could move the script to Cloud Run, but I feel we&amp;#39;re going to come up against the same issues.&lt;/p&gt;\n\n&lt;p&gt;Now, there is a /changes endpoint, which only returns records that have changed since the provided date, but in terms of EL strategy, we still need to somehow consolidate the data into a source of truth that mirrors the source.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m for a bit of a sense-check. I propose that &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Every 6 hours (or any arbitrary cycle time), we ask for any data that&amp;#39;s changed at source for the last four* cycle periods.&lt;/li&gt;\n&lt;li&gt;We take those changed records and dump them into Google Cloud Storage in JSON/Avro, whatever.&lt;/li&gt;\n&lt;li&gt;A BQ data transfer job appends all records into a table&lt;/li&gt;\n&lt;li&gt;We create a Dataform/DBT SQL pipeline that picks out all of the most recent records for every record ID into a staging table.&lt;/li&gt;\n&lt;li&gt;Downstream BI can use that staging table as a &amp;#39;mirrored&amp;#39; source of truth from the SAAS platform.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why four*? Contingency. If an EL fails, it can fail for 4 cycles before we need manual intervention.&lt;/p&gt;\n\n&lt;p&gt;Am I overcomplicating this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bkiai1", "is_robot_indexable": true, "report_reasons": null, "author": "wiktor1800", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkiai1/extracting_from_api_strategy_what_to_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkiai1/extracting_from_api_strategy_what_to_do/", "subreddit_subscribers": 170830, "created_utc": 1711057173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I was wondering if yall can explain the difference between data analyst and business intelligence analyst. From the job descriptions, they seem really similar. ", "author_fullname": "t2_y1h3t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between DA and BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkeojr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711048312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I was wondering if yall can explain the difference between data analyst and business intelligence analyst. From the job descriptions, they seem really similar. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1bkeojr", "is_robot_indexable": true, "report_reasons": null, "author": "xloserr", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkeojr/difference_between_da_and_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkeojr/difference_between_da_and_bi/", "subreddit_subscribers": 170830, "created_utc": 1711048312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AI is almost like a buzz word these days and I understand hopelessly little. What should I read to get up to speed? Ideally related to data engineering. ", "author_fullname": "t2_da8v38boh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to read to get up to speed with AI? ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bkd6t0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711044665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI is almost like a buzz word these days and I understand hopelessly little. What should I read to get up to speed? Ideally related to data engineering. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bkd6t0", "is_robot_indexable": true, "report_reasons": null, "author": "Cultural-Ideal-7924", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkd6t0/what_to_read_to_get_up_to_speed_with_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkd6t0/what_to_read_to_get_up_to_speed_with_ai/", "subreddit_subscribers": 170830, "created_utc": 1711044665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "    FROM python:3.10.6-slim-buster\n    \n    WORKDIR /orchestration\n    \n    COPY . /orchestration\n    \n    RUN apt-get update \\\n        &amp;&amp; apt-get -y install libpq-dev gcc \\\n        &amp;&amp; pip install psycopg2 \\\n        &amp;&amp; pip install --no-cache-dir -r requirements.txt\n    \n    EXPOSE 4200\n    \n    # Run the Python application\n    ENTRYPOINT [\"python\", \"data_orchestration/staging_workloads/main.py\"]\n    RUN prefect server start\n\nI can't build the docker image because it gets stuck in the \"prefect server start\" endlessly. I've seen other repos and I think there is nothing wrong with my code.\n\n&amp;#x200B;\n\nCan you please help me?", "author_fullname": "t2_a9360hkx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help deploying prefect to EC2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bk78d2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711029686.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;pre&gt;&lt;code&gt;FROM python:3.10.6-slim-buster\n\nWORKDIR /orchestration\n\nCOPY . /orchestration\n\nRUN apt-get update \\\n    &amp;amp;&amp;amp; apt-get -y install libpq-dev gcc \\\n    &amp;amp;&amp;amp; pip install psycopg2 \\\n    &amp;amp;&amp;amp; pip install --no-cache-dir -r requirements.txt\n\nEXPOSE 4200\n\n# Run the Python application\nENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;data_orchestration/staging_workloads/main.py&amp;quot;]\nRUN prefect server start\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I can&amp;#39;t build the docker image because it gets stuck in the &amp;quot;prefect server start&amp;quot; endlessly. I&amp;#39;ve seen other repos and I think there is nothing wrong with my code.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can you please help me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bk78d2", "is_robot_indexable": true, "report_reasons": null, "author": "Different_Fee6785", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bk78d2/help_deploying_prefect_to_ec2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bk78d2/help_deploying_prefect_to_ec2/", "subreddit_subscribers": 170830, "created_utc": 1711029686.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, maybe you have faced this. I have to send some data to an endpoint where the batch size limit is 10k, we were doing ok using python on airflow (single core) but now the data size increased to 10M so it\u2019s taking too long because of the 1000s requests\n\nWe could use spark but I would like to avoid it (as 10M fits in one worker perfectly) but not sure how, not really used to python multi threading or how to implement on airflow ", "author_fullname": "t2_79l5nq82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paralelize requests to insert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bk57gv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711023575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, maybe you have faced this. I have to send some data to an endpoint where the batch size limit is 10k, we were doing ok using python on airflow (single core) but now the data size increased to 10M so it\u2019s taking too long because of the 1000s requests&lt;/p&gt;\n\n&lt;p&gt;We could use spark but I would like to avoid it (as 10M fits in one worker perfectly) but not sure how, not really used to python multi threading or how to implement on airflow &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bk57gv", "is_robot_indexable": true, "report_reasons": null, "author": "Obvious-Phrase-657", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bk57gv/paralelize_requests_to_insert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bk57gv/paralelize_requests_to_insert/", "subreddit_subscribers": 170830, "created_utc": 1711023575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for information, documentation plus videos preferable in English Apache Doris. This is an upcoming field I am venturing to and would like to have as much info. as possible on the subject matter. ", "author_fullname": "t2_cxzdcift", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Doris", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1bk50f2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711022912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for information, documentation plus videos preferable in English Apache Doris. This is an upcoming field I am venturing to and would like to have as much info. as possible on the subject matter. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1bk50f2", "is_robot_indexable": true, "report_reasons": null, "author": "Lazy_Secret8626", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bk50f2/apache_doris/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bk50f2/apache_doris/", "subreddit_subscribers": 170830, "created_utc": 1711022912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! My name is Peter and I've been working on a data generation and validation tool called [Data Caterer](https://data.catering/). The latest version comes with a UI and you can now run it on Windows, Mac or Linux (also via Docker). I hope this tool makes generating and validating data across any data source simple and easy.\n\nKey features:\n- Batch and event data generation\n- Maintain relationships across any dataset\n- Create custom data generation scenarios\n- Clean up generated data\n- Advanced validation options\n- Suggest data validations\n\n[Quick start to run it yourself.](https://data.catering/get-started/docker)\n\n[Demo of the UI.](https://data.catering/sample/ui/index.html)\n\n[Github repo.](https://github.com/data-catering/data-caterer)\n\nHappy to receive any feedback.", "author_fullname": "t2_h209j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Generation and Validation Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bkuszf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1711098169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! My name is Peter and I&amp;#39;ve been working on a data generation and validation tool called &lt;a href=\"https://data.catering/\"&gt;Data Caterer&lt;/a&gt;. The latest version comes with a UI and you can now run it on Windows, Mac or Linux (also via Docker). I hope this tool makes generating and validating data across any data source simple and easy.&lt;/p&gt;\n\n&lt;p&gt;Key features:\n- Batch and event data generation\n- Maintain relationships across any dataset\n- Create custom data generation scenarios\n- Clean up generated data\n- Advanced validation options\n- Suggest data validations&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://data.catering/get-started/docker\"&gt;Quick start to run it yourself.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://data.catering/sample/ui/index.html\"&gt;Demo of the UI.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/data-catering/data-caterer\"&gt;Github repo.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to receive any feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?auto=webp&amp;s=b6dd71e86b80372c7b40d0dd1cbe8eb15713d211", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f27dad83d552b4340e4e2fba146314a746d5b6c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d63cf74d31d8fdb9c4e5e11f409c1e48920d936", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5aa706541a35748bdb248dff3f22c5b49db11f17", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=717125de4265264970e61b6f3bfce9306b99c3b2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=363a4f4536ecc43b58fff3e01f8be492e2fcdcd7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/-L6t3rMwHc6UOLVDBhPGvNt27cFh58eiGEAgg1ct_lI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff1d200561f21b331379bc1b2305b2921d385f5e", "width": 1080, "height": 567}], "variants": {}, "id": "Us7ZEZRSIqU-6GfX_0BHZ2rb6V_vyVy2N3JCeqc72S0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "1bkuszf", "is_robot_indexable": true, "report_reasons": null, "author": "Pitah7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkuszf/data_generation_and_validation_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkuszf/data_generation_and_validation_tool/", "subreddit_subscribers": 170830, "created_utc": 1711098169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nSoon I am going to start a new project where I will set up the entire architecture as sole data engineer.\n\nSome goals that are on the horizon for this data platform would be analytics and potentially a login platform/website containing all joined sources. All done within Azure. There might come streaming and/or AI usecases in the future as well.\n\nOnestop shop solution:\nUsing databricks (with ADLs) for Ingestion and using deltalive tables for the whole ELT. We have unity catalog on top of this.\nPowerBI / analysts can connect to the databricks SQL warehouse and we can use single node clusters for small workloads and scale up for big ones.\n\nSeparate modules:\nFor orchestration/ingestion ADF. Data is stored in Postgres and the T can be handles with DBT being launched from ADF. PowerBI/analysts can connect to the DB.\n\nAI solutions would be another module (preferably not AzureML) and streaming done via Azure stream analytics probably.\n\nWhat would have your preference and why? What are some things that might be easy to miss in either solution?\n\nI am heavily leaning towards the databricks solution as I think the collaboration is easier, integrations work seamlessly and we can very easily on-board data scientists as we can use both python and SQL", "author_fullname": "t2_10fsx8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Onestop shop vs separate modules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bkuc50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711096073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Soon I am going to start a new project where I will set up the entire architecture as sole data engineer.&lt;/p&gt;\n\n&lt;p&gt;Some goals that are on the horizon for this data platform would be analytics and potentially a login platform/website containing all joined sources. All done within Azure. There might come streaming and/or AI usecases in the future as well.&lt;/p&gt;\n\n&lt;p&gt;Onestop shop solution:\nUsing databricks (with ADLs) for Ingestion and using deltalive tables for the whole ELT. We have unity catalog on top of this.\nPowerBI / analysts can connect to the databricks SQL warehouse and we can use single node clusters for small workloads and scale up for big ones.&lt;/p&gt;\n\n&lt;p&gt;Separate modules:\nFor orchestration/ingestion ADF. Data is stored in Postgres and the T can be handles with DBT being launched from ADF. PowerBI/analysts can connect to the DB.&lt;/p&gt;\n\n&lt;p&gt;AI solutions would be another module (preferably not AzureML) and streaming done via Azure stream analytics probably.&lt;/p&gt;\n\n&lt;p&gt;What would have your preference and why? What are some things that might be easy to miss in either solution?&lt;/p&gt;\n\n&lt;p&gt;I am heavily leaning towards the databricks solution as I think the collaboration is easier, integrations work seamlessly and we can very easily on-board data scientists as we can use both python and SQL&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bkuc50", "is_robot_indexable": true, "report_reasons": null, "author": "juicd_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bkuc50/onestop_shop_vs_separate_modules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bkuc50/onestop_shop_vs_separate_modules/", "subreddit_subscribers": 170830, "created_utc": 1711096073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI have a quick question about command id. I need to pull cdc logs to Storage account from on prem sql server and table is badly designed so I do not have any watermark column I can grab onto. Is command id good choice for know ing what I ingested with copy activity (ADF) since I do not have any other options. Also would date without timestamp be somewhat acceptable choice? \n\nThanks! ", "author_fullname": "t2_4c7udteq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC logs - __$command_id", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bku4jp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711095103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I have a quick question about command id. I need to pull cdc logs to Storage account from on prem sql server and table is badly designed so I do not have any watermark column I can grab onto. Is command id good choice for know ing what I ingested with copy activity (ADF) since I do not have any other options. Also would date without timestamp be somewhat acceptable choice? &lt;/p&gt;\n\n&lt;p&gt;Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bku4jp", "is_robot_indexable": true, "report_reasons": null, "author": "MahoYami", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bku4jp/cdc_logs_command_id/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bku4jp/cdc_logs_command_id/", "subreddit_subscribers": 170830, "created_utc": 1711095103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an airflow DAG running with BashOperators:\n\n    default_args = {\n        'owner': 'jack',\n        'start_date': datetime(2024, 3, 1, 0, 0, 0, 0)\n    }\n    \n    with DAG(\n        'testing_dag_1209', # Name of DAG\n        description = 'An example of DAG_1209', # Description shown in UI\n        schedule_interval='0 * * * *',  # Job would start at start_date + schedule_interval\n        default_args=default_args,\n        tags = [\"Testing_dag_1209\"]\n    ) as dag:\n        start = EmptyOperator(task_id = 'start')\n        task_11 = BashOperator(task_id = 'task_11', bash_command = '{} {} --logical_date {{ params.logical_date }}'.format(stock_env_python_dir, task),\n                               params = {'logical_date':'{{ds}}'})\n        end = EmptyOperator(task_id = 'end')\n        \n        start &gt;&gt; task_1 &gt;&gt; task_2 &gt;&gt; end \n\nI run a simple python script test\\_task1.py which has content:\n\n    from airflow.decorators import dag, task\n    from airflow.operators.python import get_current_context\n    \n    \n    \n    @task\n    def get_date(**kwargs):\n      print(\"Show current keyword args: \")\n      print('Logical date: ', kwargs)\n      return kwargs\n    \n    \n    \n    print(\"Start test_task1.py...\")\n    \n    date = get_date()\n    print(date)\n\n\n\nThe python script may execute every hour from 2024/3/1 until today.\n\nWhat I want to do is to print the 'logical\\_date' which is the scheduled date for execution from 2024/3/1 until today in the log.\n\n\n\nI'm checked the airflow document about the context access, however it doesn't work as expected, the output log message didn't show anything in the Console print in test\\_task1.py.\n\n\n\n1. Can anyone give me some advice what is the simplest way to print airflow context within the task?\n2. Although I have another question, is it possible to print airflow context outside of tasks? Like printing the context as \n\n&amp;#8203;\n\n    with DAG( 'testing_dag_1209', # Name of DAG description = 'An example of DAG_1209', # Description shown in UI schedule_interval='0 * * * *',  # Job would start at start_date + schedule_interval default_args=default_args, tags = [\"Testing_dag_1209\"] ) \n    as dag: \n    \n      print(\"Current logical date is: {{ ds }}\") \n      start = EmptyOperator(task_id = 'start')  \n      task_11 = BashOperator(task_id = 'task_11', bash_command = '{} {} --logical_date {{ params.logical_date }}'.format(stock_env_python_dir, task),                         params = {'logical_date':'{{ds}}'})  \n      end = EmptyOperator(task_id = 'end')    \n      start &gt;&gt; task_1 &gt;&gt; task_2 &gt;&gt; end \n\n\n\nThank you for all the advice!\n\n", "author_fullname": "t2_11cquw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I get the context information like 'logical_date' within task?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1bku2m7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1711094858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an airflow DAG running with BashOperators:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;default_args = {\n    &amp;#39;owner&amp;#39;: &amp;#39;jack&amp;#39;,\n    &amp;#39;start_date&amp;#39;: datetime(2024, 3, 1, 0, 0, 0, 0)\n}\n\nwith DAG(\n    &amp;#39;testing_dag_1209&amp;#39;, # Name of DAG\n    description = &amp;#39;An example of DAG_1209&amp;#39;, # Description shown in UI\n    schedule_interval=&amp;#39;0 * * * *&amp;#39;,  # Job would start at start_date + schedule_interval\n    default_args=default_args,\n    tags = [&amp;quot;Testing_dag_1209&amp;quot;]\n) as dag:\n    start = EmptyOperator(task_id = &amp;#39;start&amp;#39;)\n    task_11 = BashOperator(task_id = &amp;#39;task_11&amp;#39;, bash_command = &amp;#39;{} {} --logical_date {{ params.logical_date }}&amp;#39;.format(stock_env_python_dir, task),\n                           params = {&amp;#39;logical_date&amp;#39;:&amp;#39;{{ds}}&amp;#39;})\n    end = EmptyOperator(task_id = &amp;#39;end&amp;#39;)\n\n    start &amp;gt;&amp;gt; task_1 &amp;gt;&amp;gt; task_2 &amp;gt;&amp;gt; end \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I run a simple python script test_task1.py which has content:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from airflow.decorators import dag, task\nfrom airflow.operators.python import get_current_context\n\n\n\n@task\ndef get_date(**kwargs):\n  print(&amp;quot;Show current keyword args: &amp;quot;)\n  print(&amp;#39;Logical date: &amp;#39;, kwargs)\n  return kwargs\n\n\n\nprint(&amp;quot;Start test_task1.py...&amp;quot;)\n\ndate = get_date()\nprint(date)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The python script may execute every hour from 2024/3/1 until today.&lt;/p&gt;\n\n&lt;p&gt;What I want to do is to print the &amp;#39;logical_date&amp;#39; which is the scheduled date for execution from 2024/3/1 until today in the log.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m checked the airflow document about the context access, however it doesn&amp;#39;t work as expected, the output log message didn&amp;#39;t show anything in the Console print in test_task1.py.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Can anyone give me some advice what is the simplest way to print airflow context within the task?&lt;/li&gt;\n&lt;li&gt;Although I have another question, is it possible to print airflow context outside of tasks? Like printing the context as &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;with DAG( &amp;#39;testing_dag_1209&amp;#39;, # Name of DAG description = &amp;#39;An example of DAG_1209&amp;#39;, # Description shown in UI schedule_interval=&amp;#39;0 * * * *&amp;#39;,  # Job would start at start_date + schedule_interval default_args=default_args, tags = [&amp;quot;Testing_dag_1209&amp;quot;] ) \nas dag: \n\n  print(&amp;quot;Current logical date is: {{ ds }}&amp;quot;) \n  start = EmptyOperator(task_id = &amp;#39;start&amp;#39;)  \n  task_11 = BashOperator(task_id = &amp;#39;task_11&amp;#39;, bash_command = &amp;#39;{} {} --logical_date {{ params.logical_date }}&amp;#39;.format(stock_env_python_dir, task),                         params = {&amp;#39;logical_date&amp;#39;:&amp;#39;{{ds}}&amp;#39;})  \n  end = EmptyOperator(task_id = &amp;#39;end&amp;#39;)    \n  start &amp;gt;&amp;gt; task_1 &amp;gt;&amp;gt; task_2 &amp;gt;&amp;gt; end \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thank you for all the advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1bku2m7", "is_robot_indexable": true, "report_reasons": null, "author": "Laurence-Lin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1bku2m7/how_can_i_get_the_context_information_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1bku2m7/how_can_i_get_the_context_information_like/", "subreddit_subscribers": 170830, "created_utc": 1711094858.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}