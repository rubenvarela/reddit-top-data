{"kind": "Listing", "data": {"after": null, "dist": 3, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The PM sub is asserting that PMs own the \"what and the why\" of projects and data science should stick to the \"how.\" \n\nIn my experience data scientists are in a unique position to own business outcomes, straddling technical and domain expertise and understanding the risk profile for projects so that smart bets on products can be made. \n\nDo you own business outcomes as a data scientist and if so, do you think it ought to be that way, or should the responsibility largely lie with PMs?\n\n[https://www.reddit.com/r/ProductManagement/comments/18ue8yt/pm\\_contribution\\_in\\_ml\\_projects/?utm\\_source=share&amp;utm\\_medium=web2x&amp;context=3](https://www.reddit.com/r/ProductManagement/comments/18ue8yt/pm_contribution_in_ml_projects/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\n&amp;#x200B;", "author_fullname": "t2_ggrx8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PM, not data science, owns the \"what and the why\" (cross post from PM subreddit)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18uvq41", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703986124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The PM sub is asserting that PMs own the &amp;quot;what and the why&amp;quot; of projects and data science should stick to the &amp;quot;how.&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;In my experience data scientists are in a unique position to own business outcomes, straddling technical and domain expertise and understanding the risk profile for projects so that smart bets on products can be made. &lt;/p&gt;\n\n&lt;p&gt;Do you own business outcomes as a data scientist and if so, do you think it ought to be that way, or should the responsibility largely lie with PMs?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/ProductManagement/comments/18ue8yt/pm_contribution_in_ml_projects/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;https://www.reddit.com/r/ProductManagement/comments/18ue8yt/pm_contribution_in_ml_projects/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "18uvq41", "is_robot_indexable": true, "report_reasons": null, "author": "therockhound", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18uvq41/pm_not_data_science_owns_the_what_and_the_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18uvq41/pm_not_data_science_owns_the_what_and_the_why/", "subreddit_subscribers": 1217480, "created_utc": 1703986124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm building a product for the video game, League of Legends, that will give players 3-6 distinct things to focus on in the game, that will increase their chances of winning the most.\n\nFor my technical background, I thought I wanted to be a data scientist, but transitioned to data engineering, so I have a very fundamental grasp of machine learning concepts. This is why I want input from all of you wonderfully smart people about the way I want to calculate these \"important\" columns.\n\nI know that the world of explanability is still uncertain, but here is my approach:\n\n1. I am given a dataset of matches of a single player, where each row represents the stats of this player at the end of the match. There are \\~100 columns (of things like kills, assists, damage dealt, etc) after dropping the columns with any NULLS in it.\n   1. There is a binary WIN column that shows whether the player won the match or not. This is the column we are most interested in\n2. I train a simple tree-based model on this data, and get the list of \"feature importances\" using sklearn's `permutation_importance()` function.\n   1. For some reason (maybe someone can explain), there are a large number of columns that return a ZERO feature importance after computing this.\n3. This is where I do things differently: I RETRAIN the model using the same dataset, but without the columns that returned 0 importance on the last \"run\"\n4. I basically repeat this process until the list of feature importances doesn't contain ZERO.\n   1. The end result is that there are usually 3-20 columns left (depending on the model).\n5. I take the top N (haven't decided yet) columns and \"give\" them to the user to focus on in their next game\n\nTheoretically, if \"feature importance\" really lives up to it's name, the ending model should have only the \"most important\" columns when trying to achieve a win.\n\nI've tried using SHAP/LIME, but they were more complicated that using straight feature importance.\n\nLike I mentioned, I don't have classical training in ML or Statistics, so all of this is stuff I tried to learn on my own at one point. I appreciate any helpful advice on if this approach makes sense/is valid. \n\n# The big question is: are there any problems with this approach, and are the resulting set of columns truly the \"most important?\"", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "As a non-data-scientist, assess my approach for finding the \"most important\" columns in a dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ut4si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703979106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a product for the video game, League of Legends, that will give players 3-6 distinct things to focus on in the game, that will increase their chances of winning the most.&lt;/p&gt;\n\n&lt;p&gt;For my technical background, I thought I wanted to be a data scientist, but transitioned to data engineering, so I have a very fundamental grasp of machine learning concepts. This is why I want input from all of you wonderfully smart people about the way I want to calculate these &amp;quot;important&amp;quot; columns.&lt;/p&gt;\n\n&lt;p&gt;I know that the world of explanability is still uncertain, but here is my approach:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I am given a dataset of matches of a single player, where each row represents the stats of this player at the end of the match. There are ~100 columns (of things like kills, assists, damage dealt, etc) after dropping the columns with any NULLS in it.\n\n&lt;ol&gt;\n&lt;li&gt;There is a binary WIN column that shows whether the player won the match or not. This is the column we are most interested in&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;I train a simple tree-based model on this data, and get the list of &amp;quot;feature importances&amp;quot; using sklearn&amp;#39;s &lt;code&gt;permutation_importance()&lt;/code&gt; function.\n\n&lt;ol&gt;\n&lt;li&gt;For some reason (maybe someone can explain), there are a large number of columns that return a ZERO feature importance after computing this.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;This is where I do things differently: I RETRAIN the model using the same dataset, but without the columns that returned 0 importance on the last &amp;quot;run&amp;quot;&lt;/li&gt;\n&lt;li&gt;I basically repeat this process until the list of feature importances doesn&amp;#39;t contain ZERO.\n\n&lt;ol&gt;\n&lt;li&gt;The end result is that there are usually 3-20 columns left (depending on the model).&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;I take the top N (haven&amp;#39;t decided yet) columns and &amp;quot;give&amp;quot; them to the user to focus on in their next game&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Theoretically, if &amp;quot;feature importance&amp;quot; really lives up to it&amp;#39;s name, the ending model should have only the &amp;quot;most important&amp;quot; columns when trying to achieve a win.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried using SHAP/LIME, but they were more complicated that using straight feature importance.&lt;/p&gt;\n\n&lt;p&gt;Like I mentioned, I don&amp;#39;t have classical training in ML or Statistics, so all of this is stuff I tried to learn on my own at one point. I appreciate any helpful advice on if this approach makes sense/is valid. &lt;/p&gt;\n\n&lt;h1&gt;The big question is: are there any problems with this approach, and are the resulting set of columns truly the &amp;quot;most important?&amp;quot;&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "18ut4si", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18ut4si/as_a_nondatascientist_assess_my_approach_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18ut4si/as_a_nondatascientist_assess_my_approach_for/", "subreddit_subscribers": 1217480, "created_utc": 1703979106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I possess several Python scripts that need to be executed sequentially.  The subsequent script can be initiated either manually or automatically.  Following each script execution, the output is to be stored in a  database, with the option to manually visualize the data at each step. I  am seeking recommendations for tools that facilitate building pipelines  and dashboards for visualization. An essential requirement is the  ability to maintain versioning for each run. Could you suggest some  no-code or low-code tools that align with these specifications?", "author_fullname": "t2_5owk7j7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "looking for tools to run python script execution, database storage, and visualizations with version control", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18vg8xx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1704054174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I possess several Python scripts that need to be executed sequentially.  The subsequent script can be initiated either manually or automatically.  Following each script execution, the output is to be stored in a  database, with the option to manually visualize the data at each step. I  am seeking recommendations for tools that facilitate building pipelines  and dashboards for visualization. An essential requirement is the  ability to maintain versioning for each run. Could you suggest some  no-code or low-code tools that align with these specifications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "18vg8xx", "is_robot_indexable": true, "report_reasons": null, "author": "mrtac96", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18vg8xx/looking_for_tools_to_run_python_script_execution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18vg8xx/looking_for_tools_to_run_python_script_execution/", "subreddit_subscribers": 1217480, "created_utc": 1704054174.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}