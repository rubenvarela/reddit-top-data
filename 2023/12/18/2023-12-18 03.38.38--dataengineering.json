{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is Databricks fundamentally/architecturally different from other hyper scalers when all them have launched their own Lakehouse replicas? For e.g. BigLake under GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kfn28", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702813427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18kfn28", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kfn28/how_is_databricks_fundamentallyarchitecturally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kfn28/how_is_databricks_fundamentallyarchitecturally/", "subreddit_subscribers": 146650, "created_utc": 1702813427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.instagram.com/reel/C0UMsuNrwX5/?igshid=MzRlODBiNWFlZA==\n\nI've seen it suggested on this sub a few times. Surely it's a waste of time and won't ever be directly applicable to your actual career skills beyond some network basics that you'd get tinkering with AWS anyway?\n\nOr am I missing something?", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would someone want to be building RPI clusters which have so few transferable skills when you could just be doing projects in AWS/cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18khf1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702819930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.instagram.com/reel/C0UMsuNrwX5/?igshid=MzRlODBiNWFlZA==\"&gt;https://www.instagram.com/reel/C0UMsuNrwX5/?igshid=MzRlODBiNWFlZA==&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen it suggested on this sub a few times. Surely it&amp;#39;s a waste of time and won&amp;#39;t ever be directly applicable to your actual career skills beyond some network basics that you&amp;#39;d get tinkering with AWS anyway?&lt;/p&gt;\n\n&lt;p&gt;Or am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-th6YlprDrdAa9jQoqT_f7yvdg8EEAfT44tHbx2vI1w.jpg?auto=webp&amp;s=870722f67c3bc0e2671bdff2eb5e54768215f381", "width": 540, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/-th6YlprDrdAa9jQoqT_f7yvdg8EEAfT44tHbx2vI1w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a56880ad097e4106d1bd35563580d925591b958", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/-th6YlprDrdAa9jQoqT_f7yvdg8EEAfT44tHbx2vI1w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4acc9953c0bd0089ad7e45894f33886def0be756", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/-th6YlprDrdAa9jQoqT_f7yvdg8EEAfT44tHbx2vI1w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8316399932d8a190da49675ca5749033abbadea0", "width": 320, "height": 320}], "variants": {}, "id": "AprF7A7Vjv6kqmuhxUJ-C9dMsGDdNHfawA9lPfuzvBE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18khf1r", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18khf1r/why_would_someone_want_to_be_building_rpi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18khf1r/why_would_someone_want_to_be_building_rpi/", "subreddit_subscribers": 146650, "created_utc": 1702819930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I inherited a DE team when I joined my new job as a team lead. For some reason (performance, on-premise datacenter) they are using MySql for everything, including multi-terabyte datawarehouses. This led to literally 50+ database servers with hundreds of user accounts. While I prepare a migration to a more appropriate toolchain, I still need to manage access to all of these servers. Any pointers to decent, ideally open source, solutions to manage users, roles and permissions across servers. My Google-fu is failing me.", "author_fullname": "t2_dxt8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Database user management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kj9gp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702825600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I inherited a DE team when I joined my new job as a team lead. For some reason (performance, on-premise datacenter) they are using MySql for everything, including multi-terabyte datawarehouses. This led to literally 50+ database servers with hundreds of user accounts. While I prepare a migration to a more appropriate toolchain, I still need to manage access to all of these servers. Any pointers to decent, ideally open source, solutions to manage users, roles and permissions across servers. My Google-fu is failing me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18kj9gp", "is_robot_indexable": true, "report_reasons": null, "author": "pokepip", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kj9gp/cross_database_user_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kj9gp/cross_database_user_management/", "subreddit_subscribers": 146650, "created_utc": 1702825600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are you thinking about getting new skills? What will you suggest if you want to be a updated data engineer or data manager?\n\nAny certifications? Any courses? Any local or enterprise projects? Any ideas to launch your personal brand?", "author_fullname": "t2_nobkhscod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2024 Data Engineering Top Skills that you will prepare for", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kt2fc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702852013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are you thinking about getting new skills? What will you suggest if you want to be a updated data engineer or data manager?&lt;/p&gt;\n\n&lt;p&gt;Any certifications? Any courses? Any local or enterprise projects? Any ideas to launch your personal brand?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18kt2fc", "is_robot_indexable": true, "report_reasons": null, "author": "A-Global-Citizen", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kt2fc/2024_data_engineering_top_skills_that_you_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kt2fc/2024_data_engineering_top_skills_that_you_will/", "subreddit_subscribers": 146650, "created_utc": 1702852013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm developing a data pipeline that takes data from a video game API about player's matches, transforms the data, and loads it into the database.\n\nFirst, I have to get a list of all the match IDs in order to then access their info, but in order to prevent duplicate entries to the player\\_matches table, I remove the match IDs that I already have in my database, from the list of match IDs I receive in the current call.\n\nThe problem comes when I want to do testing on any step in the pipeline, when there are no matches found. If there are no matches found, then the program rightfully exits early because all the subsequent logic is for processing new matches not already in the database.\n\nI had the idea of setting a development flag that would rollback and changes to the database if it's set. That way, I can always test the entirety of the pipeline without having to find new player that have played matches in real time since I last ran the pipeline.\n\nAfter talking about this with ChatGPT, it said this is called a \"dry run.\" I'm not seeing a whole lot of information about it online, so I'm wondering if this is normal practice when developing data pipelines.\n\nEdit: I'm fairly confident this problem has nothing to do with my environment setup or having multiple database environments either. This is a pipeline problem only", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is doing a \"dry run\" of a data pipeline - so that changes to the database are either rolled back or not committed, for testing purposes - normal practice in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kqo3p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702847010.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702845590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m developing a data pipeline that takes data from a video game API about player&amp;#39;s matches, transforms the data, and loads it into the database.&lt;/p&gt;\n\n&lt;p&gt;First, I have to get a list of all the match IDs in order to then access their info, but in order to prevent duplicate entries to the player_matches table, I remove the match IDs that I already have in my database, from the list of match IDs I receive in the current call.&lt;/p&gt;\n\n&lt;p&gt;The problem comes when I want to do testing on any step in the pipeline, when there are no matches found. If there are no matches found, then the program rightfully exits early because all the subsequent logic is for processing new matches not already in the database.&lt;/p&gt;\n\n&lt;p&gt;I had the idea of setting a development flag that would rollback and changes to the database if it&amp;#39;s set. That way, I can always test the entirety of the pipeline without having to find new player that have played matches in real time since I last ran the pipeline.&lt;/p&gt;\n\n&lt;p&gt;After talking about this with ChatGPT, it said this is called a &amp;quot;dry run.&amp;quot; I&amp;#39;m not seeing a whole lot of information about it online, so I&amp;#39;m wondering if this is normal practice when developing data pipelines.&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;m fairly confident this problem has nothing to do with my environment setup or having multiple database environments either. This is a pipeline problem only&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18kqo3p", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kqo3p/is_doing_a_dry_run_of_a_data_pipeline_so_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kqo3p/is_doing_a_dry_run_of_a_data_pipeline_so_that/", "subreddit_subscribers": 146650, "created_utc": 1702845590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer using python, AWS and gitlab in my current project. I have been working for the past 1.7 years in my current role.\n\nI earn \u00a331k per year. \n\nI have a portfolio. I am looking to focus on a new portfolio project now. I have a professional profile picture. I have AWS certifications related to my work as well Python certs.\n\nHow do you choose viable roles to apply for. I am quite gun shy about applying. I have built myself up a lot in preparation for applications next year. I am trying to understand as much as I can. So I can develop more.", "author_fullname": "t2_bkou9use8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on natural career progression. Ask from a \u00a331k UK data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k8ncd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702785645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer using python, AWS and gitlab in my current project. I have been working for the past 1.7 years in my current role.&lt;/p&gt;\n\n&lt;p&gt;I earn \u00a331k per year. &lt;/p&gt;\n\n&lt;p&gt;I have a portfolio. I am looking to focus on a new portfolio project now. I have a professional profile picture. I have AWS certifications related to my work as well Python certs.&lt;/p&gt;\n\n&lt;p&gt;How do you choose viable roles to apply for. I am quite gun shy about applying. I have built myself up a lot in preparation for applications next year. I am trying to understand as much as I can. So I can develop more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18k8ncd", "is_robot_indexable": true, "report_reasons": null, "author": "RagingCharlotte", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18k8ncd/advice_on_natural_career_progression_ask_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18k8ncd/advice_on_natural_career_progression_ask_from_a/", "subreddit_subscribers": 146650, "created_utc": 1702785645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bachelor\u2019s degree in economics and have been working as a business analyst for 2 years. Part of my job involves writing SQL queries for reports and I\u2019ve enjoyed it so far.\nNow I\u2019m looking to transition into a data engineering career. \nMy employer offers tuition assistance, so I\u2019m planning on doing an online program while working as a BA. My options are the following:\n1. MS Computer Information Systems at Boston University with a concentration in database management and business intelligence ($30,000)\n2. BS Computer Science at WGU ($15,000)\n\nWhich would prepare me more for a data engineering career?", "author_fullname": "t2_5uz2eb6mq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best degree for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18krumy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702848706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bachelor\u2019s degree in economics and have been working as a business analyst for 2 years. Part of my job involves writing SQL queries for reports and I\u2019ve enjoyed it so far.\nNow I\u2019m looking to transition into a data engineering career. \nMy employer offers tuition assistance, so I\u2019m planning on doing an online program while working as a BA. My options are the following:\n1. MS Computer Information Systems at Boston University with a concentration in database management and business intelligence ($30,000)\n2. BS Computer Science at WGU ($15,000)&lt;/p&gt;\n\n&lt;p&gt;Which would prepare me more for a data engineering career?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18krumy", "is_robot_indexable": true, "report_reasons": null, "author": "abc__901", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18krumy/best_degree_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18krumy/best_degree_for_data_engineering/", "subreddit_subscribers": 146650, "created_utc": 1702848706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work with dataflow and don\u2019t use Hadoop or its ecosystem. \n\nI am pretty familiar with how  MapReduce works and I\u2019ve read on how Hadoop accesses the HDFS, YARN, worker nodes, data replication/redundancy, etc. \n\nI also have read up on Spark and understand that it can use the Hadoop software framework to run MapReduce on data stored in HDFS. \n\nBut here is a point of confusion: people refer to Hadoop and the entire software framework, the entire ecosystem, sometimes incorrectly as simply HDFS, or incorrectly as simply MapReduce computations. Can anyone verify that Hadoop can ONLY perform computations using a MapReduce algorithm? There is NO OTHER framework it can use?\n\nSecond point of confusion: spark is frequently discussed in terms of Hadoop and MapReduce. But if I understand correctly, Spark can operate without using Hadoop, HDFS, or using MapReduce. In other words that is one series of things it can do but it has additional flexibility. Is this correct?\n\nNote: not a question about what is best to use. I just am trying to understand the underlying framework.", "author_fullname": "t2_kii3tm7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hadoop vs MapReduce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kmkf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702834679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work with dataflow and don\u2019t use Hadoop or its ecosystem. &lt;/p&gt;\n\n&lt;p&gt;I am pretty familiar with how  MapReduce works and I\u2019ve read on how Hadoop accesses the HDFS, YARN, worker nodes, data replication/redundancy, etc. &lt;/p&gt;\n\n&lt;p&gt;I also have read up on Spark and understand that it can use the Hadoop software framework to run MapReduce on data stored in HDFS. &lt;/p&gt;\n\n&lt;p&gt;But here is a point of confusion: people refer to Hadoop and the entire software framework, the entire ecosystem, sometimes incorrectly as simply HDFS, or incorrectly as simply MapReduce computations. Can anyone verify that Hadoop can ONLY perform computations using a MapReduce algorithm? There is NO OTHER framework it can use?&lt;/p&gt;\n\n&lt;p&gt;Second point of confusion: spark is frequently discussed in terms of Hadoop and MapReduce. But if I understand correctly, Spark can operate without using Hadoop, HDFS, or using MapReduce. In other words that is one series of things it can do but it has additional flexibility. Is this correct?&lt;/p&gt;\n\n&lt;p&gt;Note: not a question about what is best to use. I just am trying to understand the underlying framework.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18kmkf6", "is_robot_indexable": true, "report_reasons": null, "author": "Ornery_Vanilla1902", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kmkf6/hadoop_vs_mapreduce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kmkf6/hadoop_vs_mapreduce/", "subreddit_subscribers": 146650, "created_utc": 1702834679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\nI'm planning to launch an OpenSource project on interoperability modules between different data collection and storage sources (KoboToolBox, ODK, Monkey, DHIS2, etc.).\nToday, when you work on this different project, you have to create your own data extraction code.\nCould we work together on this project?\nThe languages to be used are R and Python.", "author_fullname": "t2_gnhzyiiu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Library for interoperability datasources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 69, "top_awarded_type": null, "hide_score": false, "name": "t3_18kg22e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-HgeX6Gn5tgepqRvzYfUYxKNoXyMi7zvKjMP4OXoREM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702815041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,\nI&amp;#39;m planning to launch an OpenSource project on interoperability modules between different data collection and storage sources (KoboToolBox, ODK, Monkey, DHIS2, etc.).\nToday, when you work on this different project, you have to create your own data extraction code.\nCould we work together on this project?\nThe languages to be used are R and Python.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/frdblj8dju6c1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?auto=webp&amp;s=a9726b7fc3a6a933fc2948f9a2dc9ce040d0c80d", "width": 784, "height": 391}, "resolutions": [{"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c65a96d42aa22cbd9e749eb01cf77be3df258fae", "width": 108, "height": 53}, {"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad480beed7fb891aa1e310f5bf88ec471fa6d680", "width": 216, "height": 107}, {"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c7d9156a2a5f220628386bea858918713c9005f", "width": 320, "height": 159}, {"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c97bbc7692690e699b23f2c0b2510277676998cf", "width": 640, "height": 319}], "variants": {}, "id": "OOIGtN6jtekJhRzY3_ykgoXStsBRU36dxSvM21Xy0ho"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18kg22e", "is_robot_indexable": true, "report_reasons": null, "author": "hlama26", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kg22e/library_for_interoperability_datasources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/frdblj8dju6c1.jpeg", "subreddit_subscribers": 146650, "created_utc": 1702815041.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow DE \n\nI would love to hear some tips about how I should proceed with star  schema implementation for DW (Synapse will serve as serving layer, while  Databricks as transformation in PySpark).\n\nI have data with partition by market, what I plan is to enforce schema in file of every market, then in enriched layer, I'll union every  market to create cross market dataset, lets call it master\\_table\n\nNow, I need to distinguish dim tables from fact\\_table, let it be for instance \"AGE\",\"GENDER\",\"LOCATION\" for \"customer\\_profile\" and \"PRODUCT\", \"BRAND\" for \"product\\_profile\"\n\nFrom each dim table, I need to create either surrogate key  (synthetic one, like integer number, incrementally) or composite key  (for instance concatenation of all columns) - Microsoft proposes INT values for joins and groups for better performance\n\nAfter I got the key, I need to have dim table keys back into fact  table and drop original values, leaving only keys and rest columns which  belongs to fact table\n\nMy questions and concerns are:\n\nWhat is the order of tasks? Should I create a view on this cross market (lets call it master\\_table) dataset from databricks and then:\n\n&amp;#x200B;\n\n1. first create VIEW for dim table customer\\_profile like SELECT DISTINCT AGE,GENDER,LOCATION from master\\_table - but then how I can get ROW\\_NUMBER OVER() for customer\\_profile\\_id?\n2. second create VIEW for dim table product\\_profile - SELECT DISTINCT PRODUCT, BRAND from master\\_table - again, how I can get ROW\\_NUMBER OVER() to get product\\_profile\\_id?\n3. third create fact\\_table : SELECT customer\\_profile\\_id, product\\_profile\\_id, \\[some fact measure\\] FROM master\\_table mt  LEFT JOIN customer\\_profile cp ON mt.AGE = cp.AGE AND mt.LOCATION=cp.LOCATION and mt.GENDER=cp.GENDER and same for product\\_profile\n\nIs it the way to go?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Star-Schema DW implementation in real life scenario", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ko96l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702839237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DE &lt;/p&gt;\n\n&lt;p&gt;I would love to hear some tips about how I should proceed with star  schema implementation for DW (Synapse will serve as serving layer, while  Databricks as transformation in PySpark).&lt;/p&gt;\n\n&lt;p&gt;I have data with partition by market, what I plan is to enforce schema in file of every market, then in enriched layer, I&amp;#39;ll union every  market to create cross market dataset, lets call it master_table&lt;/p&gt;\n\n&lt;p&gt;Now, I need to distinguish dim tables from fact_table, let it be for instance &amp;quot;AGE&amp;quot;,&amp;quot;GENDER&amp;quot;,&amp;quot;LOCATION&amp;quot; for &amp;quot;customer_profile&amp;quot; and &amp;quot;PRODUCT&amp;quot;, &amp;quot;BRAND&amp;quot; for &amp;quot;product_profile&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;From each dim table, I need to create either surrogate key  (synthetic one, like integer number, incrementally) or composite key  (for instance concatenation of all columns) - Microsoft proposes INT values for joins and groups for better performance&lt;/p&gt;\n\n&lt;p&gt;After I got the key, I need to have dim table keys back into fact  table and drop original values, leaving only keys and rest columns which  belongs to fact table&lt;/p&gt;\n\n&lt;p&gt;My questions and concerns are:&lt;/p&gt;\n\n&lt;p&gt;What is the order of tasks? Should I create a view on this cross market (lets call it master_table) dataset from databricks and then:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;first create VIEW for dim table customer_profile like SELECT DISTINCT AGE,GENDER,LOCATION from master_table - but then how I can get ROW_NUMBER OVER() for customer_profile_id?&lt;/li&gt;\n&lt;li&gt;second create VIEW for dim table product_profile - SELECT DISTINCT PRODUCT, BRAND from master_table - again, how I can get ROW_NUMBER OVER() to get product_profile_id?&lt;/li&gt;\n&lt;li&gt;third create fact_table : SELECT customer_profile_id, product_profile_id, [some fact measure] FROM master_table mt  LEFT JOIN customer_profile cp ON mt.AGE = cp.AGE AND mt.LOCATION=cp.LOCATION and mt.GENDER=cp.GENDER and same for product_profile&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is it the way to go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ko96l", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ko96l/starschema_dw_implementation_in_real_life_scenario/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ko96l/starschema_dw_implementation_in_real_life_scenario/", "subreddit_subscribers": 146650, "created_utc": 1702839237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New data engineer here (team of 1) looking for inspiration on how to improve a process!\n\nCurrently dagster open source runs on a windows machine behind our firewall. This machine orchestrates python scripts (etl, reporting, events, DBT in bigquery, etc). A primary source is sql server, which I am currently querying with this machine, loading data into pandas, then using the pandas bigquery library to upload to bigquery tables.\n\nIt has been fine with many dimensional tables but I have been running into issues adding more large table ingests. Primarily it is just exhausting memory and just slowing down the process, especially on backfills of big tables. I know that I could potentially query and store in memory using an arrow ready format vs data frames, but before I make those changes what would be the most elegant way to handle this? Should I move the execution off of this machine into either gcp or the sql server in some way?", "author_fullname": "t2_ronx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about a system design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kvzzk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "8cf4f390-e787-11ed-81a4-ca7b65282907", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702860308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New data engineer here (team of 1) looking for inspiration on how to improve a process!&lt;/p&gt;\n\n&lt;p&gt;Currently dagster open source runs on a windows machine behind our firewall. This machine orchestrates python scripts (etl, reporting, events, DBT in bigquery, etc). A primary source is sql server, which I am currently querying with this machine, loading data into pandas, then using the pandas bigquery library to upload to bigquery tables.&lt;/p&gt;\n\n&lt;p&gt;It has been fine with many dimensional tables but I have been running into issues adding more large table ingests. Primarily it is just exhausting memory and just slowing down the process, especially on backfills of big tables. I know that I could potentially query and store in memory using an arrow ready format vs data frames, but before I make those changes what would be the most elegant way to handle this? Should I move the execution off of this machine into either gcp or the sql server in some way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Lead Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18kvzzk", "is_robot_indexable": true, "report_reasons": null, "author": "seanpool3", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18kvzzk/question_about_a_system_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kvzzk/question_about_a_system_design/", "subreddit_subscribers": 146650, "created_utc": 1702860308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, suggest a data pipeline to read sales data from a website and send a notification if a product crosses x number of orders in last 1 hour. Use cloud computing.\n\nMy thought is to do something like below, please help with suggestions:\n\nWebserver &gt; aws managed kafka &gt; spark structured streaming with 5 min sliding window over 1 hour &gt; store 5 min aggregate in a time series db like influx &gt; lambda to aggregate over 1 hour and send sns notifications, schedule lambda to run every 5 mins &gt; delete data older than 1 hour in database.", "author_fullname": "t2_kfvc08j9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Design a real time notification system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18ky2wk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702866863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, suggest a data pipeline to read sales data from a website and send a notification if a product crosses x number of orders in last 1 hour. Use cloud computing.&lt;/p&gt;\n\n&lt;p&gt;My thought is to do something like below, please help with suggestions:&lt;/p&gt;\n\n&lt;p&gt;Webserver &amp;gt; aws managed kafka &amp;gt; spark structured streaming with 5 min sliding window over 1 hour &amp;gt; store 5 min aggregate in a time series db like influx &amp;gt; lambda to aggregate over 1 hour and send sns notifications, schedule lambda to run every 5 mins &amp;gt; delete data older than 1 hour in database.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ky2wk", "is_robot_indexable": true, "report_reasons": null, "author": "RepulsiveCry8412", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ky2wk/design_a_real_time_notification_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ky2wk/design_a_real_time_notification_system/", "subreddit_subscribers": 146650, "created_utc": 1702866863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have tried reading through the DBT docs but finding it hard to understand how to change the target database for DBT production runs when developing in DBT cloud. I am using Redshift, and want my prod runs to output to its own 'analytics' database, and all dev runs to go to a dbt\\_dev database.  \nTIA ", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Changing DBT Cloud Target Database (Redshift)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kazlm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702794141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried reading through the DBT docs but finding it hard to understand how to change the target database for DBT production runs when developing in DBT cloud. I am using Redshift, and want my prod runs to output to its own &amp;#39;analytics&amp;#39; database, and all dev runs to go to a dbt_dev database.&lt;br/&gt;\nTIA &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18kazlm", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kazlm/changing_dbt_cloud_target_database_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kazlm/changing_dbt_cloud_target_database_redshift/", "subreddit_subscribers": 146650, "created_utc": 1702794141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all! Data Analyst with 5 years of experience here. I have been using SQL and Python in my job, but it hasn't been anything too advanced. SQL has mainly been queries that have involved CTEs and the fundementals (WHERE vs. HAVING, INNER/LEFT JOINS, UNION vs. UNION ALL, some windows functions when needed like LAG and LEAD), and Python has been data exploration in Pandas and some web scraping.\n\nA few months ago I began getting interested in Data Engineering and doing some basic projects involving getting information out of an API, cleaning it, and having it run in Pandas. I have done 2 projects in AWS using many of their systems, but it has very much been a \"follow along\".\n\nThe job description says\n\n* Advanced SQL Skills expected\n\n* Python skills required.\n\n* AWS experienced required.\n\nI feel like I am just very basic in all of those, although my fundamentals are good.\n\nWhat are some questions you may expect to be asked in each? Just curious if anyone has any first-hand experience with interviews lately, or have interviewed folks!\n\nI'm nervous and excited! Thanks!", "author_fullname": "t2_6iptp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer Interview incoming! Can I have some advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kwb0r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702861505.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702861250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all! Data Analyst with 5 years of experience here. I have been using SQL and Python in my job, but it hasn&amp;#39;t been anything too advanced. SQL has mainly been queries that have involved CTEs and the fundementals (WHERE vs. HAVING, INNER/LEFT JOINS, UNION vs. UNION ALL, some windows functions when needed like LAG and LEAD), and Python has been data exploration in Pandas and some web scraping.&lt;/p&gt;\n\n&lt;p&gt;A few months ago I began getting interested in Data Engineering and doing some basic projects involving getting information out of an API, cleaning it, and having it run in Pandas. I have done 2 projects in AWS using many of their systems, but it has very much been a &amp;quot;follow along&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;The job description says&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Advanced SQL Skills expected&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Python skills required.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;AWS experienced required.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I feel like I am just very basic in all of those, although my fundamentals are good.&lt;/p&gt;\n\n&lt;p&gt;What are some questions you may expect to be asked in each? Just curious if anyone has any first-hand experience with interviews lately, or have interviewed folks!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m nervous and excited! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18kwb0r", "is_robot_indexable": true, "report_reasons": null, "author": "tits_mcgee_92", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kwb0r/data_engineer_interview_incoming_can_i_have_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kwb0r/data_engineer_interview_incoming_can_i_have_some/", "subreddit_subscribers": 146650, "created_utc": 1702861250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! I've put together a Docker Compose setup that includes tools like Hadoop, Hive, Spark, PySpark, Jupyter, and Airflow. It's designed to be easy for anyone to set up and start using.\n\nJust clone the [repository](https://github.com/carteakey/data-pipeline-compose) and spin up all services using \\`docker compose up -d\\`.\n\nThe purpose is to just streamline the initial configuration process without the usual setup hassles, which can often be a roadblock for someone trying to get their hands into DE.\n\nLet me know if you have any suggestions / feedback.", "author_fullname": "t2_dy2vwvki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data-pipeline-compose - Data Engineering environment setup using Docker Compose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kqgcq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702845962.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702845018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I&amp;#39;ve put together a Docker Compose setup that includes tools like Hadoop, Hive, Spark, PySpark, Jupyter, and Airflow. It&amp;#39;s designed to be easy for anyone to set up and start using.&lt;/p&gt;\n\n&lt;p&gt;Just clone the &lt;a href=\"https://github.com/carteakey/data-pipeline-compose\"&gt;repository&lt;/a&gt; and spin up all services using `docker compose up -d`.&lt;/p&gt;\n\n&lt;p&gt;The purpose is to just streamline the initial configuration process without the usual setup hassles, which can often be a roadblock for someone trying to get their hands into DE.&lt;/p&gt;\n\n&lt;p&gt;Let me know if you have any suggestions / feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?auto=webp&amp;s=36bf0a22bba6a2a7d1d4b79a57ae8a4747d31b21", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8a38719866a3a024c5cae9c40e8d7114a86d5dc1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3863951cc22e89d14d365c1a4985954eb48df551", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0742b4d74d60f7ed3d115a3ce55842fb6ef7cfb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4067d1729385147a328e04df989a90dfb686fdf6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c616e99051a82cc5406da4f2c8f4d93a70ade7d7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3889262a6c4d922ceb30d055bc7589a95ee9aa58", "width": 1080, "height": 540}], "variants": {}, "id": "5sG3ddbTVX7UyJNxaXu479stuzePSzohwXGcYKXzae0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18kqgcq", "is_robot_indexable": true, "report_reasons": null, "author": "carteakey", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kqgcq/datapipelinecompose_data_engineering_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kqgcq/datapipelinecompose_data_engineering_environment/", "subreddit_subscribers": 146650, "created_utc": 1702845018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys\n\nSo I'm migrating some data from an old DB to our new one, I need to use a custom integrated runtime on the old DB, because of that I can't use the source directly in the data flow activity to make changes, and the new DB already has data that I can't change.\n\nI've gone with this setup:\n\n&amp;#x200B;\n\n1. Copy old DB table to file in blob storage\n2. Pass that file to the data flow\n3. Add new columns/rename as needed\n4. Insert into table\n5. Save new primary keys into a table or file that maps to the old primary keys in the old DB\n\n&amp;#x200B;\n\nI've only got to step 4 so far, but I need to keep the primary keys so step 5 may end up being multiple steps if it becomes a lot of trouble.\n\n&amp;#x200B;\n\nI'm just wondering, if there's a way to simplify my work flow? As I was working on it, it feels like I'm making it overly complicated.", "author_fullname": "t2_56o0g58i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to simplify this process (Azure Data Factory)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18km4jo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702833550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m migrating some data from an old DB to our new one, I need to use a custom integrated runtime on the old DB, because of that I can&amp;#39;t use the source directly in the data flow activity to make changes, and the new DB already has data that I can&amp;#39;t change.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve gone with this setup:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Copy old DB table to file in blob storage&lt;/li&gt;\n&lt;li&gt;Pass that file to the data flow&lt;/li&gt;\n&lt;li&gt;Add new columns/rename as needed&lt;/li&gt;\n&lt;li&gt;Insert into table&lt;/li&gt;\n&lt;li&gt;Save new primary keys into a table or file that maps to the old primary keys in the old DB&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only got to step 4 so far, but I need to keep the primary keys so step 5 may end up being multiple steps if it becomes a lot of trouble.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just wondering, if there&amp;#39;s a way to simplify my work flow? As I was working on it, it feels like I&amp;#39;m making it overly complicated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18km4jo", "is_robot_indexable": true, "report_reasons": null, "author": "IG-55", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18km4jo/is_there_a_way_to_simplify_this_process_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18km4jo/is_there_a_way_to_simplify_this_process_azure/", "subreddit_subscribers": 146650, "created_utc": 1702833550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi l , I want to build a feature pipeline for real time approximation (over 90 min) and use this feature in real time inference (90 min back). The data for the feature and the inference derived from the same events. \n\nHow would you suggest to solve those kind of problems and what technology stack do you use? Do you have some examples ? \n\nThanks", "author_fullname": "t2_j15inhpup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature pipeline in stream for approximation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kiirt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702823459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi l , I want to build a feature pipeline for real time approximation (over 90 min) and use this feature in real time inference (90 min back). The data for the feature and the inference derived from the same events. &lt;/p&gt;\n\n&lt;p&gt;How would you suggest to solve those kind of problems and what technology stack do you use? Do you have some examples ? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18kiirt", "is_robot_indexable": true, "report_reasons": null, "author": "springRock88", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kiirt/feature_pipeline_in_stream_for_approximation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kiirt/feature_pipeline_in_stream_for_approximation/", "subreddit_subscribers": 146650, "created_utc": 1702823459.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}