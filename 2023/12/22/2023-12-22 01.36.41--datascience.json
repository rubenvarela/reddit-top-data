{"kind": "Listing", "data": {"after": null, "dist": 9, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "  This article will explain how to use [Pipeline](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com) and [Transformers](https://scikit-learn.org/stable/data_transforms.html?ref=dataleadsfuture.com) correctly in Scikit-Learn (sklearn) projects to speed up and reuse our model training process.\n\nThis piece complements and clarifies the official documentation on Pipeline examples and some common misunderstandings.\n\nI hope that after reading this, you'll be able to use the Pipeline, an excellent design, to better complete your machine learning tasks.\n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/) \n\n### Why use a Pipeline\n\nAs mentioned earlier, in a machine learning task, we often need to use various Transformers for data scaling and feature dimensionality reduction before training a model.\n\nThis presents several challenges:\n\n* **Code complexity**: For each use of a Transformer, we have to go through initialization, `fit_transform`, and `transform` steps. Missing one step during a transformation could derail the entire training process.\n* **Data leakage**: As we discussed, for each Transformer, we fit with train data and then transform both train and test data. We must avoid letting the distribution of the test data leak into the train data.\n* **Code reusability**: A machine learning model includes not only the trained Estimator for prediction but also the data preprocessing steps. Therefore, a machine learning task comprising Transformers and an Estimator should be atomic and indivisible.\n* **Hyperparameter tuning**: After setting up the steps of machine learning, we need to adjust hyperparameters to find the best combination of Transformer parameter values.\n\nScikit-Learn introduced the `Pipeline` module to solve these issues.\n\n### What is a Pipeline\n\nA `Pipeline` is a module in Scikit-Learn that implements the chain of responsibility design pattern.\n\nWhen creating a Pipeline, we use the `steps` parameter to chain together multiple Transformers for initialization:\n\n    from sklearn.pipeline import Pipeline\n    from sklearn.decomposition import PCA\n    from sklearn.ensemble import RandomForestClassifier\n    \n    pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('pca', PCA(n_components=2, random_state=42)),\n                               ('estimator', RandomForestClassifier(n_estimators=3, max_depth=5))])\n\nThe [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline) points out that the last Transformer must be an Estimator.\n\nIf you don't need to specify each Transformer's name, you can simplify the creation of a Pipeline with `make_pipeline`:\n\n    from sklearn.pipeline import make_pipeline\n    \n    pipeline_2 = make_pipeline(StandardScaler(),\n                               PCA(n_components=2, random_state=42),\n                               RandomForestClassifier(n_estimators=3, max_depth=5))\n\n Understanding the Pipeline's mechanism from the source code\n\nWe've mentioned the importance of not letting test data variables leak into training data when using each Transformer.\n\nThis principle is relatively easy to ensure when each data preprocessing step is independent.\n\nBut what if we integrate these steps using a Pipeline?\n\nIf we look at the [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline), we find it simply uses the fit  \n method on the entire dataset without explaining how to handle train and test data separately.\n\nWith this question in mind, I dived into the Pipeline's source code to find the answer.\n\nReading the source code revealed that although Pipeline implements `fit`, `fit_transform`, and `predict` methods, they work differently from regular Transformers.\n\nTake the following Pipeline creation process as an example:\n\n    from sklearn.pipeline import Pipeline\n    from sklearn.decomposition import PCA\n    from sklearn.ensemble import RandomForestClassifier\n    \n    pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('pca', PCA(n_components=2, random_state=42)),\n                               ('estimator', RandomForestClassifier(n_estimators=3, max_depth=5))])\n\n The internal implementation can be represented by the following diagram: \n\n[ Internal implementation of the fit and predict methods when called. Image by Author ](https://preview.redd.it/okhwyg75gl7c1.png?width=684&amp;format=png&amp;auto=webp&amp;s=8106360fcaeb17deea2adf04fc34228dd31a9fd7)\n\nAs you can see, when we call the `fit` method, Pipeline first separates Transformers from the Estimator.\n\nFor each Transformer, Pipeline checks if there's a `fit_transform` method; if so, it calls it; otherwise, it calls `fit`.\n\nFor the Estimator, it calls `fit` directly.\n\nFor the `predict` method, Pipeline separates Transformers from the Estimator.\n\nPipeline calls each Transformer's `transform` method in sequence, followed by the Estimator's `predict`  \n method.\n\nTherefore, when using a Pipeline, we still need to split train and test data. Then we simply call `fit` on the train data and `predict` on the test data.\n\nThere's a special case when combining Pipeline with `GridSearchCV` for hyperparameter tuning: you don't need to manually split train and test data. I'll explain this in more detail in the best practices section.\n\n## Best Practices for Using Transformers and Pipeline in Actual Applications\n\nNow that we've discussed the working principles of Transformers and Pipeline, it's time to fulfill the promise made in the title and talk about the best practices when combining Transformers with Pipeline in real projects.\n\n### Combining Pipeline with GridSearchCV for hyperparameter tuning\n\nIn a machine learning project, selecting the right dataset processing and algorithm is one aspect. After debugging the initial steps, it's time for parameter optimization.\n\nUsing `GridSearchCV` or `RandomizedSearchCV`, you can try different parameters for the Estimator to find the best fit:\n\n    import time\n    \n    from sklearn.model_selection import GridSearchCV\n    \n    pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('pca', PCA()),\n                               ('estimator', RandomForestClassifier())])\n    param_grid = {'pca__n_components': [2, 'mle'],\n                  'estimator__n_estimators': [3, 5, 7],\n                  'estimator__max_depth': [3, 5]}\n    \n    start = time.perf_counter()\n    clf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=4)\n    clf.fit(X, y)\n    \n    # It takes 2.39 seconds to finish the search on my laptop.\n    print(f\"It takes {time.perf_counter() - start} seconds to finish the search.\")\n\n But in machine learning, hyperparameter tuning is not limited to Estimator parameters; it also involves combinations of Transformer parameters.\n\nIntegrating all steps with Pipeline allows for hyperparameter tuning of every element with different parameter combinations.\n\nNote that during hyperparameter tuning, we no longer need to manually split train and test data. `GridSearchCV` will split the data into training and validation sets using [StratifiedKFold](https://scikit-learn.org/stable/modules/cross_validation.html?ref=dataleadsfuture.com#stratified-k-fold), which implemented a k-fold cross validation mechanism.\n\n[ StratifiedKFold iterative process of splitting train data and test data. Image by Author ](https://preview.redd.it/uqnuo8fpgl7c1.png?width=681&amp;format=png&amp;auto=webp&amp;s=bd6b5f7edb2d4f5e71786fd6c0f45745ea2095d9)\n\n We can also set the number of folds for cross-validation and choose how many workers to use. The tuning process is illustrated in the following diagram: \n\n[ Internal implementation of GridSearchCV hyperparameter tuning. Image by Author ](https://preview.redd.it/piqqv9nrgl7c1.png?width=699&amp;format=png&amp;auto=webp&amp;s=1bb2a2d4739e8100e71559fb9f9e0cfaa4846803)\n\n Due to space constraints, I won't go into detail about `GridSearchCV` and `RandomizedSearchCV` here. If you're interested, I can write another article explaining them next time. \n\n### Using the memory parameter to cache Transformer outputs\n\nOf course, hyperparameter tuning with `GridSearchCV` can be slow, but that's no worry, Pipeline provides a caching mechanism to speed up the tuning efficiency by caching the results of intermediate steps.\n\nWhen initializing a Pipeline, you can pass in a memory parameter, which will cache the results after the first call to `fit` and `transform` for each transformer.\n\nIf subsequent calls to fit and `transform` use the same parameters, which is very likely during hyperparameter tuning, these steps will directly read the results from the cache instead of recalculating, significantly speeding up the efficiency when running the same Transformer repeatedly.\n\nThe `memory` parameter can accept the following values:\n\n* The default is None: caching is not used.\n* A string: providing a path to store the cached results.\n* A `joblib.Memory` object: allows for finer-grained control, such as configuring the storage backend for the cache.\n\nNext, let's use the previous `GridSearchCV` example, this time adding `memory` to the Pipeline to see how much speed can be improved:\n\n    pipeline_m = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('pca', PCA()),\n                               ('estimator', RandomForestClassifier())],\n                          memory='./cache')\n    start = time.perf_counter()\n    clf_m = GridSearchCV(pipeline_m, param_grid=param_grid, cv=5, n_jobs=4)\n    clf_m.fit(X, y)\n    \n    # It takes 0.22 seconds to finish the search with memory parameter.\n    print(f\"It takes {time.perf_counter() - start} seconds to finish the search with memory.\")\n\nAs shown, with caching, the tuning process only takes 0.2 seconds, a significant speed increase from the previous 2.4 seconds.\n\n### How to debug Scikit-Learn Pipeline\n\nAfter integrating Transformers into a Pipeline, the entire preprocessing and transformation process becomes a black box. It can be difficult to understand which step the process is currently on.\n\nFortunately, we can solve this problem by adding logging to the Pipeline.  \nWe need to create custom transformers to add logging at each step of data transformation.\n\nHere's an example of adding logging with Python's standard logging library:\n\nFirst, you need to configure a logger:\n\n    import logging\n    \n    from sklearn.base import BaseEstimator, TransformerMixin\n    \n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger()\n\n Next, you can create a custom Transformer and add logging within its methods: \n\n    class LoggingTransformer(BaseEstimator, TransformerMixin):\n        def __init__(self, transformer):\n            self.transformer = transformer\n            self.real_name = self.transformer.__class__.__name__\n    \n        def fit(self, X, y=None):\n            logging.info(f\"Begin fit: {self.real_name}\")\n            self.transformer.fit(X, y)\n            logging.info(f\"End fit: {self.real_name}\")\n            return self\n    \n        def fit_transform(self, X, y=None):\n            logging.info(f\"Begin fit_transform: {self.real_name}\")\n            X_fit_transformed = self.transformer.fit_transform(X, y)\n            logging.info(f\"End fit_transform: {self.real_name}\")\n            return X_fit_transformed\n    \n        def transform(self, X):\n            logging.info(f\"Begin transform: {self.real_name}\")\n            X_transformed = self.transformer.transform(X)\n            logging.info(f\"End transform: {self.real_name}\")\n            return X_transformed\n\n Then you can use this `LoggingTransformer` when creating your Pipeline: \n\n    pipeline_logging = Pipeline(steps=[('scaler', LoggingTransformer(StandardScaler())),\n                                 ('pca', LoggingTransformer(PCA(n_components=2))),\n                                 ('estimator', RandomForestClassifier(n_estimators=5, max_depth=3))])\n    pipeline_logging.fit(X_train, y_train)\n\n[ The effect after adding the LoggingTransformer. Image by Author ](https://preview.redd.it/53kl3padhl7c1.png?width=664&amp;format=png&amp;auto=webp&amp;s=a3e3d9dcf25f0ee8dc7234e1db9280ce1cd15b0d)\n\nWhen you use `pipeline.fit`, it will call the `fit` and `transform` methods for each step in turn and log the appropriate messages.\n\n### Use passthrough in Scikit-Learn Pipeline\n\nIn a Pipeline, a step can be set to `'passthrough`', which means that for this specific step, the input data will pass through unchanged to the next step.\n\nThis is useful when you want to selectively enable/disable certain steps in a complex pipeline.\n\nTaking the code example above, we know that when using `DecisionTree` or `RandomForest`, standardizing the data is unnecessary, so we can use `passthrough` to skip this step.\n\nAn example would be as follows:\n\n    param_grid = {'scaler': ['passthrough'],\n                  'pca__n_components': [2, 'mle'],\n                  'estimator__n_estimators': [3, 5, 7],\n                  'estimator__max_depth': [3, 5]}\n    clf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=4)\n    clf.fit(X, y)\n\n Reusing the Pipeline\n\nAfter a journey of trials and tribulations, we finally have a well-performing machine learning model.\n\nNow, you might consider how to reuse this model, share it with colleagues, or deploy it in a production environment.\n\nHowever, the result of a model's training includes not only the model itself but also the various data processing steps, which all need to be saved.\n\nUsing `joblib` and Pipeline, we can save the entire training process for later use. The following code provides a simple example:\n\n    from joblib import dump, load\n    \n    # save pipeline\n    dump(pipeline, 'model_pipeline.joblib')\n    \n    # load pipeline\n    loaded_pipeline = load('model_pipeline.joblib')\n    \n    # predict with loaded pipeline\n    loaded_predictions = loaded_pipeline.predict(X_test)\n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/) \n\n&amp;#x200B;", "author_fullname": "t2_9r8ft2a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to correctly use sklearn Transformers in a Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"piqqv9nrgl7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 97, "x": 108, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc712dc1ff6400f697a3881d51f8d3570f829238"}, {"y": 194, "x": 216, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3a36b89acd98a75d521128b625e47005add048a"}, {"y": 288, "x": 320, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03780e7cf2e0d41698ce9f4e0ce50d301171ebb0"}, {"y": 577, "x": 640, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f33f9a7028cf3468f80742501736390b58e76681"}], "s": {"y": 631, "x": 699, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=699&amp;format=png&amp;auto=webp&amp;s=1bb2a2d4739e8100e71559fb9f9e0cfaa4846803"}, "id": "piqqv9nrgl7c1"}, "uqnuo8fpgl7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b370e612070428374a207b597a92aff82d8a147"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=94c2adf95ce18b60f16bc2cc8abc2a6c14563d8f"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=84db99e434cb5e4d32032a6ae0d7446b5193e10a"}, {"y": 320, "x": 640, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad714328a71791ed1f8ded1f1e5f63e03f1ccbaa"}], "s": {"y": 341, "x": 681, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=681&amp;format=png&amp;auto=webp&amp;s=bd6b5f7edb2d4f5e71786fd6c0f45745ea2095d9"}, "id": "uqnuo8fpgl7c1"}, "okhwyg75gl7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a3723b91df001b47a713b573280c2632814bff9"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=318fb9cf0ea6f735eaa74c3ee9f46c0a1a31d5d1"}, {"y": 196, "x": 320, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a9cd4c403b9f61f1ee9176cdfdb693a946dbe2b"}, {"y": 393, "x": 640, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1eac9f48da6a1da4c9a130d67dbc6580769e103e"}], "s": {"y": 421, "x": 684, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=684&amp;format=png&amp;auto=webp&amp;s=8106360fcaeb17deea2adf04fc34228dd31a9fd7"}, "id": "okhwyg75gl7c1"}, "53kl3padhl7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ccc85545041b723d70529bd68170283417122e9"}, {"y": 126, "x": 216, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd832e4183f64bc5cb636ae98f3cf166d672ded4"}, {"y": 187, "x": 320, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b39ac233ad51443069ed58202070dc9749e37e96"}, {"y": 375, "x": 640, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5dc09cc5b6c6fc6fd89507b8a1fbef3e85099e61"}], "s": {"y": 390, "x": 664, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=664&amp;format=png&amp;auto=webp&amp;s=a3e3d9dcf25f0ee8dc7234e1db9280ce1cd15b0d"}, "id": "53kl3padhl7c1"}}, "name": "t3_18ngsgv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Coding", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/R6xMJtVPodH3aN_i-OC2a0zZZJv0utQLFd8yB5_cBeU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1703141418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This article will explain how to use &lt;a href=\"https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com\"&gt;Pipeline&lt;/a&gt; and &lt;a href=\"https://scikit-learn.org/stable/data_transforms.html?ref=dataleadsfuture.com\"&gt;Transformers&lt;/a&gt; correctly in Scikit-Learn (sklearn) projects to speed up and reuse our model training process.&lt;/p&gt;\n\n&lt;p&gt;This piece complements and clarifies the official documentation on Pipeline examples and some common misunderstandings.&lt;/p&gt;\n\n&lt;p&gt;I hope that after reading this, you&amp;#39;ll be able to use the Pipeline, an excellent design, to better complete your machine learning tasks.&lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n\n&lt;h3&gt;Why use a Pipeline&lt;/h3&gt;\n\n&lt;p&gt;As mentioned earlier, in a machine learning task, we often need to use various Transformers for data scaling and feature dimensionality reduction before training a model.&lt;/p&gt;\n\n&lt;p&gt;This presents several challenges:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Code complexity&lt;/strong&gt;: For each use of a Transformer, we have to go through initialization, &lt;code&gt;fit_transform&lt;/code&gt;, and &lt;code&gt;transform&lt;/code&gt; steps. Missing one step during a transformation could derail the entire training process.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data leakage&lt;/strong&gt;: As we discussed, for each Transformer, we fit with train data and then transform both train and test data. We must avoid letting the distribution of the test data leak into the train data.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Code reusability&lt;/strong&gt;: A machine learning model includes not only the trained Estimator for prediction but also the data preprocessing steps. Therefore, a machine learning task comprising Transformers and an Estimator should be atomic and indivisible.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hyperparameter tuning&lt;/strong&gt;: After setting up the steps of machine learning, we need to adjust hyperparameters to find the best combination of Transformer parameter values.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Scikit-Learn introduced the &lt;code&gt;Pipeline&lt;/code&gt; module to solve these issues.&lt;/p&gt;\n\n&lt;h3&gt;What is a Pipeline&lt;/h3&gt;\n\n&lt;p&gt;A &lt;code&gt;Pipeline&lt;/code&gt; is a module in Scikit-Learn that implements the chain of responsibility design pattern.&lt;/p&gt;\n\n&lt;p&gt;When creating a Pipeline, we use the &lt;code&gt;steps&lt;/code&gt; parameter to chain together multiple Transformers for initialization:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, StandardScaler()),\n                           (&amp;#39;pca&amp;#39;, PCA(n_components=2, random_state=42)),\n                           (&amp;#39;estimator&amp;#39;, RandomForestClassifier(n_estimators=3, max_depth=5))])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The &lt;a href=\"https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline\"&gt;official documentation&lt;/a&gt; points out that the last Transformer must be an Estimator.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t need to specify each Transformer&amp;#39;s name, you can simplify the creation of a Pipeline with &lt;code&gt;make_pipeline&lt;/code&gt;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.pipeline import make_pipeline\n\npipeline_2 = make_pipeline(StandardScaler(),\n                           PCA(n_components=2, random_state=42),\n                           RandomForestClassifier(n_estimators=3, max_depth=5))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Understanding the Pipeline&amp;#39;s mechanism from the source code&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve mentioned the importance of not letting test data variables leak into training data when using each Transformer.&lt;/p&gt;\n\n&lt;p&gt;This principle is relatively easy to ensure when each data preprocessing step is independent.&lt;/p&gt;\n\n&lt;p&gt;But what if we integrate these steps using a Pipeline?&lt;/p&gt;\n\n&lt;p&gt;If we look at the &lt;a href=\"https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline\"&gt;official documentation&lt;/a&gt;, we find it simply uses the fit&lt;br/&gt;\n method on the entire dataset without explaining how to handle train and test data separately.&lt;/p&gt;\n\n&lt;p&gt;With this question in mind, I dived into the Pipeline&amp;#39;s source code to find the answer.&lt;/p&gt;\n\n&lt;p&gt;Reading the source code revealed that although Pipeline implements &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;fit_transform&lt;/code&gt;, and &lt;code&gt;predict&lt;/code&gt; methods, they work differently from regular Transformers.&lt;/p&gt;\n\n&lt;p&gt;Take the following Pipeline creation process as an example:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, StandardScaler()),\n                           (&amp;#39;pca&amp;#39;, PCA(n_components=2, random_state=42)),\n                           (&amp;#39;estimator&amp;#39;, RandomForestClassifier(n_estimators=3, max_depth=5))])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The internal implementation can be represented by the following diagram: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/okhwyg75gl7c1.png?width=684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8106360fcaeb17deea2adf04fc34228dd31a9fd7\"&gt; Internal implementation of the fit and predict methods when called. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As you can see, when we call the &lt;code&gt;fit&lt;/code&gt; method, Pipeline first separates Transformers from the Estimator.&lt;/p&gt;\n\n&lt;p&gt;For each Transformer, Pipeline checks if there&amp;#39;s a &lt;code&gt;fit_transform&lt;/code&gt; method; if so, it calls it; otherwise, it calls &lt;code&gt;fit&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;For the Estimator, it calls &lt;code&gt;fit&lt;/code&gt; directly.&lt;/p&gt;\n\n&lt;p&gt;For the &lt;code&gt;predict&lt;/code&gt; method, Pipeline separates Transformers from the Estimator.&lt;/p&gt;\n\n&lt;p&gt;Pipeline calls each Transformer&amp;#39;s &lt;code&gt;transform&lt;/code&gt; method in sequence, followed by the Estimator&amp;#39;s &lt;code&gt;predict&lt;/code&gt;&lt;br/&gt;\n method.&lt;/p&gt;\n\n&lt;p&gt;Therefore, when using a Pipeline, we still need to split train and test data. Then we simply call &lt;code&gt;fit&lt;/code&gt; on the train data and &lt;code&gt;predict&lt;/code&gt; on the test data.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a special case when combining Pipeline with &lt;code&gt;GridSearchCV&lt;/code&gt; for hyperparameter tuning: you don&amp;#39;t need to manually split train and test data. I&amp;#39;ll explain this in more detail in the best practices section.&lt;/p&gt;\n\n&lt;h2&gt;Best Practices for Using Transformers and Pipeline in Actual Applications&lt;/h2&gt;\n\n&lt;p&gt;Now that we&amp;#39;ve discussed the working principles of Transformers and Pipeline, it&amp;#39;s time to fulfill the promise made in the title and talk about the best practices when combining Transformers with Pipeline in real projects.&lt;/p&gt;\n\n&lt;h3&gt;Combining Pipeline with GridSearchCV for hyperparameter tuning&lt;/h3&gt;\n\n&lt;p&gt;In a machine learning project, selecting the right dataset processing and algorithm is one aspect. After debugging the initial steps, it&amp;#39;s time for parameter optimization.&lt;/p&gt;\n\n&lt;p&gt;Using &lt;code&gt;GridSearchCV&lt;/code&gt; or &lt;code&gt;RandomizedSearchCV&lt;/code&gt;, you can try different parameters for the Estimator to find the best fit:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import time\n\nfrom sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, StandardScaler()),\n                           (&amp;#39;pca&amp;#39;, PCA()),\n                           (&amp;#39;estimator&amp;#39;, RandomForestClassifier())])\nparam_grid = {&amp;#39;pca__n_components&amp;#39;: [2, &amp;#39;mle&amp;#39;],\n              &amp;#39;estimator__n_estimators&amp;#39;: [3, 5, 7],\n              &amp;#39;estimator__max_depth&amp;#39;: [3, 5]}\n\nstart = time.perf_counter()\nclf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=4)\nclf.fit(X, y)\n\n# It takes 2.39 seconds to finish the search on my laptop.\nprint(f&amp;quot;It takes {time.perf_counter() - start} seconds to finish the search.&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But in machine learning, hyperparameter tuning is not limited to Estimator parameters; it also involves combinations of Transformer parameters.&lt;/p&gt;\n\n&lt;p&gt;Integrating all steps with Pipeline allows for hyperparameter tuning of every element with different parameter combinations.&lt;/p&gt;\n\n&lt;p&gt;Note that during hyperparameter tuning, we no longer need to manually split train and test data. &lt;code&gt;GridSearchCV&lt;/code&gt; will split the data into training and validation sets using &lt;a href=\"https://scikit-learn.org/stable/modules/cross_validation.html?ref=dataleadsfuture.com#stratified-k-fold\"&gt;StratifiedKFold&lt;/a&gt;, which implemented a k-fold cross validation mechanism.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uqnuo8fpgl7c1.png?width=681&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd6b5f7edb2d4f5e71786fd6c0f45745ea2095d9\"&gt; StratifiedKFold iterative process of splitting train data and test data. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We can also set the number of folds for cross-validation and choose how many workers to use. The tuning process is illustrated in the following diagram: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/piqqv9nrgl7c1.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bb2a2d4739e8100e71559fb9f9e0cfaa4846803\"&gt; Internal implementation of GridSearchCV hyperparameter tuning. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Due to space constraints, I won&amp;#39;t go into detail about &lt;code&gt;GridSearchCV&lt;/code&gt; and &lt;code&gt;RandomizedSearchCV&lt;/code&gt; here. If you&amp;#39;re interested, I can write another article explaining them next time. &lt;/p&gt;\n\n&lt;h3&gt;Using the memory parameter to cache Transformer outputs&lt;/h3&gt;\n\n&lt;p&gt;Of course, hyperparameter tuning with &lt;code&gt;GridSearchCV&lt;/code&gt; can be slow, but that&amp;#39;s no worry, Pipeline provides a caching mechanism to speed up the tuning efficiency by caching the results of intermediate steps.&lt;/p&gt;\n\n&lt;p&gt;When initializing a Pipeline, you can pass in a memory parameter, which will cache the results after the first call to &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; for each transformer.&lt;/p&gt;\n\n&lt;p&gt;If subsequent calls to fit and &lt;code&gt;transform&lt;/code&gt; use the same parameters, which is very likely during hyperparameter tuning, these steps will directly read the results from the cache instead of recalculating, significantly speeding up the efficiency when running the same Transformer repeatedly.&lt;/p&gt;\n\n&lt;p&gt;The &lt;code&gt;memory&lt;/code&gt; parameter can accept the following values:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The default is None: caching is not used.&lt;/li&gt;\n&lt;li&gt;A string: providing a path to store the cached results.&lt;/li&gt;\n&lt;li&gt;A &lt;code&gt;joblib.Memory&lt;/code&gt; object: allows for finer-grained control, such as configuring the storage backend for the cache.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Next, let&amp;#39;s use the previous &lt;code&gt;GridSearchCV&lt;/code&gt; example, this time adding &lt;code&gt;memory&lt;/code&gt; to the Pipeline to see how much speed can be improved:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipeline_m = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, StandardScaler()),\n                           (&amp;#39;pca&amp;#39;, PCA()),\n                           (&amp;#39;estimator&amp;#39;, RandomForestClassifier())],\n                      memory=&amp;#39;./cache&amp;#39;)\nstart = time.perf_counter()\nclf_m = GridSearchCV(pipeline_m, param_grid=param_grid, cv=5, n_jobs=4)\nclf_m.fit(X, y)\n\n# It takes 0.22 seconds to finish the search with memory parameter.\nprint(f&amp;quot;It takes {time.perf_counter() - start} seconds to finish the search with memory.&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;As shown, with caching, the tuning process only takes 0.2 seconds, a significant speed increase from the previous 2.4 seconds.&lt;/p&gt;\n\n&lt;h3&gt;How to debug Scikit-Learn Pipeline&lt;/h3&gt;\n\n&lt;p&gt;After integrating Transformers into a Pipeline, the entire preprocessing and transformation process becomes a black box. It can be difficult to understand which step the process is currently on.&lt;/p&gt;\n\n&lt;p&gt;Fortunately, we can solve this problem by adding logging to the Pipeline.&lt;br/&gt;\nWe need to create custom transformers to add logging at each step of data transformation.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an example of adding logging with Python&amp;#39;s standard logging library:&lt;/p&gt;\n\n&lt;p&gt;First, you need to configure a logger:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import logging\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nlogging.basicConfig(level=logging.INFO, format=&amp;#39;%(asctime)s - %(levelname)s - %(message)s&amp;#39;)\nlogger = logging.getLogger()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Next, you can create a custom Transformer and add logging within its methods: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class LoggingTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, transformer):\n        self.transformer = transformer\n        self.real_name = self.transformer.__class__.__name__\n\n    def fit(self, X, y=None):\n        logging.info(f&amp;quot;Begin fit: {self.real_name}&amp;quot;)\n        self.transformer.fit(X, y)\n        logging.info(f&amp;quot;End fit: {self.real_name}&amp;quot;)\n        return self\n\n    def fit_transform(self, X, y=None):\n        logging.info(f&amp;quot;Begin fit_transform: {self.real_name}&amp;quot;)\n        X_fit_transformed = self.transformer.fit_transform(X, y)\n        logging.info(f&amp;quot;End fit_transform: {self.real_name}&amp;quot;)\n        return X_fit_transformed\n\n    def transform(self, X):\n        logging.info(f&amp;quot;Begin transform: {self.real_name}&amp;quot;)\n        X_transformed = self.transformer.transform(X)\n        logging.info(f&amp;quot;End transform: {self.real_name}&amp;quot;)\n        return X_transformed\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then you can use this &lt;code&gt;LoggingTransformer&lt;/code&gt; when creating your Pipeline: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipeline_logging = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, LoggingTransformer(StandardScaler())),\n                             (&amp;#39;pca&amp;#39;, LoggingTransformer(PCA(n_components=2))),\n                             (&amp;#39;estimator&amp;#39;, RandomForestClassifier(n_estimators=5, max_depth=3))])\npipeline_logging.fit(X_train, y_train)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/53kl3padhl7c1.png?width=664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3e3d9dcf25f0ee8dc7234e1db9280ce1cd15b0d\"&gt; The effect after adding the LoggingTransformer. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;When you use &lt;code&gt;pipeline.fit&lt;/code&gt;, it will call the &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; methods for each step in turn and log the appropriate messages.&lt;/p&gt;\n\n&lt;h3&gt;Use passthrough in Scikit-Learn Pipeline&lt;/h3&gt;\n\n&lt;p&gt;In a Pipeline, a step can be set to &lt;code&gt;&amp;#39;passthrough&lt;/code&gt;&amp;#39;, which means that for this specific step, the input data will pass through unchanged to the next step.&lt;/p&gt;\n\n&lt;p&gt;This is useful when you want to selectively enable/disable certain steps in a complex pipeline.&lt;/p&gt;\n\n&lt;p&gt;Taking the code example above, we know that when using &lt;code&gt;DecisionTree&lt;/code&gt; or &lt;code&gt;RandomForest&lt;/code&gt;, standardizing the data is unnecessary, so we can use &lt;code&gt;passthrough&lt;/code&gt; to skip this step.&lt;/p&gt;\n\n&lt;p&gt;An example would be as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;param_grid = {&amp;#39;scaler&amp;#39;: [&amp;#39;passthrough&amp;#39;],\n              &amp;#39;pca__n_components&amp;#39;: [2, &amp;#39;mle&amp;#39;],\n              &amp;#39;estimator__n_estimators&amp;#39;: [3, 5, 7],\n              &amp;#39;estimator__max_depth&amp;#39;: [3, 5]}\nclf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=4)\nclf.fit(X, y)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Reusing the Pipeline&lt;/p&gt;\n\n&lt;p&gt;After a journey of trials and tribulations, we finally have a well-performing machine learning model.&lt;/p&gt;\n\n&lt;p&gt;Now, you might consider how to reuse this model, share it with colleagues, or deploy it in a production environment.&lt;/p&gt;\n\n&lt;p&gt;However, the result of a model&amp;#39;s training includes not only the model itself but also the various data processing steps, which all need to be saved.&lt;/p&gt;\n\n&lt;p&gt;Using &lt;code&gt;joblib&lt;/code&gt; and Pipeline, we can save the entire training process for later use. The following code provides a simple example:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from joblib import dump, load\n\n# save pipeline\ndump(pipeline, &amp;#39;model_pipeline.joblib&amp;#39;)\n\n# load pipeline\nloaded_pipeline = load(&amp;#39;model_pipeline.joblib&amp;#39;)\n\n# predict with loaded pipeline\nloaded_predictions = loaded_pipeline.predict(X_test)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3QeLrvmrkTlith0rWod1RvJb84uIyQ_ooxhR4GAxxyk.jpg?auto=webp&amp;s=0898caa546f8aacdf897f436bb227a3544bf2c83", "width": 160, "height": 58}, "resolutions": [{"url": "https://external-preview.redd.it/3QeLrvmrkTlith0rWod1RvJb84uIyQ_ooxhR4GAxxyk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54fa7103ac1fa7c5fa4f7488aed2f8f183533aa7", "width": 108, "height": 39}], "variants": {}, "id": "aRDTtkw0_GBCGb9E7JLBzDXeEp2pWa53pBC0AILtbPw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4ab9c418-70eb-11ee-8a37-4a495429ae82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18ngsgv", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18ngsgv/how_to_correctly_use_sklearn_transformers_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18ngsgv/how_to_correctly_use_sklearn_transformers_in_a/", "subreddit_subscribers": 1197809, "created_utc": 1703141418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "UPDATE TO TITLE: I meant, \u201cheading down the management path.\u201d \n\nIt\u2019s been 6 months since starting a data science management role, and now have been laid off. \n\nThe role was sold as a data science manager, yet ended up doing admin work and touched on very small amounts of actual data science projects. \n\nI was upset about the role but my boss assured me there were \u201cbig things\u201d in the pipeline. \n\nI got my break and finally took responsibility for a project and coordinated with a team from the US. Things went smoothly, and as a native English speaker (I\u2019m EU based), I sat in on more global meetings. \n\nFinally, things seemed to take off and work well\u2026 until I got fired today. \n\nThe result was: \u201cbusiness wasn\u2019t happy with your results.\u201d (We acted as data science manager consultants for different Business Units within the company). I asked why, the only feedback was a comment about a presentation not working out. \n\nBusiness never communicated with me directly, only with my boss. There was a layer of reasoning missing. \n\nSo, here I am: questions unanswered, out of a job, and feel like I\u2019ve missed my chance at DS management. \n\nI have one week left. What could I best do with this time?\n\nThanks all :)", "author_fullname": "t2_mzwvpkmks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got laid off as I was heading the management path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18natsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703121843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;UPDATE TO TITLE: I meant, \u201cheading down the management path.\u201d &lt;/p&gt;\n\n&lt;p&gt;It\u2019s been 6 months since starting a data science management role, and now have been laid off. &lt;/p&gt;\n\n&lt;p&gt;The role was sold as a data science manager, yet ended up doing admin work and touched on very small amounts of actual data science projects. &lt;/p&gt;\n\n&lt;p&gt;I was upset about the role but my boss assured me there were \u201cbig things\u201d in the pipeline. &lt;/p&gt;\n\n&lt;p&gt;I got my break and finally took responsibility for a project and coordinated with a team from the US. Things went smoothly, and as a native English speaker (I\u2019m EU based), I sat in on more global meetings. &lt;/p&gt;\n\n&lt;p&gt;Finally, things seemed to take off and work well\u2026 until I got fired today. &lt;/p&gt;\n\n&lt;p&gt;The result was: \u201cbusiness wasn\u2019t happy with your results.\u201d (We acted as data science manager consultants for different Business Units within the company). I asked why, the only feedback was a comment about a presentation not working out. &lt;/p&gt;\n\n&lt;p&gt;Business never communicated with me directly, only with my boss. There was a layer of reasoning missing. &lt;/p&gt;\n\n&lt;p&gt;So, here I am: questions unanswered, out of a job, and feel like I\u2019ve missed my chance at DS management. &lt;/p&gt;\n\n&lt;p&gt;I have one week left. What could I best do with this time?&lt;/p&gt;\n\n&lt;p&gt;Thanks all :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18natsq", "is_robot_indexable": true, "report_reasons": null, "author": "Hot-hentai-cum-papi", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18natsq/just_got_laid_off_as_i_was_heading_the_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18natsq/just_got_laid_off_as_i_was_heading_the_management/", "subreddit_subscribers": 1197809, "created_utc": 1703121843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_yjyfc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some of the most \u201cconfidently incorrect\u201d data science opinions you have heard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nxz71", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Statistics", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703194468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "370e8fc0-70eb-11ee-b58a-86a96bfd3389", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#94e044", "id": "18nxz71", "is_robot_indexable": true, "report_reasons": null, "author": "Stauce52", "discussion_type": null, "num_comments": 79, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18nxz71/what_are_some_of_the_most_confidently_incorrect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18nxz71/what_are_some_of_the_most_confidently_incorrect/", "subreddit_subscribers": 1197809, "created_utc": 1703194468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m divided myself on this topic because I get that it\u2019s more for data exploration. But at the same time, the code will eventually be put into production even if it\u2019s not the current iteration.", "author_fullname": "t2_soqhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should notebooks follow software engineering best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nulu1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703185709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m divided myself on this topic because I get that it\u2019s more for data exploration. But at the same time, the code will eventually be put into production even if it\u2019s not the current iteration.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "18nulu1", "is_robot_indexable": true, "report_reasons": null, "author": "n1k0h1k0", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18nulu1/should_notebooks_follow_software_engineering_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18nulu1/should_notebooks_follow_software_engineering_best/", "subreddit_subscribers": 1197809, "created_utc": 1703185709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve written a couple of times about my manager and how toxic it has been on the team. I had a chance to re-org to a different team, but he made it difficult to change and kept putting so many hoops.\n\nAnd then he became interim AVP because AVP got fed up and left, which meant that I wasn\u2019t switching teams.\n\nHowever, I had a random chance meeting with the someone higher and I spoke to them about everything, which he wasn\u2019t aware of. They have a brand new team that I\u2019ll be a part of. \n\nAnd best of all, my current manager can\u2019t say anything and I\u2019ll be leaving! However, since it\u2019s internal, there\u2019ll be transition time since there is no team. There\u2019s no one to replace me on my team, unfortunately.\n\nHowever, the new role comes with a promotion and a 20% salary increase, so this is truly the best Christmas gift ever! The projects won\u2019t be as interesting, but I want to take a break from LLMs and stick to classification. LLMs in a corporation is really\u2026 political and the data sucks.\n\nAnd then I can look for a new role externally with the title change, so definitely excited! I definitely have imposter syndrome about the new role, but very excited to start having more than 4 hours of sleep and not waking up to a million rude messages from my manager. It\u2019s ridiculously hard to prepare for a new job when your current job is so toxic and you\u2019re so burnt out.\n\nThanks so much for the advice and encouragement!!", "author_fullname": "t2_6lukipdd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thank you for the advice on my manager! I\u2019m finally switching teams!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ns3lu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703183371.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703179234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve written a couple of times about my manager and how toxic it has been on the team. I had a chance to re-org to a different team, but he made it difficult to change and kept putting so many hoops.&lt;/p&gt;\n\n&lt;p&gt;And then he became interim AVP because AVP got fed up and left, which meant that I wasn\u2019t switching teams.&lt;/p&gt;\n\n&lt;p&gt;However, I had a random chance meeting with the someone higher and I spoke to them about everything, which he wasn\u2019t aware of. They have a brand new team that I\u2019ll be a part of. &lt;/p&gt;\n\n&lt;p&gt;And best of all, my current manager can\u2019t say anything and I\u2019ll be leaving! However, since it\u2019s internal, there\u2019ll be transition time since there is no team. There\u2019s no one to replace me on my team, unfortunately.&lt;/p&gt;\n\n&lt;p&gt;However, the new role comes with a promotion and a 20% salary increase, so this is truly the best Christmas gift ever! The projects won\u2019t be as interesting, but I want to take a break from LLMs and stick to classification. LLMs in a corporation is really\u2026 political and the data sucks.&lt;/p&gt;\n\n&lt;p&gt;And then I can look for a new role externally with the title change, so definitely excited! I definitely have imposter syndrome about the new role, but very excited to start having more than 4 hours of sleep and not waking up to a million rude messages from my manager. It\u2019s ridiculously hard to prepare for a new job when your current job is so toxic and you\u2019re so burnt out.&lt;/p&gt;\n\n&lt;p&gt;Thanks so much for the advice and encouragement!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ns3lu", "is_robot_indexable": true, "report_reasons": null, "author": "Much-Focus-1408", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18ns3lu/thank_you_for_the_advice_on_my_manager_im_finally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18ns3lu/thank_you_for_the_advice_on_my_manager_im_finally/", "subreddit_subscribers": 1197809, "created_utc": 1703179234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey data science people, \n\nI've been a data scientist now for several years, and am currently in somewhat of a leadership position. I work for a smaller company, for which I was their very first data scientist. Overall, I'd describe my situation as somewhat of a massive double-edged sword. I've developed all their infrastructure and have really set the scope of data science at the organization. This last year, I started a push into actual data products. In that sense, it's been very rewarding getting to experience multiple hats, mostly be my own boss, and to be able to create something across not only the modeling side, but also the infrastructure and product management sides. \n\nThat said, the other side of the sword is all that comes with being the first data scientist at a smaller organization. The being a one-man startup within a company thing can definitely be a fair amount of exhausting and isolating. Hiring other positions itself has been very tricky. Company buy-in has been a weird mix... they definitely make me feel heavily valued, and I'm not afraid of losing my job. The last presentation I gave, the COO was practically salivating. Yet, I also feel like they haven't committed too much buy-in to data science in general. I've told my boss repeatedly how it all can feel really tenuous. Even scheduling meetings to go over data science success with various company stakeholders, feels like they really have to fit it in. \n\nWith all that said, my boss has for a long time been very interested in scaling up data science at the organization. He recently informally offered me a title bump up to Director of Data Science, and I'm considering it. I've also been thinking about changing companies to experience how others do it, or else taking somewhat of a sabbatical to think out the next stage of my life, what direction I want to go in. Overall I'm torn about the limitations of my current company... had originally gotten into data science to do NLP, and now that LLMs are taking off, I feel like I'm falling farther and farther away from that working here. I'm also wondering on whether it would be smart to take on a director role from the first company I started my ds career with. I'm a little worried I would get shoehorned from now on into director roles, when I'm not sure I'm ready to give up a more raw position. That said, my company has overall treated me well, and it IS a director role, which essentially I'm already doing. \n\nSo yeah, overall I think taking the director role would be an interesting experience and might play into my underlying motivations around creativity, BUT I'm also wary of the limitations of my own company in the field, and whether it overall aligns with my interests in things like LLMs. \n\nWhat do you guys think? :) ", "author_fullname": "t2_q91gszwod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Taking a Director of Data Science Position from First Company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nfk9b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703137081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data science people, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been a data scientist now for several years, and am currently in somewhat of a leadership position. I work for a smaller company, for which I was their very first data scientist. Overall, I&amp;#39;d describe my situation as somewhat of a massive double-edged sword. I&amp;#39;ve developed all their infrastructure and have really set the scope of data science at the organization. This last year, I started a push into actual data products. In that sense, it&amp;#39;s been very rewarding getting to experience multiple hats, mostly be my own boss, and to be able to create something across not only the modeling side, but also the infrastructure and product management sides. &lt;/p&gt;\n\n&lt;p&gt;That said, the other side of the sword is all that comes with being the first data scientist at a smaller organization. The being a one-man startup within a company thing can definitely be a fair amount of exhausting and isolating. Hiring other positions itself has been very tricky. Company buy-in has been a weird mix... they definitely make me feel heavily valued, and I&amp;#39;m not afraid of losing my job. The last presentation I gave, the COO was practically salivating. Yet, I also feel like they haven&amp;#39;t committed too much buy-in to data science in general. I&amp;#39;ve told my boss repeatedly how it all can feel really tenuous. Even scheduling meetings to go over data science success with various company stakeholders, feels like they really have to fit it in. &lt;/p&gt;\n\n&lt;p&gt;With all that said, my boss has for a long time been very interested in scaling up data science at the organization. He recently informally offered me a title bump up to Director of Data Science, and I&amp;#39;m considering it. I&amp;#39;ve also been thinking about changing companies to experience how others do it, or else taking somewhat of a sabbatical to think out the next stage of my life, what direction I want to go in. Overall I&amp;#39;m torn about the limitations of my current company... had originally gotten into data science to do NLP, and now that LLMs are taking off, I feel like I&amp;#39;m falling farther and farther away from that working here. I&amp;#39;m also wondering on whether it would be smart to take on a director role from the first company I started my ds career with. I&amp;#39;m a little worried I would get shoehorned from now on into director roles, when I&amp;#39;m not sure I&amp;#39;m ready to give up a more raw position. That said, my company has overall treated me well, and it IS a director role, which essentially I&amp;#39;m already doing. &lt;/p&gt;\n\n&lt;p&gt;So yeah, overall I think taking the director role would be an interesting experience and might play into my underlying motivations around creativity, BUT I&amp;#39;m also wary of the limitations of my own company in the field, and whether it overall aligns with my interests in things like LLMs. &lt;/p&gt;\n\n&lt;p&gt;What do you guys think? :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nfk9b", "is_robot_indexable": true, "report_reasons": null, "author": "dsthrowaway1337", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18nfk9b/taking_a_director_of_data_science_position_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18nfk9b/taking_a_director_of_data_science_position_from/", "subreddit_subscribers": 1197809, "created_utc": 1703137081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone, \n\nSorry to be posting this here but I really need some feedback on my CV. Ive been applying to entry level data roles and machine learning engineer roles and cant seem to land a single interview call. Any feedback would be greatly appreciated.", "author_fullname": "t2_5lbg30ew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Need feedback on my CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"955y8pvo6q7c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 137, "x": 108, "u": "https://preview.redd.it/955y8pvo6q7c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4cf1d63aac67a86a9c3cbe1db4abfef97d619946"}, {"y": 274, "x": 216, "u": "https://preview.redd.it/955y8pvo6q7c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6bff6651ce5f833c241b65c53aa22e23adf7aa9c"}, {"y": 406, "x": 320, "u": "https://preview.redd.it/955y8pvo6q7c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25c3cbcc2fdcd7df06b10b2b5107d75e0e4d4a12"}, {"y": 813, "x": 640, "u": "https://preview.redd.it/955y8pvo6q7c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7096610931ff792c1a1b3ee349e71906a08c94a1"}], "s": {"y": 1053, "x": 828, "u": "https://preview.redd.it/955y8pvo6q7c1.jpg?width=828&amp;format=pjpg&amp;auto=webp&amp;s=b28de451d42b1f323f3c1da0abb881e38c1f1bf4"}, "id": "955y8pvo6q7c1"}, "66poopvo6q7c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 140, "x": 108, "u": "https://preview.redd.it/66poopvo6q7c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=16d293e28340bc76fddb4ec5fea752685e298416"}, {"y": 281, "x": 216, "u": "https://preview.redd.it/66poopvo6q7c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=028ceee91aa0b9abc265a9d241cfafbde7e13831"}, {"y": 416, "x": 320, "u": "https://preview.redd.it/66poopvo6q7c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db36e7daf347724ff3457b54013bad9e03e44b1e"}, {"y": 833, "x": 640, "u": "https://preview.redd.it/66poopvo6q7c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41dd1049118a685c55bd08e50d242e87eddf5eea"}], "s": {"y": 1078, "x": 828, "u": "https://preview.redd.it/66poopvo6q7c1.jpg?width=828&amp;format=pjpg&amp;auto=webp&amp;s=78a323b2efca9e6a91c9069510a4b5eee0e5d061"}, "id": "66poopvo6q7c1"}}, "name": "t3_18nzcxc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 7, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "955y8pvo6q7c1", "id": 377286653}, {"media_id": "66poopvo6q7c1", "id": 377286654}]}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8blSrPK5Dexnj6dDuvI3bCm4nTs_6_C_o9GwkGpWihU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703198197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;Sorry to be posting this here but I really need some feedback on my CV. Ive been applying to entry level data roles and machine learning engineer roles and cant seem to land a single interview call. Any feedback would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/18nzcxc", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nzcxc", "is_robot_indexable": true, "report_reasons": null, "author": "Blahblahblakha", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18nzcxc/need_feedback_on_my_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/18nzcxc", "subreddit_subscribers": 1197809, "created_utc": 1703198197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My 2018 laptop was damage last month and I had to quickly purchase a new MacBook. I just finished my first semester as a data science grad student. In the future, I will be taking higher level machine learning courses, and may want to pursue side projects with computer vision, NLP, and LLMs. \n\nI initially purchased a 14 inch M3 pro MacBook with 18gb of ram. I noticed with VScode, Slack, Zoom, Notion, Safari and two monitors connected I\u2019m using about 15 gb of ram. ChatGPT even resets when I go to a different program.\n\n I wanted to know how important ram use is for machine learning and LLMs. I\u2019m debating returning my MacBook for the same form factor but with 36 gb of ram. Is it necessary, or will I have the same issues?", "author_fullname": "t2_8fplm045", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Minimum MacBook ram for machine learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ntu3d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703183733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My 2018 laptop was damage last month and I had to quickly purchase a new MacBook. I just finished my first semester as a data science grad student. In the future, I will be taking higher level machine learning courses, and may want to pursue side projects with computer vision, NLP, and LLMs. &lt;/p&gt;\n\n&lt;p&gt;I initially purchased a 14 inch M3 pro MacBook with 18gb of ram. I noticed with VScode, Slack, Zoom, Notion, Safari and two monitors connected I\u2019m using about 15 gb of ram. ChatGPT even resets when I go to a different program.&lt;/p&gt;\n\n&lt;p&gt;I wanted to know how important ram use is for machine learning and LLMs. I\u2019m debating returning my MacBook for the same form factor but with 36 gb of ram. Is it necessary, or will I have the same issues?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "18ntu3d", "is_robot_indexable": true, "report_reasons": null, "author": "Fair-Assist-3553", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18ntu3d/minimum_macbook_ram_for_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18ntu3d/minimum_macbook_ram_for_machine_learning/", "subreddit_subscribers": 1197809, "created_utc": 1703183733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi. I have the opportunity to define my own data role at a very very large company. I'm currently a Sr. DS and I manage a team at a startup. This seems like a trick - the company is tech and data heavy and they have data scientists on staff, in abundance - *I'm* not that great.    \n\n Is this a **trap**? ", "author_fullname": "t2_4ciy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice needed, (not a humble brag, I swear)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nccs1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.32, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703126609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I have the opportunity to define my own data role at a very very large company. I&amp;#39;m currently a Sr. DS and I manage a team at a startup. This seems like a trick - the company is tech and data heavy and they have data scientists on staff, in abundance - &lt;em&gt;I&amp;#39;m&lt;/em&gt; not that great.    &lt;/p&gt;\n\n&lt;p&gt;Is this a &lt;strong&gt;trap&lt;/strong&gt;? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nccs1", "is_robot_indexable": true, "report_reasons": null, "author": "tmotytmoty", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18nccs1/advice_needed_not_a_humble_brag_i_swear/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18nccs1/advice_needed_not_a_humble_brag_i_swear/", "subreddit_subscribers": 1197809, "created_utc": 1703126609.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}