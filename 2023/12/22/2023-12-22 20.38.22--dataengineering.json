{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im a contractor and I was asked by someone at my client's to help out with another project I wasn't originally contracted for. It's an ERP migration project, so I really only have to write SQL statements and extract some data based on the new ERP system's template from the legacy ERP's backend (SQL Server).\n\nI've never done this kind of work before and largely unfamiliar with ERP systems, but since it's mostly just SQL work I thought I could help out. Project Manager gave me a list of fields I need to look for, but there are about 200 tables in the backend. I said I'd like to speak with the finance team managing this legacy ERP system to narrow down my search, and Project Manager says \"well I saw the table names, they all look pretty straightforward, I think you can just look through all those tables\". But I said \"I've never worked on this domain before so I would really appreciate some guidance\". And she didnt' respond.  I tried searching blindly but ended up finding a table with 10 addresses per customer and I don't know which of these are needed etc. \n\nWas I being unprofessional for wanting to ask for help to make sure I'm looking for the data in the right places? ", "author_fullname": "t2_7m2ues2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was I unprofessional when I said I needed help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18of52n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703252357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a contractor and I was asked by someone at my client&amp;#39;s to help out with another project I wasn&amp;#39;t originally contracted for. It&amp;#39;s an ERP migration project, so I really only have to write SQL statements and extract some data based on the new ERP system&amp;#39;s template from the legacy ERP&amp;#39;s backend (SQL Server).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never done this kind of work before and largely unfamiliar with ERP systems, but since it&amp;#39;s mostly just SQL work I thought I could help out. Project Manager gave me a list of fields I need to look for, but there are about 200 tables in the backend. I said I&amp;#39;d like to speak with the finance team managing this legacy ERP system to narrow down my search, and Project Manager says &amp;quot;well I saw the table names, they all look pretty straightforward, I think you can just look through all those tables&amp;quot;. But I said &amp;quot;I&amp;#39;ve never worked on this domain before so I would really appreciate some guidance&amp;quot;. And she didnt&amp;#39; respond.  I tried searching blindly but ended up finding a table with 10 addresses per customer and I don&amp;#39;t know which of these are needed etc. &lt;/p&gt;\n\n&lt;p&gt;Was I being unprofessional for wanting to ask for help to make sure I&amp;#39;m looking for the data in the right places? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18of52n", "is_robot_indexable": true, "report_reasons": null, "author": "DataScienceIsScience", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18of52n/was_i_unprofessional_when_i_said_i_needed_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18of52n/was_i_unprofessional_when_i_said_i_needed_help/", "subreddit_subscribers": 147695, "created_utc": 1703252357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi people. So currently my team is running bunch of python scripts on Virtual Machine scheduled on Task Scheduler.\n\nWe dont use github. Just the scripts, some .bat files and task scheduler.\n\nSo im thinking of adding the element of SWE/ or proper way to do this. \n\nI'm looking into GitLab and GitHub but have no actual direction on whats a good practise and whats not. Or what we're doing wrong.\n\nNeed some experts to throw in on how to improve this process. Thanks!", "author_fullname": "t2_7nk0x7fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whats the best practise to automating python scripts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o2zzq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703208284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people. So currently my team is running bunch of python scripts on Virtual Machine scheduled on Task Scheduler.&lt;/p&gt;\n\n&lt;p&gt;We dont use github. Just the scripts, some .bat files and task scheduler.&lt;/p&gt;\n\n&lt;p&gt;So im thinking of adding the element of SWE/ or proper way to do this. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking into GitLab and GitHub but have no actual direction on whats a good practise and whats not. Or what we&amp;#39;re doing wrong.&lt;/p&gt;\n\n&lt;p&gt;Need some experts to throw in on how to improve this process. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o2zzq", "is_robot_indexable": true, "report_reasons": null, "author": "Nopal97", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o2zzq/whats_the_best_practise_to_automating_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o2zzq/whats_the_best_practise_to_automating_python/", "subreddit_subscribers": 147695, "created_utc": 1703208284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody,\n\nI recently accepted a PhD position focusing on the development of a database solution for the laboratory of a university.\n\nThis lab is characterized by a high turnover of projects and students, with a rich diversity of data that holds the potential for significant insights. However, as of now, there's a notable absence of a structured data warehouse. I want to take the opportunity to set up a 21st century solution.\n\nGiven these circumstances, I'm reaching out to this you guys for advice. I'm interested in learning about:\n\n1. **Best Practices and Considerations:** What are the key factors I should consider when implementing a data warehouse from scratch? Are there specific best practices tailored for high-turnover, diverse data environments?\n2. **Appropriateness of a Data Warehouse:** Is a data warehouse the ideal solution for the lab? Or should we explore other data management systems?\n\n**About Me:** My background includes experience in Database QA and moderate programming skills. However, I'm relatively new to in-depth data management. I'm currently reading \"Fundamentals of Data Engineering\" and actively seeking resources to improve my core SQL skills and overall data handling expertise.\n\nI'd appreciate any insights, resources, or experiences you could share that could serve as a guide.\n\nThanks!", "author_fullname": "t2_ddwwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ground Up Database Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18obiz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703239979.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703239306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,&lt;/p&gt;\n\n&lt;p&gt;I recently accepted a PhD position focusing on the development of a database solution for the laboratory of a university.&lt;/p&gt;\n\n&lt;p&gt;This lab is characterized by a high turnover of projects and students, with a rich diversity of data that holds the potential for significant insights. However, as of now, there&amp;#39;s a notable absence of a structured data warehouse. I want to take the opportunity to set up a 21st century solution.&lt;/p&gt;\n\n&lt;p&gt;Given these circumstances, I&amp;#39;m reaching out to this you guys for advice. I&amp;#39;m interested in learning about:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Best Practices and Considerations:&lt;/strong&gt; What are the key factors I should consider when implementing a data warehouse from scratch? Are there specific best practices tailored for high-turnover, diverse data environments?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Appropriateness of a Data Warehouse:&lt;/strong&gt; Is a data warehouse the ideal solution for the lab? Or should we explore other data management systems?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;About Me:&lt;/strong&gt; My background includes experience in Database QA and moderate programming skills. However, I&amp;#39;m relatively new to in-depth data management. I&amp;#39;m currently reading &amp;quot;Fundamentals of Data Engineering&amp;quot; and actively seeking resources to improve my core SQL skills and overall data handling expertise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate any insights, resources, or experiences you could share that could serve as a guide.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18obiz9", "is_robot_indexable": true, "report_reasons": null, "author": "Jimmyfatz", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18obiz9/ground_up_database_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18obiz9/ground_up_database_design/", "subreddit_subscribers": 147695, "created_utc": 1703239306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our most recent blog on Real-time Change Data Capture from Postgres 16 Read Replicas [https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas](https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas)  \nStart replicating data from Postgres to Data Warehouses, Queues and Storage using Read Replicas instead of Primaries. No worry of additional load or outages of the Primary database.\n\nWhile building this feature, a few of our learnings on logical decoding on Postgres Read Replicas:  \n1\ufe0f\u20e3 CREATE\\_REPLICATION\\_SLOT works as expected  \n2\ufe0f\u20e3 SNAPSHOT can be created and used on Read Replicas  \n3\ufe0f\u20e3 START\\_REPLICATION worked as expected  \n4\ufe0f\u20e3 Publications cannot be created on the Replica. They need to be created on the primary.  \n5\ufe0f\u20e3 WAL control functions cannot run on Read Replicas. Use pg\\_last\\_wal\\_receive\\_lsn() instead of pg\\_current\\_wal\\_lsn()", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Change Data Capture from Postgres 16 Read Replicas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nzdi8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703198239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our most recent blog on Real-time Change Data Capture from Postgres 16 Read Replicas &lt;a href=\"https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas\"&gt;https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas&lt;/a&gt;&lt;br/&gt;\nStart replicating data from Postgres to Data Warehouses, Queues and Storage using Read Replicas instead of Primaries. No worry of additional load or outages of the Primary database.&lt;/p&gt;\n\n&lt;p&gt;While building this feature, a few of our learnings on logical decoding on Postgres Read Replicas:&lt;br/&gt;\n1\ufe0f\u20e3 CREATE_REPLICATION_SLOT works as expected&lt;br/&gt;\n2\ufe0f\u20e3 SNAPSHOT can be created and used on Read Replicas&lt;br/&gt;\n3\ufe0f\u20e3 START_REPLICATION worked as expected&lt;br/&gt;\n4\ufe0f\u20e3 Publications cannot be created on the Replica. They need to be created on the primary.&lt;br/&gt;\n5\ufe0f\u20e3 WAL control functions cannot run on Read Replicas. Use pg_last_wal_receive_lsn() instead of pg_current_wal_lsn()&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?auto=webp&amp;s=2b914bd51e6fe4fee7388cd4feab65531c9c1376", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd79830ce32aad0a840914f23c6003fe0c7c1ae6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a206c8f190043c41f3128dd1b71d29de6c21f112", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d552aa43457c0da8f5013ebe653391036f5332a", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d6d36f451b4354a0aa9dc47d5753f1edfc2a73b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cdfed35c434a9c8f014e7ee8eba77ad545bf114", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec5d413b0521c2eec24d14cf32571b0179e4ef0a", "width": 1080, "height": 567}], "variants": {}, "id": "xCM7b2TzQAsK8mVeahKijq6AT8KMhyAVbQr5BhJjngE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nzdi8", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nzdi8/realtime_change_data_capture_from_postgres_16/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nzdi8/realtime_change_data_capture_from_postgres_16/", "subreddit_subscribers": 147695, "created_utc": 1703198239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a newbie intern in business analytics, and my manager recently wrote in my performance review that he wants me to improve in SQL. I\u2019ve written a few queries in SQL Server throughout the internship, but that\u2019s the extent to which I\u2019ve used it (I\u2019ve mostly worked in Tableau and Excel otherwise).\n\nFor an upcoming project, he said I\u2019ll need to connect Azure Databricks to either SQL Server or DBeaver. I\u2019ve done some online research, but I\u2019m still not understanding what Databricks and DBeaver are. What is the purpose of each, and how do they relate to SQL Server?", "author_fullname": "t2_6zej24pk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5: How do SQL Server, Databricks, and DBeaver work together?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o3fek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703209596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a newbie intern in business analytics, and my manager recently wrote in my performance review that he wants me to improve in SQL. I\u2019ve written a few queries in SQL Server throughout the internship, but that\u2019s the extent to which I\u2019ve used it (I\u2019ve mostly worked in Tableau and Excel otherwise).&lt;/p&gt;\n\n&lt;p&gt;For an upcoming project, he said I\u2019ll need to connect Azure Databricks to either SQL Server or DBeaver. I\u2019ve done some online research, but I\u2019m still not understanding what Databricks and DBeaver are. What is the purpose of each, and how do they relate to SQL Server?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o3fek", "is_robot_indexable": true, "report_reasons": null, "author": "jiminforthewin", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o3fek/eli5_how_do_sql_server_databricks_and_dbeaver/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o3fek/eli5_how_do_sql_server_databricks_and_dbeaver/", "subreddit_subscribers": 147695, "created_utc": 1703209596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nThe company I work at allows for $5000 annual education stipend. Currently I am a data scientist that works mostly on Analytics -- I use SQL 95% of the time, and I can hack my way around Pandas and R with googling (used them both in the past).\n\nI would like to get more technical and would transition into Data Engineering -- I wanted to sign up for a Data Engineering course to learn key softwares and tools that DE teams look for. I've seen several free camps like Zoomcamp and paid courses by MIT -- since I'lll get the course reimbursed, can anyone recommend a good option for a DS trying to switch to DE? I'm having a hard time understanding what a paid course will offer over a free one, but I want to make sure I select the highest quality course since budget is no issue.\n\n&amp;#x200B;\n\nEdit: what works great for me is bigger project style structures. I liked The Odin Project for Web Dev... not a fan of purely video lectures with quizzes like most Coursera courses.", "author_fullname": "t2_923nyofu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paid vs Free Data Engineering courses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o6qap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703246005.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703220266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;The company I work at allows for $5000 annual education stipend. Currently I am a data scientist that works mostly on Analytics -- I use SQL 95% of the time, and I can hack my way around Pandas and R with googling (used them both in the past).&lt;/p&gt;\n\n&lt;p&gt;I would like to get more technical and would transition into Data Engineering -- I wanted to sign up for a Data Engineering course to learn key softwares and tools that DE teams look for. I&amp;#39;ve seen several free camps like Zoomcamp and paid courses by MIT -- since I&amp;#39;lll get the course reimbursed, can anyone recommend a good option for a DS trying to switch to DE? I&amp;#39;m having a hard time understanding what a paid course will offer over a free one, but I want to make sure I select the highest quality course since budget is no issue.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: what works great for me is bigger project style structures. I liked The Odin Project for Web Dev... not a fan of purely video lectures with quizzes like most Coursera courses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18o6qap", "is_robot_indexable": true, "report_reasons": null, "author": "FairAd6062", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o6qap/paid_vs_free_data_engineering_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o6qap/paid_vs_free_data_engineering_courses/", "subreddit_subscribers": 147695, "created_utc": 1703220266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "has anyone worked on both dbt and Delta Live Tables with Databricks? What's the pros/cons?\n\nRecently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline\n\nWe had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo'd us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)", "author_fullname": "t2_ghlgg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: dbt or Delta Live Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ny8tf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703195187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;has anyone worked on both dbt and Delta Live Tables with Databricks? What&amp;#39;s the pros/cons?&lt;/p&gt;\n\n&lt;p&gt;Recently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline&lt;/p&gt;\n\n&lt;p&gt;We had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo&amp;#39;d us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ny8tf", "is_robot_indexable": true, "report_reasons": null, "author": "y45hiro", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ny8tf/databricks_dbt_or_delta_live_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ny8tf/databricks_dbt_or_delta_live_tables/", "subreddit_subscribers": 147695, "created_utc": 1703195187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company we have some pipelines that do simple processing on raw data from S3 before storing them back as cleaned and partitioned (by day) datasets on S3, so users can effectively read clean data by ranges of days. Raw and partitioned data is stored as parquet.\n\nRight now this is done with Pandas on Airflow pipelines, running with KubernetesExecutor. This raw data only *occasionally* gets large: we do incremental syncs from data sources, so usually we have 1 larger dataset -- the first sync -- followed by small batches. These small batches tend to be really small on most sources: around 100 records. The larger datasets might grow up to millions of records. I would like to have a solution that scales out to this extraordinary cases, while still being used effectively for the small ones.\n\nFrom what I've seen, Spark has been losing popularity due to the hard work in maintaining the clusters and the need to study and learn it from scratch, while simpler tools rise. Polars, Dask or DuckDB seem to be good alternatives.\n\nWhat are the current low setup, easy to learn, cost effective solutions that scale to this wide range of sizes of data, and their pros and cons?", "author_fullname": "t2_c3hoz70b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to scale pipelines to large datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oftc0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703254296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company we have some pipelines that do simple processing on raw data from S3 before storing them back as cleaned and partitioned (by day) datasets on S3, so users can effectively read clean data by ranges of days. Raw and partitioned data is stored as parquet.&lt;/p&gt;\n\n&lt;p&gt;Right now this is done with Pandas on Airflow pipelines, running with KubernetesExecutor. This raw data only &lt;em&gt;occasionally&lt;/em&gt; gets large: we do incremental syncs from data sources, so usually we have 1 larger dataset -- the first sync -- followed by small batches. These small batches tend to be really small on most sources: around 100 records. The larger datasets might grow up to millions of records. I would like to have a solution that scales out to this extraordinary cases, while still being used effectively for the small ones.&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve seen, Spark has been losing popularity due to the hard work in maintaining the clusters and the need to study and learn it from scratch, while simpler tools rise. Polars, Dask or DuckDB seem to be good alternatives.&lt;/p&gt;\n\n&lt;p&gt;What are the current low setup, easy to learn, cost effective solutions that scale to this wide range of sizes of data, and their pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oftc0", "is_robot_indexable": true, "report_reasons": null, "author": "henriquemeloo_", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oftc0/best_way_to_scale_pipelines_to_large_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oftc0/best_way_to_scale_pipelines_to_large_datasets/", "subreddit_subscribers": 147695, "created_utc": 1703254296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently develop in Azure where we use ADF, ADLS and Databricks. Currently we have a few environment json files in a parameters container that map credentials to secrets in key vault across dev/uat/prd to reduce hardcoding.\n\nWhen creating a common pipeline I create a pipeline folder in our parameters container and store a config file with constants specific to the trigger. For example this file might could contain a list of tables that we want to extract from sql. This way we can easily add or remove tables to that trigger and reuse the pipeline for other sources and schedules by simply pointing a trigger to a different config file.\n\nMy question is, have I over complicated this? Is anyone else doing anything similar?", "author_fullname": "t2_qc6h84ym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Parameters &amp; Environment Variables in Azure to Enable CICD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oapzd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703235960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently develop in Azure where we use ADF, ADLS and Databricks. Currently we have a few environment json files in a parameters container that map credentials to secrets in key vault across dev/uat/prd to reduce hardcoding.&lt;/p&gt;\n\n&lt;p&gt;When creating a common pipeline I create a pipeline folder in our parameters container and store a config file with constants specific to the trigger. For example this file might could contain a list of tables that we want to extract from sql. This way we can easily add or remove tables to that trigger and reuse the pipeline for other sources and schedules by simply pointing a trigger to a different config file.&lt;/p&gt;\n\n&lt;p&gt;My question is, have I over complicated this? Is anyone else doing anything similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oapzd", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated-Western1788", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oapzd/managing_parameters_environment_variables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oapzd/managing_parameters_environment_variables_in/", "subreddit_subscribers": 147695, "created_utc": 1703235960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My application runs in a AWS service called Glue which is based on spark.\n\nI am reading in millions of AWS S3 objects using spark but by the time it lists all the files and starts to process them some have been deleted by another application.\n\nIs there a way to ignore these? I have tried the flag ignoreMissingFiles to no avail?\n\nAlso, semi related, does spark need to list all files before starting processing? I would have thought there was a way to load each file, process and then save since each file is independent. Is seems like all files have to be loaded first which seems crazy to me. Can I not tell spark to load one at a time?", "author_fullname": "t2_vlw0gd8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to skip non existent S3 objects loading in Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o2a06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703206153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My application runs in a AWS service called Glue which is based on spark.&lt;/p&gt;\n\n&lt;p&gt;I am reading in millions of AWS S3 objects using spark but by the time it lists all the files and starts to process them some have been deleted by another application.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to ignore these? I have tried the flag ignoreMissingFiles to no avail?&lt;/p&gt;\n\n&lt;p&gt;Also, semi related, does spark need to list all files before starting processing? I would have thought there was a way to load each file, process and then save since each file is independent. Is seems like all files have to be loaded first which seems crazy to me. Can I not tell spark to load one at a time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o2a06", "is_robot_indexable": true, "report_reasons": null, "author": "atticusfinch975", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o2a06/how_to_skip_non_existent_s3_objects_loading_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o2a06/how_to_skip_non_existent_s3_objects_loading_in/", "subreddit_subscribers": 147695, "created_utc": 1703206153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u8kebhp6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UCX v0.7.0 by Databricks Labs \u2014 new release with CLI commands", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_18o0v7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1JCa-RXvcaGTvAt8kVlN1TX89yfRYzrOl1z-ECM9Pe8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703202177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/databricks-labs/ucx-v0-7-0-by-databricks-labs-new-release-with-cli-commands-b4ddd0786598", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?auto=webp&amp;s=75068d1c59038b869069b8e2b570835cf4d87d4d", "width": 768, "height": 496}, "resolutions": [{"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b011d28294feecf4496a5702eabac9aeed870ba2", "width": 108, "height": 69}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9337fb1551a33d5aea7eacee28a7e59f1f928fba", "width": 216, "height": 139}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d1ae8366cf3b35c3b97c4abce0c58ed9b760442", "width": 320, "height": 206}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83f79a393771ca5cefcc9ae8c33f59729fa81fa4", "width": 640, "height": 413}], "variants": {}, "id": "lTHuVblV7M4EdxXC6wPm6GrWFFAemY5_V7f-E_jIDNw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18o0v7c", "is_robot_indexable": true, "report_reasons": null, "author": "serge_databricks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o0v7c/ucx_v070_by_databricks_labs_new_release_with_cli/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/databricks-labs/ucx-v0-7-0-by-databricks-labs-new-release-with-cli-commands-b4ddd0786598", "subreddit_subscribers": 147695, "created_utc": 1703202177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datafusion SQL CLI - Look Ma, I made a new ETL tool.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18nxjfu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wHTKvyWGxYvyFCZodMyJnMQjzmMoZMB_jKzsGGrWrcU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703193326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "confessionsofadataguy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.confessionsofadataguy.com/datafusion-sql-cli-look-ma-i-made-a-new-etl-tool/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?auto=webp&amp;s=41423b348e6ad563cd9ec3c93bb2ef568d601d94", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6770d83ea8f1b52b4202f91465721a61ab368c3c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f52f431ce010bf1d78004cd087fb77ff52933da0", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35c938305c576391e1e83f1d8ba5ae5f653d4605", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30da6f3130dcc719059d8f1e646c19d64c041394", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b7bdb03463a1206d2006d958847620dccc9f170", "width": 960, "height": 960}], "variants": {}, "id": "yA9pk-IMBm6egneN5vdpyDs0g2ml4ezsDBMsp_nGF0w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nxjfu", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nxjfu/datafusion_sql_cli_look_ma_i_made_a_new_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.confessionsofadataguy.com/datafusion-sql-cli-look-ma-i-made-a-new-etl-tool/", "subreddit_subscribers": 147695, "created_utc": 1703193326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It would be helpful to see if anyone has been able to get meaningful discounts from vendors. \n\nIf you've received a discount on a vendor -- can you state vendor // annual spend and what discount you received? I'm particularly interested in Snowflake.", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vendor discounts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18okok3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703267444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It would be helpful to see if anyone has been able to get meaningful discounts from vendors. &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve received a discount on a vendor -- can you state vendor // annual spend and what discount you received? I&amp;#39;m particularly interested in Snowflake.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18okok3", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18okok3/vendor_discounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18okok3/vendor_discounts/", "subreddit_subscribers": 147695, "created_utc": 1703267444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\nI recently joined a company that has a Mac only policy. I only ever used windows+WSL2 for work (or at all, tbh). Never used a Mac in my life.\n\nWhat advice would you give me to make the transition smoother?\n\nFor instance, things that are a must-have for me are:\n- remap keys (e,g, Sharpkeys for windows)\n- edit system shortcuts\n- having multiple keyboard layouts installed (Italian, English, etc...)\n- install everything with package manager (does brew suffice? Is there a better alternative?)\n\nAny advice is more than welcome!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving from windows+WSL2 to Mac. Halp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ojz1f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703265564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,\nI recently joined a company that has a Mac only policy. I only ever used windows+WSL2 for work (or at all, tbh). Never used a Mac in my life.&lt;/p&gt;\n\n&lt;p&gt;What advice would you give me to make the transition smoother?&lt;/p&gt;\n\n&lt;p&gt;For instance, things that are a must-have for me are:\n- remap keys (e,g, Sharpkeys for windows)\n- edit system shortcuts\n- having multiple keyboard layouts installed (Italian, English, etc...)\n- install everything with package manager (does brew suffice? Is there a better alternative?)&lt;/p&gt;\n\n&lt;p&gt;Any advice is more than welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ojz1f", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ojz1f/moving_from_windowswsl2_to_mac_halp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ojz1f/moving_from_windowswsl2_to_mac_halp/", "subreddit_subscribers": 147695, "created_utc": 1703265564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I am working on a small project to create a simple real-time pipeline of posts on Reddit. I want to allow the user to query the subreddit name. But then the problem appears, **how should the new query subreddit change be written to the Kafka producer?**\n\n\\- Should I create a new topic for every new subreddit query, although that will *theoretically* create hundreds of thousands of topics (given that there are \\~100k subreddits) and would be a bad practice (although this is not going to happen in my use case)?    \n\\- Should I write everything to the same stream, but use some kind of ID to differentiate between topics? Would this aggregation of data make it harder to process the stream? Since it is relatively large now, my stream processor (e.g. Spark) will have to process everything just to select the rows with the intended subreddit name.\n\nI am quite stuck here and would appreciate any insight on this, thank you so much.", "author_fullname": "t2_38msj796", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategy to approach query-based Kafka producer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oefke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703250871.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703250152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I am working on a small project to create a simple real-time pipeline of posts on Reddit. I want to allow the user to query the subreddit name. But then the problem appears, &lt;strong&gt;how should the new query subreddit change be written to the Kafka producer?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Should I create a new topic for every new subreddit query, although that will &lt;em&gt;theoretically&lt;/em&gt; create hundreds of thousands of topics (given that there are ~100k subreddits) and would be a bad practice (although this is not going to happen in my use case)?&lt;br/&gt;\n- Should I write everything to the same stream, but use some kind of ID to differentiate between topics? Would this aggregation of data make it harder to process the stream? Since it is relatively large now, my stream processor (e.g. Spark) will have to process everything just to select the rows with the intended subreddit name.&lt;/p&gt;\n\n&lt;p&gt;I am quite stuck here and would appreciate any insight on this, thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oefke", "is_robot_indexable": true, "report_reasons": null, "author": "Babe_My_Name_Is_Hung", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oefke/strategy_to_approach_querybased_kafka_producer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oefke/strategy_to_approach_querybased_kafka_producer/", "subreddit_subscribers": 147695, "created_utc": 1703250152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nJust finished up a project I have been working on for the last few weeks.  Like the title says, it's a data pipeline to a Tableau dashboard containing world economic data.  \n\nHere's a link to the project:\n\nhttps://github.com/johnchandlerbaldwin/data-modeling-project\n\nHere's a link to the Tableau dashboard:\n\nhttps://public.tableau.com/app/profile/john.baldwin4618/viz/EconomicIndicators_17029599995600/FinalDashboard?publish=yes\n\nIt was great to get experience with Tableau and the Modern Data Stack.  I'm hoping this will help me transition into data engineering - career data analyst with a graduate degree in data science here trying to transition into the field.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Project: Data Pipeline from Wikipedia to BigQuery to Create a Tableau Dashboard of World Economic Indicators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18omyz7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703273615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;Just finished up a project I have been working on for the last few weeks.  Like the title says, it&amp;#39;s a data pipeline to a Tableau dashboard containing world economic data.  &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a link to the project:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/johnchandlerbaldwin/data-modeling-project\"&gt;https://github.com/johnchandlerbaldwin/data-modeling-project&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a link to the Tableau dashboard:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://public.tableau.com/app/profile/john.baldwin4618/viz/EconomicIndicators_17029599995600/FinalDashboard?publish=yes\"&gt;https://public.tableau.com/app/profile/john.baldwin4618/viz/EconomicIndicators_17029599995600/FinalDashboard?publish=yes&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It was great to get experience with Tableau and the Modern Data Stack.  I&amp;#39;m hoping this will help me transition into data engineering - career data analyst with a graduate degree in data science here trying to transition into the field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?auto=webp&amp;s=421642422a1e58da69fdf0dc300c050f6b141810", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2133b15c60695dc6f1b4d449193ff128e4190ae", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee9451e1c09607a592eac5807067544a170cdb3b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4183f52c1577942283e215f63de66f8a563129db", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc78d1c0e952e148090277a141f985ccdb2a1355", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc01910a5212eb1195da94a653086341beba14f3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90559908a68b7090de2eb6e24b69d7e55863d5ff", "width": 1080, "height": 540}], "variants": {}, "id": "TWDB43vlvDLUgVcdtOELJ74yCcMtQ5udPCeTdvIoslA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18omyz7", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18omyz7/personal_project_data_pipeline_from_wikipedia_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18omyz7/personal_project_data_pipeline_from_wikipedia_to/", "subreddit_subscribers": 147695, "created_utc": 1703273615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently tackling a project to shift our operations from an outdated stack to a proper data engineering platform. Our current system, designed mainly by software engineers, is a mix of Java/Go microservices, overly dependent on manual heavy lifting  , and honestly, it's barely hanging on.\n\nHere's what I'm dealing with:\n\n1. I need to pull a list of stock prices that our clients are tracking, which comes from Cassandra. The twist is, both the client roster and the stock list are always in flux.\n2. For every customer, every five minutes or so, I've got to ping a bunch of REST APIs for each client to grab the latest stock info.\n3. Then, there's some basic data transformation before I shoot this data over to Kinesis for more heavy-lifting.\n\nBasically,\n\nEvery 5 minutes --&gt; Customer 1 --&gt; REST APIs A,B,C ---&gt; Transform and send to Kinesis\n\nEvery 5 minutes --&gt; Customer 2  --&gt; REST APIs D,E,F. --&gt; Transform and send to Kinesis\n\nI tried setting up the first step in a Master DAG in Airflow and then handling the next two steps in Nifi as part of Sub DAGs(corresponding to each customer). But, boy, am I hitting a wall trying to pass all the client-specific info through the Sub DAGs using the TriggerDagRunOperator. Trying to jam this all into a single DAG? Not gonna work \u2013 it just can't keep up with our five-minute deadline.\n\nNow, I'm starting to wonder if we even need an orchestrator in this puzzle, considering none seem up to snuff for our needs. Is it crazy to think about running all three stages directly in Nifi? My experience with Nifi has mostly been about data ingestion and routing, but here, it might have to take on a scheduler role too. Need some opinions on how to design this flow !", "author_fullname": "t2_8023gpp1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Options for migrating from a legacy pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18omla1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703272526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently tackling a project to shift our operations from an outdated stack to a proper data engineering platform. Our current system, designed mainly by software engineers, is a mix of Java/Go microservices, overly dependent on manual heavy lifting  , and honestly, it&amp;#39;s barely hanging on.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m dealing with:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I need to pull a list of stock prices that our clients are tracking, which comes from Cassandra. The twist is, both the client roster and the stock list are always in flux.&lt;/li&gt;\n&lt;li&gt;For every customer, every five minutes or so, I&amp;#39;ve got to ping a bunch of REST APIs for each client to grab the latest stock info.&lt;/li&gt;\n&lt;li&gt;Then, there&amp;#39;s some basic data transformation before I shoot this data over to Kinesis for more heavy-lifting.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Basically,&lt;/p&gt;\n\n&lt;p&gt;Every 5 minutes --&amp;gt; Customer 1 --&amp;gt; REST APIs A,B,C ---&amp;gt; Transform and send to Kinesis&lt;/p&gt;\n\n&lt;p&gt;Every 5 minutes --&amp;gt; Customer 2  --&amp;gt; REST APIs D,E,F. --&amp;gt; Transform and send to Kinesis&lt;/p&gt;\n\n&lt;p&gt;I tried setting up the first step in a Master DAG in Airflow and then handling the next two steps in Nifi as part of Sub DAGs(corresponding to each customer). But, boy, am I hitting a wall trying to pass all the client-specific info through the Sub DAGs using the TriggerDagRunOperator. Trying to jam this all into a single DAG? Not gonna work \u2013 it just can&amp;#39;t keep up with our five-minute deadline.&lt;/p&gt;\n\n&lt;p&gt;Now, I&amp;#39;m starting to wonder if we even need an orchestrator in this puzzle, considering none seem up to snuff for our needs. Is it crazy to think about running all three stages directly in Nifi? My experience with Nifi has mostly been about data ingestion and routing, but here, it might have to take on a scheduler role too. Need some opinions on how to design this flow !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18omla1", "is_robot_indexable": true, "report_reasons": null, "author": "keroomi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18omla1/options_for_migrating_from_a_legacy_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18omla1/options_for_migrating_from_a_legacy_pipeline/", "subreddit_subscribers": 147695, "created_utc": 1703272526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am looking for resources for the  AWS Certified Data Engineer - Associate certification. I cannot see anything linked on the certification's page and at the moment I am going through the normal AWS documentation. Any suggestion for some extra content?", "author_fullname": "t2_utk7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best resources to study for AWS Certified Data Engineer - Associate - cert?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18oma86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703271689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am looking for resources for the  AWS Certified Data Engineer - Associate certification. I cannot see anything linked on the certification&amp;#39;s page and at the moment I am going through the normal AWS documentation. Any suggestion for some extra content?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oma86", "is_robot_indexable": true, "report_reasons": null, "author": "df016", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oma86/what_are_the_best_resources_to_study_for_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oma86/what_are_the_best_resources_to_study_for_aws/", "subreddit_subscribers": 147695, "created_utc": 1703271689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data engineers, I am at a dilemma.  \nHow do you organize your transformation code?\n\nFor airflow to run spark jobs, it needs access to the .py scripts  \nI would like to keep using Gitlab for managing the code base.\n\nMy initial thought was to build a custom operator that will pull the latest version of the code then use the command spark-submit to run the transformation.\n\nHow did you toggle this problem?", "author_fullname": "t2_j9pcxbcbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Apache Airflow + Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ojyhx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703265519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data engineers, I am at a dilemma.&lt;br/&gt;\nHow do you organize your transformation code?&lt;/p&gt;\n\n&lt;p&gt;For airflow to run spark jobs, it needs access to the .py scripts&lt;br/&gt;\nI would like to keep using Gitlab for managing the code base.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was to build a custom operator that will pull the latest version of the code then use the command spark-submit to run the transformation.&lt;/p&gt;\n\n&lt;p&gt;How did you toggle this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ojyhx", "is_robot_indexable": true, "report_reasons": null, "author": "Human-Failure-99", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ojyhx/help_with_apache_airflow_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ojyhx/help_with_apache_airflow_spark/", "subreddit_subscribers": 147695, "created_utc": 1703265519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Video \ud83e\udd73 Creating Pipelines without Airflow Knowledge \ud83d\ude33", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18oj17p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/uv-SZKjups8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18oj17p", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0YD4vBYIg6hW0KSK0YeJfmgVV0e-0de5UNZVlkRQlgs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703263076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/uv-SZKjups8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?auto=webp&amp;s=b8adcd0145fcb2d8cdb0aafd758c42ffb1ca3aa6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9e4e609b5416d661f1ef191e2b3d5d8c215a289", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed487119963730863ca04759c0cfd159528d036b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc690e5ec6feefab112123e44a213f99bf1d31db", "width": 320, "height": 240}], "variants": {}, "id": "B45PQTstzD0z7ab5oanv9VuTrBUi41DB_w7yZlDT-sA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18oj17p", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oj17p/new_video_creating_pipelines_without_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/uv-SZKjups8", "subreddit_subscribers": 147695, "created_utc": 1703263076.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/uv-SZKjups8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Morning all,\n\nI need some out the box ideas on ways I can set up a process to send us notifications to a Teams channel that outline what pipeline failed and the error message. \n\nAny good ideas on how to do this are welcome. Originally I was thinking Power Automate through a web activity in a pipeline with the POST option however this is proving to be more difficult then I thought. \n\nPlatform I\u2019m using is Azure Synapse.\n\nCheers", "author_fullname": "t2_173udy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Notifications on Pipeline failures - best method?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oberj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703238860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Morning all,&lt;/p&gt;\n\n&lt;p&gt;I need some out the box ideas on ways I can set up a process to send us notifications to a Teams channel that outline what pipeline failed and the error message. &lt;/p&gt;\n\n&lt;p&gt;Any good ideas on how to do this are welcome. Originally I was thinking Power Automate through a web activity in a pipeline with the POST option however this is proving to be more difficult then I thought. &lt;/p&gt;\n\n&lt;p&gt;Platform I\u2019m using is Azure Synapse.&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oberj", "is_robot_indexable": true, "report_reasons": null, "author": "prodigypro", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oberj/notifications_on_pipeline_failures_best_method/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oberj/notifications_on_pipeline_failures_best_method/", "subreddit_subscribers": 147695, "created_utc": 1703238860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just read an article says that building applications on top of a data warehouse is a very bad idea and data warehouses are \u201ca terrible application backend\u201d \n\nthen whats the best approach to build an application? \n\ni have an idea of using \u201cRetool\u201d to build an application just for internal use, and as i know this will be built on top of the data warehouse. \n\nnote: our developers are all engaged in a big project and we\u2019re planning to do it with the IT. trying to help each other. the idea is to build an application for us to monitor the vacations calendar. in certain period who will be on vacation and who will be delighted\u2026 etc we want to automate the process as much we can because this is not going right with us and my manager and I believe this is the solution \ud83e\udd37\ud83c\udffb\u200d\u2640\ufe0f", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building applications on DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ojz9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703265583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just read an article says that building applications on top of a data warehouse is a very bad idea and data warehouses are \u201ca terrible application backend\u201d &lt;/p&gt;\n\n&lt;p&gt;then whats the best approach to build an application? &lt;/p&gt;\n\n&lt;p&gt;i have an idea of using \u201cRetool\u201d to build an application just for internal use, and as i know this will be built on top of the data warehouse. &lt;/p&gt;\n\n&lt;p&gt;note: our developers are all engaged in a big project and we\u2019re planning to do it with the IT. trying to help each other. the idea is to build an application for us to monitor the vacations calendar. in certain period who will be on vacation and who will be delighted\u2026 etc we want to automate the process as much we can because this is not going right with us and my manager and I believe this is the solution \ud83e\udd37\ud83c\udffb\u200d\u2640\ufe0f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ojz9z", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ojz9z/building_applications_on_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ojz9z/building_applications_on_dwh/", "subreddit_subscribers": 147695, "created_utc": 1703265583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I \u201clearned\u201d pySpark as part of my masters\u2018 cloud computing class but I didn\u2019t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark\u2019s. I don\u2019t know what pySpark is really. It isn\u2019t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven\u2019t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.", "author_fullname": "t2_6jnra7z5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Differences between just python and pySpark? What is pySpark even and do I need it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nzg9f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703198939.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703198436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I \u201clearned\u201d pySpark as part of my masters\u2018 cloud computing class but I didn\u2019t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark\u2019s. I don\u2019t know what pySpark is really. It isn\u2019t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven\u2019t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18nzg9f", "is_robot_indexable": true, "report_reasons": null, "author": "Ngachate", "discussion_type": null, "num_comments": 30, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nzg9f/differences_between_just_python_and_pyspark_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nzg9f/differences_between_just_python_and_pyspark_what/", "subreddit_subscribers": 147695, "created_utc": 1703198436.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}