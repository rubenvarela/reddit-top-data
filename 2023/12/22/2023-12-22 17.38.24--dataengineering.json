{"kind": "Listing", "data": {"after": null, "dist": 19, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im a contractor and I was asked by someone at my client's to help out with another project I wasn't originally contracted for. It's an ERP migration project, so I really only have to write SQL statements and extract some data based on the new ERP system's template from the legacy ERP's backend (SQL Server).\n\nI've never done this kind of work before and largely unfamiliar with ERP systems, but since it's mostly just SQL work I thought I could help out. Project Manager gave me a list of fields I need to look for, but there are about 200 tables in the backend. I said I'd like to speak with the finance team managing this legacy ERP system to narrow down my search, and Project Manager says \"well I saw the table names, they all look pretty straightforward, I think you can just look through all those tables\". But I said \"I've never worked on this domain before so I would really appreciate some guidance\". And she didnt' respond.  I tried searching blindly but ended up finding a table with 10 addresses per customer and I don't know which of these are needed etc. \n\nWas I being unprofessional for wanting to ask for help to make sure I'm looking for the data in the right places? ", "author_fullname": "t2_7m2ues2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was I unprofessional when I said I needed help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18of52n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703252357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a contractor and I was asked by someone at my client&amp;#39;s to help out with another project I wasn&amp;#39;t originally contracted for. It&amp;#39;s an ERP migration project, so I really only have to write SQL statements and extract some data based on the new ERP system&amp;#39;s template from the legacy ERP&amp;#39;s backend (SQL Server).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never done this kind of work before and largely unfamiliar with ERP systems, but since it&amp;#39;s mostly just SQL work I thought I could help out. Project Manager gave me a list of fields I need to look for, but there are about 200 tables in the backend. I said I&amp;#39;d like to speak with the finance team managing this legacy ERP system to narrow down my search, and Project Manager says &amp;quot;well I saw the table names, they all look pretty straightforward, I think you can just look through all those tables&amp;quot;. But I said &amp;quot;I&amp;#39;ve never worked on this domain before so I would really appreciate some guidance&amp;quot;. And she didnt&amp;#39; respond.  I tried searching blindly but ended up finding a table with 10 addresses per customer and I don&amp;#39;t know which of these are needed etc. &lt;/p&gt;\n\n&lt;p&gt;Was I being unprofessional for wanting to ask for help to make sure I&amp;#39;m looking for the data in the right places? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18of52n", "is_robot_indexable": true, "report_reasons": null, "author": "DataScienceIsScience", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18of52n/was_i_unprofessional_when_i_said_i_needed_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18of52n/was_i_unprofessional_when_i_said_i_needed_help/", "subreddit_subscribers": 147665, "created_utc": 1703252357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody,\n\nI recently accepted a PhD position focusing on the development of a database solution for the laboratory of a university.\n\nThis lab is characterized by a high turnover of projects and students, with a rich diversity of data that holds the potential for significant insights. However, as of now, there's a notable absence of a structured data warehouse. I want to take the opportunity to set up a 21st century solution.\n\nGiven these circumstances, I'm reaching out to this you guys for advice. I'm interested in learning about:\n\n1. **Best Practices and Considerations:** What are the key factors I should consider when implementing a data warehouse from scratch? Are there specific best practices tailored for high-turnover, diverse data environments?\n2. **Appropriateness of a Data Warehouse:** Is a data warehouse the ideal solution for the lab? Or should we explore other data management systems?\n\n**About Me:** My background includes experience in Database QA and moderate programming skills. However, I'm relatively new to in-depth data management. I'm currently reading \"Fundamentals of Data Engineering\" and actively seeking resources to improve my core SQL skills and overall data handling expertise.\n\nI'd appreciate any insights, resources, or experiences you could share that could serve as a guide.\n\nThanks!", "author_fullname": "t2_ddwwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ground Up Database Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18obiz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703239979.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703239306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,&lt;/p&gt;\n\n&lt;p&gt;I recently accepted a PhD position focusing on the development of a database solution for the laboratory of a university.&lt;/p&gt;\n\n&lt;p&gt;This lab is characterized by a high turnover of projects and students, with a rich diversity of data that holds the potential for significant insights. However, as of now, there&amp;#39;s a notable absence of a structured data warehouse. I want to take the opportunity to set up a 21st century solution.&lt;/p&gt;\n\n&lt;p&gt;Given these circumstances, I&amp;#39;m reaching out to this you guys for advice. I&amp;#39;m interested in learning about:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Best Practices and Considerations:&lt;/strong&gt; What are the key factors I should consider when implementing a data warehouse from scratch? Are there specific best practices tailored for high-turnover, diverse data environments?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Appropriateness of a Data Warehouse:&lt;/strong&gt; Is a data warehouse the ideal solution for the lab? Or should we explore other data management systems?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;About Me:&lt;/strong&gt; My background includes experience in Database QA and moderate programming skills. However, I&amp;#39;m relatively new to in-depth data management. I&amp;#39;m currently reading &amp;quot;Fundamentals of Data Engineering&amp;quot; and actively seeking resources to improve my core SQL skills and overall data handling expertise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate any insights, resources, or experiences you could share that could serve as a guide.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18obiz9", "is_robot_indexable": true, "report_reasons": null, "author": "Jimmyfatz", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18obiz9/ground_up_database_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18obiz9/ground_up_database_design/", "subreddit_subscribers": 147665, "created_utc": 1703239306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi people. So currently my team is running bunch of python scripts on Virtual Machine scheduled on Task Scheduler.\n\nWe dont use github. Just the scripts, some .bat files and task scheduler.\n\nSo im thinking of adding the element of SWE/ or proper way to do this. \n\nI'm looking into GitLab and GitHub but have no actual direction on whats a good practise and whats not. Or what we're doing wrong.\n\nNeed some experts to throw in on how to improve this process. Thanks!", "author_fullname": "t2_7nk0x7fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whats the best practise to automating python scripts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o2zzq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703208284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people. So currently my team is running bunch of python scripts on Virtual Machine scheduled on Task Scheduler.&lt;/p&gt;\n\n&lt;p&gt;We dont use github. Just the scripts, some .bat files and task scheduler.&lt;/p&gt;\n\n&lt;p&gt;So im thinking of adding the element of SWE/ or proper way to do this. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking into GitLab and GitHub but have no actual direction on whats a good practise and whats not. Or what we&amp;#39;re doing wrong.&lt;/p&gt;\n\n&lt;p&gt;Need some experts to throw in on how to improve this process. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o2zzq", "is_robot_indexable": true, "report_reasons": null, "author": "Nopal97", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o2zzq/whats_the_best_practise_to_automating_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o2zzq/whats_the_best_practise_to_automating_python/", "subreddit_subscribers": 147665, "created_utc": 1703208284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our most recent blog on Real-time Change Data Capture from Postgres 16 Read Replicas [https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas](https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas)  \nStart replicating data from Postgres to Data Warehouses, Queues and Storage using Read Replicas instead of Primaries. No worry of additional load or outages of the Primary database.\n\nWhile building this feature, a few of our learnings on logical decoding on Postgres Read Replicas:  \n1\ufe0f\u20e3 CREATE\\_REPLICATION\\_SLOT works as expected  \n2\ufe0f\u20e3 SNAPSHOT can be created and used on Read Replicas  \n3\ufe0f\u20e3 START\\_REPLICATION worked as expected  \n4\ufe0f\u20e3 Publications cannot be created on the Replica. They need to be created on the primary.  \n5\ufe0f\u20e3 WAL control functions cannot run on Read Replicas. Use pg\\_last\\_wal\\_receive\\_lsn() instead of pg\\_current\\_wal\\_lsn()", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Change Data Capture from Postgres 16 Read Replicas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nzdi8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703198239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our most recent blog on Real-time Change Data Capture from Postgres 16 Read Replicas &lt;a href=\"https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas\"&gt;https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas&lt;/a&gt;&lt;br/&gt;\nStart replicating data from Postgres to Data Warehouses, Queues and Storage using Read Replicas instead of Primaries. No worry of additional load or outages of the Primary database.&lt;/p&gt;\n\n&lt;p&gt;While building this feature, a few of our learnings on logical decoding on Postgres Read Replicas:&lt;br/&gt;\n1\ufe0f\u20e3 CREATE_REPLICATION_SLOT works as expected&lt;br/&gt;\n2\ufe0f\u20e3 SNAPSHOT can be created and used on Read Replicas&lt;br/&gt;\n3\ufe0f\u20e3 START_REPLICATION worked as expected&lt;br/&gt;\n4\ufe0f\u20e3 Publications cannot be created on the Replica. They need to be created on the primary.&lt;br/&gt;\n5\ufe0f\u20e3 WAL control functions cannot run on Read Replicas. Use pg_last_wal_receive_lsn() instead of pg_current_wal_lsn()&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?auto=webp&amp;s=2b914bd51e6fe4fee7388cd4feab65531c9c1376", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd79830ce32aad0a840914f23c6003fe0c7c1ae6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a206c8f190043c41f3128dd1b71d29de6c21f112", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d552aa43457c0da8f5013ebe653391036f5332a", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d6d36f451b4354a0aa9dc47d5753f1edfc2a73b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cdfed35c434a9c8f014e7ee8eba77ad545bf114", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec5d413b0521c2eec24d14cf32571b0179e4ef0a", "width": 1080, "height": 567}], "variants": {}, "id": "xCM7b2TzQAsK8mVeahKijq6AT8KMhyAVbQr5BhJjngE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nzdi8", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nzdi8/realtime_change_data_capture_from_postgres_16/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nzdi8/realtime_change_data_capture_from_postgres_16/", "subreddit_subscribers": 147665, "created_utc": 1703198239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a newbie intern in business analytics, and my manager recently wrote in my performance review that he wants me to improve in SQL. I\u2019ve written a few queries in SQL Server throughout the internship, but that\u2019s the extent to which I\u2019ve used it (I\u2019ve mostly worked in Tableau and Excel otherwise).\n\nFor an upcoming project, he said I\u2019ll need to connect Azure Databricks to either SQL Server or DBeaver. I\u2019ve done some online research, but I\u2019m still not understanding what Databricks and DBeaver are. What is the purpose of each, and how do they relate to SQL Server?", "author_fullname": "t2_6zej24pk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5: How do SQL Server, Databricks, and DBeaver work together?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o3fek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703209596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a newbie intern in business analytics, and my manager recently wrote in my performance review that he wants me to improve in SQL. I\u2019ve written a few queries in SQL Server throughout the internship, but that\u2019s the extent to which I\u2019ve used it (I\u2019ve mostly worked in Tableau and Excel otherwise).&lt;/p&gt;\n\n&lt;p&gt;For an upcoming project, he said I\u2019ll need to connect Azure Databricks to either SQL Server or DBeaver. I\u2019ve done some online research, but I\u2019m still not understanding what Databricks and DBeaver are. What is the purpose of each, and how do they relate to SQL Server?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o3fek", "is_robot_indexable": true, "report_reasons": null, "author": "jiminforthewin", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o3fek/eli5_how_do_sql_server_databricks_and_dbeaver/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o3fek/eli5_how_do_sql_server_databricks_and_dbeaver/", "subreddit_subscribers": 147665, "created_utc": 1703209596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nThe company I work at allows for $5000 annual education stipend. Currently I am a data scientist that works mostly on Analytics -- I use SQL 95% of the time, and I can hack my way around Pandas and R with googling (used them both in the past).\n\nI would like to get more technical and would transition into Data Engineering -- I wanted to sign up for a Data Engineering course to learn key softwares and tools that DE teams look for. I've seen several free camps like Zoomcamp and paid courses by MIT -- since I'lll get the course reimbursed, can anyone recommend a good option for a DS trying to switch to DE? I'm having a hard time understanding what a paid course will offer over a free one, but I want to make sure I select the highest quality course since budget is no issue.\n\n&amp;#x200B;\n\nEdit: what works great for me is bigger project style structures. I liked The Odin Project for Web Dev... not a fan of purely video lectures with quizzes like most Coursera courses.", "author_fullname": "t2_923nyofu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paid vs Free Data Engineering courses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o6qap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703246005.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703220266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;The company I work at allows for $5000 annual education stipend. Currently I am a data scientist that works mostly on Analytics -- I use SQL 95% of the time, and I can hack my way around Pandas and R with googling (used them both in the past).&lt;/p&gt;\n\n&lt;p&gt;I would like to get more technical and would transition into Data Engineering -- I wanted to sign up for a Data Engineering course to learn key softwares and tools that DE teams look for. I&amp;#39;ve seen several free camps like Zoomcamp and paid courses by MIT -- since I&amp;#39;lll get the course reimbursed, can anyone recommend a good option for a DS trying to switch to DE? I&amp;#39;m having a hard time understanding what a paid course will offer over a free one, but I want to make sure I select the highest quality course since budget is no issue.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: what works great for me is bigger project style structures. I liked The Odin Project for Web Dev... not a fan of purely video lectures with quizzes like most Coursera courses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18o6qap", "is_robot_indexable": true, "report_reasons": null, "author": "FairAd6062", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o6qap/paid_vs_free_data_engineering_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o6qap/paid_vs_free_data_engineering_courses/", "subreddit_subscribers": 147665, "created_utc": 1703220266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "has anyone worked on both dbt and Delta Live Tables with Databricks? What's the pros/cons?\n\nRecently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline\n\nWe had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo'd us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)", "author_fullname": "t2_ghlgg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: dbt or Delta Live Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ny8tf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703195187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;has anyone worked on both dbt and Delta Live Tables with Databricks? What&amp;#39;s the pros/cons?&lt;/p&gt;\n\n&lt;p&gt;Recently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline&lt;/p&gt;\n\n&lt;p&gt;We had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo&amp;#39;d us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ny8tf", "is_robot_indexable": true, "report_reasons": null, "author": "y45hiro", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ny8tf/databricks_dbt_or_delta_live_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ny8tf/databricks_dbt_or_delta_live_tables/", "subreddit_subscribers": 147665, "created_utc": 1703195187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently develop in Azure where we use ADF, ADLS and Databricks. Currently we have a few environment json files in a parameters container that map credentials to secrets in key vault across dev/uat/prd to reduce hardcoding.\n\nWhen creating a common pipeline I create a pipeline folder in our parameters container and store a config file with constants specific to the trigger. For example this file might could contain a list of tables that we want to extract from sql. This way we can easily add or remove tables to that trigger and reuse the pipeline for other sources and schedules by simply pointing a trigger to a different config file.\n\nMy question is, have I over complicated this? Is anyone else doing anything similar?", "author_fullname": "t2_qc6h84ym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Parameters &amp; Environment Variables in Azure to Enable CICD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oapzd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703235960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently develop in Azure where we use ADF, ADLS and Databricks. Currently we have a few environment json files in a parameters container that map credentials to secrets in key vault across dev/uat/prd to reduce hardcoding.&lt;/p&gt;\n\n&lt;p&gt;When creating a common pipeline I create a pipeline folder in our parameters container and store a config file with constants specific to the trigger. For example this file might could contain a list of tables that we want to extract from sql. This way we can easily add or remove tables to that trigger and reuse the pipeline for other sources and schedules by simply pointing a trigger to a different config file.&lt;/p&gt;\n\n&lt;p&gt;My question is, have I over complicated this? Is anyone else doing anything similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oapzd", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated-Western1788", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oapzd/managing_parameters_environment_variables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oapzd/managing_parameters_environment_variables_in/", "subreddit_subscribers": 147665, "created_utc": 1703235960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My application runs in a AWS service called Glue which is based on spark.\n\nI am reading in millions of AWS S3 objects using spark but by the time it lists all the files and starts to process them some have been deleted by another application.\n\nIs there a way to ignore these? I have tried the flag ignoreMissingFiles to no avail?\n\nAlso, semi related, does spark need to list all files before starting processing? I would have thought there was a way to load each file, process and then save since each file is independent. Is seems like all files have to be loaded first which seems crazy to me. Can I not tell spark to load one at a time?", "author_fullname": "t2_vlw0gd8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to skip non existent S3 objects loading in Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o2a06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703206153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My application runs in a AWS service called Glue which is based on spark.&lt;/p&gt;\n\n&lt;p&gt;I am reading in millions of AWS S3 objects using spark but by the time it lists all the files and starts to process them some have been deleted by another application.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to ignore these? I have tried the flag ignoreMissingFiles to no avail?&lt;/p&gt;\n\n&lt;p&gt;Also, semi related, does spark need to list all files before starting processing? I would have thought there was a way to load each file, process and then save since each file is independent. Is seems like all files have to be loaded first which seems crazy to me. Can I not tell spark to load one at a time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o2a06", "is_robot_indexable": true, "report_reasons": null, "author": "atticusfinch975", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o2a06/how_to_skip_non_existent_s3_objects_loading_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o2a06/how_to_skip_non_existent_s3_objects_loading_in/", "subreddit_subscribers": 147665, "created_utc": 1703206153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u8kebhp6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UCX v0.7.0 by Databricks Labs \u2014 new release with CLI commands", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_18o0v7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1JCa-RXvcaGTvAt8kVlN1TX89yfRYzrOl1z-ECM9Pe8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703202177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/databricks-labs/ucx-v0-7-0-by-databricks-labs-new-release-with-cli-commands-b4ddd0786598", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?auto=webp&amp;s=75068d1c59038b869069b8e2b570835cf4d87d4d", "width": 768, "height": 496}, "resolutions": [{"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b011d28294feecf4496a5702eabac9aeed870ba2", "width": 108, "height": 69}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9337fb1551a33d5aea7eacee28a7e59f1f928fba", "width": 216, "height": 139}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d1ae8366cf3b35c3b97c4abce0c58ed9b760442", "width": 320, "height": 206}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83f79a393771ca5cefcc9ae8c33f59729fa81fa4", "width": 640, "height": 413}], "variants": {}, "id": "lTHuVblV7M4EdxXC6wPm6GrWFFAemY5_V7f-E_jIDNw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18o0v7c", "is_robot_indexable": true, "report_reasons": null, "author": "serge_databricks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o0v7c/ucx_v070_by_databricks_labs_new_release_with_cli/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/databricks-labs/ucx-v0-7-0-by-databricks-labs-new-release-with-cli-commands-b4ddd0786598", "subreddit_subscribers": 147665, "created_utc": 1703202177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does databricks have its own operational database? It primarily plays in the analytical space, right?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nvfch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703187807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nvfch", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nvfch/does_databricks_have_its_own_operational_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nvfch/does_databricks_have_its_own_operational_database/", "subreddit_subscribers": 147665, "created_utc": 1703187807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, if Apache Flink and Stream Processing is your jam, Ververica just pushed all the session videos from Flink Forward Seattle from November.  You can watch them [here](https://www.ververica.academy/app/videos).", "author_fullname": "t2_8k4wqq0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flink Forward 2023 Session Videos are LIVE!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ntdgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703182539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, if Apache Flink and Stream Processing is your jam, Ververica just pushed all the session videos from Flink Forward Seattle from November.  You can watch them &lt;a href=\"https://www.ververica.academy/app/videos\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?auto=webp&amp;s=587f667acd6f8dc058ad81caa52f478370bec78f", "width": 288, "height": 72}, "resolutions": [{"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc40a585a78df51a77b29097e282812a54342421", "width": 108, "height": 27}, {"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7db7b8904146e8109c4834cdfae1b8fb7b494500", "width": 216, "height": 54}], "variants": {}, "id": "YFXgszUcZBc4zgvyAizrbeETuOI6zuEJCxmOQS9yBBM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ntdgo", "is_robot_indexable": true, "report_reasons": null, "author": "wildbreaker", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ntdgo/flink_forward_2023_session_videos_are_live/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ntdgo/flink_forward_2023_session_videos_are_live/", "subreddit_subscribers": 147665, "created_utc": 1703182539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company we have some pipelines that do simple processing on raw data from S3 before storing them back as cleaned and partitioned (by day) datasets on S3, so users can effectively read clean data by ranges of days. Raw and partitioned data is stored as parquet.\n\nRight now this is done with Pandas on Airflow pipelines, running with KubernetesExecutor. This raw data only *occasionally* gets large: we do incremental syncs from data sources, so usually we have 1 larger dataset -- the first sync -- followed by small batches. These small batches tend to be really small on most sources: around 100 records. The larger datasets might grow up to millions of records. I would like to have a solution that scales out to this extraordinary cases, while still being used effectively for the small ones.\n\nFrom what I've seen, Spark has been losing popularity due to the hard work in maintaining the clusters and the need to study and learn it from scratch, while simpler tools rise. Polars, Dask or DuckDB seem to be good alternatives.\n\nWhat are the current low setup, easy to learn, cost effective solutions that scale to this wide range of sizes of data, and their pros and cons?", "author_fullname": "t2_c3hoz70b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to scale pipelines to large datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oftc0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703254296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company we have some pipelines that do simple processing on raw data from S3 before storing them back as cleaned and partitioned (by day) datasets on S3, so users can effectively read clean data by ranges of days. Raw and partitioned data is stored as parquet.&lt;/p&gt;\n\n&lt;p&gt;Right now this is done with Pandas on Airflow pipelines, running with KubernetesExecutor. This raw data only &lt;em&gt;occasionally&lt;/em&gt; gets large: we do incremental syncs from data sources, so usually we have 1 larger dataset -- the first sync -- followed by small batches. These small batches tend to be really small on most sources: around 100 records. The larger datasets might grow up to millions of records. I would like to have a solution that scales out to this extraordinary cases, while still being used effectively for the small ones.&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve seen, Spark has been losing popularity due to the hard work in maintaining the clusters and the need to study and learn it from scratch, while simpler tools rise. Polars, Dask or DuckDB seem to be good alternatives.&lt;/p&gt;\n\n&lt;p&gt;What are the current low setup, easy to learn, cost effective solutions that scale to this wide range of sizes of data, and their pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oftc0", "is_robot_indexable": true, "report_reasons": null, "author": "henriquemeloo_", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oftc0/best_way_to_scale_pipelines_to_large_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oftc0/best_way_to_scale_pipelines_to_large_datasets/", "subreddit_subscribers": 147665, "created_utc": 1703254296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datafusion SQL CLI - Look Ma, I made a new ETL tool.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18nxjfu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wHTKvyWGxYvyFCZodMyJnMQjzmMoZMB_jKzsGGrWrcU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703193326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "confessionsofadataguy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.confessionsofadataguy.com/datafusion-sql-cli-look-ma-i-made-a-new-etl-tool/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?auto=webp&amp;s=41423b348e6ad563cd9ec3c93bb2ef568d601d94", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6770d83ea8f1b52b4202f91465721a61ab368c3c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f52f431ce010bf1d78004cd087fb77ff52933da0", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35c938305c576391e1e83f1d8ba5ae5f653d4605", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30da6f3130dcc719059d8f1e646c19d64c041394", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b7bdb03463a1206d2006d958847620dccc9f170", "width": 960, "height": 960}], "variants": {}, "id": "yA9pk-IMBm6egneN5vdpyDs0g2ml4ezsDBMsp_nGF0w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nxjfu", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nxjfu/datafusion_sql_cli_look_ma_i_made_a_new_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.confessionsofadataguy.com/datafusion-sql-cli-look-ma-i-made-a-new-etl-tool/", "subreddit_subscribers": 147665, "created_utc": 1703193326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I am working on a small project to create a simple real-time pipeline of posts on Reddit. I want to allow the user to query the subreddit name. But then the problem appears, **how should the new query subreddit change be written to the Kafka producer?**\n\n\\- Should I create a new topic for every new subreddit query, although that will *theoretically* create hundreds of thousands of topics (given that there are \\~100k subreddits) and would be a bad practice (although this is not going to happen in my use case)?    \n\\- Should I write everything to the same stream, but use some kind of ID to differentiate between topics? Would this aggregation of data make it harder to process the stream? Since it is relatively large now, my stream processor (e.g. Spark) will have to process everything just to select the rows with the intended subreddit name.\n\nI am quite stuck here and would appreciate any insight on this, thank you so much.", "author_fullname": "t2_38msj796", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategy to approach query-based Kafka producer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oefke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703250871.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703250152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I am working on a small project to create a simple real-time pipeline of posts on Reddit. I want to allow the user to query the subreddit name. But then the problem appears, &lt;strong&gt;how should the new query subreddit change be written to the Kafka producer?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Should I create a new topic for every new subreddit query, although that will &lt;em&gt;theoretically&lt;/em&gt; create hundreds of thousands of topics (given that there are ~100k subreddits) and would be a bad practice (although this is not going to happen in my use case)?&lt;br/&gt;\n- Should I write everything to the same stream, but use some kind of ID to differentiate between topics? Would this aggregation of data make it harder to process the stream? Since it is relatively large now, my stream processor (e.g. Spark) will have to process everything just to select the rows with the intended subreddit name.&lt;/p&gt;\n\n&lt;p&gt;I am quite stuck here and would appreciate any insight on this, thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oefke", "is_robot_indexable": true, "report_reasons": null, "author": "Babe_My_Name_Is_Hung", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oefke/strategy_to_approach_querybased_kafka_producer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oefke/strategy_to_approach_querybased_kafka_producer/", "subreddit_subscribers": 147665, "created_utc": 1703250152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Video \ud83e\udd73 Creating Pipelines without Airflow Knowledge \ud83d\ude33", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_18oj17p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/uv-SZKjups8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18oj17p", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0YD4vBYIg6hW0KSK0YeJfmgVV0e-0de5UNZVlkRQlgs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703263076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/uv-SZKjups8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?auto=webp&amp;s=b8adcd0145fcb2d8cdb0aafd758c42ffb1ca3aa6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9e4e609b5416d661f1ef191e2b3d5d8c215a289", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed487119963730863ca04759c0cfd159528d036b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc690e5ec6feefab112123e44a213f99bf1d31db", "width": 320, "height": 240}], "variants": {}, "id": "B45PQTstzD0z7ab5oanv9VuTrBUi41DB_w7yZlDT-sA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18oj17p", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oj17p/new_video_creating_pipelines_without_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/uv-SZKjups8", "subreddit_subscribers": 147665, "created_utc": 1703263076.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/uv-SZKjups8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Morning all,\n\nI need some out the box ideas on ways I can set up a process to send us notifications to a Teams channel that outline what pipeline failed and the error message. \n\nAny good ideas on how to do this are welcome. Originally I was thinking Power Automate through a web activity in a pipeline with the POST option however this is proving to be more difficult then I thought. \n\nPlatform I\u2019m using is Azure Synapse.\n\nCheers", "author_fullname": "t2_173udy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Notifications on Pipeline failures - best method?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oberj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703238860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Morning all,&lt;/p&gt;\n\n&lt;p&gt;I need some out the box ideas on ways I can set up a process to send us notifications to a Teams channel that outline what pipeline failed and the error message. &lt;/p&gt;\n\n&lt;p&gt;Any good ideas on how to do this are welcome. Originally I was thinking Power Automate through a web activity in a pipeline with the POST option however this is proving to be more difficult then I thought. &lt;/p&gt;\n\n&lt;p&gt;Platform I\u2019m using is Azure Synapse.&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oberj", "is_robot_indexable": true, "report_reasons": null, "author": "prodigypro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oberj/notifications_on_pipeline_failures_best_method/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oberj/notifications_on_pipeline_failures_best_method/", "subreddit_subscribers": 147665, "created_utc": 1703238860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4s3pxn3ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source: Apache Flink Snowflake Connector", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18nu9le", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kLq3s-CLXUH2VOW7K8zgUhCQUC3JJ3SaTTnBE9UouDw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703184848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/deltastreaminc/flink-connector-snowflake", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?auto=webp&amp;s=ede12520963f3fd8dfac3d1c268b1a96d24aa28b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=94e9e8dbccb6777b133732cf88f0a5ddd24f82f6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=42b7c35aa1a79b17830e1a332d86ea5f7274c542", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ecd5cff96be211c37660d91770726f11999c9b4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d811589af1fa84d7df29106efdee027355e87b8e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f1c2b48e3bd28a1f71b9f98069b886ae9f72ee5b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1924e218a53fcc3a1dd802c00d16c3bf4ab257ec", "width": 1080, "height": 540}], "variants": {}, "id": "RGBgY8rhE9yMTve-s0JAWriC0RRtN-gWb_G5M5rDRuI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18nu9le", "is_robot_indexable": true, "report_reasons": null, "author": "DeltaStream_io", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nu9le/open_source_apache_flink_snowflake_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/deltastreaminc/flink-connector-snowflake", "subreddit_subscribers": 147665, "created_utc": 1703184848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I \u201clearned\u201d pySpark as part of my masters\u2018 cloud computing class but I didn\u2019t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark\u2019s. I don\u2019t know what pySpark is really. It isn\u2019t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven\u2019t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.", "author_fullname": "t2_6jnra7z5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Differences between just python and pySpark? What is pySpark even and do I need it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nzg9f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703198939.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703198436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I \u201clearned\u201d pySpark as part of my masters\u2018 cloud computing class but I didn\u2019t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark\u2019s. I don\u2019t know what pySpark is really. It isn\u2019t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven\u2019t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18nzg9f", "is_robot_indexable": true, "report_reasons": null, "author": "Ngachate", "discussion_type": null, "num_comments": 30, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nzg9f/differences_between_just_python_and_pyspark_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nzg9f/differences_between_just_python_and_pyspark_what/", "subreddit_subscribers": 147665, "created_utc": 1703198436.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}