{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I thought I was an intermediate user of SQL until I started this current job. In the past, I just did simple joins, translations using coalesque, trims, uppercase, CTE things like that. Occasionally I'd use a window function. I was using BigQuery, SQL server, DB2. \n\n\n**My current company is astronomically different**. I am just a data analyst. We use a flavor of SQL called Teradata. when we are starting a massive project with a billion rows of data, we pull each section of data into smaller pieces using volatile table. So for example, employees are put into an employee volatile table, orders are put into their own volatile table. Then we merge the volatile tables together and restrict them using where clauses for region, department, if we are looking at a specific hierarchy under a single VP.\n\n\nThe part that really gets me is that we are not allowed to use window functions or CTE. Everything has to be a temporary table. Then, after it's all done, we just select from the temporary table and insert it into an actual database table that we create. Keep in mind that I am just a data analyst, so I'm doing some pretty hardcore transformations and ETL, and creating my own tables, updating tables, having to manage primary indexes, query optimization and table enhancement... It's pretty crazy. \n\n\nWhat I want to know though is why are CTE's bad and tempt tables are the solution to everything?", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My company's ETL process is extremely confusing to me", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nmv06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703164777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought I was an intermediate user of SQL until I started this current job. In the past, I just did simple joins, translations using coalesque, trims, uppercase, CTE things like that. Occasionally I&amp;#39;d use a window function. I was using BigQuery, SQL server, DB2. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My current company is astronomically different&lt;/strong&gt;. I am just a data analyst. We use a flavor of SQL called Teradata. when we are starting a massive project with a billion rows of data, we pull each section of data into smaller pieces using volatile table. So for example, employees are put into an employee volatile table, orders are put into their own volatile table. Then we merge the volatile tables together and restrict them using where clauses for region, department, if we are looking at a specific hierarchy under a single VP.&lt;/p&gt;\n\n&lt;p&gt;The part that really gets me is that we are not allowed to use window functions or CTE. Everything has to be a temporary table. Then, after it&amp;#39;s all done, we just select from the temporary table and insert it into an actual database table that we create. Keep in mind that I am just a data analyst, so I&amp;#39;m doing some pretty hardcore transformations and ETL, and creating my own tables, updating tables, having to manage primary indexes, query optimization and table enhancement... It&amp;#39;s pretty crazy. &lt;/p&gt;\n\n&lt;p&gt;What I want to know though is why are CTE&amp;#39;s bad and tempt tables are the solution to everything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nmv06", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nmv06/my_companys_etl_process_is_extremely_confusing_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nmv06/my_companys_etl_process_is_extremely_confusing_to/", "subreddit_subscribers": 147598, "created_utc": 1703164777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our most recent blog on Real-time Change Data Capture from Postgres 16 Read Replicas [https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas](https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas)  \nStart replicating data from Postgres to Data Warehouses, Queues and Storage using Read Replicas instead of Primaries. No worry of additional load or outages of the Primary database.\n\nWhile building this feature, a few of our learnings on logical decoding on Postgres Read Replicas:  \n1\ufe0f\u20e3 CREATE\\_REPLICATION\\_SLOT works as expected  \n2\ufe0f\u20e3 SNAPSHOT can be created and used on Read Replicas  \n3\ufe0f\u20e3 START\\_REPLICATION worked as expected  \n4\ufe0f\u20e3 Publications cannot be created on the Replica. They need to be created on the primary.  \n5\ufe0f\u20e3 WAL control functions cannot run on Read Replicas. Use pg\\_last\\_wal\\_receive\\_lsn() instead of pg\\_current\\_wal\\_lsn()", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Change Data Capture from Postgres 16 Read Replicas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nzdi8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703198239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our most recent blog on Real-time Change Data Capture from Postgres 16 Read Replicas &lt;a href=\"https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas\"&gt;https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas&lt;/a&gt;&lt;br/&gt;\nStart replicating data from Postgres to Data Warehouses, Queues and Storage using Read Replicas instead of Primaries. No worry of additional load or outages of the Primary database.&lt;/p&gt;\n\n&lt;p&gt;While building this feature, a few of our learnings on logical decoding on Postgres Read Replicas:&lt;br/&gt;\n1\ufe0f\u20e3 CREATE_REPLICATION_SLOT works as expected&lt;br/&gt;\n2\ufe0f\u20e3 SNAPSHOT can be created and used on Read Replicas&lt;br/&gt;\n3\ufe0f\u20e3 START_REPLICATION worked as expected&lt;br/&gt;\n4\ufe0f\u20e3 Publications cannot be created on the Replica. They need to be created on the primary.&lt;br/&gt;\n5\ufe0f\u20e3 WAL control functions cannot run on Read Replicas. Use pg_last_wal_receive_lsn() instead of pg_current_wal_lsn()&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?auto=webp&amp;s=2b914bd51e6fe4fee7388cd4feab65531c9c1376", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd79830ce32aad0a840914f23c6003fe0c7c1ae6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a206c8f190043c41f3128dd1b71d29de6c21f112", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d552aa43457c0da8f5013ebe653391036f5332a", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d6d36f451b4354a0aa9dc47d5753f1edfc2a73b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cdfed35c434a9c8f014e7ee8eba77ad545bf114", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec5d413b0521c2eec24d14cf32571b0179e4ef0a", "width": 1080, "height": 567}], "variants": {}, "id": "xCM7b2TzQAsK8mVeahKijq6AT8KMhyAVbQr5BhJjngE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nzdi8", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nzdi8/realtime_change_data_capture_from_postgres_16/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nzdi8/realtime_change_data_capture_from_postgres_16/", "subreddit_subscribers": 147598, "created_utc": 1703198239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi people. So currently my team is running bunch of python scripts on Virtual Machine scheduled on Task Scheduler.\n\nWe dont use github. Just the scripts, some .bat files and task scheduler.\n\nSo im thinking of adding the element of SWE/ or proper way to do this. \n\nI'm looking into GitLab and GitHub but have no actual direction on whats a good practise and whats not. Or what we're doing wrong.\n\nNeed some experts to throw in on how to improve this process. Thanks!", "author_fullname": "t2_7nk0x7fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whats the best practise to automating python scripts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o2zzq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703208284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people. So currently my team is running bunch of python scripts on Virtual Machine scheduled on Task Scheduler.&lt;/p&gt;\n\n&lt;p&gt;We dont use github. Just the scripts, some .bat files and task scheduler.&lt;/p&gt;\n\n&lt;p&gt;So im thinking of adding the element of SWE/ or proper way to do this. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking into GitLab and GitHub but have no actual direction on whats a good practise and whats not. Or what we&amp;#39;re doing wrong.&lt;/p&gt;\n\n&lt;p&gt;Need some experts to throw in on how to improve this process. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o2zzq", "is_robot_indexable": true, "report_reasons": null, "author": "Nopal97", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o2zzq/whats_the_best_practise_to_automating_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o2zzq/whats_the_best_practise_to_automating_python/", "subreddit_subscribers": 147598, "created_utc": 1703208284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a newbie intern in business analytics, and my manager recently wrote in my performance review that he wants me to improve in SQL. I\u2019ve written a few queries in SQL Server throughout the internship, but that\u2019s the extent to which I\u2019ve used it (I\u2019ve mostly worked in Tableau and Excel otherwise).\n\nFor an upcoming project, he said I\u2019ll need to connect Azure Databricks to either SQL Server or DBeaver. I\u2019ve done some online research, but I\u2019m still not understanding what Databricks and DBeaver are. What is the purpose of each, and how do they relate to SQL Server?", "author_fullname": "t2_6zej24pk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5: How do SQL Server, Databricks, and DBeaver work together?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o3fek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703209596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a newbie intern in business analytics, and my manager recently wrote in my performance review that he wants me to improve in SQL. I\u2019ve written a few queries in SQL Server throughout the internship, but that\u2019s the extent to which I\u2019ve used it (I\u2019ve mostly worked in Tableau and Excel otherwise).&lt;/p&gt;\n\n&lt;p&gt;For an upcoming project, he said I\u2019ll need to connect Azure Databricks to either SQL Server or DBeaver. I\u2019ve done some online research, but I\u2019m still not understanding what Databricks and DBeaver are. What is the purpose of each, and how do they relate to SQL Server?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o3fek", "is_robot_indexable": true, "report_reasons": null, "author": "jiminforthewin", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o3fek/eli5_how_do_sql_server_databricks_and_dbeaver/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o3fek/eli5_how_do_sql_server_databricks_and_dbeaver/", "subreddit_subscribers": 147598, "created_utc": 1703209596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My application runs in a AWS service called Glue which is based on spark.\n\nI am reading in millions of AWS S3 objects using spark but by the time it lists all the files and starts to process them some have been deleted by another application.\n\nIs there a way to ignore these? I have tried the flag ignoreMissingFiles to no avail?\n\nAlso, semi related, does spark need to list all files before starting processing? I would have thought there was a way to load each file, process and then save since each file is independent. Is seems like all files have to be loaded first which seems crazy to me. Can I not tell spark to load one at a time?", "author_fullname": "t2_vlw0gd8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to skip non existent S3 objects loading in Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o2a06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703206153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My application runs in a AWS service called Glue which is based on spark.&lt;/p&gt;\n\n&lt;p&gt;I am reading in millions of AWS S3 objects using spark but by the time it lists all the files and starts to process them some have been deleted by another application.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to ignore these? I have tried the flag ignoreMissingFiles to no avail?&lt;/p&gt;\n\n&lt;p&gt;Also, semi related, does spark need to list all files before starting processing? I would have thought there was a way to load each file, process and then save since each file is independent. Is seems like all files have to be loaded first which seems crazy to me. Can I not tell spark to load one at a time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18o2a06", "is_robot_indexable": true, "report_reasons": null, "author": "atticusfinch975", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o2a06/how_to_skip_non_existent_s3_objects_loading_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o2a06/how_to_skip_non_existent_s3_objects_loading_in/", "subreddit_subscribers": 147598, "created_utc": 1703206153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "has anyone worked on both dbt and Delta Live Tables with Databricks? What's the pros/cons?\n\nRecently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline\n\nWe had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo'd us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)", "author_fullname": "t2_ghlgg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: dbt or Delta Live Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ny8tf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703195187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;has anyone worked on both dbt and Delta Live Tables with Databricks? What&amp;#39;s the pros/cons?&lt;/p&gt;\n\n&lt;p&gt;Recently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline&lt;/p&gt;\n\n&lt;p&gt;We had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo&amp;#39;d us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ny8tf", "is_robot_indexable": true, "report_reasons": null, "author": "y45hiro", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ny8tf/databricks_dbt_or_delta_live_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ny8tf/databricks_dbt_or_delta_live_tables/", "subreddit_subscribers": 147598, "created_utc": 1703195187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, the company where I am working currently, are offering AWS Certifications, for their cloud related projects. And I have showed interest in doing that. As my plan is to do the certifications, get hands-on with some projects, meanwhile learning other data engineering related skills on my own, and then make a transition towards a data engineering role. \n\nIs this a good approach? What do you guys think?", "author_fullname": "t2_pt8qqht7l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can a Cloud Engineer with Solution Architect Associate Certification, get a job as a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nkbl0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703155850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, the company where I am working currently, are offering AWS Certifications, for their cloud related projects. And I have showed interest in doing that. As my plan is to do the certifications, get hands-on with some projects, meanwhile learning other data engineering related skills on my own, and then make a transition towards a data engineering role. &lt;/p&gt;\n\n&lt;p&gt;Is this a good approach? What do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nkbl0", "is_robot_indexable": true, "report_reasons": null, "author": "Abdullah6600", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nkbl0/can_a_cloud_engineer_with_solution_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nkbl0/can_a_cloud_engineer_with_solution_architect/", "subreddit_subscribers": 147598, "created_utc": 1703155850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u8kebhp6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UCX v0.7.0 by Databricks Labs \u2014 new release with CLI commands", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_18o0v7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1JCa-RXvcaGTvAt8kVlN1TX89yfRYzrOl1z-ECM9Pe8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703202177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/databricks-labs/ucx-v0-7-0-by-databricks-labs-new-release-with-cli-commands-b4ddd0786598", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?auto=webp&amp;s=75068d1c59038b869069b8e2b570835cf4d87d4d", "width": 768, "height": 496}, "resolutions": [{"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b011d28294feecf4496a5702eabac9aeed870ba2", "width": 108, "height": 69}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9337fb1551a33d5aea7eacee28a7e59f1f928fba", "width": 216, "height": 139}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d1ae8366cf3b35c3b97c4abce0c58ed9b760442", "width": 320, "height": 206}, {"url": "https://external-preview.redd.it/kYwulMP_vwupdyjWBWhDH3PAGlWBoEqyRuRkfVrYkXI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83f79a393771ca5cefcc9ae8c33f59729fa81fa4", "width": 640, "height": 413}], "variants": {}, "id": "lTHuVblV7M4EdxXC6wPm6GrWFFAemY5_V7f-E_jIDNw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18o0v7c", "is_robot_indexable": true, "report_reasons": null, "author": "serge_databricks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o0v7c/ucx_v070_by_databricks_labs_new_release_with_cli/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/databricks-labs/ucx-v0-7-0-by-databricks-labs-new-release-with-cli-commands-b4ddd0786598", "subreddit_subscribers": 147598, "created_utc": 1703202177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does databricks have its own operational database? It primarily plays in the analytical space, right?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nvfch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703187807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nvfch", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nvfch/does_databricks_have_its_own_operational_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nvfch/does_databricks_have_its_own_operational_database/", "subreddit_subscribers": 147598, "created_utc": 1703187807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! \n\nThe company I work at allows for $5000 annual education stipend. Currently I am a data scientist that works mostly on Analytics -- I use SQL 95% of the time, and I can hack my way around Pandas and R with googling (used them both in the past).   \n\n\nI would like to get more technical and would transition into Data Engineering -- I wanted to sign up for a Data Engineering course to learn key softwares and tools that DE teams look for. I've seen several free camps like Zoomcamp and paid courses by MIT -- since I'lll get the course reimbursed, can anyone recommend a good option for a DS trying to switch to DE? I'm having a hard time understanding what a paid course will offer over a free one, but I want to make sure I select the highest quality course since budget is no issue.", "author_fullname": "t2_923nyofu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paid vs Free Data Engineering courses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18o6qap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703220266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;The company I work at allows for $5000 annual education stipend. Currently I am a data scientist that works mostly on Analytics -- I use SQL 95% of the time, and I can hack my way around Pandas and R with googling (used them both in the past).   &lt;/p&gt;\n\n&lt;p&gt;I would like to get more technical and would transition into Data Engineering -- I wanted to sign up for a Data Engineering course to learn key softwares and tools that DE teams look for. I&amp;#39;ve seen several free camps like Zoomcamp and paid courses by MIT -- since I&amp;#39;lll get the course reimbursed, can anyone recommend a good option for a DS trying to switch to DE? I&amp;#39;m having a hard time understanding what a paid course will offer over a free one, but I want to make sure I select the highest quality course since budget is no issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18o6qap", "is_robot_indexable": true, "report_reasons": null, "author": "FairAd6062", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18o6qap/paid_vs_free_data_engineering_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18o6qap/paid_vs_free_data_engineering_courses/", "subreddit_subscribers": 147598, "created_utc": 1703220266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datafusion SQL CLI - Look Ma, I made a new ETL tool.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18nxjfu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wHTKvyWGxYvyFCZodMyJnMQjzmMoZMB_jKzsGGrWrcU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703193326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "confessionsofadataguy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.confessionsofadataguy.com/datafusion-sql-cli-look-ma-i-made-a-new-etl-tool/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?auto=webp&amp;s=41423b348e6ad563cd9ec3c93bb2ef568d601d94", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6770d83ea8f1b52b4202f91465721a61ab368c3c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f52f431ce010bf1d78004cd087fb77ff52933da0", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35c938305c576391e1e83f1d8ba5ae5f653d4605", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30da6f3130dcc719059d8f1e646c19d64c041394", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b7bdb03463a1206d2006d958847620dccc9f170", "width": 960, "height": 960}], "variants": {}, "id": "yA9pk-IMBm6egneN5vdpyDs0g2ml4ezsDBMsp_nGF0w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nxjfu", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nxjfu/datafusion_sql_cli_look_ma_i_made_a_new_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.confessionsofadataguy.com/datafusion-sql-cli-look-ma-i-made-a-new-etl-tool/", "subreddit_subscribers": 147598, "created_utc": 1703193326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, if Apache Flink and Stream Processing is your jam, Ververica just pushed all the session videos from Flink Forward Seattle from November.  You can watch them [here](https://www.ververica.academy/app/videos).", "author_fullname": "t2_8k4wqq0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flink Forward 2023 Session Videos are LIVE!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ntdgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703182539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, if Apache Flink and Stream Processing is your jam, Ververica just pushed all the session videos from Flink Forward Seattle from November.  You can watch them &lt;a href=\"https://www.ververica.academy/app/videos\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?auto=webp&amp;s=587f667acd6f8dc058ad81caa52f478370bec78f", "width": 288, "height": 72}, "resolutions": [{"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc40a585a78df51a77b29097e282812a54342421", "width": 108, "height": 27}, {"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7db7b8904146e8109c4834cdfae1b8fb7b494500", "width": 216, "height": 54}], "variants": {}, "id": "YFXgszUcZBc4zgvyAizrbeETuOI6zuEJCxmOQS9yBBM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ntdgo", "is_robot_indexable": true, "report_reasons": null, "author": "wildbreaker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ntdgo/flink_forward_2023_session_videos_are_live/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ntdgo/flink_forward_2023_session_videos_are_live/", "subreddit_subscribers": 147598, "created_utc": 1703182539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, Parquet is well known for allowing projection and predicate pushdown. I know most parquet readers implement them. However, parquet readers read the data. What is I just want to reorganize the files based on predicates and columns, without reading them... is that possible?  \n\nFor example, suppose we have 2 files and I want to reorganize them into 5 smaller files that each will contain a subset of rows and a subset of columns.\n\nIf I use a parquet reader, I will need to read, reorganize in memory and write as I wish. I'll benefit from pushdown, but still will need to read. But considering I don't need to work on the data, only to reorganize it in different files based on predicates and columns, if there is a way to do so better, it will save a lot of IO and memory. \n\nAppreciate any resource.", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to reorganize parquet files with predicate and projections pushdown?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nravf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703177109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, Parquet is well known for allowing projection and predicate pushdown. I know most parquet readers implement them. However, parquet readers read the data. What is I just want to reorganize the files based on predicates and columns, without reading them... is that possible?  &lt;/p&gt;\n\n&lt;p&gt;For example, suppose we have 2 files and I want to reorganize them into 5 smaller files that each will contain a subset of rows and a subset of columns.&lt;/p&gt;\n\n&lt;p&gt;If I use a parquet reader, I will need to read, reorganize in memory and write as I wish. I&amp;#39;ll benefit from pushdown, but still will need to read. But considering I don&amp;#39;t need to work on the data, only to reorganize it in different files based on predicates and columns, if there is a way to do so better, it will save a lot of IO and memory. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any resource.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18nravf", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nravf/how_to_reorganize_parquet_files_with_predicate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nravf/how_to_reorganize_parquet_files_with_predicate/", "subreddit_subscribers": 147598, "created_utc": 1703177109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial](https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=a4ddbb06ac319a3328da49a7ee41596b857b9b9a)\n\n[Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial](https://spatial-dev.guru/2023/12/17/exploring-3d-terrain-visualization-with-python-a-dem-and-pyvista-tutorial/)", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 61, "top_awarded_type": null, "hide_score": false, "media_metadata": {"j5mpe1ek1o7c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb749d4d6b158b3920467e93ca12992f0ebe54d0"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06bd8bb9610be8fb37ad4db5ae52175c6accc8b1"}, {"y": 139, "x": 320, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=70c9f275871fd8d7b414b8a457b917c1ec87d7a4"}, {"y": 279, "x": 640, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=220c4b38d1bb2f118bc5020c2d6ad8a344363aab"}, {"y": 419, "x": 960, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=750caa82e1f98abf161376941f287f071430b39a"}], "s": {"y": 447, "x": 1024, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=a4ddbb06ac319a3328da49a7ee41596b857b9b9a"}, "id": "j5mpe1ek1o7c1"}}, "name": "t3_18npho9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FEzntdzYQD8bvhSuHxJuzmCzLpmmw6MzWeeVZq1FV8c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1703172273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a4ddbb06ac319a3328da49a7ee41596b857b9b9a\"&gt;Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2023/12/17/exploring-3d-terrain-visualization-with-python-a-dem-and-pyvista-tutorial/\"&gt;Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?auto=webp&amp;s=d716dbc723271fd0bd0c981e2d46b69434e0ebaa", "width": 1023, "height": 447}, "resolutions": [{"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b88e41bb4f458332ca9644d2332e8b253147d295", "width": 108, "height": 47}, {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2efd502ef60fe672163b69d418a7e64f55de8c6c", "width": 216, "height": 94}, {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60b06a376720168c101f25c2f3b2e41b9bbfd6cb", "width": 320, "height": 139}, {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69ccf8a73d405e774792ae0ce5b5d75d7631e0a5", "width": 640, "height": 279}, {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3dc3567266d66203f81a095e60bb56ad5d0467f6", "width": 960, "height": 419}], "variants": {}, "id": "u9N9iTbyonfgAIhUyGLDZmp072_g2o3C4fBK-A8sQEc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18npho9", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18npho9/exploring_3d_terrain_visualization_with_python_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18npho9/exploring_3d_terrain_visualization_with_python_a/", "subreddit_subscribers": 147598, "created_utc": 1703172273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4s3pxn3ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source: Apache Flink Snowflake Connector", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18nu9le", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kLq3s-CLXUH2VOW7K8zgUhCQUC3JJ3SaTTnBE9UouDw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703184848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/deltastreaminc/flink-connector-snowflake", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?auto=webp&amp;s=ede12520963f3fd8dfac3d1c268b1a96d24aa28b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=94e9e8dbccb6777b133732cf88f0a5ddd24f82f6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=42b7c35aa1a79b17830e1a332d86ea5f7274c542", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ecd5cff96be211c37660d91770726f11999c9b4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d811589af1fa84d7df29106efdee027355e87b8e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f1c2b48e3bd28a1f71b9f98069b886ae9f72ee5b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1924e218a53fcc3a1dd802c00d16c3bf4ab257ec", "width": 1080, "height": 540}], "variants": {}, "id": "RGBgY8rhE9yMTve-s0JAWriC0RRtN-gWb_G5M5rDRuI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18nu9le", "is_robot_indexable": true, "report_reasons": null, "author": "DeltaStream_io", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nu9le/open_source_apache_flink_snowflake_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/deltastreaminc/flink-connector-snowflake", "subreddit_subscribers": 147598, "created_utc": 1703184848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " The  decimator is a function that removes points in the plot while keeping  all the \"value/information\" of a chart. The post features examples with  times series and clustering.\n\n[https://www.taipy.io/posts/big-data-charting-strategies-in-python](https://www.taipy.io/posts/big-data-charting-strategies-in-python)", "author_fullname": "t2_tfe7ylgn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plotting Big Data in Python with the Decimator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nqchi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703174552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The  decimator is a function that removes points in the plot while keeping  all the &amp;quot;value/information&amp;quot; of a chart. The post features examples with  times series and clustering.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.taipy.io/posts/big-data-charting-strategies-in-python\"&gt;https://www.taipy.io/posts/big-data-charting-strategies-in-python&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nqchi", "is_robot_indexable": true, "report_reasons": null, "author": "quicklyalienated76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nqchi/plotting_big_data_in_python_with_the_decimator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nqchi/plotting_big_data_in_python_with_the_decimator/", "subreddit_subscribers": 147598, "created_utc": 1703174552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I \u201clearned\u201d pySpark as part of my masters\u2018 cloud computing class but I didn\u2019t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark\u2019s. I don\u2019t know what pySpark is really. It isn\u2019t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven\u2019t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.", "author_fullname": "t2_6jnra7z5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Differences between just python and pySpark? What is pySpark even and do I need it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nzg9f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703198939.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703198436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I \u201clearned\u201d pySpark as part of my masters\u2018 cloud computing class but I didn\u2019t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark\u2019s. I don\u2019t know what pySpark is really. It isn\u2019t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven\u2019t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18nzg9f", "is_robot_indexable": true, "report_reasons": null, "author": "Ngachate", "discussion_type": null, "num_comments": 26, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nzg9f/differences_between_just_python_and_pyspark_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nzg9f/differences_between_just_python_and_pyspark_what/", "subreddit_subscribers": 147598, "created_utc": 1703198436.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}