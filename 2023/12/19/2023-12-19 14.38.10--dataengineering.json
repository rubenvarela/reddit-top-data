{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!\n\n[https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra\\_5PGH&amp;index=6&amp;t=689s](https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;index=6&amp;t=689s)", "author_fullname": "t2_me12im5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lsb9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702957460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;amp;index=6&amp;amp;t=689s\"&gt;https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;amp;index=6&amp;amp;t=689s&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?auto=webp&amp;s=7297230f4639915c7e57af7b9a255708da17bfa9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=784a54eeca77266e76144dcc7477cdae8da54cbb", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76e755d0518dfef7107139a615d64e1857715e31", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bcd0428c65436c58727868ae2eae5a9b4a6d541d", "width": 320, "height": 240}], "variants": {}, "id": "_rVKQNaxcPqL1qZYn-JkINnf7oHCvdPuQq4k6I3ej4A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18lsb9p", "is_robot_indexable": true, "report_reasons": null, "author": "onurbaltaci", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lsb9p/i_recorded_a_crash_course_on_polars_library_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lsb9p/i_recorded_a_crash_course_on_polars_library_of/", "subreddit_subscribers": 146959, "created_utc": 1702957460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in charge of, or rather I volunteered to create the documentation that will be the foundation for the development of data pipelines and ETL processes using Python. Part of that documentation includes establishing the configuration/environment used for development. (We currently use SSIS and SQL Server for our pipelines)\n\nI will be using Visual Studio Code and Python 3.\n\nWhat extensions would you recommend that would help with Data Engineering with Python?\n\nIn addition, what extensions would help with other necessary steps in development, such as Version Control, syntax (PEP8), and Testing?", "author_fullname": "t2_aiumhm29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python for Data Engineering VSCode Extensions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lcjqp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702916251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in charge of, or rather I volunteered to create the documentation that will be the foundation for the development of data pipelines and ETL processes using Python. Part of that documentation includes establishing the configuration/environment used for development. (We currently use SSIS and SQL Server for our pipelines)&lt;/p&gt;\n\n&lt;p&gt;I will be using Visual Studio Code and Python 3.&lt;/p&gt;\n\n&lt;p&gt;What extensions would you recommend that would help with Data Engineering with Python?&lt;/p&gt;\n\n&lt;p&gt;In addition, what extensions would help with other necessary steps in development, such as Version Control, syntax (PEP8), and Testing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lcjqp", "is_robot_indexable": true, "report_reasons": null, "author": "Upbeat_Count_7568", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lcjqp/python_for_data_engineering_vscode_extensions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lcjqp/python_for_data_engineering_vscode_extensions/", "subreddit_subscribers": 146959, "created_utc": 1702916251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious as to where Databricks SQL and Warehouses actually fit into the lakehouse medallion architecture? Databricks material says that Databricks SQL is used for data warehousing on the lakehouse. Great, but it really just seems like Databricks SQL is designed for analysts who just want to query existing tables and objects to support a report. However, when I think of \"data warehousing\", I think of transforming raw data into dimensional models to support analysts and their queries.\n\nDoes Databricks just refer to \"Data Warehousing\" as the ability to query tables in the lakehouse and provide an endpoint for BI tools? That doesn't really seem like \"data warehousing\" to me, but rather just analytics. or am I wrong here?\n\nLets assume DBMS -&gt; Landing (Ephemeral) -&gt; Bronze (CDC Appends) -&gt; Silver (Integrated 3NF) -&gt; Gold (facts &amp; dimensions). What is exactly is the role of Databricks SQL and Warehouses here? Is Databricks SQL just being used as an interactive query engine for analysts that want to write ad-hoc SQL queries on top of silver or gold tables? Or does Databricks SQL have a larger role to be used by BI Developers to build and update the Fact and Dimensions in Gold?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks SQL and Warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lery0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702921811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious as to where Databricks SQL and Warehouses actually fit into the lakehouse medallion architecture? Databricks material says that Databricks SQL is used for data warehousing on the lakehouse. Great, but it really just seems like Databricks SQL is designed for analysts who just want to query existing tables and objects to support a report. However, when I think of &amp;quot;data warehousing&amp;quot;, I think of transforming raw data into dimensional models to support analysts and their queries.&lt;/p&gt;\n\n&lt;p&gt;Does Databricks just refer to &amp;quot;Data Warehousing&amp;quot; as the ability to query tables in the lakehouse and provide an endpoint for BI tools? That doesn&amp;#39;t really seem like &amp;quot;data warehousing&amp;quot; to me, but rather just analytics. or am I wrong here?&lt;/p&gt;\n\n&lt;p&gt;Lets assume DBMS -&amp;gt; Landing (Ephemeral) -&amp;gt; Bronze (CDC Appends) -&amp;gt; Silver (Integrated 3NF) -&amp;gt; Gold (facts &amp;amp; dimensions). What is exactly is the role of Databricks SQL and Warehouses here? Is Databricks SQL just being used as an interactive query engine for analysts that want to write ad-hoc SQL queries on top of silver or gold tables? Or does Databricks SQL have a larger role to be used by BI Developers to build and update the Fact and Dimensions in Gold?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lery0", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lery0/databricks_sql_and_warehouses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lery0/databricks_sql_and_warehouses/", "subreddit_subscribers": 146959, "created_utc": 1702921811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nMy company allocates some budget for training purpose, we can make use of this budget to buy some learning materials. Im looking for courses that take my data engineering from intermediate to advanced level. ( I would like to go indepth of advance concepts). Are there any course which can help me?  \n\n\nArea of interest:  \n\n\n1. Python ( coding )\n2. Data Warehouse / data modelling\n3. AWS for data analytics/engineering\n4. Distributed systems\n\netc....  \n", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paid courses for excelling data engineering skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18l9vbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702909389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;My company allocates some budget for training purpose, we can make use of this budget to buy some learning materials. Im looking for courses that take my data engineering from intermediate to advanced level. ( I would like to go indepth of advance concepts). Are there any course which can help me?  &lt;/p&gt;\n\n&lt;p&gt;Area of interest:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Python ( coding )&lt;/li&gt;\n&lt;li&gt;Data Warehouse / data modelling&lt;/li&gt;\n&lt;li&gt;AWS for data analytics/engineering&lt;/li&gt;\n&lt;li&gt;Distributed systems&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;etc....  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18l9vbb", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18l9vbb/paid_courses_for_excelling_data_engineering_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18l9vbb/paid_courses_for_excelling_data_engineering_skills/", "subreddit_subscribers": 146959, "created_utc": 1702909389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When we were a smaller company, Fivetran made sense due to the value vs effort. We still like them but we are definitely being penalized for high amounts of MAR.\n\nBasically, we have activity data that we record and would like to do analytics on top of it. This data was low at one point and has now increased. This data also lives in the same relational db as the other tables so ideally I'd want a connector that can reliably sync all the data.\n\nWhat alternatives would you recommend for Aurora PostgreSQL database sync to snowflake? Small team so need something as decent as fivetran without any headaches. We are willing to pay, just way less than what they're charging us.", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran alternatives for Amazon Aurora PostgreSQL to Snowflake ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lda1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702918104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When we were a smaller company, Fivetran made sense due to the value vs effort. We still like them but we are definitely being penalized for high amounts of MAR.&lt;/p&gt;\n\n&lt;p&gt;Basically, we have activity data that we record and would like to do analytics on top of it. This data was low at one point and has now increased. This data also lives in the same relational db as the other tables so ideally I&amp;#39;d want a connector that can reliably sync all the data.&lt;/p&gt;\n\n&lt;p&gt;What alternatives would you recommend for Aurora PostgreSQL database sync to snowflake? Small team so need something as decent as fivetran without any headaches. We are willing to pay, just way less than what they&amp;#39;re charging us.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lda1q", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lda1q/fivetran_alternatives_for_amazon_aurora/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lda1q/fivetran_alternatives_for_amazon_aurora/", "subreddit_subscribers": 146959, "created_utc": 1702918104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, Right now I am working on a project that needs Databricks and BI experience. Can you guys suggest some real time experience courses for both Databricks and Power BI", "author_fullname": "t2_nlmrmy2ow", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best course for Databricks and BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lhjsr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702928583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, Right now I am working on a project that needs Databricks and BI experience. Can you guys suggest some real time experience courses for both Databricks and Power BI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18lhjsr", "is_robot_indexable": true, "report_reasons": null, "author": "Sufficient_Koala_609", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lhjsr/best_course_for_databricks_and_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lhjsr/best_course_for_databricks_and_bi/", "subreddit_subscribers": 146959, "created_utc": 1702928583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recap of 2023's Transformative Data Landscape", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_18luokv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QC-u87qQqiWYhhkIC7zp7h8awQeZx1F8z1WVloZ5my4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702965072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/recap-of-2023s-transformative-data", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?auto=webp&amp;s=05cf70f29567b297be9005cc1fe6c52634c548d1", "width": 852, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8235b4ab19fc2ff32de42dcad115918f2e142ea", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac56b2d2c72cfcf52f1d78a8b06cc0971bef6d20", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d507577ef1336581efee9304ad98a549d20d63fe", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e89944c98598044365a90f48cd565f877fa7541c", "width": 640, "height": 450}], "variants": {}, "id": "bU0KynVYqdS53QNQHO7TTIr2Kuty1FCLaOXHnUOQcI0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18luokv", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18luokv/recap_of_2023s_transformative_data_landscape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/recap-of-2023s-transformative-data", "subreddit_subscribers": 146959, "created_utc": 1702965072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why is the cost for hightouch so high? Are there cheaper options? I am trying to figure out how to lower cost. Is Rudderstack a better solution?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hightouch Cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lmjjz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702941056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why is the cost for hightouch so high? Are there cheaper options? I am trying to figure out how to lower cost. Is Rudderstack a better solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lmjjz", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lmjjz/hightouch_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lmjjz/hightouch_cost/", "subreddit_subscribers": 146959, "created_utc": 1702941056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://blog.peerdb.io/pg-slot-notify-monitor-postgres-slot-growth-in-slack](https://blog.peerdb.io/pg-slot-notify-monitor-postgres-slot-growth-in-slack)\n\nWe open sourced PG Slot Notify, a tool we've been actively using to monitor replication slot size and alert us when there are abnormalities.\n\nIf you have Postgres databases with replication slots, this tool would come very handy!! It involves 5 mins of setup time and you should be good to go!", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PG Slot Notify: Monitor Postgres Replication Slot Growth in Slack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ll3w9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702937409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://blog.peerdb.io/pg-slot-notify-monitor-postgres-slot-growth-in-slack\"&gt;https://blog.peerdb.io/pg-slot-notify-monitor-postgres-slot-growth-in-slack&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We open sourced PG Slot Notify, a tool we&amp;#39;ve been actively using to monitor replication slot size and alert us when there are abnormalities.&lt;/p&gt;\n\n&lt;p&gt;If you have Postgres databases with replication slots, this tool would come very handy!! It involves 5 mins of setup time and you should be good to go!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?auto=webp&amp;s=f2c640396c48ee4e87e81497df05c6f09b0481db", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=584fe42ee441fed66d044e42f0bc778750228572", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83938a1b51284fa54354b4819bd85c7322d70795", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a53d612801375bd9e1406e912037e2df91c2803", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f42ba9b2be0e2be031475dd9c3d9c7bcaf856525", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fecc1bf0eed3569e71cb61e79f62d243659df902", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a06cbf070c88c8326046b408d2c5be8c1af54614", "width": 1080, "height": 567}], "variants": {}, "id": "CXHCBuyAiQklJzG9M75ycT_nl9F2gQjEziJ40F9-sp8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18ll3w9", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ll3w9/pg_slot_notify_monitor_postgres_replication_slot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ll3w9/pg_slot_notify_monitor_postgres_replication_slot/", "subreddit_subscribers": 146959, "created_utc": 1702937409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am looking to switch my career and move into DE. \n\nI've only covered basics of Python, SQL, MySQL, PostgreSQL, Linux, bash scripting, Database administration + a few rdbms tools like pgadmin4, phpmyadmin or dbeaver. \n\nThere's still a long way to go for me, but I am already looking at some job offers to see what specific tools and skills companies in my area require.\n\nWhat are some red flags that when you see in a job offer you're like \"This company has no structure / it needs three different people for that role / etc.\"?\n\nI am looking for ways to weed out those offers that I shouldn't be using as a baseline for gathering my skills.\n\nThanks.", "author_fullname": "t2_7xz6r2in", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Red flags in DE job offers (beginner)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18m14z5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702990200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am looking to switch my career and move into DE. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only covered basics of Python, SQL, MySQL, PostgreSQL, Linux, bash scripting, Database administration + a few rdbms tools like pgadmin4, phpmyadmin or dbeaver. &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s still a long way to go for me, but I am already looking at some job offers to see what specific tools and skills companies in my area require.&lt;/p&gt;\n\n&lt;p&gt;What are some red flags that when you see in a job offer you&amp;#39;re like &amp;quot;This company has no structure / it needs three different people for that role / etc.&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;I am looking for ways to weed out those offers that I shouldn&amp;#39;t be using as a baseline for gathering my skills.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18m14z5", "is_robot_indexable": true, "report_reasons": null, "author": "Consistent-Drink-235", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m14z5/red_flags_in_de_job_offers_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m14z5/red_flags_in_de_job_offers_beginner/", "subreddit_subscribers": 146959, "created_utc": 1702990200.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need some advice on best approach/ practise.\n\nBackground :\n\nIm a data analyst working in a team. Breakdown of our reporting flows are :\n\n1. We remote into VM and pull data using SQL. Then do the necessary conditioning or merging or etc and automate these stuffs using the task scheduler. We need python because we interact with google sheet a lot.\n\nProblem i have with this is approach is that currently we dont have a proper Git framework whatsoever. Everybody just remote and change the code directly on the VM. \n\n2. We have internal data tools that connects to the datawarehouse. Mainly the workflows are being done on SQL which has proper logging, code versioning etc as they were built and maintained in house. \n\nThis tool is actually quite solid, it even supports pyspark and spark sql. The only problem is that our team have never leveraged spark on this internal data platform. Just SQL. \n\nAnd when we need massive update on google sheets reporting flows we just use the no.1 approach, do everything on the VM.\n\nSo the advice i need is, i want to understand if approach no.1 is bad practise and should we bring everything over on approach no.2? \n\nMy concern for keeping no.1 is that there is no proper Git framework to it. Everything is just so messy. Should i introduce Git and keep it around?\n\nAs for to just use no.2 approach, im not sure if pyspark can do exactly what we're doing in no.1 since we lack of the exposure. \n\nSorry for lack of knowledge, im new to these kinds of infra. I dont know much other than just writing good SQL queries.", "author_fullname": "t2_7nk0x7fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18m0x3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702989497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need some advice on best approach/ practise.&lt;/p&gt;\n\n&lt;p&gt;Background :&lt;/p&gt;\n\n&lt;p&gt;Im a data analyst working in a team. Breakdown of our reporting flows are :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We remote into VM and pull data using SQL. Then do the necessary conditioning or merging or etc and automate these stuffs using the task scheduler. We need python because we interact with google sheet a lot.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Problem i have with this is approach is that currently we dont have a proper Git framework whatsoever. Everybody just remote and change the code directly on the VM. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We have internal data tools that connects to the datawarehouse. Mainly the workflows are being done on SQL which has proper logging, code versioning etc as they were built and maintained in house. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This tool is actually quite solid, it even supports pyspark and spark sql. The only problem is that our team have never leveraged spark on this internal data platform. Just SQL. &lt;/p&gt;\n\n&lt;p&gt;And when we need massive update on google sheets reporting flows we just use the no.1 approach, do everything on the VM.&lt;/p&gt;\n\n&lt;p&gt;So the advice i need is, i want to understand if approach no.1 is bad practise and should we bring everything over on approach no.2? &lt;/p&gt;\n\n&lt;p&gt;My concern for keeping no.1 is that there is no proper Git framework to it. Everything is just so messy. Should i introduce Git and keep it around?&lt;/p&gt;\n\n&lt;p&gt;As for to just use no.2 approach, im not sure if pyspark can do exactly what we&amp;#39;re doing in no.1 since we lack of the exposure. &lt;/p&gt;\n\n&lt;p&gt;Sorry for lack of knowledge, im new to these kinds of infra. I dont know much other than just writing good SQL queries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m0x3c", "is_robot_indexable": true, "report_reasons": null, "author": "Nopal97", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m0x3c/advice_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m0x3c/advice_needed/", "subreddit_subscribers": 146959, "created_utc": 1702989497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the company I am currently contracting in they have a central data warehouse running on Postgres with about 30 terabytes of data. Users of the warehouse are expected to pull data out of the database into their own environments. \nAs many of the tables are too large to do a full sync, we need to do incremental sync on new/updated data. The person that created the central warehouse has created a system where every record is tagged with the id of the batch of data that it was ingested from. These are not necessarily in incrementing order. This is done per table. Thus, every system working with the data warehouse has to maintain their own logic for every table that they use of which transactions they have already processed. When I look at their ETL scripts about half of the code is spent on this logic. \nTo me this seems overly convoluted compared to a database generated timestamp that I can query for changed data. It also means that I cannot use default functionality for incremental loads in dbt and meltano. Am I overlooking something here?", "author_fullname": "t2_dxt8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting new records/changes: timestamp vs ingest I\u2019d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m00kd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702986364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the company I am currently contracting in they have a central data warehouse running on Postgres with about 30 terabytes of data. Users of the warehouse are expected to pull data out of the database into their own environments. \nAs many of the tables are too large to do a full sync, we need to do incremental sync on new/updated data. The person that created the central warehouse has created a system where every record is tagged with the id of the batch of data that it was ingested from. These are not necessarily in incrementing order. This is done per table. Thus, every system working with the data warehouse has to maintain their own logic for every table that they use of which transactions they have already processed. When I look at their ETL scripts about half of the code is spent on this logic. \nTo me this seems overly convoluted compared to a database generated timestamp that I can query for changed data. It also means that I cannot use default functionality for incremental loads in dbt and meltano. Am I overlooking something here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m00kd", "is_robot_indexable": true, "report_reasons": null, "author": "pokepip", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m00kd/detecting_new_recordschanges_timestamp_vs_ingest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m00kd/detecting_new_recordschanges_timestamp_vs_ingest/", "subreddit_subscribers": 146959, "created_utc": 1702986364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why storage and compute can't be separated for an OLTP database?", "author_fullname": "t2_fulplt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OLTP vs OLAP databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lyljw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702980821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why storage and compute can&amp;#39;t be separated for an OLTP database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lyljw", "is_robot_indexable": true, "report_reasons": null, "author": "ankit_goyal", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lyljw/oltp_vs_olap_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lyljw/oltp_vs_olap_databases/", "subreddit_subscribers": 146959, "created_utc": 1702980821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dose hosting the system on your environment will change the source you\u2019re reading from? \n\nFor example System x is hosted now on the provider side, if the IT said we need to host it on our environment, dose that means the data source in fivetran connector have to be changed? I\u2019m I gonna establish a connector to my own environment rather that to x system? \n\ni\u2019m not familiar to cloud however I assume no need to do that it sounds weird.. however I heard people here say they have to \ud83e\udd14", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting the tool on your environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ltbyp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702960586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dose hosting the system on your environment will change the source you\u2019re reading from? &lt;/p&gt;\n\n&lt;p&gt;For example System x is hosted now on the provider side, if the IT said we need to host it on our environment, dose that means the data source in fivetran connector have to be changed? I\u2019m I gonna establish a connector to my own environment rather that to x system? &lt;/p&gt;\n\n&lt;p&gt;i\u2019m not familiar to cloud however I assume no need to do that it sounds weird.. however I heard people here say they have to \ud83e\udd14&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ltbyp", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ltbyp/hosting_the_tool_on_your_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ltbyp/hosting_the_tool_on_your_environment/", "subreddit_subscribers": 146959, "created_utc": 1702960586.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In this tutorial, we share an approach to web scraping that leverages a distributed cloud network with thousands of residential gaming PCs. \n\nYou can read the tutorial here: [**https://substack.thewebscraping.club/p/web-dragon-llm-powered-web-scraping**](https://substack.thewebscraping.club/p/web-dragon-llm-powered-web-scraping) \n\nBut why do this? \n\nThe AI boom and the resulting increase &amp; importance of web scraping has two challenges: \n\n\\- Website technology preventing automatic extraction\n\n\\- Website changes breaking manually coded screapers\n\nThe nature of a distributed network offers solutions to both of the main challenges faced by scrapers. \n\nIn this tutorial, we use SaladCloud but there are also other distributed cloud networks for you to choose from.\n\nSince every Salad node is an actual residential computer, loading a page from these nodes ***looks*** like legitimate human traffic to most websites, and is much less likely to get blocked. \n\nAdditionally, these gaming PCs have GPUs capable of running modern Large Language Models(LLMs), providing a potentially much more flexible and resilient option for parsing website content. \n\n## Key Takeaways \n\n* Actually accessing page content was extremely easy on a distributed cloud, with more than 99% of pages viewed successfully within 2 tries. Just as we humans are occasionally presented with CAPTCHAs, our crawlers also ran into CAPTCHAs from time to time. We dealt with this by retrying the URL with a different queue worker (and therefore a different IP in a different location). \n* Once the boilerplate is set up, the process of writing the data extraction layer is a lot simpler and more intuitive than traditional web-scraping techniques, consisting mostly of asking natural language questions. It\u2019s easy to imagine giving this framework a simple web interface that would allow the creation of custom web scrapers with no coding knowledge required at all.\n* LLM\u2019s are not a magic bullet for the problems faced by Web Scraping. Prompts that work reliably with one website may not work very well at all with another site. You\u2019re still going to do some per-site customization.\n* Setting up a proof-of-concept RAG application is pretty straightforward with off-the-shelf open-source tools. As is the case for most software, developing that proof-of-concept into something production-ready is non-trivial.\n* There are a lot of choices to make, knobs to adjust, variables to tweak in using the LLM approach to data extraction. There\u2019s going to be a significant amount of experimentation involved in reliably getting the results you want. Everything from what models to use, how to split your webpage into chunks, and how you write your prompts can have a major impact on both the efficiency and accuracy of data extraction.\n* Using LLMs and Vector Databases is much more computationally intensive than traditional methods of web scraping, which rely on highly efficient HTML selectors and other low-cost text processing techniques. In our test run with [Harrods.com](http://harrods.com/), each page took about 9s to process on an RTX 3080 Ti, yielding a JSON object like this:\n\n&amp;#8203;\n\n    {   \"name\": \"Moncler 999 x adidas Originals NMD Padded Boots\",   \"manufacturer\": \"Moncler, adidas Originals\",   \"price\": 637,   \"currency\": \"USD\",   \"description\": \"The Moncler x adidas Originals NMD Padded Boots are a collaboration between the two renowned brands. These boots are designed with a focus on functionality and style, featuring a unique combination of materials. The lining is made of other materials, providing added comfort and warmth, while the sole is also crafted from other materials. These boots are currently available for personal shopping and in-store purchase only, as they are not currently available for online purchase.\",   \"is_available_online\": false }\n\n* Whether this technique is worth the extra computational cost will depend a lot on your particular use case. At scale, it\u2019s likely that your savings in engineering time would significantly outweigh the additional costs from computing, especially given Salad\u2019s low costs compared to traditional clouds.\n* The LLM space is developing at a breakneck pace. It\u2019s possible (likely even) that by the time you\u2019re reading this article, none of the models I\u2019ve chosen are still considered state-of-the-art. The good news is that open-source inference servers like [\ud83e\udd17Text Embeddings Inference](https://huggingface.co/docs/text-embeddings-inference/index) and [\ud83e\udd17Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index) make it very easy to try out new models without changing the rest of your code, each requiring only a single environment variable (MODEL\\_ID  \n) to configure models from Huggingface Hub.\n\nYou can try it yourself here: [www.salad.com](https://www.salad.com)", "author_fullname": "t2_2ju1icfv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLM-powered web scraping with 1000s of residential gaming PCs on a distributed cloud [Tutorial with Workflow included]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18m1v88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702992462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this tutorial, we share an approach to web scraping that leverages a distributed cloud network with thousands of residential gaming PCs. &lt;/p&gt;\n\n&lt;p&gt;You can read the tutorial here: &lt;a href=\"https://substack.thewebscraping.club/p/web-dragon-llm-powered-web-scraping\"&gt;&lt;strong&gt;https://substack.thewebscraping.club/p/web-dragon-llm-powered-web-scraping&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;But why do this? &lt;/p&gt;\n\n&lt;p&gt;The AI boom and the resulting increase &amp;amp; importance of web scraping has two challenges: &lt;/p&gt;\n\n&lt;p&gt;- Website technology preventing automatic extraction&lt;/p&gt;\n\n&lt;p&gt;- Website changes breaking manually coded screapers&lt;/p&gt;\n\n&lt;p&gt;The nature of a distributed network offers solutions to both of the main challenges faced by scrapers. &lt;/p&gt;\n\n&lt;p&gt;In this tutorial, we use SaladCloud but there are also other distributed cloud networks for you to choose from.&lt;/p&gt;\n\n&lt;p&gt;Since every Salad node is an actual residential computer, loading a page from these nodes &lt;strong&gt;&lt;em&gt;looks&lt;/em&gt;&lt;/strong&gt; like legitimate human traffic to most websites, and is much less likely to get blocked. &lt;/p&gt;\n\n&lt;p&gt;Additionally, these gaming PCs have GPUs capable of running modern Large Language Models(LLMs), providing a potentially much more flexible and resilient option for parsing website content. &lt;/p&gt;\n\n&lt;h2&gt;Key Takeaways&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Actually accessing page content was extremely easy on a distributed cloud, with more than 99% of pages viewed successfully within 2 tries. Just as we humans are occasionally presented with CAPTCHAs, our crawlers also ran into CAPTCHAs from time to time. We dealt with this by retrying the URL with a different queue worker (and therefore a different IP in a different location). &lt;/li&gt;\n&lt;li&gt;Once the boilerplate is set up, the process of writing the data extraction layer is a lot simpler and more intuitive than traditional web-scraping techniques, consisting mostly of asking natural language questions. It\u2019s easy to imagine giving this framework a simple web interface that would allow the creation of custom web scrapers with no coding knowledge required at all.&lt;/li&gt;\n&lt;li&gt;LLM\u2019s are not a magic bullet for the problems faced by Web Scraping. Prompts that work reliably with one website may not work very well at all with another site. You\u2019re still going to do some per-site customization.&lt;/li&gt;\n&lt;li&gt;Setting up a proof-of-concept RAG application is pretty straightforward with off-the-shelf open-source tools. As is the case for most software, developing that proof-of-concept into something production-ready is non-trivial.&lt;/li&gt;\n&lt;li&gt;There are a lot of choices to make, knobs to adjust, variables to tweak in using the LLM approach to data extraction. There\u2019s going to be a significant amount of experimentation involved in reliably getting the results you want. Everything from what models to use, how to split your webpage into chunks, and how you write your prompts can have a major impact on both the efficiency and accuracy of data extraction.&lt;/li&gt;\n&lt;li&gt;Using LLMs and Vector Databases is much more computationally intensive than traditional methods of web scraping, which rely on highly efficient HTML selectors and other low-cost text processing techniques. In our test run with &lt;a href=\"http://harrods.com/\"&gt;Harrods.com&lt;/a&gt;, each page took about 9s to process on an RTX 3080 Ti, yielding a JSON object like this:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{   &amp;quot;name&amp;quot;: &amp;quot;Moncler 999 x adidas Originals NMD Padded Boots&amp;quot;,   &amp;quot;manufacturer&amp;quot;: &amp;quot;Moncler, adidas Originals&amp;quot;,   &amp;quot;price&amp;quot;: 637,   &amp;quot;currency&amp;quot;: &amp;quot;USD&amp;quot;,   &amp;quot;description&amp;quot;: &amp;quot;The Moncler x adidas Originals NMD Padded Boots are a collaboration between the two renowned brands. These boots are designed with a focus on functionality and style, featuring a unique combination of materials. The lining is made of other materials, providing added comfort and warmth, while the sole is also crafted from other materials. These boots are currently available for personal shopping and in-store purchase only, as they are not currently available for online purchase.&amp;quot;,   &amp;quot;is_available_online&amp;quot;: false }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Whether this technique is worth the extra computational cost will depend a lot on your particular use case. At scale, it\u2019s likely that your savings in engineering time would significantly outweigh the additional costs from computing, especially given Salad\u2019s low costs compared to traditional clouds.&lt;/li&gt;\n&lt;li&gt;The LLM space is developing at a breakneck pace. It\u2019s possible (likely even) that by the time you\u2019re reading this article, none of the models I\u2019ve chosen are still considered state-of-the-art. The good news is that open-source inference servers like &lt;a href=\"https://huggingface.co/docs/text-embeddings-inference/index\"&gt;\ud83e\udd17Text Embeddings Inference&lt;/a&gt; and &lt;a href=\"https://huggingface.co/docs/text-generation-inference/index\"&gt;\ud83e\udd17Text Generation Inference&lt;/a&gt; make it very easy to try out new models without changing the rest of your code, each requiring only a single environment variable (MODEL_ID&lt;br/&gt;\n) to configure models from Huggingface Hub.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can try it yourself here: &lt;a href=\"https://www.salad.com\"&gt;www.salad.com&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Nq9QnvdZOrmmnPIPwCcUYYGq3yIvf7RQpKmgZSZegBs.jpg?auto=webp&amp;s=a034b88769582cf9f063a2ade2be0ea3ad4624f9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Nq9QnvdZOrmmnPIPwCcUYYGq3yIvf7RQpKmgZSZegBs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=394162b253cd95a1f26a3d9165d0c3c4d7171de7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Nq9QnvdZOrmmnPIPwCcUYYGq3yIvf7RQpKmgZSZegBs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff9a546836814d89b09482b7bc1defcea8067339", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Nq9QnvdZOrmmnPIPwCcUYYGq3yIvf7RQpKmgZSZegBs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=401cc6c5f7d41ba533dbcf05406a34f434a24169", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Nq9QnvdZOrmmnPIPwCcUYYGq3yIvf7RQpKmgZSZegBs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb56830ef1ebb4173f88cbbe5548c26943d8c26e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Nq9QnvdZOrmmnPIPwCcUYYGq3yIvf7RQpKmgZSZegBs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d8fec7f1d5728157a5c0cba7b7394c0590274ffb", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Nq9QnvdZOrmmnPIPwCcUYYGq3yIvf7RQpKmgZSZegBs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0b90e8ac6cf3e94f212088ef0b0d2e71a9f6977", "width": 1080, "height": 540}], "variants": {}, "id": "msV-zSy9fohwGXKfs_vyRiEzzu8VT8oE-Ix3N9KdV88"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18m1v88", "is_robot_indexable": true, "report_reasons": null, "author": "SaladChefs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m1v88/llmpowered_web_scraping_with_1000s_of_residential/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m1v88/llmpowered_web_scraping_with_1000s_of_residential/", "subreddit_subscribers": 146959, "created_utc": 1702992462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m not a data engineer, am a BI analyst whos interested in becoming a DE, so currently I\u2019m trying to observe, ask questions, search\u2026 but I noticed it\u2019s really stressful job and there a lot of things i need to learn and i feel overwhelmed. \n\nHow you guys dealing with this pressure and what strategies are you using to balance between doing work and continue learning and sharpening your skills?\n\nI also want to ask about how you learned DE and if you know a geed source teaching the basics of DE for beginners. \nnote that I\u2019m a BI analyst for almost 3 years :)", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Work pressure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lzawl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702983685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not a data engineer, am a BI analyst whos interested in becoming a DE, so currently I\u2019m trying to observe, ask questions, search\u2026 but I noticed it\u2019s really stressful job and there a lot of things i need to learn and i feel overwhelmed. &lt;/p&gt;\n\n&lt;p&gt;How you guys dealing with this pressure and what strategies are you using to balance between doing work and continue learning and sharpening your skills?&lt;/p&gt;\n\n&lt;p&gt;I also want to ask about how you learned DE and if you know a geed source teaching the basics of DE for beginners. \nnote that I\u2019m a BI analyst for almost 3 years :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18lzawl", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lzawl/work_pressure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lzawl/work_pressure/", "subreddit_subscribers": 146959, "created_utc": 1702983685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_qvzmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expedia Uses WebSockets and Kafka to Query Near Real-Time Streaming Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18lz0me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_-95beDt0eTU61sjw3MyytCoW7zyk-9mjFOz5cqSkbM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702982547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoq.com/news/2023/12/expedia-websockets-kafka-query/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?auto=webp&amp;s=22be1127b3a032c0428d4bee24b8d2cb2a2233c0", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74b528758f8dac132781b7ba52cec3194953941b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b5018e5a478380623b37087dbf9dc4b43bbf70c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1b372ee2d8ab5031fb9f92e22e20238371990e25", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec9e2d3c657a70106b4608c93841f75070af8d79", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3de9be364444c886057dff0f382a0ea14630cb01", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7820fff738d4f7d7dcf3a548b2e8c36161176b6", "width": 1080, "height": 567}], "variants": {}, "id": "BmdzDPEbzLW9dI8d-7eePz_SJf3gyF91hmqD-LM6Od4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18lz0me", "is_robot_indexable": true, "report_reasons": null, "author": "rgancarz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lz0me/expedia_uses_websockets_and_kafka_to_query_near/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoq.com/news/2023/12/expedia-websockets-kafka-query/", "subreddit_subscribers": 146959, "created_utc": 1702982547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, my company is in the process of evaluating a new data platform (think DWH, DL, LH), they've asked for me to help, and it's clear they haven't thought this through much (both the type of architecture they want and how to structure an evaluation). I'm not looking for specific product recommendations, but advice on the process you go through to make a decision and not just wander in circles doing trials and POCs for the next 18 months. \n\nHere are some of the specific questions I have, but I'm all ears for other advice too:\n\n* Do you prefer trials that are fully-managed or self-managed? What are the pros/cons to each? When products have both, how do I choose? \n* When you first trial a piece of data infra, what do you focus on in the first 30-60min?\n\nI'm thinking mostly about the beginning of the evaluation right now since we're just starting, so advice on that stage would be most helpful. Thanks!\n\n&amp;#x200B;", "author_fullname": "t2_vad5s0up", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for Evaluating new data platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lj7yt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702932727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, my company is in the process of evaluating a new data platform (think DWH, DL, LH), they&amp;#39;ve asked for me to help, and it&amp;#39;s clear they haven&amp;#39;t thought this through much (both the type of architecture they want and how to structure an evaluation). I&amp;#39;m not looking for specific product recommendations, but advice on the process you go through to make a decision and not just wander in circles doing trials and POCs for the next 18 months. &lt;/p&gt;\n\n&lt;p&gt;Here are some of the specific questions I have, but I&amp;#39;m all ears for other advice too:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do you prefer trials that are fully-managed or self-managed? What are the pros/cons to each? When products have both, how do I choose? &lt;/li&gt;\n&lt;li&gt;When you first trial a piece of data infra, what do you focus on in the first 30-60min?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m thinking mostly about the beginning of the evaluation right now since we&amp;#39;re just starting, so advice on that stage would be most helpful. Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18lj7yt", "is_robot_indexable": true, "report_reasons": null, "author": "Curious_Gorges", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lj7yt/tips_for_evaluating_new_data_platforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lj7yt/tips_for_evaluating_new_data_platforms/", "subreddit_subscribers": 146959, "created_utc": 1702932727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a complex challenge where I'm exploring ways to simplify our orchestration structure where some \"tasks\" are more traditional running of small scripts or processes, but others are data transformations involving sometimes thousands of hardcoded conditional steps in a single task. \n\nI was thinking about refactoring the more complex workflows to be more independent when the possibly bad idea struck me to store the refactored code in a table format and use code injection or maybe grouped and composed in a pre-compilation step. So this is a sort of data-driven programming paradigm where the code is stored as a data object that is parsed and applied using a general bit of code.  that way I could test the code snippets atomically, generate data transformation tasks dynamically...\n\nSo has anyone experimented with storing code as data in a table format for complex workflows? What about using code injection or compilation for orchestration tasks? Any insights or warnings you can share would be incredibly helpful!", "author_fullname": "t2_4hepb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data-driven programming?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18limft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702931216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a complex challenge where I&amp;#39;m exploring ways to simplify our orchestration structure where some &amp;quot;tasks&amp;quot; are more traditional running of small scripts or processes, but others are data transformations involving sometimes thousands of hardcoded conditional steps in a single task. &lt;/p&gt;\n\n&lt;p&gt;I was thinking about refactoring the more complex workflows to be more independent when the possibly bad idea struck me to store the refactored code in a table format and use code injection or maybe grouped and composed in a pre-compilation step. So this is a sort of data-driven programming paradigm where the code is stored as a data object that is parsed and applied using a general bit of code.  that way I could test the code snippets atomically, generate data transformation tasks dynamically...&lt;/p&gt;\n\n&lt;p&gt;So has anyone experimented with storing code as data in a table format for complex workflows? What about using code injection or compilation for orchestration tasks? Any insights or warnings you can share would be incredibly helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18limft", "is_robot_indexable": true, "report_reasons": null, "author": "theinexplicablefuzz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18limft/datadriven_programming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18limft/datadriven_programming/", "subreddit_subscribers": 146959, "created_utc": 1702931216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey DE!\n\nI'm on the lookout for the perfect orchestration solution, but I'm not quite seasoned enough to be aware of all the options out there. Here's the challenge: I have a customer who lays out ETL/ELT tasks in extensive flow diagrams with intricate conditions and interactions.\n\nWhat I'm after is a solution that allows these tasks to be relatively atomic for easy testing, yet straightforward to define and code. Additionally, I need support for ML workflows, flow versioning, and the flexibility to deploy both in the cloud and locally. Ideally, I'm aiming for something lightweight that can effortlessly work with minimal setup, drawing from a configuration source featuring DAGs and tasks.\n\nI'm aware of another team in my company working on a substantial system for a similar task, leveraging Kafka, Kubernetes, and node-red. However, this setup seems a bit too hefty for my needs, and I'm not a fan of how they are hardcoding custom tasks in node-red.\n\nI also found this awesome list of [workflow engines](https://github.com/meirwah/awesome-workflow-engines)\n\nAny suggestions or recommendations for a solution that strikes the right balance for my requirements? Thanks in advance!", "author_fullname": "t2_7i2kfbec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best user friendly orchestration tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lf3uj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702922607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DE!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on the lookout for the perfect orchestration solution, but I&amp;#39;m not quite seasoned enough to be aware of all the options out there. Here&amp;#39;s the challenge: I have a customer who lays out ETL/ELT tasks in extensive flow diagrams with intricate conditions and interactions.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m after is a solution that allows these tasks to be relatively atomic for easy testing, yet straightforward to define and code. Additionally, I need support for ML workflows, flow versioning, and the flexibility to deploy both in the cloud and locally. Ideally, I&amp;#39;m aiming for something lightweight that can effortlessly work with minimal setup, drawing from a configuration source featuring DAGs and tasks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware of another team in my company working on a substantial system for a similar task, leveraging Kafka, Kubernetes, and node-red. However, this setup seems a bit too hefty for my needs, and I&amp;#39;m not a fan of how they are hardcoding custom tasks in node-red.&lt;/p&gt;\n\n&lt;p&gt;I also found this awesome list of &lt;a href=\"https://github.com/meirwah/awesome-workflow-engines\"&gt;workflow engines&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or recommendations for a solution that strikes the right balance for my requirements? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?auto=webp&amp;s=b1fe2cc2ffd4873a5d343debf9808bb93b2dda1d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43acdbac9b761d7856283ff0546bcc148ceed9d2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f37a04aebefac1e04fdf4aa4f2f317eed939547", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1fe2d902f40d902f2a7aba37b9bdfecdd241d14", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c756d6046c053bb95216b4c3b5497adba34e393d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6e1147407d0942d0dd9a5abd75bd67b94c3c0f87", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a74d378d3d7f80dc71c79d33ee779f49074f9a70", "width": 1080, "height": 540}], "variants": {}, "id": "AEPkqSVO-ZS9X-zj_UTJDoVNL-W4vCzHWkMh32X3mbc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18lf3uj", "is_robot_indexable": true, "report_reasons": null, "author": "Responsible-Rule3619", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lf3uj/best_user_friendly_orchestration_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lf3uj/best_user_friendly_orchestration_tool/", "subreddit_subscribers": 146959, "created_utc": 1702922607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi,\n\nI've been itching to start a project but I do not have any idea on what type of project I wanted. I wanted to focused on DE projects that barely touches or at least touches \"classical models\".  \nPlease suggest a project if you have one hehe.\n\nWhat are your thoughts or thought process when starting a project? I get stuck whenever I do not know how to solve the problem. I tend to google it and at the end I solved it but I get a guilty feeling whenever I encounter this type of problem again and solved it but I forget about it after many months but I know what to search for it. Is that normal? I've been battling my thoughts about people who code straight or at least search less and code more.\n\nI wanted to finish a project and only then I'll be called a junior DA. (at least for myself)  \nI just need to prove it to myself and I'll be confident for the rest of my life as junior DA.", "author_fullname": "t2_d1r7vfsbx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for projects, that focuses on Data Engineering and a little help please", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lxlpj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702976673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been itching to start a project but I do not have any idea on what type of project I wanted. I wanted to focused on DE projects that barely touches or at least touches &amp;quot;classical models&amp;quot;.&lt;br/&gt;\nPlease suggest a project if you have one hehe.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts or thought process when starting a project? I get stuck whenever I do not know how to solve the problem. I tend to google it and at the end I solved it but I get a guilty feeling whenever I encounter this type of problem again and solved it but I forget about it after many months but I know what to search for it. Is that normal? I&amp;#39;ve been battling my thoughts about people who code straight or at least search less and code more.&lt;/p&gt;\n\n&lt;p&gt;I wanted to finish a project and only then I&amp;#39;ll be called a junior DA. (at least for myself)&lt;br/&gt;\nI just need to prove it to myself and I&amp;#39;ll be confident for the rest of my life as junior DA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18lxlpj", "is_robot_indexable": true, "report_reasons": null, "author": "Cold-Investment-5517", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lxlpj/looking_for_projects_that_focuses_on_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lxlpj/looking_for_projects_that_focuses_on_data/", "subreddit_subscribers": 146959, "created_utc": 1702976673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As we\u2019re starting to use DBT more, and are using it for codifying our business rules, I\u2019m wondering if anyone\u2019s had success training GPT on a DBT project. My initial attempts were lack luster. GPT seemed to know a fair amount about DBT, and of SQL, but struggled to be of much use when I trained it on a DBT project (zipped, uploaded to OpenAI GPT4).  My next step is to create a prep doc with some basic support/prompt engineering. For example, our mart \u201cLegal First Name\u201d and \u201cPreferred First Name\u201d fields come from different source tables, have different staging models. I asked GPT to tell me the difference between the two, as well as derive what SQL each would be off of source tables. Another approach I\u2019m going to try is to do a DBT run materializing as much as I can ephemerally, and see if that helps get useful responses. If this marriage of well-designed DBT project/models and GPT/LLM can be cracked, I think (at least our org) will enter a new level of accessible data literacy.", "author_fullname": "t2_f1kbimm96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT and ChatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lt3d0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702959841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As we\u2019re starting to use DBT more, and are using it for codifying our business rules, I\u2019m wondering if anyone\u2019s had success training GPT on a DBT project. My initial attempts were lack luster. GPT seemed to know a fair amount about DBT, and of SQL, but struggled to be of much use when I trained it on a DBT project (zipped, uploaded to OpenAI GPT4).  My next step is to create a prep doc with some basic support/prompt engineering. For example, our mart \u201cLegal First Name\u201d and \u201cPreferred First Name\u201d fields come from different source tables, have different staging models. I asked GPT to tell me the difference between the two, as well as derive what SQL each would be off of source tables. Another approach I\u2019m going to try is to do a DBT run materializing as much as I can ephemerally, and see if that helps get useful responses. If this marriage of well-designed DBT project/models and GPT/LLM can be cracked, I think (at least our org) will enter a new level of accessible data literacy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lt3d0", "is_robot_indexable": true, "report_reasons": null, "author": "No-Database2068", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lt3d0/dbt_and_chatgpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lt3d0/dbt_and_chatgpt/", "subreddit_subscribers": 146959, "created_utc": 1702959841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi - I'm completely new to data engineering and am hoping to take a boot camp through Data Engineering Camp ([https://dataengineercamp.com/](https://dataengineercamp.com/)). To get in, it looks like I need to take a short 15 min quiz for SQL and Python. I've been learning through online videos, but for anyone who took the course, can you give me advice on what I would need to know/focus on in order to pass this quiz and get in? Any additional advice would be greatly appreciated, too. Thanks!   ", "author_fullname": "t2_6693m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Camp - Boot Camp Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lnyj1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702944792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi - I&amp;#39;m completely new to data engineering and am hoping to take a boot camp through Data Engineering Camp (&lt;a href=\"https://dataengineercamp.com/\"&gt;https://dataengineercamp.com/&lt;/a&gt;). To get in, it looks like I need to take a short 15 min quiz for SQL and Python. I&amp;#39;ve been learning through online videos, but for anyone who took the course, can you give me advice on what I would need to know/focus on in order to pass this quiz and get in? Any additional advice would be greatly appreciated, too. Thanks!   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cLzfdrM3D2Lwr_3fRzyoMtfBIe7szsP9q1Qcnr9R4aY.jpg?auto=webp&amp;s=efdc5f7bfd74e8d15daba8c32f73d0a62675a3f7", "width": 60, "height": 60}, "resolutions": [], "variants": {}, "id": "3TFVUMEeWn_N7TXgLTo7g140VW2LvuxMtnRQ0_5fwIM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18lnyj1", "is_robot_indexable": true, "report_reasons": null, "author": "af2ninja", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lnyj1/data_engineering_camp_boot_camp_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lnyj1/data_engineering_camp_boot_camp_question/", "subreddit_subscribers": 146959, "created_utc": 1702944792.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}