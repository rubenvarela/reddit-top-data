{"kind": "Listing", "data": {"after": "t3_18l8fk5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How can someone with 2 years of experience with knowledge of frontend ,backend, data science, data engineering . \nAnd with a salary of fresher \ud83d\ude02", "author_fullname": "t2_7yh1jlaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are they looking for with title data science full stack engineer \ud83d\ude02", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18l9aak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": "transparent", "ups": 110, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 110, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/I2DODH87LArpY7Q0No33HAnH9HM8iSSkxTp5zR0q_BM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702907789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How can someone with 2 years of experience with knowledge of frontend ,backend, data science, data engineering . \nAnd with a salary of fresher \ud83d\ude02&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nj9g27m5727c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nj9g27m5727c1.png?auto=webp&amp;s=347c1914017958f6b4f7e9894143d00a13f2ff12", "width": 864, "height": 1920}, "resolutions": [{"url": "https://preview.redd.it/nj9g27m5727c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=53eef121b6d3d770eacbc4c1cd77816f1117811a", "width": 108, "height": 216}, {"url": "https://preview.redd.it/nj9g27m5727c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63cc156ebcc7d8bf0ba7938ff8ef3573e3675ab5", "width": 216, "height": 432}, {"url": "https://preview.redd.it/nj9g27m5727c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6044c57acc15a31afe2151b098699f2ef2299a97", "width": 320, "height": 640}, {"url": "https://preview.redd.it/nj9g27m5727c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9fc2085fe28631b831089554e8f0e0b7c293bca", "width": 640, "height": 1280}], "variants": {}, "id": "CpK-Ssd2TVq8YW3JUm933R4rYDFyIwjJ9KqAr6dmjdY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18l9aak", "is_robot_indexable": true, "report_reasons": null, "author": "Foot_Straight", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18l9aak/what_are_they_looking_for_with_title_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nj9g27m5727c1.png", "subreddit_subscribers": 146948, "created_utc": 1702907789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!\n\n[https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra\\_5PGH&amp;index=6&amp;t=689s](https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;index=6&amp;t=689s)", "author_fullname": "t2_me12im5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lsb9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702957460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;amp;index=6&amp;amp;t=689s\"&gt;https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;amp;index=6&amp;amp;t=689s&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?auto=webp&amp;s=7297230f4639915c7e57af7b9a255708da17bfa9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=784a54eeca77266e76144dcc7477cdae8da54cbb", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76e755d0518dfef7107139a615d64e1857715e31", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bcd0428c65436c58727868ae2eae5a9b4a6d541d", "width": 320, "height": 240}], "variants": {}, "id": "_rVKQNaxcPqL1qZYn-JkINnf7oHCvdPuQq4k6I3ej4A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18lsb9p", "is_robot_indexable": true, "report_reasons": null, "author": "onurbaltaci", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lsb9p/i_recorded_a_crash_course_on_polars_library_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lsb9p/i_recorded_a_crash_course_on_polars_library_of/", "subreddit_subscribers": 146948, "created_utc": 1702957460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in charge of, or rather I volunteered to create the documentation that will be the foundation for the development of data pipelines and ETL processes using Python. Part of that documentation includes establishing the configuration/environment used for development. (We currently use SSIS and SQL Server for our pipelines)\n\nI will be using Visual Studio Code and Python 3.\n\nWhat extensions would you recommend that would help with Data Engineering with Python?\n\nIn addition, what extensions would help with other necessary steps in development, such as Version Control, syntax (PEP8), and Testing?", "author_fullname": "t2_aiumhm29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python for Data Engineering VSCode Extensions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lcjqp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702916251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in charge of, or rather I volunteered to create the documentation that will be the foundation for the development of data pipelines and ETL processes using Python. Part of that documentation includes establishing the configuration/environment used for development. (We currently use SSIS and SQL Server for our pipelines)&lt;/p&gt;\n\n&lt;p&gt;I will be using Visual Studio Code and Python 3.&lt;/p&gt;\n\n&lt;p&gt;What extensions would you recommend that would help with Data Engineering with Python?&lt;/p&gt;\n\n&lt;p&gt;In addition, what extensions would help with other necessary steps in development, such as Version Control, syntax (PEP8), and Testing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lcjqp", "is_robot_indexable": true, "report_reasons": null, "author": "Upbeat_Count_7568", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lcjqp/python_for_data_engineering_vscode_extensions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lcjqp/python_for_data_engineering_vscode_extensions/", "subreddit_subscribers": 146948, "created_utc": 1702916251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious as to where Databricks SQL and Warehouses actually fit into the lakehouse medallion architecture? Databricks material says that Databricks SQL is used for data warehousing on the lakehouse. Great, but it really just seems like Databricks SQL is designed for analysts who just want to query existing tables and objects to support a report. However, when I think of \"data warehousing\", I think of transforming raw data into dimensional models to support analysts and their queries.\n\nDoes Databricks just refer to \"Data Warehousing\" as the ability to query tables in the lakehouse and provide an endpoint for BI tools? That doesn't really seem like \"data warehousing\" to me, but rather just analytics. or am I wrong here?\n\nLets assume DBMS -&gt; Landing (Ephemeral) -&gt; Bronze (CDC Appends) -&gt; Silver (Integrated 3NF) -&gt; Gold (facts &amp; dimensions). What is exactly is the role of Databricks SQL and Warehouses here? Is Databricks SQL just being used as an interactive query engine for analysts that want to write ad-hoc SQL queries on top of silver or gold tables? Or does Databricks SQL have a larger role to be used by BI Developers to build and update the Fact and Dimensions in Gold?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks SQL and Warehouses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lery0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702921811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious as to where Databricks SQL and Warehouses actually fit into the lakehouse medallion architecture? Databricks material says that Databricks SQL is used for data warehousing on the lakehouse. Great, but it really just seems like Databricks SQL is designed for analysts who just want to query existing tables and objects to support a report. However, when I think of &amp;quot;data warehousing&amp;quot;, I think of transforming raw data into dimensional models to support analysts and their queries.&lt;/p&gt;\n\n&lt;p&gt;Does Databricks just refer to &amp;quot;Data Warehousing&amp;quot; as the ability to query tables in the lakehouse and provide an endpoint for BI tools? That doesn&amp;#39;t really seem like &amp;quot;data warehousing&amp;quot; to me, but rather just analytics. or am I wrong here?&lt;/p&gt;\n\n&lt;p&gt;Lets assume DBMS -&amp;gt; Landing (Ephemeral) -&amp;gt; Bronze (CDC Appends) -&amp;gt; Silver (Integrated 3NF) -&amp;gt; Gold (facts &amp;amp; dimensions). What is exactly is the role of Databricks SQL and Warehouses here? Is Databricks SQL just being used as an interactive query engine for analysts that want to write ad-hoc SQL queries on top of silver or gold tables? Or does Databricks SQL have a larger role to be used by BI Developers to build and update the Fact and Dimensions in Gold?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lery0", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lery0/databricks_sql_and_warehouses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lery0/databricks_sql_and_warehouses/", "subreddit_subscribers": 146948, "created_utc": 1702921811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nMy company allocates some budget for training purpose, we can make use of this budget to buy some learning materials. Im looking for courses that take my data engineering from intermediate to advanced level. ( I would like to go indepth of advance concepts). Are there any course which can help me?  \n\n\nArea of interest:  \n\n\n1. Python ( coding )\n2. Data Warehouse / data modelling\n3. AWS for data analytics/engineering\n4. Distributed systems\n\netc....  \n", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paid courses for excelling data engineering skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18l9vbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702909389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;My company allocates some budget for training purpose, we can make use of this budget to buy some learning materials. Im looking for courses that take my data engineering from intermediate to advanced level. ( I would like to go indepth of advance concepts). Are there any course which can help me?  &lt;/p&gt;\n\n&lt;p&gt;Area of interest:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Python ( coding )&lt;/li&gt;\n&lt;li&gt;Data Warehouse / data modelling&lt;/li&gt;\n&lt;li&gt;AWS for data analytics/engineering&lt;/li&gt;\n&lt;li&gt;Distributed systems&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;etc....  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18l9vbb", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18l9vbb/paid_courses_for_excelling_data_engineering_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18l9vbb/paid_courses_for_excelling_data_engineering_skills/", "subreddit_subscribers": 146948, "created_utc": 1702909389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When we were a smaller company, Fivetran made sense due to the value vs effort. We still like them but we are definitely being penalized for high amounts of MAR.\n\nBasically, we have activity data that we record and would like to do analytics on top of it. This data was low at one point and has now increased. This data also lives in the same relational db as the other tables so ideally I'd want a connector that can reliably sync all the data.\n\nWhat alternatives would you recommend for Aurora PostgreSQL database sync to snowflake? Small team so need something as decent as fivetran without any headaches. We are willing to pay, just way less than what they're charging us.", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran alternatives for Amazon Aurora PostgreSQL to Snowflake ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lda1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702918104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When we were a smaller company, Fivetran made sense due to the value vs effort. We still like them but we are definitely being penalized for high amounts of MAR.&lt;/p&gt;\n\n&lt;p&gt;Basically, we have activity data that we record and would like to do analytics on top of it. This data was low at one point and has now increased. This data also lives in the same relational db as the other tables so ideally I&amp;#39;d want a connector that can reliably sync all the data.&lt;/p&gt;\n\n&lt;p&gt;What alternatives would you recommend for Aurora PostgreSQL database sync to snowflake? Small team so need something as decent as fivetran without any headaches. We are willing to pay, just way less than what they&amp;#39;re charging us.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lda1q", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lda1q/fivetran_alternatives_for_amazon_aurora/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lda1q/fivetran_alternatives_for_amazon_aurora/", "subreddit_subscribers": 146948, "created_utc": 1702918104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, Right now I am working on a project that needs Databricks and BI experience. Can you guys suggest some real time experience courses for both Databricks and Power BI", "author_fullname": "t2_nlmrmy2ow", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best course for Databricks and BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lhjsr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702928583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, Right now I am working on a project that needs Databricks and BI experience. Can you guys suggest some real time experience courses for both Databricks and Power BI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18lhjsr", "is_robot_indexable": true, "report_reasons": null, "author": "Sufficient_Koala_609", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lhjsr/best_course_for_databricks_and_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lhjsr/best_course_for_databricks_and_bi/", "subreddit_subscribers": 146948, "created_utc": 1702928583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recap of 2023's Transformative Data Landscape", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_18luokv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QC-u87qQqiWYhhkIC7zp7h8awQeZx1F8z1WVloZ5my4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702965072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/recap-of-2023s-transformative-data", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?auto=webp&amp;s=05cf70f29567b297be9005cc1fe6c52634c548d1", "width": 852, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8235b4ab19fc2ff32de42dcad115918f2e142ea", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac56b2d2c72cfcf52f1d78a8b06cc0971bef6d20", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d507577ef1336581efee9304ad98a549d20d63fe", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e89944c98598044365a90f48cd565f877fa7541c", "width": 640, "height": 450}], "variants": {}, "id": "bU0KynVYqdS53QNQHO7TTIr2Kuty1FCLaOXHnUOQcI0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18luokv", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18luokv/recap_of_2023s_transformative_data_landscape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/recap-of-2023s-transformative-data", "subreddit_subscribers": 146948, "created_utc": 1702965072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why is the cost for hightouch so high? Are there cheaper options? I am trying to figure out how to lower cost. Is Rudderstack a better solution?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hightouch Cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lmjjz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702941056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why is the cost for hightouch so high? Are there cheaper options? I am trying to figure out how to lower cost. Is Rudderstack a better solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lmjjz", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lmjjz/hightouch_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lmjjz/hightouch_cost/", "subreddit_subscribers": 146948, "created_utc": 1702941056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://blog.peerdb.io/pg-slot-notify-monitor-postgres-slot-growth-in-slack](https://blog.peerdb.io/pg-slot-notify-monitor-postgres-slot-growth-in-slack)\n\nWe open sourced PG Slot Notify, a tool we've been actively using to monitor replication slot size and alert us when there are abnormalities.\n\nIf you have Postgres databases with replication slots, this tool would come very handy!! It involves 5 mins of setup time and you should be good to go!", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PG Slot Notify: Monitor Postgres Replication Slot Growth in Slack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ll3w9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702937409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://blog.peerdb.io/pg-slot-notify-monitor-postgres-slot-growth-in-slack\"&gt;https://blog.peerdb.io/pg-slot-notify-monitor-postgres-slot-growth-in-slack&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We open sourced PG Slot Notify, a tool we&amp;#39;ve been actively using to monitor replication slot size and alert us when there are abnormalities.&lt;/p&gt;\n\n&lt;p&gt;If you have Postgres databases with replication slots, this tool would come very handy!! It involves 5 mins of setup time and you should be good to go!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?auto=webp&amp;s=f2c640396c48ee4e87e81497df05c6f09b0481db", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=584fe42ee441fed66d044e42f0bc778750228572", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83938a1b51284fa54354b4819bd85c7322d70795", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a53d612801375bd9e1406e912037e2df91c2803", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f42ba9b2be0e2be031475dd9c3d9c7bcaf856525", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fecc1bf0eed3569e71cb61e79f62d243659df902", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/10TWVuDGR9WBJnT0IViO5YZTX6U_RoIKWXRXNHdU6Xo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a06cbf070c88c8326046b408d2c5be8c1af54614", "width": 1080, "height": 567}], "variants": {}, "id": "CXHCBuyAiQklJzG9M75ycT_nl9F2gQjEziJ40F9-sp8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18ll3w9", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ll3w9/pg_slot_notify_monitor_postgres_replication_slot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ll3w9/pg_slot_notify_monitor_postgres_replication_slot/", "subreddit_subscribers": 146948, "created_utc": 1702937409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the company I am currently contracting in they have a central data warehouse running on Postgres with about 30 terabytes of data. Users of the warehouse are expected to pull data out of the database into their own environments. \nAs many of the tables are too large to do a full sync, we need to do incremental sync on new/updated data. The person that created the central warehouse has created a system where every record is tagged with the id of the batch of data that it was ingested from. These are not necessarily in incrementing order. This is done per table. Thus, every system working with the data warehouse has to maintain their own logic for every table that they use of which transactions they have already processed. When I look at their ETL scripts about half of the code is spent on this logic. \nTo me this seems overly convoluted compared to a database generated timestamp that I can query for changed data. It also means that I cannot use default functionality for incremental loads in dbt and meltano. Am I overlooking something here?", "author_fullname": "t2_dxt8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting new records/changes: timestamp vs ingest I\u2019d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18m00kd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702986364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the company I am currently contracting in they have a central data warehouse running on Postgres with about 30 terabytes of data. Users of the warehouse are expected to pull data out of the database into their own environments. \nAs many of the tables are too large to do a full sync, we need to do incremental sync on new/updated data. The person that created the central warehouse has created a system where every record is tagged with the id of the batch of data that it was ingested from. These are not necessarily in incrementing order. This is done per table. Thus, every system working with the data warehouse has to maintain their own logic for every table that they use of which transactions they have already processed. When I look at their ETL scripts about half of the code is spent on this logic. \nTo me this seems overly convoluted compared to a database generated timestamp that I can query for changed data. It also means that I cannot use default functionality for incremental loads in dbt and meltano. Am I overlooking something here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m00kd", "is_robot_indexable": true, "report_reasons": null, "author": "pokepip", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m00kd/detecting_new_recordschanges_timestamp_vs_ingest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m00kd/detecting_new_recordschanges_timestamp_vs_ingest/", "subreddit_subscribers": 146948, "created_utc": 1702986364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why storage and compute can't be separated for an OLTP database?", "author_fullname": "t2_fulplt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OLTP vs OLAP databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lyljw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702980821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why storage and compute can&amp;#39;t be separated for an OLTP database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lyljw", "is_robot_indexable": true, "report_reasons": null, "author": "ankit_goyal", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lyljw/oltp_vs_olap_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lyljw/oltp_vs_olap_databases/", "subreddit_subscribers": 146948, "created_utc": 1702980821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dose hosting the system on your environment will change the source you\u2019re reading from? \n\nFor example System x is hosted now on the provider side, if the IT said we need to host it on our environment, dose that means the data source in fivetran connector have to be changed? I\u2019m I gonna establish a connector to my own environment rather that to x system? \n\ni\u2019m not familiar to cloud however I assume no need to do that it sounds weird.. however I heard people here say they have to \ud83e\udd14", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting the tool on your environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ltbyp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702960586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dose hosting the system on your environment will change the source you\u2019re reading from? &lt;/p&gt;\n\n&lt;p&gt;For example System x is hosted now on the provider side, if the IT said we need to host it on our environment, dose that means the data source in fivetran connector have to be changed? I\u2019m I gonna establish a connector to my own environment rather that to x system? &lt;/p&gt;\n\n&lt;p&gt;i\u2019m not familiar to cloud however I assume no need to do that it sounds weird.. however I heard people here say they have to \ud83e\udd14&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ltbyp", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ltbyp/hosting_the_tool_on_your_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ltbyp/hosting_the_tool_on_your_environment/", "subreddit_subscribers": 146948, "created_utc": 1702960586.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am looking to switch my career and move into DE. \n\nI've only covered basics of Python, SQL, MySQL, PostgreSQL, Linux, bash scripting, Database administration + a few rdbms tools like pgadmin4, phpmyadmin or dbeaver. \n\nThere's still a long way to go for me, but I am already looking at some job offers to see what specific tools and skills companies in my area require.\n\nWhat are some red flags that when you see in a job offer you're like \"This company has no structure / it needs three different people for that role / etc.\"?\n\nI am looking for ways to weed out those offers that I shouldn't be using as a baseline for gathering my skills.\n\nThanks.", "author_fullname": "t2_7xz6r2in", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Red flags in DE job offers (beginner)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18m14z5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702990200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am looking to switch my career and move into DE. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only covered basics of Python, SQL, MySQL, PostgreSQL, Linux, bash scripting, Database administration + a few rdbms tools like pgadmin4, phpmyadmin or dbeaver. &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s still a long way to go for me, but I am already looking at some job offers to see what specific tools and skills companies in my area require.&lt;/p&gt;\n\n&lt;p&gt;What are some red flags that when you see in a job offer you&amp;#39;re like &amp;quot;This company has no structure / it needs three different people for that role / etc.&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;I am looking for ways to weed out those offers that I shouldn&amp;#39;t be using as a baseline for gathering my skills.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18m14z5", "is_robot_indexable": true, "report_reasons": null, "author": "Consistent-Drink-235", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m14z5/red_flags_in_de_job_offers_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m14z5/red_flags_in_de_job_offers_beginner/", "subreddit_subscribers": 146948, "created_utc": 1702990200.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need some advice on best approach/ practise.\n\nBackground :\n\nIm a data analyst working in a team. Breakdown of our reporting flows are :\n\n1. We remote into VM and pull data using SQL. Then do the necessary conditioning or merging or etc and automate these stuffs using the task scheduler. We need python because we interact with google sheet a lot.\n\nProblem i have with this is approach is that currently we dont have a proper Git framework whatsoever. Everybody just remote and change the code directly on the VM. \n\n2. We have internal data tools that connects to the datawarehouse. Mainly the workflows are being done on SQL which has proper logging, code versioning etc as they were built and maintained in house. \n\nThis tool is actually quite solid, it even supports pyspark and spark sql. The only problem is that our team have never leveraged spark on this internal data platform. Just SQL. \n\nAnd when we need massive update on google sheets reporting flows we just use the no.1 approach, do everything on the VM.\n\nSo the advice i need is, i want to understand if approach no.1 is bad practise and should we bring everything over on approach no.2? \n\nMy concern for keeping no.1 is that there is no proper Git framework to it. Everything is just so messy. Should i introduce Git and keep it around?\n\nAs for to just use no.2 approach, im not sure if pyspark can do exactly what we're doing in no.1 since we lack of the exposure. \n\nSorry for lack of knowledge, im new to these kinds of infra. I dont know much other than just writing good SQL queries.", "author_fullname": "t2_7nk0x7fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18m0x3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702989497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need some advice on best approach/ practise.&lt;/p&gt;\n\n&lt;p&gt;Background :&lt;/p&gt;\n\n&lt;p&gt;Im a data analyst working in a team. Breakdown of our reporting flows are :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We remote into VM and pull data using SQL. Then do the necessary conditioning or merging or etc and automate these stuffs using the task scheduler. We need python because we interact with google sheet a lot.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Problem i have with this is approach is that currently we dont have a proper Git framework whatsoever. Everybody just remote and change the code directly on the VM. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We have internal data tools that connects to the datawarehouse. Mainly the workflows are being done on SQL which has proper logging, code versioning etc as they were built and maintained in house. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This tool is actually quite solid, it even supports pyspark and spark sql. The only problem is that our team have never leveraged spark on this internal data platform. Just SQL. &lt;/p&gt;\n\n&lt;p&gt;And when we need massive update on google sheets reporting flows we just use the no.1 approach, do everything on the VM.&lt;/p&gt;\n\n&lt;p&gt;So the advice i need is, i want to understand if approach no.1 is bad practise and should we bring everything over on approach no.2? &lt;/p&gt;\n\n&lt;p&gt;My concern for keeping no.1 is that there is no proper Git framework to it. Everything is just so messy. Should i introduce Git and keep it around?&lt;/p&gt;\n\n&lt;p&gt;As for to just use no.2 approach, im not sure if pyspark can do exactly what we&amp;#39;re doing in no.1 since we lack of the exposure. &lt;/p&gt;\n\n&lt;p&gt;Sorry for lack of knowledge, im new to these kinds of infra. I dont know much other than just writing good SQL queries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m0x3c", "is_robot_indexable": true, "report_reasons": null, "author": "Nopal97", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m0x3c/advice_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m0x3c/advice_needed/", "subreddit_subscribers": 146948, "created_utc": 1702989497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m not a data engineer, am a BI analyst whos interested in becoming a DE, so currently I\u2019m trying to observe, ask questions, search\u2026 but I noticed it\u2019s really stressful job and there a lot of things i need to learn and i feel overwhelmed. \n\nHow you guys dealing with this pressure and what strategies are you using to balance between doing work and continue learning and sharpening your skills?\n\nI also want to ask about how you learned DE and if you know a geed source teaching the basics of DE for beginners. \nnote that I\u2019m a BI analyst for almost 3 years :)", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Work pressure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lzawl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702983685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not a data engineer, am a BI analyst whos interested in becoming a DE, so currently I\u2019m trying to observe, ask questions, search\u2026 but I noticed it\u2019s really stressful job and there a lot of things i need to learn and i feel overwhelmed. &lt;/p&gt;\n\n&lt;p&gt;How you guys dealing with this pressure and what strategies are you using to balance between doing work and continue learning and sharpening your skills?&lt;/p&gt;\n\n&lt;p&gt;I also want to ask about how you learned DE and if you know a geed source teaching the basics of DE for beginners. \nnote that I\u2019m a BI analyst for almost 3 years :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18lzawl", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lzawl/work_pressure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lzawl/work_pressure/", "subreddit_subscribers": 146948, "created_utc": 1702983685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_qvzmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expedia Uses WebSockets and Kafka to Query Near Real-Time Streaming Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18lz0me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_-95beDt0eTU61sjw3MyytCoW7zyk-9mjFOz5cqSkbM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702982547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoq.com/news/2023/12/expedia-websockets-kafka-query/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?auto=webp&amp;s=22be1127b3a032c0428d4bee24b8d2cb2a2233c0", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74b528758f8dac132781b7ba52cec3194953941b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b5018e5a478380623b37087dbf9dc4b43bbf70c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1b372ee2d8ab5031fb9f92e22e20238371990e25", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec9e2d3c657a70106b4608c93841f75070af8d79", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3de9be364444c886057dff0f382a0ea14630cb01", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7820fff738d4f7d7dcf3a548b2e8c36161176b6", "width": 1080, "height": 567}], "variants": {}, "id": "BmdzDPEbzLW9dI8d-7eePz_SJf3gyF91hmqD-LM6Od4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18lz0me", "is_robot_indexable": true, "report_reasons": null, "author": "rgancarz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lz0me/expedia_uses_websockets_and_kafka_to_query_near/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoq.com/news/2023/12/expedia-websockets-kafka-query/", "subreddit_subscribers": 146948, "created_utc": 1702982547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, my company is in the process of evaluating a new data platform (think DWH, DL, LH), they've asked for me to help, and it's clear they haven't thought this through much (both the type of architecture they want and how to structure an evaluation). I'm not looking for specific product recommendations, but advice on the process you go through to make a decision and not just wander in circles doing trials and POCs for the next 18 months. \n\nHere are some of the specific questions I have, but I'm all ears for other advice too:\n\n* Do you prefer trials that are fully-managed or self-managed? What are the pros/cons to each? When products have both, how do I choose? \n* When you first trial a piece of data infra, what do you focus on in the first 30-60min?\n\nI'm thinking mostly about the beginning of the evaluation right now since we're just starting, so advice on that stage would be most helpful. Thanks!\n\n&amp;#x200B;", "author_fullname": "t2_vad5s0up", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for Evaluating new data platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lj7yt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702932727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, my company is in the process of evaluating a new data platform (think DWH, DL, LH), they&amp;#39;ve asked for me to help, and it&amp;#39;s clear they haven&amp;#39;t thought this through much (both the type of architecture they want and how to structure an evaluation). I&amp;#39;m not looking for specific product recommendations, but advice on the process you go through to make a decision and not just wander in circles doing trials and POCs for the next 18 months. &lt;/p&gt;\n\n&lt;p&gt;Here are some of the specific questions I have, but I&amp;#39;m all ears for other advice too:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do you prefer trials that are fully-managed or self-managed? What are the pros/cons to each? When products have both, how do I choose? &lt;/li&gt;\n&lt;li&gt;When you first trial a piece of data infra, what do you focus on in the first 30-60min?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m thinking mostly about the beginning of the evaluation right now since we&amp;#39;re just starting, so advice on that stage would be most helpful. Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18lj7yt", "is_robot_indexable": true, "report_reasons": null, "author": "Curious_Gorges", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lj7yt/tips_for_evaluating_new_data_platforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lj7yt/tips_for_evaluating_new_data_platforms/", "subreddit_subscribers": 146948, "created_utc": 1702932727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a complex challenge where I'm exploring ways to simplify our orchestration structure where some \"tasks\" are more traditional running of small scripts or processes, but others are data transformations involving sometimes thousands of hardcoded conditional steps in a single task. \n\nI was thinking about refactoring the more complex workflows to be more independent when the possibly bad idea struck me to store the refactored code in a table format and use code injection or maybe grouped and composed in a pre-compilation step. So this is a sort of data-driven programming paradigm where the code is stored as a data object that is parsed and applied using a general bit of code.  that way I could test the code snippets atomically, generate data transformation tasks dynamically...\n\nSo has anyone experimented with storing code as data in a table format for complex workflows? What about using code injection or compilation for orchestration tasks? Any insights or warnings you can share would be incredibly helpful!", "author_fullname": "t2_4hepb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data-driven programming?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18limft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702931216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a complex challenge where I&amp;#39;m exploring ways to simplify our orchestration structure where some &amp;quot;tasks&amp;quot; are more traditional running of small scripts or processes, but others are data transformations involving sometimes thousands of hardcoded conditional steps in a single task. &lt;/p&gt;\n\n&lt;p&gt;I was thinking about refactoring the more complex workflows to be more independent when the possibly bad idea struck me to store the refactored code in a table format and use code injection or maybe grouped and composed in a pre-compilation step. So this is a sort of data-driven programming paradigm where the code is stored as a data object that is parsed and applied using a general bit of code.  that way I could test the code snippets atomically, generate data transformation tasks dynamically...&lt;/p&gt;\n\n&lt;p&gt;So has anyone experimented with storing code as data in a table format for complex workflows? What about using code injection or compilation for orchestration tasks? Any insights or warnings you can share would be incredibly helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18limft", "is_robot_indexable": true, "report_reasons": null, "author": "theinexplicablefuzz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18limft/datadriven_programming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18limft/datadriven_programming/", "subreddit_subscribers": 146948, "created_utc": 1702931216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey DE!\n\nI'm on the lookout for the perfect orchestration solution, but I'm not quite seasoned enough to be aware of all the options out there. Here's the challenge: I have a customer who lays out ETL/ELT tasks in extensive flow diagrams with intricate conditions and interactions.\n\nWhat I'm after is a solution that allows these tasks to be relatively atomic for easy testing, yet straightforward to define and code. Additionally, I need support for ML workflows, flow versioning, and the flexibility to deploy both in the cloud and locally. Ideally, I'm aiming for something lightweight that can effortlessly work with minimal setup, drawing from a configuration source featuring DAGs and tasks.\n\nI'm aware of another team in my company working on a substantial system for a similar task, leveraging Kafka, Kubernetes, and node-red. However, this setup seems a bit too hefty for my needs, and I'm not a fan of how they are hardcoding custom tasks in node-red.\n\nI also found this awesome list of [workflow engines](https://github.com/meirwah/awesome-workflow-engines)\n\nAny suggestions or recommendations for a solution that strikes the right balance for my requirements? Thanks in advance!", "author_fullname": "t2_7i2kfbec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best user friendly orchestration tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lf3uj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702922607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DE!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on the lookout for the perfect orchestration solution, but I&amp;#39;m not quite seasoned enough to be aware of all the options out there. Here&amp;#39;s the challenge: I have a customer who lays out ETL/ELT tasks in extensive flow diagrams with intricate conditions and interactions.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m after is a solution that allows these tasks to be relatively atomic for easy testing, yet straightforward to define and code. Additionally, I need support for ML workflows, flow versioning, and the flexibility to deploy both in the cloud and locally. Ideally, I&amp;#39;m aiming for something lightweight that can effortlessly work with minimal setup, drawing from a configuration source featuring DAGs and tasks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware of another team in my company working on a substantial system for a similar task, leveraging Kafka, Kubernetes, and node-red. However, this setup seems a bit too hefty for my needs, and I&amp;#39;m not a fan of how they are hardcoding custom tasks in node-red.&lt;/p&gt;\n\n&lt;p&gt;I also found this awesome list of &lt;a href=\"https://github.com/meirwah/awesome-workflow-engines\"&gt;workflow engines&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or recommendations for a solution that strikes the right balance for my requirements? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?auto=webp&amp;s=b1fe2cc2ffd4873a5d343debf9808bb93b2dda1d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43acdbac9b761d7856283ff0546bcc148ceed9d2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f37a04aebefac1e04fdf4aa4f2f317eed939547", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1fe2d902f40d902f2a7aba37b9bdfecdd241d14", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c756d6046c053bb95216b4c3b5497adba34e393d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6e1147407d0942d0dd9a5abd75bd67b94c3c0f87", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Gg5M0EsvrCF9UFGDlN_Z23sUeFVDuqN-H2DKSif_gdo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a74d378d3d7f80dc71c79d33ee779f49074f9a70", "width": 1080, "height": 540}], "variants": {}, "id": "AEPkqSVO-ZS9X-zj_UTJDoVNL-W4vCzHWkMh32X3mbc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18lf3uj", "is_robot_indexable": true, "report_reasons": null, "author": "Responsible-Rule3619", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lf3uj/best_user_friendly_orchestration_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lf3uj/best_user_friendly_orchestration_tool/", "subreddit_subscribers": 146948, "created_utc": 1702922607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  I work in finance and I\u2019m on a B.I. team which mainly uses Power BI as its analysis tool and Dremio to obtain this data; there\u2019s a table  that\u2019s used by most apps and it\u2019s heavy, which means that many times the load will break due to version recovery conflicts.\n\n  The problem is that 100s of apps use slightly different versions of this query and all of them are refreshed daily, making it really heavy on the server, kind of redundant and also very prone to NOT update correctly.\n\n  The current data pipeline is PostgreSQL &gt; Dremio &gt; ODBC connection in PBI. We want to have one main source so that the table is refreshed only once per day, avoiding useless refreshes; dataflow + incremental refresh is out of questions since manager is not willing to pay for PBI premium.\n\n  I\u2019m fairly proficient with python, which means that using airflow I could create a script that performs this incremental refresh and saves the table as .parquet, however, this is kind of reinventing the wheel, since the database and datalake should already be performing this part of the task.\n\n  My question is: how should I approach this? I\u2019m willing to use/learn any technology that helps me solve this issue (since I want to eventually become a data engineer).\n\n  My current idea would be saving the whole data to a .parquet and have all the apps reading from this file, since it will be updated daily and PBI is good at reading .parquet files. I understand that having the apps use incremental refresh could solve this issue, however we can almost never get it to do the initial load.", "author_fullname": "t2_h8ts7kb34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help to avoid having multiple apps refreshing the same information separately", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18l8sr3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702906428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in finance and I\u2019m on a B.I. team which mainly uses Power BI as its analysis tool and Dremio to obtain this data; there\u2019s a table  that\u2019s used by most apps and it\u2019s heavy, which means that many times the load will break due to version recovery conflicts.&lt;/p&gt;\n\n&lt;p&gt;The problem is that 100s of apps use slightly different versions of this query and all of them are refreshed daily, making it really heavy on the server, kind of redundant and also very prone to NOT update correctly.&lt;/p&gt;\n\n&lt;p&gt;The current data pipeline is PostgreSQL &amp;gt; Dremio &amp;gt; ODBC connection in PBI. We want to have one main source so that the table is refreshed only once per day, avoiding useless refreshes; dataflow + incremental refresh is out of questions since manager is not willing to pay for PBI premium.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m fairly proficient with python, which means that using airflow I could create a script that performs this incremental refresh and saves the table as .parquet, however, this is kind of reinventing the wheel, since the database and datalake should already be performing this part of the task.&lt;/p&gt;\n\n&lt;p&gt;My question is: how should I approach this? I\u2019m willing to use/learn any technology that helps me solve this issue (since I want to eventually become a data engineer).&lt;/p&gt;\n\n&lt;p&gt;My current idea would be saving the whole data to a .parquet and have all the apps reading from this file, since it will be updated daily and PBI is good at reading .parquet files. I understand that having the apps use incremental refresh could solve this issue, however we can almost never get it to do the initial load.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18l8sr3", "is_robot_indexable": true, "report_reasons": null, "author": "gbarza", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18l8sr3/need_help_to_avoid_having_multiple_apps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18l8sr3/need_help_to_avoid_having_multiple_apps/", "subreddit_subscribers": 146948, "created_utc": 1702906428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi,\n\nI've been itching to start a project but I do not have any idea on what type of project I wanted. I wanted to focused on DE projects that barely touches or at least touches \"classical models\".  \nPlease suggest a project if you have one hehe.\n\nWhat are your thoughts or thought process when starting a project? I get stuck whenever I do not know how to solve the problem. I tend to google it and at the end I solved it but I get a guilty feeling whenever I encounter this type of problem again and solved it but I forget about it after many months but I know what to search for it. Is that normal? I've been battling my thoughts about people who code straight or at least search less and code more.\n\nI wanted to finish a project and only then I'll be called a junior DA. (at least for myself)  \nI just need to prove it to myself and I'll be confident for the rest of my life as junior DA.", "author_fullname": "t2_d1r7vfsbx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for projects, that focuses on Data Engineering and a little help please", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lxlpj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702976673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been itching to start a project but I do not have any idea on what type of project I wanted. I wanted to focused on DE projects that barely touches or at least touches &amp;quot;classical models&amp;quot;.&lt;br/&gt;\nPlease suggest a project if you have one hehe.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts or thought process when starting a project? I get stuck whenever I do not know how to solve the problem. I tend to google it and at the end I solved it but I get a guilty feeling whenever I encounter this type of problem again and solved it but I forget about it after many months but I know what to search for it. Is that normal? I&amp;#39;ve been battling my thoughts about people who code straight or at least search less and code more.&lt;/p&gt;\n\n&lt;p&gt;I wanted to finish a project and only then I&amp;#39;ll be called a junior DA. (at least for myself)&lt;br/&gt;\nI just need to prove it to myself and I&amp;#39;ll be confident for the rest of my life as junior DA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18lxlpj", "is_robot_indexable": true, "report_reasons": null, "author": "Cold-Investment-5517", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lxlpj/looking_for_projects_that_focuses_on_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lxlpj/looking_for_projects_that_focuses_on_data/", "subreddit_subscribers": 146948, "created_utc": 1702976673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As we\u2019re starting to use DBT more, and are using it for codifying our business rules, I\u2019m wondering if anyone\u2019s had success training GPT on a DBT project. My initial attempts were lack luster. GPT seemed to know a fair amount about DBT, and of SQL, but struggled to be of much use when I trained it on a DBT project (zipped, uploaded to OpenAI GPT4).  My next step is to create a prep doc with some basic support/prompt engineering. For example, our mart \u201cLegal First Name\u201d and \u201cPreferred First Name\u201d fields come from different source tables, have different staging models. I asked GPT to tell me the difference between the two, as well as derive what SQL each would be off of source tables. Another approach I\u2019m going to try is to do a DBT run materializing as much as I can ephemerally, and see if that helps get useful responses. If this marriage of well-designed DBT project/models and GPT/LLM can be cracked, I think (at least our org) will enter a new level of accessible data literacy.", "author_fullname": "t2_f1kbimm96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT and ChatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lt3d0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.52, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702959841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As we\u2019re starting to use DBT more, and are using it for codifying our business rules, I\u2019m wondering if anyone\u2019s had success training GPT on a DBT project. My initial attempts were lack luster. GPT seemed to know a fair amount about DBT, and of SQL, but struggled to be of much use when I trained it on a DBT project (zipped, uploaded to OpenAI GPT4).  My next step is to create a prep doc with some basic support/prompt engineering. For example, our mart \u201cLegal First Name\u201d and \u201cPreferred First Name\u201d fields come from different source tables, have different staging models. I asked GPT to tell me the difference between the two, as well as derive what SQL each would be off of source tables. Another approach I\u2019m going to try is to do a DBT run materializing as much as I can ephemerally, and see if that helps get useful responses. If this marriage of well-designed DBT project/models and GPT/LLM can be cracked, I think (at least our org) will enter a new level of accessible data literacy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lt3d0", "is_robot_indexable": true, "report_reasons": null, "author": "No-Database2068", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lt3d0/dbt_and_chatgpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lt3d0/dbt_and_chatgpt/", "subreddit_subscribers": 146948, "created_utc": 1702959841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\n\nFor better or for worse,I have 31 tables under a dataset in GCP that are tied to an individual tab in a gsheet which automatically pulls in data once it is entered.\n\nAll I really need to do is union them all and do some transformation on them on this intermediate table before finally pushing out a final reporting table.\n\nI can't do the wildcard * select that GCP supports because the tables have slightly different schemas but they do have a common set of columns.\n\nI don't want to code it all in SQL since new tables may land in this dataset so I was thinking of something more automated via Python on GCP.\n\nHas anyone dealt with something like this, any thoughts?\n\nSome ideas I had were:\n\n* Using cloud functions and the big frames API to loop through each table and combine them and pushing them out to an intermediate table.\n\n* (Less Preferred) but use procedural SQL that BigQuery supports and create an automatic sql string that way", "author_fullname": "t2_zhg21", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle 31 mini tables in GCP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18l8wm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702906736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;For better or for worse,I have 31 tables under a dataset in GCP that are tied to an individual tab in a gsheet which automatically pulls in data once it is entered.&lt;/p&gt;\n\n&lt;p&gt;All I really need to do is union them all and do some transformation on them on this intermediate table before finally pushing out a final reporting table.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t do the wildcard * select that GCP supports because the tables have slightly different schemas but they do have a common set of columns.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to code it all in SQL since new tables may land in this dataset so I was thinking of something more automated via Python on GCP.&lt;/p&gt;\n\n&lt;p&gt;Has anyone dealt with something like this, any thoughts?&lt;/p&gt;\n\n&lt;p&gt;Some ideas I had were:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Using cloud functions and the big frames API to loop through each table and combine them and pushing them out to an intermediate table.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;(Less Preferred) but use procedural SQL that BigQuery supports and create an automatic sql string that way&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18l8wm1", "is_robot_indexable": true, "report_reasons": null, "author": "studentofarkad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18l8wm1/how_to_handle_31_mini_tables_in_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18l8wm1/how_to_handle_31_mini_tables_in_gcp/", "subreddit_subscribers": 146948, "created_utc": 1702906736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking to get thoughts on how to best solve this problem. \n\nThe platform I\u2019m working on has about 10m records that are searchable by the user. We currently showcase all attributes in this view on the UI and can include up to 140 attributes but creating a complex query that pulls in the data from all the various tables into one big view. \n\nWhen the user undertakes a CRUD on the main\ndatabase (MSSQL) changes happen instantly. \n\nWe then have an app function that goes off an d generates the massive SQL query to execute and then we push that result back into elasticsearch. This process is the bottleneck. \n\nI\u2019m think of using CDC or something similar to stream changes into elasticsearch instead. \n\nAny thoughts or experiences with data steaming from MSSQL into elasticsearch ?", "author_fullname": "t2_xhvhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MSSQL to ElasticSearch streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18l8fk5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702905351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to get thoughts on how to best solve this problem. &lt;/p&gt;\n\n&lt;p&gt;The platform I\u2019m working on has about 10m records that are searchable by the user. We currently showcase all attributes in this view on the UI and can include up to 140 attributes but creating a complex query that pulls in the data from all the various tables into one big view. &lt;/p&gt;\n\n&lt;p&gt;When the user undertakes a CRUD on the main\ndatabase (MSSQL) changes happen instantly. &lt;/p&gt;\n\n&lt;p&gt;We then have an app function that goes off an d generates the massive SQL query to execute and then we push that result back into elasticsearch. This process is the bottleneck. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m think of using CDC or something similar to stream changes into elasticsearch instead. &lt;/p&gt;\n\n&lt;p&gt;Any thoughts or experiences with data steaming from MSSQL into elasticsearch ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18l8fk5", "is_robot_indexable": true, "report_reasons": null, "author": "jinsy1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18l8fk5/mssql_to_elasticsearch_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18l8fk5/mssql_to_elasticsearch_streaming/", "subreddit_subscribers": 146948, "created_utc": 1702905351.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}