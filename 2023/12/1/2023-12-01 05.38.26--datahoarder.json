{"kind": "Listing", "data": {"after": "t3_187jotk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1cjqfkwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Update on the Tokyo Lab Film situation: \"Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away!\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_187iixe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 311, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away! &lt;a href=\"https://t.co/YBZ4VDupty\"&gt;https://t.co/YBZ4VDupty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Go\u2605Tanks (@tanks404) &lt;a href=\"https://twitter.com/tanks404/status/1730202703102353526?ref_src=twsrc%5Etfw\"&gt;November 30, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/tanks404/status/1730202703102353526", "author_name": "Go\u2605Tanks", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away! &lt;a href=\"https://t.co/YBZ4VDupty\"&gt;https://t.co/YBZ4VDupty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Go\u2605Tanks (@tanks404) &lt;a href=\"https://twitter.com/tanks404/status/1730202703102353526?ref_src=twsrc%5Etfw\"&gt;November 30, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "author_url": "https://twitter.com/tanks404", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away! &lt;a href=\"https://t.co/YBZ4VDupty\"&gt;https://t.co/YBZ4VDupty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Go\u2605Tanks (@tanks404) &lt;a href=\"https://twitter.com/tanks404/status/1730202703102353526?ref_src=twsrc%5Etfw\"&gt;November 30, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/187iixe", "height": 200}, "link_flair_text": "News", "can_mod_post": false, "score": 311, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HUDdx7lLHSkxWJN_9IefbOV1AOXLfy7P9VdaDSQ2vCk.jpg", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701350903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/tanks404/status/1730202703102353526", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DwnJo79WeQcmI1ZPa9wXr-ul5WOo3hJFRGHqWQZ1x0k.jpg?auto=webp&amp;s=df5ff545e212b3ceab86560758132b52efec12ac", "width": 140, "height": 140}, "resolutions": [{"url": "https://external-preview.redd.it/DwnJo79WeQcmI1ZPa9wXr-ul5WOo3hJFRGHqWQZ1x0k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cde168eca60c12c204f34ca931507c07ebd143c1", "width": 108, "height": 108}], "variants": {}, "id": "_SMiF5GKmYLGf4y56QLZ9rC9av05q7OKN2PNbSg4Zg0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DVD", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187iixe", "is_robot_indexable": true, "report_reasons": null, "author": "koempleh", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/187iixe/update_on_the_tokyo_lab_film_situation_tokyo_lab/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/tanks404/status/1730202703102353526", "subreddit_subscribers": 715423, "created_utc": 1701350903.0, "num_crossposts": 0, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/tanks404/status/1730202703102353526", "author_name": "Go\u2605Tanks", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away! &lt;a href=\"https://t.co/YBZ4VDupty\"&gt;https://t.co/YBZ4VDupty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Go\u2605Tanks (@tanks404) &lt;a href=\"https://twitter.com/tanks404/status/1730202703102353526?ref_src=twsrc%5Etfw\"&gt;November 30, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "author_url": "https://twitter.com/tanks404", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I run a video production company. We have 300TB of archived projects (and growing daily).\n\nMany years ago, our old solution for archiving was simply to dump old projects off onto an external drive, duplicate that, and have one drive at the office, one offsite elsewhere. This was ok, but not ideal. Relatively expensive per TB, and just a shit ton of physical drives.\n\nA few years ago, we had an unlimited Google Drive and 1000/1000 fibre internet. So we moved to a system where we would drop a project onto an external drive, keep that offsite, and have a duplicate of it uploaded to Google Drive. This worked ok until we reached a hidden file number limit on Google Drive. Then they removed the unlimited sizing of Google Drive accounts completely. So that was a dead end.\n\nSo then we moved that system to Dropbox a couple of years ago, as they were offering an unlimited account. This was the perfect situation. Dropbox was feature rich, fast, integrated beautifully into finder/explorer and just a great solution all round. It meant it was easy to give clients access to old data directly if they needed, etc. Anyway, as you all know, that gravy train has come to an end recently, and we now have 12 months grace with out storage on there before we have to have this sorted back to another sytem.\n\nOur options seem to be:\n\n* Go back to our old system of duplicated external drives, with one living offsite. We'd need \\~$7500AUD worth of new drives to duplicate what we currently have.\n* Buy a couple of LTO-9 tape drives (2 offices in different cities) and keep one copy on an external drive and one copy on a tape archive. This would be \\~$20000AUD of hardware upfront + media costs of \\~$2000AUD (assuming we'd get maybe 30TB per tape on the 18TB raw LTO 9 tapes). So more expensive upfront but would maybe pay off eventually?\n* Build a linustechtips style beast of a NAS. Raw drive cost would be similar to the external drives, but would have the advantage of being accessible remotely. Would then need to spend $5000-10000AUD on the actual hardware on top of the drives. Also have the problem of ever growing storage needs. This solution we could potentially not duplicate the data to external drives though and live with RAID as only form of redundancy...\n* Another clour storage service? Anything fast and decent enough that comes at a reasonable cost?\n\nAny advice here would be appreciated!", "author_fullname": "t2_8o00p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "300TB of data. Dropbox and Google are dead to me. Next options. Cloud? Tape? NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187b44e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 102, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 102, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701322947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I run a video production company. We have 300TB of archived projects (and growing daily).&lt;/p&gt;\n\n&lt;p&gt;Many years ago, our old solution for archiving was simply to dump old projects off onto an external drive, duplicate that, and have one drive at the office, one offsite elsewhere. This was ok, but not ideal. Relatively expensive per TB, and just a shit ton of physical drives.&lt;/p&gt;\n\n&lt;p&gt;A few years ago, we had an unlimited Google Drive and 1000/1000 fibre internet. So we moved to a system where we would drop a project onto an external drive, keep that offsite, and have a duplicate of it uploaded to Google Drive. This worked ok until we reached a hidden file number limit on Google Drive. Then they removed the unlimited sizing of Google Drive accounts completely. So that was a dead end.&lt;/p&gt;\n\n&lt;p&gt;So then we moved that system to Dropbox a couple of years ago, as they were offering an unlimited account. This was the perfect situation. Dropbox was feature rich, fast, integrated beautifully into finder/explorer and just a great solution all round. It meant it was easy to give clients access to old data directly if they needed, etc. Anyway, as you all know, that gravy train has come to an end recently, and we now have 12 months grace with out storage on there before we have to have this sorted back to another sytem.&lt;/p&gt;\n\n&lt;p&gt;Our options seem to be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Go back to our old system of duplicated external drives, with one living offsite. We&amp;#39;d need ~$7500AUD worth of new drives to duplicate what we currently have.&lt;/li&gt;\n&lt;li&gt;Buy a couple of LTO-9 tape drives (2 offices in different cities) and keep one copy on an external drive and one copy on a tape archive. This would be ~$20000AUD of hardware upfront + media costs of ~$2000AUD (assuming we&amp;#39;d get maybe 30TB per tape on the 18TB raw LTO 9 tapes). So more expensive upfront but would maybe pay off eventually?&lt;/li&gt;\n&lt;li&gt;Build a linustechtips style beast of a NAS. Raw drive cost would be similar to the external drives, but would have the advantage of being accessible remotely. Would then need to spend $5000-10000AUD on the actual hardware on top of the drives. Also have the problem of ever growing storage needs. This solution we could potentially not duplicate the data to external drives though and live with RAID as only form of redundancy...&lt;/li&gt;\n&lt;li&gt;Another clour storage service? Anything fast and decent enough that comes at a reasonable cost?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice here would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187b44e", "is_robot_indexable": true, "report_reasons": null, "author": "campster123", "discussion_type": null, "num_comments": 144, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187b44e/300tb_of_data_dropbox_and_google_are_dead_to_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187b44e/300tb_of_data_dropbox_and_google_are_dead_to_me/", "subreddit_subscribers": 715423, "created_utc": 1701322947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im wondering if anyone has ever scraped pricing data for the major supermarkets?\n\nWith the cost of living crisis currently im interested to see which products have increased dramatically and if theres any correlation across brands or sectors.\n\nThanks!", "author_fullname": "t2_eq196", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UK Datahoarders - Has anyone ever scraped supermarket price files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187ivbk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701351907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im wondering if anyone has ever scraped pricing data for the major supermarkets?&lt;/p&gt;\n\n&lt;p&gt;With the cost of living crisis currently im interested to see which products have increased dramatically and if theres any correlation across brands or sectors.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187ivbk", "is_robot_indexable": true, "report_reasons": null, "author": "jpjapers", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187ivbk/uk_datahoarders_has_anyone_ever_scraped/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187ivbk/uk_datahoarders_has_anyone_ever_scraped/", "subreddit_subscribers": 715423, "created_utc": 1701351907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m in search of a small, travel-sized way to store massive amounts of data, primarily 4K remux movies. I\u2019ll be going to Antarctica, where my internet will be limited. I\u2019ve been told that they have plenty of standard-def movies, a handful of blue rays, and a small-ish collection of HD movies digitally. What they\u2019re missing, they say, is full-bitrate 4K movies, and I want to bring some. I know these films can grow upwards of 30-50GB.\n\nI have been looking at M.2 NVME drives\u2026 grabbing a few 2TB drives and enclosures seem like a good option, I think I should be able to squeeze about 40 movies on each (assuming 50gb) drive. Is there a better option? Spinning disk drives are not an option, otherwise I\u2019d grab a 20tb HDD and be done with it.\nEdit: I see the typo in the title\u2026 I can\u2019t change it\u2026 it\u2019s supposed to say Travel-Sized Data Hoarding. ", "author_fullname": "t2_vdxjkybi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Travel-Sized says hoarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187rse3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701374953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in search of a small, travel-sized way to store massive amounts of data, primarily 4K remux movies. I\u2019ll be going to Antarctica, where my internet will be limited. I\u2019ve been told that they have plenty of standard-def movies, a handful of blue rays, and a small-ish collection of HD movies digitally. What they\u2019re missing, they say, is full-bitrate 4K movies, and I want to bring some. I know these films can grow upwards of 30-50GB.&lt;/p&gt;\n\n&lt;p&gt;I have been looking at M.2 NVME drives\u2026 grabbing a few 2TB drives and enclosures seem like a good option, I think I should be able to squeeze about 40 movies on each (assuming 50gb) drive. Is there a better option? Spinning disk drives are not an option, otherwise I\u2019d grab a 20tb HDD and be done with it.\nEdit: I see the typo in the title\u2026 I can\u2019t change it\u2026 it\u2019s supposed to say Travel-Sized Data Hoarding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187rse3", "is_robot_indexable": true, "report_reasons": null, "author": "DisposabIeHuman", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187rse3/travelsized_says_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187rse3/travelsized_says_hoarding/", "subreddit_subscribers": 715423, "created_utc": 1701374953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_32ywoabe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often does server part deals restock / does it restock on recertified drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 62, "top_awarded_type": null, "hide_score": false, "name": "t3_187bar0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iwCBiRezStZy1nV0nrfwes-tcwpuobFCoagbHyQe4kw.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701323579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ebuzyuw8cf3c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?auto=webp&amp;s=0effe9c41dc3e4ae189bab531c0d23d4ef7f21d3", "width": 1892, "height": 848}, "resolutions": [{"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a0584255087489f9c1fb9b7dd91cae942c93b8c", "width": 108, "height": 48}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c787f1c3ea4535aba5eacc760512f31be6189260", "width": 216, "height": 96}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b098bc52e3afdb699f739b6c4c44bbd2c70ce50", "width": 320, "height": 143}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8be3668603d7fe909e7253220bd484fd5e7e7799", "width": 640, "height": 286}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b374578fa8f6620e443c94603391ac8f9c2a3e4", "width": 960, "height": 430}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ee083859db6095bf2d13a935dbbe0830534e72c", "width": 1080, "height": 484}], "variants": {}, "id": "pL5FDZKw5hYWcV0CxlhT5h9J4amN0SyKGWZLUdLT0H0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "14TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187bar0", "is_robot_indexable": true, "report_reasons": null, "author": "Beanconscriptog", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/187bar0/how_often_does_server_part_deals_restock_does_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ebuzyuw8cf3c1.png", "subreddit_subscribers": 715423, "created_utc": 1701323579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My HDD is failing and manually copying files to an external HDD is taking forever. We're talking about KB/s transfer speeds. I have around 1.3TB in it, but only really need around 400-500gb. I'm afraid this method is only hastening its death.\n\nI used to use a dos program, Norton Ghost, to clone partitions. Would a similar program be better for backing up my files vs copy/pasting? I feel like the manual copying is wearing the drive out faster. On the other hand, cloning might just copy over corrupted files on bad sectors and all (I know nothing about this so I could be wrong).\n\nThanks in advance.", "author_fullname": "t2_bgpdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it better to clone a failing HDD than manually backing it up via copy/paste?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187aus1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701322095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My HDD is failing and manually copying files to an external HDD is taking forever. We&amp;#39;re talking about KB/s transfer speeds. I have around 1.3TB in it, but only really need around 400-500gb. I&amp;#39;m afraid this method is only hastening its death.&lt;/p&gt;\n\n&lt;p&gt;I used to use a dos program, Norton Ghost, to clone partitions. Would a similar program be better for backing up my files vs copy/pasting? I feel like the manual copying is wearing the drive out faster. On the other hand, cloning might just copy over corrupted files on bad sectors and all (I know nothing about this so I could be wrong).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187aus1", "is_robot_indexable": true, "report_reasons": null, "author": "imanol1898", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187aus1/is_it_better_to_clone_a_failing_hdd_than_manually/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187aus1/is_it_better_to_clone_a_failing_hdd_than_manually/", "subreddit_subscribers": 715423, "created_utc": 1701322095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m in search of a small, travel-sized way to store massive amounts of data, primarily 4K remux movies. I\u2019ll be going to Antarctica, where my internet will be limited. I\u2019ve been told that they have plenty of standard-def movies, a handful of blue rays, and a small-ish collection of HD movies digitally. What they\u2019re missing, they say, is full-bitrate 4K movies, and I want to bring some. I know these films can grow upwards of 30-50GB.\n\nI have been looking at M.2 NVME drives\u2026 grabbing a few 2TB drives and enclosures seem like a good option when you compare $/GB, I think I should be able to squeeze about 40 movies on each (assuming 50gb) drive. Is there a better option? Spinning disk drives are not an option due to the size, weight, and fragility, otherwise I\u2019d grab a 20tb HDD and be done with it.", "author_fullname": "t2_o87j0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Travel-Sized Data Hoarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187smk5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701377059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in search of a small, travel-sized way to store massive amounts of data, primarily 4K remux movies. I\u2019ll be going to Antarctica, where my internet will be limited. I\u2019ve been told that they have plenty of standard-def movies, a handful of blue rays, and a small-ish collection of HD movies digitally. What they\u2019re missing, they say, is full-bitrate 4K movies, and I want to bring some. I know these films can grow upwards of 30-50GB.&lt;/p&gt;\n\n&lt;p&gt;I have been looking at M.2 NVME drives\u2026 grabbing a few 2TB drives and enclosures seem like a good option when you compare $/GB, I think I should be able to squeeze about 40 movies on each (assuming 50gb) drive. Is there a better option? Spinning disk drives are not an option due to the size, weight, and fragility, otherwise I\u2019d grab a 20tb HDD and be done with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187smk5", "is_robot_indexable": true, "report_reasons": null, "author": "shootingcharlie8", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187smk5/travelsized_data_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187smk5/travelsized_data_hoarding/", "subreddit_subscribers": 715423, "created_utc": 1701377059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Built my first Server. 8, 6TB drives using Storage Spaces with Parity for 29TB usable. Utilized the M.2 slot for a 6 slot SATA adapter. This leaves me with a free PCIE slot for the future. Also installed is a blue ray drive to backup my physical disks and a hot swap bay. Looking forward to using it with Plex and beyond! \n\nBuild Config:\n\n-Asus Z170M\n-i7 6700K (OC to 4.2GHz)\n-16gb ddr4 2400MHz in XMP\n-8x MG04ACA600E Toshiba 6TB HDD\n-GTX1650LP\n-M.2 to SATA3.0, Expansion Card, 6 Port", "author_fullname": "t2_wpy3t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "New Plex Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"priik62x9l3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/priik62x9l3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=06e8cfc005062039de70c9d52f3f7bedf1bbb91f"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/priik62x9l3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e75b1b2c4e5c2e2967d18ab3785bdd5a23be9bb5"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/priik62x9l3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27f1e538fe633147fe5ea884a4c9d808029c2604"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/priik62x9l3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e04327686d6236507426d75a1a2b05ecef753d1"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/priik62x9l3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6afdfe4c4e397d2d1072321045ac7f20b0f1ac5f"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/priik62x9l3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3ddf76658a374dcf39583f4364a0c51ef0b8964b"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/priik62x9l3c1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=e8c3c5f82227b2b5199bb042142c7c9ae65248fe"}, "id": "priik62x9l3c1"}, "hwe2v62x9l3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/hwe2v62x9l3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea15754c8693856706d4fb9e6e9d18fe8182f998"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/hwe2v62x9l3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c8e7497f4ba9b240a89c77a11359347f5d2d284"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/hwe2v62x9l3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f693afec4d6529be4e4e69c7b4919d1aef43e003"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/hwe2v62x9l3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=67e56b95c3b32f7d10a218c2e651f7dcb76a29f7"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/hwe2v62x9l3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e80c02a329e89fe2990ac28cd594d37b1664c922"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/hwe2v62x9l3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1cec69121f5c15e0f240d58d0131cfe81fedf25d"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/hwe2v62x9l3c1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=227bab96fcd37ea83b7387a6e735d4bae2c02ec2"}, "id": "hwe2v62x9l3c1"}, "cdhvc82x9l3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/cdhvc82x9l3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6480b5ce8ede457d994fe8545eb12d5871d40efc"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/cdhvc82x9l3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c5ad6a5ceeca2e43dba47ba2e18c4a9886a7d34f"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/cdhvc82x9l3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=29114a5530a5917f30e780d084921c867f5d7afb"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/cdhvc82x9l3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9a24c13ab631d566ad19e93f2f5ce38994e1913"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/cdhvc82x9l3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e1265104d02907998aa7d80fb959d9be0cb25c0"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/cdhvc82x9l3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c355dc4c3c7907996acb2a6a73ad26ae8edd6aa7"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/cdhvc82x9l3c1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=eb434018431a51494b075bf5f54cb45532e9e24f"}, "id": "cdhvc82x9l3c1"}, "1e87s62x9l3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/1e87s62x9l3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f8a7e546d6a8176cc04826629da8b0ea8ee86a6"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/1e87s62x9l3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0da6fd7e1a53c8c45bf8b5150d27bfcc1013208e"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/1e87s62x9l3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=351147487f370aac6d0b4e22f2f6247b4ce49949"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/1e87s62x9l3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8d6b6c0b85d7d2133ef1249bf0c51d299f14806"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/1e87s62x9l3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba12466c946adb4e3e33dcb2ccf4dcb64de94a7d"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/1e87s62x9l3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8c878532dfbe61651d66aa1c64a6da6881f81ca9"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/1e87s62x9l3c1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=0bdaee05878a3b0292573c5eca9116754b84de0c"}, "id": "1e87s62x9l3c1"}}, "name": "t3_187zq5b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 4, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "hwe2v62x9l3c1", "id": 367799771}, {"media_id": "priik62x9l3c1", "id": 367799772}, {"media_id": "cdhvc82x9l3c1", "id": 367799773}, {"media_id": "1e87s62x9l3c1", "id": 367799774}]}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ILIB_TvBdr-e164Vmoghj8DMgfhM_axwz5gcLh1RCl4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701395362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Built my first Server. 8, 6TB drives using Storage Spaces with Parity for 29TB usable. Utilized the M.2 slot for a 6 slot SATA adapter. This leaves me with a free PCIE slot for the future. Also installed is a blue ray drive to backup my physical disks and a hot swap bay. Looking forward to using it with Plex and beyond! &lt;/p&gt;\n\n&lt;p&gt;Build Config:&lt;/p&gt;\n\n&lt;p&gt;-Asus Z170M\n-i7 6700K (OC to 4.2GHz)\n-16gb ddr4 2400MHz in XMP\n-8x MG04ACA600E Toshiba 6TB HDD\n-GTX1650LP\n-M.2 to SATA3.0, Expansion Card, 6 Port&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/187zq5b", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187zq5b", "is_robot_indexable": true, "report_reasons": null, "author": "LegoPaco", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187zq5b/new_plex_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/187zq5b", "subreddit_subscribers": 715423, "created_utc": 1701395362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an Unraid system with 14 drives in the NAS, and a DAS with 8 drives connected.\n\nCurrently I have 6 drives in NAS connected via motherboard sata, and the other 8 via a pcie to sata card.\n\nI then have an 8e HBA in IT mode connected to a SFF-8087 to SFF-8088 adapter in the DAS, connecting to the 8 drives.\n\nI am changing the motherboard and CPU, and stupidly brought a motherboard with only 4 sata, 1 x pcie x16, and 1 pcie x4.\n\nIs there a SAS expander I could get, along with an internal HBA, that could connect to the internal drives of the NAS, and then have external ports to go to the DAS? I looked at the Adaptec 82885T, and apparently the 2 external ports are input only. Would it maybe better to get 2 of those, 1 in NAS, and 1 in the DAS, and use my current HBA 8e to connect to them both?", "author_fullname": "t2_aodccc8g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAS expander options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187br0y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701325077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an Unraid system with 14 drives in the NAS, and a DAS with 8 drives connected.&lt;/p&gt;\n\n&lt;p&gt;Currently I have 6 drives in NAS connected via motherboard sata, and the other 8 via a pcie to sata card.&lt;/p&gt;\n\n&lt;p&gt;I then have an 8e HBA in IT mode connected to a SFF-8087 to SFF-8088 adapter in the DAS, connecting to the 8 drives.&lt;/p&gt;\n\n&lt;p&gt;I am changing the motherboard and CPU, and stupidly brought a motherboard with only 4 sata, 1 x pcie x16, and 1 pcie x4.&lt;/p&gt;\n\n&lt;p&gt;Is there a SAS expander I could get, along with an internal HBA, that could connect to the internal drives of the NAS, and then have external ports to go to the DAS? I looked at the Adaptec 82885T, and apparently the 2 external ports are input only. Would it maybe better to get 2 of those, 1 in NAS, and 1 in the DAS, and use my current HBA 8e to connect to them both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187br0y", "is_robot_indexable": true, "report_reasons": null, "author": "minimaddnz", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/187br0y/sas_expander_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187br0y/sas_expander_options/", "subreddit_subscribers": 715423, "created_utc": 1701325077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_m4m7dc3z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Figured this might be a slightly better sub to ask around for this information than r/homelab even though that\u2019s what this is around. New build, and although the server posts into unraid, no HDDs/SSDs are being picked up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1pr04pfe8l3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/1pr04pfe8l3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b76ab79f9667e39d4882b5f5b8d9272d751d1bf2"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/1pr04pfe8l3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c2bfec64cd81a7f9cb0be0c099b5ae9a58730db"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/1pr04pfe8l3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=da51b6e04d57fa36bf8d76e2ba10a8420431fca1"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/1pr04pfe8l3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7217afa33ada0610d2c18abdeaa0343f93101055"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/1pr04pfe8l3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=320924bfa2e165d7970c4e643e6001668be70587"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/1pr04pfe8l3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9993278df4fe5e1e97cd96923e9ffd58346c95b0"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/1pr04pfe8l3c1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=3453b1f7da3292899aedcb4f54a8dd9705f8391c"}, "id": "1pr04pfe8l3c1"}, "cy1zwnfe8l3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/cy1zwnfe8l3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=55734e017a2a4c8ad18a1feaba85dfe413f58183"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/cy1zwnfe8l3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b980ed9449c5ac604c2793a19a60bbcaf9c2765"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/cy1zwnfe8l3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1dcf57505edf0922a22afb4043b285a1afee4cde"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/cy1zwnfe8l3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3def40111fd6810266f3e2e565d64eda71b2ad6"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/cy1zwnfe8l3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca3ff78d1c596865e770bf99ede11faee8a728b0"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/cy1zwnfe8l3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e93b8a1135eb12b46da7758e48ae6c34103ced6c"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/cy1zwnfe8l3c1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=65d8bb7d8a933a41d689b1e6c0e89550263ca73e"}, "id": "cy1zwnfe8l3c1"}, "x8nwsxfe8l3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/x8nwsxfe8l3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c9cf35d05011a70bb9238b6a0e2f7d5b667b1d3"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/x8nwsxfe8l3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d55d68c9903836428c65567a7bc9b76a516b3f4"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/x8nwsxfe8l3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d0091a76b201369e86b688fb18c11f081611230"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/x8nwsxfe8l3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f941e15e88d474b1431514624863f2103626b95"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/x8nwsxfe8l3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b654d0fd5810022f7ca2ca053f209455537196d5"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/x8nwsxfe8l3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c2d35698c685fa9215ae89e2264d4c39c3a39fc"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/x8nwsxfe8l3c1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=98b90c3e6b2be36180dbb8a711147d3d1381b5b8"}, "id": "x8nwsxfe8l3c1"}}, "name": "t3_187zzk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "Overall build", "media_id": "cy1zwnfe8l3c1", "id": 367804557}, {"caption": "HBA Card", "media_id": "1pr04pfe8l3c1", "id": 367804558}, {"caption": "HDDs/SSDs plugged into the HBA", "media_id": "x8nwsxfe8l3c1", "id": 367804559}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/C3GCTERf8Fr0dbV9PFTYLpBjObFBsI10z5M8mYmFVS8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701396099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/187zzk8", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187zzk8", "is_robot_indexable": true, "report_reasons": null, "author": "WeetBixMiloAndMilk", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187zzk8/figured_this_might_be_a_slightly_better_sub_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/187zzk8", "subreddit_subscribers": 715423, "created_utc": 1701396099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What is truly wrong with individual drives representing individual volumes with manual redundancy only on data needing it?\n\nFor instance, let\u2019s say I have a 20TB, a 12TB, and an 8TB drive for a 4-bay NAS, then most RAID setups don\u2019t make sense. Additionally, let\u2019s say I really only have &lt;8TB of data needing redundancy too\u2026 So, in this case, is there a real reason to not setup as three individual drives, with manual redundancy of necessary data; like a nightly rsync, etc.?\n\nI get why RAID 5, say, is the most recommended, and also understand that RAID isn\u2019t a true, completely safe backup, nor is the setup described above.\n\nJust curious for perspectives from longer-time datahoarders. I\u2019m a newbie to multi-drive NAS hardware.\n\n(Also, is the scenario I described above even possible on, say, QNap or Synology; or is it only RAID or JBOD?)", "author_fullname": "t2_8nrw6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why not RAID sometimes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187y1lj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701390752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is truly wrong with individual drives representing individual volumes with manual redundancy only on data needing it?&lt;/p&gt;\n\n&lt;p&gt;For instance, let\u2019s say I have a 20TB, a 12TB, and an 8TB drive for a 4-bay NAS, then most RAID setups don\u2019t make sense. Additionally, let\u2019s say I really only have &amp;lt;8TB of data needing redundancy too\u2026 So, in this case, is there a real reason to not setup as three individual drives, with manual redundancy of necessary data; like a nightly rsync, etc.?&lt;/p&gt;\n\n&lt;p&gt;I get why RAID 5, say, is the most recommended, and also understand that RAID isn\u2019t a true, completely safe backup, nor is the setup described above.&lt;/p&gt;\n\n&lt;p&gt;Just curious for perspectives from longer-time datahoarders. I\u2019m a newbie to multi-drive NAS hardware.&lt;/p&gt;\n\n&lt;p&gt;(Also, is the scenario I described above even possible on, say, QNap or Synology; or is it only RAID or JBOD?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187y1lj", "is_robot_indexable": true, "report_reasons": null, "author": "rgthree", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187y1lj/why_not_raid_sometimes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187y1lj/why_not_raid_sometimes/", "subreddit_subscribers": 715423, "created_utc": 1701390752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m trying to expand my ZFS pool on the cheap, and am torn between shucking the $150 dollar Seagate 14TB externals vs buying a manufactured refurbished Seagate 16TB from ServerPartsDeals for the same price. What would y\u2019all\u2019s recommendation be based on experience or longevity data? \n\nThanks for your help!", "author_fullname": "t2_2mooplap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shucked vs Refurbished", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187lveh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701359934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to expand my ZFS pool on the cheap, and am torn between shucking the $150 dollar Seagate 14TB externals vs buying a manufactured refurbished Seagate 16TB from ServerPartsDeals for the same price. What would y\u2019all\u2019s recommendation be based on experience or longevity data? &lt;/p&gt;\n\n&lt;p&gt;Thanks for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187lveh", "is_robot_indexable": true, "report_reasons": null, "author": "techromancer1", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187lveh/shucked_vs_refurbished/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187lveh/shucked_vs_refurbished/", "subreddit_subscribers": 715423, "created_utc": 1701359934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, in purchasing or re-purposing used drives to add to a large veeam repository, I've run into various ways that drives have been locked. So far, I've seen three different ways that sas drives have been locked in a way that I could not initially use them in my KTL3 shelves attached to TrueNAS SCALE servers.\n\n&amp;nbsp;\n\n**1. SED locking:**\nI have disks from retired tegile disk arrays. I went to spin these drives up, and they were basically unreadable, and dmesg was spitting out a bunch of errors. You can identify these disks physically because they have a PSID printed on the top of them.\nTo clear these locks, you can use the sedutil-cli utility to wipe the disk and clear the lock:\n\n    sedutil-cli --yesIreallywanttoERASEALLmydatausingthePSID &lt;PSID key goes here&gt; /dev/sdr\n\nThis process ran in seconds, and access was gained to the drive.\n\n&amp;nbsp;\n\n**2. T10-DIF:**\nThis wasn't obvious to me, as I've seen netapp formatted drives that won't read because linux won't read 520 byte block size drives. These drives, tho, reported as 512B formatted drives, but I couldn't read/write them. Upon further inspection, I found that they were indeed formatted at 512B blocks, but then there was another 8 bits of data, resulting in 520B blocks. I was able to reformat them with:\n\n    sg_format --format --size=512 /dev/sdl\n\nThis process takes time, nearly 12-24 hours, per drive (12TB disks). After the format was complete, i pulled and reinserted the drives into the shelf and was able to access them successfully. tmux was helpful in formatting the disks in parallel so i didn't have to wait for each one to finish before starting the next.\n\n&amp;nbsp;\n\n**3. SCSI reservation:**\nThis was even more obscure. I popped a drive in the shelf, and unlike the other drives that spun up and the activity light went out, these drives out of another tegile array came up and the activity light came on, blinked a few times as linux identified it, and then remained on. I was seeing the following in dmesg:\n\n    [174050.317023]  sdt: unable to read partition table\n    [174050.318401] sd 8:0:25:0: [sdt] Attached SCSI disk\n    [174050.899555] hpsa 0000:05:00.0: cp 0000000004843087 has status 0x18 Sense: 0xff, ASC: 0xff, ASCQ: 0xff, Returning result: 0x18\n    [174050.901621] sd 8:0:25:0: reservation conflict\n\nThese were locked in a way that the disk was inaccessible because the access was reserved at a SCSI command level by the firmware of the drive, so this wasn't about the format of the disk, or an encryption key, but at a lower level where the host simply is denied access to the drive because another host as some time set a reservation, and the current host can't automatically clear it to gain access. I was able to gain access by using sg_persist to set a new reservation, and then clear all reservations with it:\n\n    sg_persist --out --register-ignore  --param-sark=abc1234 /dev/sdr\n    sg_persist /dev/sdr\n    No service action given; assume Persistent Reserve In command\n    with Read Keys service action\n      HGST      HUS726040ALS211   BD05\n      Peripheral device type: disk\n      PR generation=0x2, 2 registered reservation keys follow:\n        0xabc1234\n        0x5bf43c4200000001\n    sg_persist --out -C --param-rk=abc1234 /dev/sdr\n\n&amp;nbsp;&amp;nbsp;\n\nFor the tegile drives, they were ALL sed locked, but three disk out of each system had scsi reservations set. I had to clear the scsi reservation, then clear the disk with SED PSID, and then the drives were accessable. I could not see, via sedutil-cli, that they were SED locked until the scsi reservation was cleared.\n\n&amp;nbsp;&amp;nbsp;\n\nI don't know who this will help, but thought I'd throw it out there for us folks not paying for new drives.", "author_fullname": "t2_16lp28n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "read only / locked hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187lcm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701360530.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701358571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, in purchasing or re-purposing used drives to add to a large veeam repository, I&amp;#39;ve run into various ways that drives have been locked. So far, I&amp;#39;ve seen three different ways that sas drives have been locked in a way that I could not initially use them in my KTL3 shelves attached to TrueNAS SCALE servers.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. SED locking:&lt;/strong&gt;\nI have disks from retired tegile disk arrays. I went to spin these drives up, and they were basically unreadable, and dmesg was spitting out a bunch of errors. You can identify these disks physically because they have a PSID printed on the top of them.\nTo clear these locks, you can use the sedutil-cli utility to wipe the disk and clear the lock:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sedutil-cli --yesIreallywanttoERASEALLmydatausingthePSID &amp;lt;PSID key goes here&amp;gt; /dev/sdr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This process ran in seconds, and access was gained to the drive.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. T10-DIF:&lt;/strong&gt;\nThis wasn&amp;#39;t obvious to me, as I&amp;#39;ve seen netapp formatted drives that won&amp;#39;t read because linux won&amp;#39;t read 520 byte block size drives. These drives, tho, reported as 512B formatted drives, but I couldn&amp;#39;t read/write them. Upon further inspection, I found that they were indeed formatted at 512B blocks, but then there was another 8 bits of data, resulting in 520B blocks. I was able to reformat them with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sg_format --format --size=512 /dev/sdl\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This process takes time, nearly 12-24 hours, per drive (12TB disks). After the format was complete, i pulled and reinserted the drives into the shelf and was able to access them successfully. tmux was helpful in formatting the disks in parallel so i didn&amp;#39;t have to wait for each one to finish before starting the next.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. SCSI reservation:&lt;/strong&gt;\nThis was even more obscure. I popped a drive in the shelf, and unlike the other drives that spun up and the activity light went out, these drives out of another tegile array came up and the activity light came on, blinked a few times as linux identified it, and then remained on. I was seeing the following in dmesg:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[174050.317023]  sdt: unable to read partition table\n[174050.318401] sd 8:0:25:0: [sdt] Attached SCSI disk\n[174050.899555] hpsa 0000:05:00.0: cp 0000000004843087 has status 0x18 Sense: 0xff, ASC: 0xff, ASCQ: 0xff, Returning result: 0x18\n[174050.901621] sd 8:0:25:0: reservation conflict\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;These were locked in a way that the disk was inaccessible because the access was reserved at a SCSI command level by the firmware of the drive, so this wasn&amp;#39;t about the format of the disk, or an encryption key, but at a lower level where the host simply is denied access to the drive because another host as some time set a reservation, and the current host can&amp;#39;t automatically clear it to gain access. I was able to gain access by using sg_persist to set a new reservation, and then clear all reservations with it:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sg_persist --out --register-ignore  --param-sark=abc1234 /dev/sdr\nsg_persist /dev/sdr\nNo service action given; assume Persistent Reserve In command\nwith Read Keys service action\n  HGST      HUS726040ALS211   BD05\n  Peripheral device type: disk\n  PR generation=0x2, 2 registered reservation keys follow:\n    0xabc1234\n    0x5bf43c4200000001\nsg_persist --out -C --param-rk=abc1234 /dev/sdr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;For the tegile drives, they were ALL sed locked, but three disk out of each system had scsi reservations set. I had to clear the scsi reservation, then clear the disk with SED PSID, and then the drives were accessable. I could not see, via sedutil-cli, that they were SED locked until the scsi reservation was cleared.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know who this will help, but thought I&amp;#39;d throw it out there for us folks not paying for new drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187lcm0", "is_robot_indexable": true, "report_reasons": null, "author": "gmc_5303", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187lcm0/read_only_locked_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187lcm0/read_only_locked_hard_drives/", "subreddit_subscribers": 715423, "created_utc": 1701358571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\n&amp;#x200B;\n\nI would like some advice on how to archive a large number of download links from the [https://uloz.to](https://uloz.to) website.\n\n&amp;#x200B;\n\nIt's a public file sharing website which unfortunately disables searching between their files and then it should only be possible to download if you have a link to the file you want to download.\n\nIs there any way to back up the links to the files that are in their database? Is there a script that can go through it one by one and write them to an excel file?\n\n&amp;#x200B;\n\nThank you for your help.", "author_fullname": "t2_f6ydydphm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup links to files on uloz.to", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187epf1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701340394.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701336801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would like some advice on how to archive a large number of download links from the &lt;a href=\"https://uloz.to\"&gt;https://uloz.to&lt;/a&gt; website.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a public file sharing website which unfortunately disables searching between their files and then it should only be possible to download if you have a link to the file you want to download.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to back up the links to the files that are in their database? Is there a script that can go through it one by one and write them to an excel file?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3Q97mNEkTbNb6kLsRXcZnSnjZDfXCltsjWeb7wsPKMs.jpg?auto=webp&amp;s=1c3764a4072f31b6ea14d42c813aee98e03f8c6a", "width": 600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3Q97mNEkTbNb6kLsRXcZnSnjZDfXCltsjWeb7wsPKMs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0c3c1ed6e2d39c413f1fc4a7628138ad38863d3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/3Q97mNEkTbNb6kLsRXcZnSnjZDfXCltsjWeb7wsPKMs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7094c96b5ad9a7c958820029341084039e88097e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/3Q97mNEkTbNb6kLsRXcZnSnjZDfXCltsjWeb7wsPKMs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24bc78655d394adaba1e32e7faad7399bed4dbf6", "width": 320, "height": 320}], "variants": {}, "id": "pRqK59bWm7VS-3qKQR_j8gMutFJRCzJjMa9gYDelECk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187epf1", "is_robot_indexable": true, "report_reasons": null, "author": "FeelsGoodBlok", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187epf1/how_to_backup_links_to_files_on_ulozto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187epf1/how_to_backup_links_to_files_on_ulozto/", "subreddit_subscribers": 715423, "created_utc": 1701336801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/s9ntzdmc4m3c1.png?width=1358&amp;format=png&amp;auto=webp&amp;s=36949e56771882252eeda19a2cd004149a8c3139\n\nI reinstalled my win 11 os and migrated the exported IDM file (\"task\"--&gt;export-&gt;to IDM export) to the new installation. same version. but it gets those yellow error bubbles and none of the files can be opened even though the path is the same.\n\ni mainly need it to index my downloads.\n\nty\n\n(it seemed perhaps I clicked \"cancel importing\" when the app found downloaded files with the same directory path already existing there, so the app ignores them altogether. those files are what i think, the actual downloaded files i want to index my downloads. i was worried it might override those existing files with empty ones and start to redo the downloads\n\nis there a way to change importing settings? how do i get those files back)", "author_fullname": "t2_pd6uct2y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "trying to restore exported IDM file but got those error bubbles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": true, "media_metadata": {"s9ntzdmc4m3c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/s9ntzdmc4m3c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=05874ee1164f5b13d82877cf23d099736026cec2"}, {"y": 128, "x": 216, "u": "https://preview.redd.it/s9ntzdmc4m3c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a6995c7aa480ed85545b4b931b96c2cc17ffdf7"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/s9ntzdmc4m3c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c45cb6978bde28e4de9ad830abc6add256af479a"}, {"y": 379, "x": 640, "u": "https://preview.redd.it/s9ntzdmc4m3c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=da70fa96bc3b61f0fcf37a4bdf45e454623a8128"}, {"y": 569, "x": 960, "u": "https://preview.redd.it/s9ntzdmc4m3c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=309b81bdc80bafe72ac5f382cf8097f854ebf5f2"}, {"y": 641, "x": 1080, "u": "https://preview.redd.it/s9ntzdmc4m3c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cbb745d0cf6048efa744dfb7333c4df4ea5c8a80"}], "s": {"y": 806, "x": 1358, "u": "https://preview.redd.it/s9ntzdmc4m3c1.png?width=1358&amp;format=png&amp;auto=webp&amp;s=36949e56771882252eeda19a2cd004149a8c3139"}, "id": "s9ntzdmc4m3c1"}}, "name": "t3_188365z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/nqlBTHPa2mSSa24HfHVezb5zj5GYFWaS-SnGGtBp2L4.jpg", "edited": 1701406456.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701405716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s9ntzdmc4m3c1.png?width=1358&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=36949e56771882252eeda19a2cd004149a8c3139\"&gt;https://preview.redd.it/s9ntzdmc4m3c1.png?width=1358&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=36949e56771882252eeda19a2cd004149a8c3139&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I reinstalled my win 11 os and migrated the exported IDM file (&amp;quot;task&amp;quot;--&amp;gt;export-&amp;gt;to IDM export) to the new installation. same version. but it gets those yellow error bubbles and none of the files can be opened even though the path is the same.&lt;/p&gt;\n\n&lt;p&gt;i mainly need it to index my downloads.&lt;/p&gt;\n\n&lt;p&gt;ty&lt;/p&gt;\n\n&lt;p&gt;(it seemed perhaps I clicked &amp;quot;cancel importing&amp;quot; when the app found downloaded files with the same directory path already existing there, so the app ignores them altogether. those files are what i think, the actual downloaded files i want to index my downloads. i was worried it might override those existing files with empty ones and start to redo the downloads&lt;/p&gt;\n\n&lt;p&gt;is there a way to change importing settings? how do i get those files back)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188365z", "is_robot_indexable": true, "report_reasons": null, "author": "jacklhoward", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188365z/trying_to_restore_exported_idm_file_but_got_those/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188365z/trying_to_restore_exported_idm_file_but_got_those/", "subreddit_subscribers": 715423, "created_utc": 1701405716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have a folder with 12k+ songs that were from my old computer and when I was only a kid and didn't know how to keep music in a good filing system and its just a whole folder with just music file and a lot are untagged. From my searches through here I found MusicBrainz Picard as the best tagging software for me and this task, it added the tags. \n\n&amp;#x200B;\n\nI then create a playlist on Windows Media Player and save all of these now tagged music files to it. Load in a large hard drive to sync to then sync and it creates a decent file hierarchy on the hard dive with albums and artists all separated. Then add this new library on the hard drive as your main library in Windows Media player and let it add info to the files but not overwrite.\n\n&amp;#x200B;\n\nThis has saved me an immeasurable amount of time but it still takes days to get through even with a half decent computer. 32GB Ram, 8 core Ryzen buts its the slow 4TB HDD I am working off of that is bottlenecking me at the moment. I went from having only one folder for these files to 1000+ that are labelled in the correct artists and albums (mostly, it's not perfect).\n\n&amp;#x200B;\n\nThanks community I've always hoped I could eventually do this to my old music.", "author_fullname": "t2_qdnksay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way I catalogue and tag music in folders with large amounts of music files.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18831as", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701405274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a folder with 12k+ songs that were from my old computer and when I was only a kid and didn&amp;#39;t know how to keep music in a good filing system and its just a whole folder with just music file and a lot are untagged. From my searches through here I found MusicBrainz Picard as the best tagging software for me and this task, it added the tags. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I then create a playlist on Windows Media Player and save all of these now tagged music files to it. Load in a large hard drive to sync to then sync and it creates a decent file hierarchy on the hard dive with albums and artists all separated. Then add this new library on the hard drive as your main library in Windows Media player and let it add info to the files but not overwrite.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This has saved me an immeasurable amount of time but it still takes days to get through even with a half decent computer. 32GB Ram, 8 core Ryzen buts its the slow 4TB HDD I am working off of that is bottlenecking me at the moment. I went from having only one folder for these files to 1000+ that are labelled in the correct artists and albums (mostly, it&amp;#39;s not perfect).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks community I&amp;#39;ve always hoped I could eventually do this to my old music.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18831as", "is_robot_indexable": true, "report_reasons": null, "author": "GewdMewd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18831as/best_way_i_catalogue_and_tag_music_in_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18831as/best_way_i_catalogue_and_tag_music_in_folders/", "subreddit_subscribers": 715423, "created_utc": 1701405274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We got a USB flash drive at my work to update some software. After it had updated, the flash drive was just gonna get thrown in the trash, but I saw that it has 230GB of space on it, so I took it with me home instead. \n\nUnfortunately it's write protected and can't be used or reformatted. \n\nI tried using a software called MiniTool Partition Wizard to do something with it, but wasn't able to.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nIs there any workaround for write protection?", "author_fullname": "t2_162i24hw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Write protected 230GB USB flash drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187yl37", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701392223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We got a USB flash drive at my work to update some software. After it had updated, the flash drive was just gonna get thrown in the trash, but I saw that it has 230GB of space on it, so I took it with me home instead. &lt;/p&gt;\n\n&lt;p&gt;Unfortunately it&amp;#39;s write protected and can&amp;#39;t be used or reformatted. &lt;/p&gt;\n\n&lt;p&gt;I tried using a software called MiniTool Partition Wizard to do something with it, but wasn&amp;#39;t able to.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there any workaround for write protection?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187yl37", "is_robot_indexable": true, "report_reasons": null, "author": "Marcus_WR", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187yl37/write_protected_230gb_usb_flash_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187yl37/write_protected_230gb_usb_flash_drive/", "subreddit_subscribers": 715423, "created_utc": 1701392223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought 4 new Seagate Exos 18 drives, smart scanned, ran a burn-in test, and then long smart scanned all of them. I was a little concerned about high amounts of Raw_Read_Error_Rate and Seek_Error_Rate. Turns out, the raw_value needs to be converted to determine the actual amount of errors. There is a good post on [superuser](https://superuser.com/questions/1377466/hdd-smart-interpretation-possible-impending-drive-failure/) explaining it in detail.\n\n&amp;nbsp;\n\nThe calculator below shows me that I don't actually have any errors on all 4 drives. I'm mostly posting this as a PSA but also hoping to get confirmation that my drives look healthy.\n\n&amp;nbsp;\n\nExample Values:\n&amp;nbsp;\n\nRaw_Read_Error_Rate = 2036232\n&amp;nbsp;\n\nSeek_Error_Rate = 146673163\n\n&amp;nbsp;\n\nSeagate Error Rate Calculator - https://s.i.wtf/\n\n&amp;nbsp;\n\nMy HDD smartscans after the burn-in: \n&amp;nbsp;\n\nhttps://pastebin.com/Cxwv24bQ\n&amp;nbsp;\n\nhttps://pastebin.com/621pwqrG\n&amp;nbsp;\n\nhttps://pastebin.com/kz9tPzzC\n&amp;nbsp;\n\nhttps://pastebin.com/JBMVajya", "author_fullname": "t2_127oje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate's SMART error values need converted", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187xy1e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701390505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought 4 new Seagate Exos 18 drives, smart scanned, ran a burn-in test, and then long smart scanned all of them. I was a little concerned about high amounts of Raw_Read_Error_Rate and Seek_Error_Rate. Turns out, the raw_value needs to be converted to determine the actual amount of errors. There is a good post on &lt;a href=\"https://superuser.com/questions/1377466/hdd-smart-interpretation-possible-impending-drive-failure/\"&gt;superuser&lt;/a&gt; explaining it in detail.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;The calculator below shows me that I don&amp;#39;t actually have any errors on all 4 drives. I&amp;#39;m mostly posting this as a PSA but also hoping to get confirmation that my drives look healthy.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Example Values:\n&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Raw_Read_Error_Rate = 2036232\n&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Seek_Error_Rate = 146673163&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Seagate Error Rate Calculator - &lt;a href=\"https://s.i.wtf/\"&gt;https://s.i.wtf/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;My HDD smartscans after the burn-in: \n&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/Cxwv24bQ\"&gt;https://pastebin.com/Cxwv24bQ&lt;/a&gt;\n&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/621pwqrG\"&gt;https://pastebin.com/621pwqrG&lt;/a&gt;\n&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/kz9tPzzC\"&gt;https://pastebin.com/kz9tPzzC&lt;/a&gt;\n&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/JBMVajya\"&gt;https://pastebin.com/JBMVajya&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b70p4mTSXo6owKc8J_y2-zPKzY1db6KMase6lTxy1nM.jpg?auto=webp&amp;s=f5c7d0b03f76df3af6a9365922d04d5bd2583811", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/b70p4mTSXo6owKc8J_y2-zPKzY1db6KMase6lTxy1nM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fac6ce2a333d0513b26437b0c86f4ad3a792fa5", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/b70p4mTSXo6owKc8J_y2-zPKzY1db6KMase6lTxy1nM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=136104314e6105ef277f5fa8147e92b5533b4014", "width": 216, "height": 216}], "variants": {}, "id": "PJ2AnV34BC-Bc5jLZ5IhQtzJPf-P0PW2LhpwYCeTNyE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187xy1e", "is_robot_indexable": true, "report_reasons": null, "author": "_doesnt_matter_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187xy1e/seagates_smart_error_values_need_converted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187xy1e/seagates_smart_error_values_need_converted/", "subreddit_subscribers": 715423, "created_utc": 1701390505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've heard of people complaining how they easily break but so far i never had any problems with internal ones even though i moved my PC from time to time and as they are the exact same hardware as external i was wondering if having one and moving it from from one city to another every week would really be that bad for it?\n\nFor context, I'd just be moved from one place to another and then be left there until i move back and repeat and I would use it just for music, movies and that kind of stuff for my laptop nothing really THAT important i would need a backup.", "author_fullname": "t2_9mwwl9ll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are external HDDs really that prone to breaking? (Looking to buy one)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187uv22", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701382557.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve heard of people complaining how they easily break but so far i never had any problems with internal ones even though i moved my PC from time to time and as they are the exact same hardware as external i was wondering if having one and moving it from from one city to another every week would really be that bad for it?&lt;/p&gt;\n\n&lt;p&gt;For context, I&amp;#39;d just be moved from one place to another and then be left there until i move back and repeat and I would use it just for music, movies and that kind of stuff for my laptop nothing really THAT important i would need a backup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187uv22", "is_robot_indexable": true, "report_reasons": null, "author": "Previous_Tailor7569", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187uv22/are_external_hdds_really_that_prone_to_breaking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187uv22/are_external_hdds_really_that_prone_to_breaking/", "subreddit_subscribers": 715423, "created_utc": 1701382557.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have just got a CRC error on an SSD running as a cache pool, raid 0, on unraid. Should I be concerned and RMA the SSD or ignore it? I have just started an extended SMART test.", "author_fullname": "t2_ju7i9anj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CRC error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187s1ni", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701375602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have just got a CRC error on an SSD running as a cache pool, raid 0, on unraid. Should I be concerned and RMA the SSD or ignore it? I have just started an extended SMART test.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187s1ni", "is_robot_indexable": true, "report_reasons": null, "author": "ParticularGiraffe174", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187s1ni/crc_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187s1ni/crc_error/", "subreddit_subscribers": 715423, "created_utc": 1701375602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently added a Sabrent Dual Bay Raid HDD enclosure to my setup for backups. \n\nThe device has a USB-A 3.2 port on the front that I decided to test with a T7 SSD. \n\nWhat I\u2019ve learned is that the read/write speeds are not equivalent when using a random USB A adaptor that I had laying on a shelf. As in, the Read ~700MBs and the Write ~125MBs.\n\nThe port performs well in other scenarios (R-700 W-700) So I\u2019m curious, can the same USB adaptor have differing R/W performance or is mine a dud?", "author_fullname": "t2_4h0rbq1j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB Read/Write", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187rxj5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701375316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently added a Sabrent Dual Bay Raid HDD enclosure to my setup for backups. &lt;/p&gt;\n\n&lt;p&gt;The device has a USB-A 3.2 port on the front that I decided to test with a T7 SSD. &lt;/p&gt;\n\n&lt;p&gt;What I\u2019ve learned is that the read/write speeds are not equivalent when using a random USB A adaptor that I had laying on a shelf. As in, the Read ~700MBs and the Write ~125MBs.&lt;/p&gt;\n\n&lt;p&gt;The port performs well in other scenarios (R-700 W-700) So I\u2019m curious, can the same USB adaptor have differing R/W performance or is mine a dud?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187rxj5", "is_robot_indexable": true, "report_reasons": null, "author": "ZakkLacksRhythm", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187rxj5/usb_readwrite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187rxj5/usb_readwrite/", "subreddit_subscribers": 715423, "created_utc": 1701375316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just got a WD 18TB External Hard Drive that I basically want to use as part of my \"server\". I put it in quotes, because it's not what you would call a proper server. It's pretty much just an external drive connected to a 2010 MacBook Pro (2010MBP) that is connected to our home network. \n\nI'm a MacOS user. My main laptop is a M1 MacBook Pro. I currently have two 2TB drives connected to the 2010MBP via USB. This new 18TB drive will be replacing the two 2TB drives. These drives are where I store my music files for iTunes Match and my movie files for my local Plex server. The rest of the files are random work files and family files from over the years.\n\nMy overall question is what do I need to do to test this new 18TB drive to make sure it's OK to start using? I've see a lot of talk about testing drives before using. But please keep in mind, I'm not really a command line guy. To be honest, a lot of the command line talk goes over my head. I can run commands in Terminal if I'm given the exact commands to type or I can follow guides step by step. But I am very much a beginner when it comes to the command line. \n\nI've seen mention of CrystalDiskInfo. But unfortunately, there is no Mac app. I did see some software that PC Magazine recommended for the Mac called DriveDX (http://binaryfruit.com/driverx). \n\nIf DriveDX isn't good enough, is anyone able to guide me step by step in Terminal to test the drive? If that's asking a lot, I apologize. Or even a site that I can read how to do this on a Mac? \n\nThank you. I appreciate any help or guidance. \n\nTL;DR How can I test a new WD 18TB External Hard Drive using a MacOS or Terminal before using it?", "author_fullname": "t2_lu9n5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I prepare and/or test a new WD 18TB External Hard Drive? (MacOS/Unix)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187rwam", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701375230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got a WD 18TB External Hard Drive that I basically want to use as part of my &amp;quot;server&amp;quot;. I put it in quotes, because it&amp;#39;s not what you would call a proper server. It&amp;#39;s pretty much just an external drive connected to a 2010 MacBook Pro (2010MBP) that is connected to our home network. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a MacOS user. My main laptop is a M1 MacBook Pro. I currently have two 2TB drives connected to the 2010MBP via USB. This new 18TB drive will be replacing the two 2TB drives. These drives are where I store my music files for iTunes Match and my movie files for my local Plex server. The rest of the files are random work files and family files from over the years.&lt;/p&gt;\n\n&lt;p&gt;My overall question is what do I need to do to test this new 18TB drive to make sure it&amp;#39;s OK to start using? I&amp;#39;ve see a lot of talk about testing drives before using. But please keep in mind, I&amp;#39;m not really a command line guy. To be honest, a lot of the command line talk goes over my head. I can run commands in Terminal if I&amp;#39;m given the exact commands to type or I can follow guides step by step. But I am very much a beginner when it comes to the command line. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen mention of CrystalDiskInfo. But unfortunately, there is no Mac app. I did see some software that PC Magazine recommended for the Mac called DriveDX (&lt;a href=\"http://binaryfruit.com/driverx\"&gt;http://binaryfruit.com/driverx&lt;/a&gt;). &lt;/p&gt;\n\n&lt;p&gt;If DriveDX isn&amp;#39;t good enough, is anyone able to guide me step by step in Terminal to test the drive? If that&amp;#39;s asking a lot, I apologize. Or even a site that I can read how to do this on a Mac? &lt;/p&gt;\n\n&lt;p&gt;Thank you. I appreciate any help or guidance. &lt;/p&gt;\n\n&lt;p&gt;TL;DR How can I test a new WD 18TB External Hard Drive using a MacOS or Terminal before using it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187rwam", "is_robot_indexable": true, "report_reasons": null, "author": "blackicehawk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187rwam/how_can_i_prepare_andor_test_a_new_wd_18tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187rwam/how_can_i_prepare_andor_test_a_new_wd_18tb/", "subreddit_subscribers": 715423, "created_utc": 1701375230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My issue is very simple.\n\nI run a small sports team and I would like to set up a cloud storage for videos of practice, that the athletes could watch in their spare time.\n\nWhat are my options? So far it has been unbelievably complicated to do and I am not that old.\n\nTrying to be the most efficient data hoarder that has ever lived.\n\nThanks again.", "author_fullname": "t2_19hby7xp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most simple cloud storage option?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187rng8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701374603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My issue is very simple.&lt;/p&gt;\n\n&lt;p&gt;I run a small sports team and I would like to set up a cloud storage for videos of practice, that the athletes could watch in their spare time.&lt;/p&gt;\n\n&lt;p&gt;What are my options? So far it has been unbelievably complicated to do and I am not that old.&lt;/p&gt;\n\n&lt;p&gt;Trying to be the most efficient data hoarder that has ever lived.&lt;/p&gt;\n\n&lt;p&gt;Thanks again.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187rng8", "is_robot_indexable": true, "report_reasons": null, "author": "imaverycringeguy", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187rng8/most_simple_cloud_storage_option/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187rng8/most_simple_cloud_storage_option/", "subreddit_subscribers": 715423, "created_utc": 1701374603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellow Hoarders, \n\nI want to backup from Windows to multiple hard drives as if they were tape cartridges. In other words I want it to ask for a destination for my backup, allow me to mount it, have it fill it up and then allow me to unmount it, mount another blank drive and continue with the backup and repeat until done. I don't care if the backup allows me to individually browse through and restore from files on the backup from each of the individual drives without having to use the backup program.\n\nI searched the wiki and didn't see a mention of software that might be able to do this however some months ago I thought I saw some threads indicating that some commercial software may support this usage but that they don't advertise it.  I've seen similar questions asked in past years but no clear answers or posts saying they found a working solution. \n\nI purchased an OWC SATA to USB Drive Dock that I intend to use to insert and eject the drives that I want to use as my \"backup media\".  I'm trying to create a backup of my Plex server that I can perform occasionally (once a year?) and then send the drives that I backed up to offsite. I'm using several drives that I've decommissioned from my server as I've migrated to larger drives (the plex is now built from 20TB drive and the old drives are 16Tb drives).  I figure I have enough lower capacity drives (i.e. 5-10TB drives) laying around that I can use to make up the difference so that the drives will hold all of my data (besides which, the Plex is currently only about 80% full).\n\nI don't mind paying for the software if it supports this usage (i.e. commercial software is fine). I much prefer paying a one time fee for the software over a subscription however although if a subscription is the only answer so be it.\n\nSo, does anyone have any suggestions on software to try or at least to inquire with the company about whether it will do what I want? I'm hoping that if it works I can create a good \"how-to\" post on accomplishing this to answer future questions on this subject (at least for Windows Users).\n\nThanks for any tips and or suggestions on my endeavor.\n\nJ.T.\n\n&amp;#x200B;", "author_fullname": "t2_3kbt33xi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to backup from Windows to multiple hard drives as if they were tape cartridges...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187ms0h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701362183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow Hoarders, &lt;/p&gt;\n\n&lt;p&gt;I want to backup from Windows to multiple hard drives as if they were tape cartridges. In other words I want it to ask for a destination for my backup, allow me to mount it, have it fill it up and then allow me to unmount it, mount another blank drive and continue with the backup and repeat until done. I don&amp;#39;t care if the backup allows me to individually browse through and restore from files on the backup from each of the individual drives without having to use the backup program.&lt;/p&gt;\n\n&lt;p&gt;I searched the wiki and didn&amp;#39;t see a mention of software that might be able to do this however some months ago I thought I saw some threads indicating that some commercial software may support this usage but that they don&amp;#39;t advertise it.  I&amp;#39;ve seen similar questions asked in past years but no clear answers or posts saying they found a working solution. &lt;/p&gt;\n\n&lt;p&gt;I purchased an OWC SATA to USB Drive Dock that I intend to use to insert and eject the drives that I want to use as my &amp;quot;backup media&amp;quot;.  I&amp;#39;m trying to create a backup of my Plex server that I can perform occasionally (once a year?) and then send the drives that I backed up to offsite. I&amp;#39;m using several drives that I&amp;#39;ve decommissioned from my server as I&amp;#39;ve migrated to larger drives (the plex is now built from 20TB drive and the old drives are 16Tb drives).  I figure I have enough lower capacity drives (i.e. 5-10TB drives) laying around that I can use to make up the difference so that the drives will hold all of my data (besides which, the Plex is currently only about 80% full).&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind paying for the software if it supports this usage (i.e. commercial software is fine). I much prefer paying a one time fee for the software over a subscription however although if a subscription is the only answer so be it.&lt;/p&gt;\n\n&lt;p&gt;So, does anyone have any suggestions on software to try or at least to inquire with the company about whether it will do what I want? I&amp;#39;m hoping that if it works I can create a good &amp;quot;how-to&amp;quot; post on accomplishing this to answer future questions on this subject (at least for Windows Users).&lt;/p&gt;\n\n&lt;p&gt;Thanks for any tips and or suggestions on my endeavor.&lt;/p&gt;\n\n&lt;p&gt;J.T.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187ms0h", "is_robot_indexable": true, "report_reasons": null, "author": "jtessier66", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187ms0h/i_want_to_backup_from_windows_to_multiple_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187ms0h/i_want_to_backup_from_windows_to_multiple_hard/", "subreddit_subscribers": 715423, "created_utc": 1701362183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a deal on this raid storage and am trying to figure out how to hook it up to my PC. Any help would be greatly appreciated.", "author_fullname": "t2_e68gb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Raid Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"o365e97ivh3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/o365e97ivh3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56fe9abf2842c10de40acb323c29b88903ed4114"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/o365e97ivh3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1943463dba45df56af17fd28c70248e9ec9830dc"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/o365e97ivh3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e2166851ea5533fb41b88338e0e486c05e958d1"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/o365e97ivh3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=78d5a07c597a55117d4f722368403fefba0d1ea8"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/o365e97ivh3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5857222ae748d863d7a641fd6908210ccb59645"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/o365e97ivh3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0da2b20c3c2dd2b7526110531a1a505d749fdba"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/o365e97ivh3c1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=f36b842bb3ba7c4c2bb336f7ae10034b88b97bb5"}, "id": "o365e97ivh3c1"}, "mmd2c87ivh3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/mmd2c87ivh3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=185a5e3e40b0e7166e920e74f60c9e4102f0909c"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/mmd2c87ivh3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1187521293ead63fe4eb995290b4a065a57210e3"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/mmd2c87ivh3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5f5e96e139728210afee42b7ce12a4c617abcf0"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/mmd2c87ivh3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee1102fa2fe0dc2faf0fc2a820f6fc72851d0f2b"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/mmd2c87ivh3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=29eccfd5b936f321f312eb83dfeeac2a7aa4dea1"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/mmd2c87ivh3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=76b69100c8ab998295922dd3b7bff58dd210e7ec"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/mmd2c87ivh3c1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=09a70e0f0d7977afeeb448786a68f3bfed720eae"}, "id": "mmd2c87ivh3c1"}}, "name": "t3_187jotk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "mmd2c87ivh3c1", "id": 367526650}, {"media_id": "o365e97ivh3c1", "id": 367526651}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UdRQj63YjpVgNJ9EaNCDuAK7xEAWB8d8dxC5CngoP5g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701354189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a deal on this raid storage and am trying to figure out how to hook it up to my PC. Any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/187jotk", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187jotk", "is_robot_indexable": true, "report_reasons": null, "author": "Jas1621", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187jotk/raid_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/187jotk", "subreddit_subscribers": 715423, "created_utc": 1701354189.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}