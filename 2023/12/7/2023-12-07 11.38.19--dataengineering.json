{"kind": "Listing", "data": {"after": "t3_18cak4a", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Keep in mind the following when reading about anything tech online lol", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "name": "t3_18cj54r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/eRXOuuSq1JEToL8zZWwv0lwd6L5Mho_KW8RYyxS4xtI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701910021.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ny2e7tayrr4c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ny2e7tayrr4c1.png?auto=webp&amp;s=acd89637bc12f77bc45643bc9468ffcc522b7bd6", "width": 556, "height": 286}, "resolutions": [{"url": "https://preview.redd.it/ny2e7tayrr4c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2b2a26f02900a6bec00b8be205667984500da88", "width": 108, "height": 55}, {"url": "https://preview.redd.it/ny2e7tayrr4c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72611ebfeaf3783ba613b19a91b61052ef6d7ccd", "width": 216, "height": 111}, {"url": "https://preview.redd.it/ny2e7tayrr4c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a3f2e231d43f3dc47734ffa293a1ddc9a0e4d69", "width": 320, "height": 164}], "variants": {}, "id": "-XEOfbD2MMIbyaGeSeB-AOzP5CCIQXaIvlaPStCbY7E"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18cj54r", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cj54r/keep_in_mind_the_following_when_reading_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ny2e7tayrr4c1.png", "subreddit_subscribers": 144362, "created_utc": 1701910021.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I expect job openings to grow by mid-year and salaries to stagnate. What do you think?", "author_fullname": "t2_p9gvk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forecasts for 2024 Job Market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ca2sc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701886087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I expect job openings to grow by mid-year and salaries to stagnate. What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18ca2sc", "is_robot_indexable": true, "report_reasons": null, "author": "marcelorojas56", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ca2sc/forecasts_for_2024_job_market/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ca2sc/forecasts_for_2024_job_market/", "subreddit_subscribers": 144362, "created_utc": 1701886087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I am reading Kimball's The Data Warehouse Toolkit (chap. 1 done), and I understand that fact tables in a dimensional model are supposed to represent granular events that occur from various business processes (sales orders, deliveries, purchase orders, quotes, manufacturing sequences, account payables, and so on). Dimensions complement the fact tables to provide textual context to events as an entry point for analysis (Number of order by &lt;some dimensional attribute(s)&gt; Got it, super clear and makes sense. \n\nKimball says that these Fact and Dimension tables are used in the BI Applications layer for analytical needs, such as Power BI for a report. However, I am confused because these fact tables (as-is) aren't really too valuable to people within a department who want to analyze a specific problem or process. I hardly see the case where I am using a fact table as is in a reporting tool, I would need to perform additional transformations, joins, and business logic before the reporting tool. In a true Kimball warehouse, are these just views that belong in the BI Application Layer and the reporting tool just uses these views instead of the fact tables directly?\n\nFor example, a pricing department would want to have a table that joins the sales orders fact to the quotes fact, filters on certain criteria, and calculates several conditional and mathematical columns. This would then be the table (or view?) that they would use in Power BI to report on conversion rates by region, product, customer, etc...).\n\nAnother example would be a logistics department wanting a report for On Time Deliveries. They would want a table (or view?) that joins the sales orders fact to the deliveries fact to obtain the date that an order line was supposed to be delivered by, create a binary column for if on time, and filter out deliveries of internal or sample orders.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kimball Fact Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c6fnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701876582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am reading Kimball&amp;#39;s The Data Warehouse Toolkit (chap. 1 done), and I understand that fact tables in a dimensional model are supposed to represent granular events that occur from various business processes (sales orders, deliveries, purchase orders, quotes, manufacturing sequences, account payables, and so on). Dimensions complement the fact tables to provide textual context to events as an entry point for analysis (Number of order by &amp;lt;some dimensional attribute(s)&amp;gt; Got it, super clear and makes sense. &lt;/p&gt;\n\n&lt;p&gt;Kimball says that these Fact and Dimension tables are used in the BI Applications layer for analytical needs, such as Power BI for a report. However, I am confused because these fact tables (as-is) aren&amp;#39;t really too valuable to people within a department who want to analyze a specific problem or process. I hardly see the case where I am using a fact table as is in a reporting tool, I would need to perform additional transformations, joins, and business logic before the reporting tool. In a true Kimball warehouse, are these just views that belong in the BI Application Layer and the reporting tool just uses these views instead of the fact tables directly?&lt;/p&gt;\n\n&lt;p&gt;For example, a pricing department would want to have a table that joins the sales orders fact to the quotes fact, filters on certain criteria, and calculates several conditional and mathematical columns. This would then be the table (or view?) that they would use in Power BI to report on conversion rates by region, product, customer, etc...).&lt;/p&gt;\n\n&lt;p&gt;Another example would be a logistics department wanting a report for On Time Deliveries. They would want a table (or view?) that joins the sales orders fact to the deliveries fact to obtain the date that an order line was supposed to be delivered by, create a binary column for if on time, and filter out deliveries of internal or sample orders.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18c6fnt", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c6fnt/kimball_fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c6fnt/kimball_fact_tables/", "subreddit_subscribers": 144362, "created_utc": 1701876582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Fun project: I have created an ETL pipeline that pulls sales from an Adidas xlsx file containing 2020-2021 sales data..I have also created visualizations in PowerBI. One showing all sales data and another Cali sales data, feel free to critique.. \nI am attempting to strengthen my Python skills along with my visualization. Eventually I will make these a bit more complicated. I\u2019m currently trying to make sure I understand all that I am doing before moving on.  Full code is on my GitHub! https://github.com/bfraz33", "author_fullname": "t2_8txv38ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Adidas Sales data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0692atnhzs4c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/0692atnhzs4c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d6ce00d63c41dced775803103e62e875c155816"}, {"y": 101, "x": 216, "u": "https://preview.redd.it/0692atnhzs4c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b776e73a267abf1bdcdb0b8f94842ec9d40e6ab8"}, {"y": 150, "x": 320, "u": "https://preview.redd.it/0692atnhzs4c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ad5ed96adc040dfbea93d767309e988b618ffd8"}, {"y": 300, "x": 640, "u": "https://preview.redd.it/0692atnhzs4c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c1d358052a1c664b1811f87cec4c3dd5d3b19894"}], "s": {"y": 302, "x": 644, "u": "https://preview.redd.it/0692atnhzs4c1.jpg?width=644&amp;format=pjpg&amp;auto=webp&amp;s=7431b05052f59de7df5f8400177d094f4ab87748"}, "id": "0692atnhzs4c1"}, "qh97ctnhzs4c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/qh97ctnhzs4c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=284f0ec46415067b65bb6b89f661de5f43d2bb20"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/qh97ctnhzs4c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40606e54250853fa87e7dd35a73021a5fb0495ce"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/qh97ctnhzs4c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2bcc5fbfd991cb059c53c307d7982e370bbdb93f"}], "s": {"y": 1246, "x": 543, "u": "https://preview.redd.it/qh97ctnhzs4c1.jpg?width=543&amp;format=pjpg&amp;auto=webp&amp;s=d269c43252ce4ee96176914f94a80a59cdb67d44"}, "id": "qh97ctnhzs4c1"}, "nrdi9tnhzs4c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/nrdi9tnhzs4c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ddb49ff86ae9d483a6f1665549053934997bf00b"}, {"y": 101, "x": 216, "u": "https://preview.redd.it/nrdi9tnhzs4c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3bff4ba961a79f4a4489a0630a3351266637c949"}, {"y": 151, "x": 320, "u": "https://preview.redd.it/nrdi9tnhzs4c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=310ffbde99a92a7a2d6bddf31cc3204c16f23666"}, {"y": 302, "x": 640, "u": "https://preview.redd.it/nrdi9tnhzs4c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc6a4129dee2043227564dd5144d8b0608dc3d68"}], "s": {"y": 302, "x": 640, "u": "https://preview.redd.it/nrdi9tnhzs4c1.jpg?width=640&amp;format=pjpg&amp;auto=webp&amp;s=32c08ac1859f92814b4f2881025c0a35a0bde41f"}, "id": "nrdi9tnhzs4c1"}}, "name": "t3_18cnsae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 14, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "qh97ctnhzs4c1", "id": 370537590}, {"media_id": "0692atnhzs4c1", "id": 370537591}, {"media_id": "nrdi9tnhzs4c1", "id": 370537592}]}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/mpJFuVF45UyKb0Pt6enPoXwi6k75IzWSUEDMizUPdP4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701924552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fun project: I have created an ETL pipeline that pulls sales from an Adidas xlsx file containing 2020-2021 sales data..I have also created visualizations in PowerBI. One showing all sales data and another Cali sales data, feel free to critique.. \nI am attempting to strengthen my Python skills along with my visualization. Eventually I will make these a bit more complicated. I\u2019m currently trying to make sure I understand all that I am doing before moving on.  Full code is on my GitHub! &lt;a href=\"https://github.com/bfraz33\"&gt;https://github.com/bfraz33&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/18cnsae", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18cnsae", "is_robot_indexable": true, "report_reasons": null, "author": "Fraiz24", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cnsae/adidas_sales_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/18cnsae", "subreddit_subscribers": 144362, "created_utc": 1701924552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This post was getting interesting answers, but OP deleted it. Starting this thread so we can re-start this conversation and save the contributions by multiple community members. (Including mine, that I will repeat below)\n\n/r/dataengineering/comments/18bwe2f/how_do_streaming_aggregation_pipelines_work/\n\nThe question was:\n\nLet's assume we are building a data pipeline for Uber Eats where we keep getting orders. We want a dashboard which shows each restaurant owner\n\n- How many orders?\n- What are the Top 3 ordered dishes (with order count)?\n- What\u2019s the total sale amount?\n\nThere are 3 buttons on the UI which allow the users (restaurant owners) to see these numbers for different time periods:\n\n- Last 1 hour\n- Last 24 hours (= 1 day)\n- Last 168 hours (= 24h * 7 = 1 week)\n\nThe dashboard should get new data every 5 minutes. How do you collect, store and serve data?\n\nMy Solution:We have a Kafka topic which gets all the order events. We have a Flink job with 5 min window which aggregates the data.\n\n\nI am not sure how to go ahead from here. Like how do I store the data efficiently so that it can answer the question from all time periods and the kind of db to use, how to partition etc.", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do Streaming Aggregation Pipelines work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ciwxo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701909585.0, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701909334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post was getting interesting answers, but OP deleted it. Starting this thread so we can re-start this conversation and save the contributions by multiple community members. (Including mine, that I will repeat below)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/r/dataengineering/comments/18bwe2f/how_do_streaming_aggregation_pipelines_work/\"&gt;/r/dataengineering/comments/18bwe2f/how_do_streaming_aggregation_pipelines_work/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The question was:&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s assume we are building a data pipeline for Uber Eats where we keep getting orders. We want a dashboard which shows each restaurant owner&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How many orders?&lt;/li&gt;\n&lt;li&gt;What are the Top 3 ordered dishes (with order count)?&lt;/li&gt;\n&lt;li&gt;What\u2019s the total sale amount?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;There are 3 buttons on the UI which allow the users (restaurant owners) to see these numbers for different time periods:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Last 1 hour&lt;/li&gt;\n&lt;li&gt;Last 24 hours (= 1 day)&lt;/li&gt;\n&lt;li&gt;Last 168 hours (= 24h * 7 = 1 week)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The dashboard should get new data every 5 minutes. How do you collect, store and serve data?&lt;/p&gt;\n\n&lt;p&gt;My Solution:We have a Kafka topic which gets all the order events. We have a Flink job with 5 min window which aggregates the data.&lt;/p&gt;\n\n&lt;p&gt;I am not sure how to go ahead from here. Like how do I store the data efficiently so that it can answer the question from all time periods and the kind of db to use, how to partition etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ciwxo", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/18ciwxo/how_do_streaming_aggregation_pipelines_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ciwxo/how_do_streaming_aggregation_pipelines_work/", "subreddit_subscribers": 144362, "created_utc": 1701909334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You can use MD5\\_NUMBER\\_LOWER64 or MD5\\_NUMBER\\_UPPER64 to generate keys, at the theoretical risk of collision. \n\nIs this a real practical risk though, with a number of unique IDs to be generated at say less than 100 million?\n\nHow I got to this question: The requirement is to use integers, but also to make the keys idempotent. \n\n  \nAnyone doing this?", "author_fullname": "t2_i7h1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MD5_NUMBER_LOWER64 to produce Integer Keys", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c5pno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701874582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can use MD5_NUMBER_LOWER64 or MD5_NUMBER_UPPER64 to generate keys, at the theoretical risk of collision. &lt;/p&gt;\n\n&lt;p&gt;Is this a real practical risk though, with a number of unique IDs to be generated at say less than 100 million?&lt;/p&gt;\n\n&lt;p&gt;How I got to this question: The requirement is to use integers, but also to make the keys idempotent. &lt;/p&gt;\n\n&lt;p&gt;Anyone doing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18c5pno", "is_robot_indexable": true, "report_reasons": null, "author": "TheLordSaves", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c5pno/md5_number_lower64_to_produce_integer_keys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c5pno/md5_number_lower64_to_produce_integer_keys/", "subreddit_subscribers": 144362, "created_utc": 1701874582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# Background\n\nI work for a small company, we're mostly focussing on small retail and e-commerce customers. We provides data analysis and automated data connections between their platforms.\n\n# Our Problem\n\nMost of our datasets are things like order data, google ads click data, and are quite small. Most of them are in the range of few megabytes up to a gigabyte of data.\n\nProblem 1 is that we have a LOT of daily pipelines (around 2000 pipeline runs each day), which all need custom configuration. These are not very standard pipelines because of lots of 'custom' data sources specific to our customers' needs. These data sources are pulled by us using in-house developed connectors to, for example, REST api's, external databases, protobuf, GraphQL etc. We do operations like merging data on data that can only be retrieved from an API, for example, order data with a customer ID merge on customer data, by retrieving customers one by one from their retail platform service.\n\nProblem 2 is that because of the limitations of these APIs, most take quite long, some up to an hour to load in the entire dataset. Because of this our server is idle for a long time, waiting for new data to come in.\n\nIn a nutshell, we have a large amount of ETL pipelines that need to be orchestrated to run on a schedule,  daily, weekly, and sometimes hourly. The ETL pipelines have a long runtime and very little memory/cpu usage because most of it is data ingestion.\n\n# Current system\n\nUp until this point, we used our in-house developed DAG application, where the data engineers developed the actual application and the backend. The data scientists use the application to create new pipelines. \n\nWhen we require new data operations, we develop and release them to our data scientists in the form of nodes to use in our graphical click-and-drag interface where nodes in the DAG represent data operations. Our backend then orchestrates the scheduling and executions of these pipelines and runs them in parallel on our servers.\n\nThis system is quite old and predates most ETL standards. We want to move on because we want to start working with bigger datasets and creating the click-and-drag pipelines is a long and tedious process in an interface written in 2010. We also want a more flexible way of writing ETL pipelines, like with Python notebooks and data lakes so we can work and innovate faster in modern data analysis areas like machine learning.\n\n# Current proposed solution / Cost issues\n\nThat is why we decided to move to an advanced data platform, namely Azure Databricks. While it seems like the perfect platform for our future workloads, migrating our current pipelines is resulting in quite a problem. Running one is extremely inefficient on the Databricks architecture. Spinning up a cluster takes a long time, because of all our libraries needed for the API connectors. Then loading in the data takes even longer, 20 minutes up to an hour. Then the memory will spike slightly up to 200 MB of the available 14 GB and the server performs the actual transformations within a few seconds. All of this can take half an hour, where a few seconds are effectively used. With 2000 daily tasks the cost of this will grow to a ridiculous size compared to our old system.\n\nWe know that there are ways to run your jobs in parallel in Databricks on all-purpose clusters, however, it doesn't seem like there is a good system in place for load balancing. The system is not built for loads of minuscule long waiting tasks at once, but rather huge, compute-optimized tasks.\n\nWhat would be the best and simplest way for us to move on? We are looking at things like utilizing Azure functions/Containers to load our data and write it to the datalake, then using Databricks for the actual transformation. This can work but would result in some serious architectural headaches. How would you tackle this problem? Move away from Databricks entirely?\n\n&amp;#x200B;", "author_fullname": "t2_3pywt6jw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best automated data pipeline platform for lots of small Datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c1o7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701863905.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701860952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Background&lt;/h1&gt;\n\n&lt;p&gt;I work for a small company, we&amp;#39;re mostly focussing on small retail and e-commerce customers. We provides data analysis and automated data connections between their platforms.&lt;/p&gt;\n\n&lt;h1&gt;Our Problem&lt;/h1&gt;\n\n&lt;p&gt;Most of our datasets are things like order data, google ads click data, and are quite small. Most of them are in the range of few megabytes up to a gigabyte of data.&lt;/p&gt;\n\n&lt;p&gt;Problem 1 is that we have a LOT of daily pipelines (around 2000 pipeline runs each day), which all need custom configuration. These are not very standard pipelines because of lots of &amp;#39;custom&amp;#39; data sources specific to our customers&amp;#39; needs. These data sources are pulled by us using in-house developed connectors to, for example, REST api&amp;#39;s, external databases, protobuf, GraphQL etc. We do operations like merging data on data that can only be retrieved from an API, for example, order data with a customer ID merge on customer data, by retrieving customers one by one from their retail platform service.&lt;/p&gt;\n\n&lt;p&gt;Problem 2 is that because of the limitations of these APIs, most take quite long, some up to an hour to load in the entire dataset. Because of this our server is idle for a long time, waiting for new data to come in.&lt;/p&gt;\n\n&lt;p&gt;In a nutshell, we have a large amount of ETL pipelines that need to be orchestrated to run on a schedule,  daily, weekly, and sometimes hourly. The ETL pipelines have a long runtime and very little memory/cpu usage because most of it is data ingestion.&lt;/p&gt;\n\n&lt;h1&gt;Current system&lt;/h1&gt;\n\n&lt;p&gt;Up until this point, we used our in-house developed DAG application, where the data engineers developed the actual application and the backend. The data scientists use the application to create new pipelines. &lt;/p&gt;\n\n&lt;p&gt;When we require new data operations, we develop and release them to our data scientists in the form of nodes to use in our graphical click-and-drag interface where nodes in the DAG represent data operations. Our backend then orchestrates the scheduling and executions of these pipelines and runs them in parallel on our servers.&lt;/p&gt;\n\n&lt;p&gt;This system is quite old and predates most ETL standards. We want to move on because we want to start working with bigger datasets and creating the click-and-drag pipelines is a long and tedious process in an interface written in 2010. We also want a more flexible way of writing ETL pipelines, like with Python notebooks and data lakes so we can work and innovate faster in modern data analysis areas like machine learning.&lt;/p&gt;\n\n&lt;h1&gt;Current proposed solution / Cost issues&lt;/h1&gt;\n\n&lt;p&gt;That is why we decided to move to an advanced data platform, namely Azure Databricks. While it seems like the perfect platform for our future workloads, migrating our current pipelines is resulting in quite a problem. Running one is extremely inefficient on the Databricks architecture. Spinning up a cluster takes a long time, because of all our libraries needed for the API connectors. Then loading in the data takes even longer, 20 minutes up to an hour. Then the memory will spike slightly up to 200 MB of the available 14 GB and the server performs the actual transformations within a few seconds. All of this can take half an hour, where a few seconds are effectively used. With 2000 daily tasks the cost of this will grow to a ridiculous size compared to our old system.&lt;/p&gt;\n\n&lt;p&gt;We know that there are ways to run your jobs in parallel in Databricks on all-purpose clusters, however, it doesn&amp;#39;t seem like there is a good system in place for load balancing. The system is not built for loads of minuscule long waiting tasks at once, but rather huge, compute-optimized tasks.&lt;/p&gt;\n\n&lt;p&gt;What would be the best and simplest way for us to move on? We are looking at things like utilizing Azure functions/Containers to load our data and write it to the datalake, then using Databricks for the actual transformation. This can work but would result in some serious architectural headaches. How would you tackle this problem? Move away from Databricks entirely?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18c1o7y", "is_robot_indexable": true, "report_reasons": null, "author": "polioio", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c1o7y/best_automated_data_pipeline_platform_for_lots_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c1o7y/best_automated_data_pipeline_platform_for_lots_of/", "subreddit_subscribers": 144362, "created_utc": 1701860952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello.\n\nSituation is this: I need to make replication stream from one postgres (db1) to another (db2). In db2 I will have analytical workload with creation of datamarts.\n\nAlso db1 needs to send replication stream to datalake somewhere.\n\nPossible solutions:\n\n1. Logical replication pg-to-pg. This one is good to transfer data between db1 and db2, but seems like additional load because it seems not suitable for db1 to datalake stream. Also DBA on my project thinks that logical replication is very hard to deal with if some problems occur.\n2. Logical replication pg-to-debezium-to-kafka. After that kafka topic is consumed by another debezium to replicate all data to db2 and by datalake consumer. Seems ok despite \"DBA on my project thinks that logical replication is very hard to deal with if some problems occur.\"\n3. Db1.Outbox -&gt; send to kafka.\n\nThe architect on my project chose option 3.\n\nI've seen some articles about 3rd option, but I don't understand why would I use debezium in this case. If all debezium does is sending outbox to kafka and then kafka event (INSERT/UPDATE/DELETE) to db2, than it seems not that useful - maybe it is not that hard to do it manually. What additional functionality that is hard to implement (or is unknown to a common developer) there is?\n\nAnd if you have also any opinions about option 1 and 2, I'll gladly read them.\n\nThanks. ", "author_fullname": "t2_7nzkf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is debezium worth the trouble?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c3yvr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701869300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;Situation is this: I need to make replication stream from one postgres (db1) to another (db2). In db2 I will have analytical workload with creation of datamarts.&lt;/p&gt;\n\n&lt;p&gt;Also db1 needs to send replication stream to datalake somewhere.&lt;/p&gt;\n\n&lt;p&gt;Possible solutions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Logical replication pg-to-pg. This one is good to transfer data between db1 and db2, but seems like additional load because it seems not suitable for db1 to datalake stream. Also DBA on my project thinks that logical replication is very hard to deal with if some problems occur.&lt;/li&gt;\n&lt;li&gt;Logical replication pg-to-debezium-to-kafka. After that kafka topic is consumed by another debezium to replicate all data to db2 and by datalake consumer. Seems ok despite &amp;quot;DBA on my project thinks that logical replication is very hard to deal with if some problems occur.&amp;quot;&lt;/li&gt;\n&lt;li&gt;Db1.Outbox -&amp;gt; send to kafka.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The architect on my project chose option 3.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen some articles about 3rd option, but I don&amp;#39;t understand why would I use debezium in this case. If all debezium does is sending outbox to kafka and then kafka event (INSERT/UPDATE/DELETE) to db2, than it seems not that useful - maybe it is not that hard to do it manually. What additional functionality that is hard to implement (or is unknown to a common developer) there is?&lt;/p&gt;\n\n&lt;p&gt;And if you have also any opinions about option 1 and 2, I&amp;#39;ll gladly read them.&lt;/p&gt;\n\n&lt;p&gt;Thanks. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18c3yvr", "is_robot_indexable": true, "report_reasons": null, "author": "popfalushi", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c3yvr/is_debezium_worth_the_trouble/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c3yvr/is_debezium_worth_the_trouble/", "subreddit_subscribers": 144362, "created_utc": 1701869300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Has anyone been successfully placed as a Data Engineering manager in the past 4 to 5 months ? I see positions open for a long time. I am located in the Chicago region. My background includes initial 12 years in Data Engineering and the past 3 years in project management related to Data Engineering and Web development projects. I receive calls when I apply for full-time DE Manager positions, but either they go on hold, or I am informed that the position is canceled. Additionally, I believe I need my profile and interview techniques evaluated. I have heard a lot about Interview Quickstart, but it is terribly expensive, around 10k USD. Are there any other recommendations that can help me prepare for a DE Manager role or, in the future, a DE Director role? ", "author_fullname": "t2_a55amval", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prepare and apply for Data Engineering manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cl6pd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701916209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been successfully placed as a Data Engineering manager in the past 4 to 5 months ? I see positions open for a long time. I am located in the Chicago region. My background includes initial 12 years in Data Engineering and the past 3 years in project management related to Data Engineering and Web development projects. I receive calls when I apply for full-time DE Manager positions, but either they go on hold, or I am informed that the position is canceled. Additionally, I believe I need my profile and interview techniques evaluated. I have heard a lot about Interview Quickstart, but it is terribly expensive, around 10k USD. Are there any other recommendations that can help me prepare for a DE Manager role or, in the future, a DE Director role? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18cl6pd", "is_robot_indexable": true, "report_reasons": null, "author": "No-Competition729", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cl6pd/prepare_and_apply_for_data_engineering_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cl6pd/prepare_and_apply_for_data_engineering_manager/", "subreddit_subscribers": 144362, "created_utc": 1701916209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello \ud83e\udd17\n\nLately I've been really interested in the topic of virtual data warehouses (what I understand are 'lake houses' - virtual data models built on top of a data lake).\n\nI'm looking for a solid reference (like Kimball on Dimensional Modeling) to know more about this.\n\nI am specifically interested in the practical part : best practices, examples of scripts, detailed use cases with a lot of explaination, implementation frameworks, how to manage SCD, snapshoting, etc \n\nDo you know what would be a good reference?\n\nThank you !", "author_fullname": "t2_rz2xn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a good book on virtual DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cfxqv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701901141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello \ud83e\udd17&lt;/p&gt;\n\n&lt;p&gt;Lately I&amp;#39;ve been really interested in the topic of virtual data warehouses (what I understand are &amp;#39;lake houses&amp;#39; - virtual data models built on top of a data lake).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a solid reference (like Kimball on Dimensional Modeling) to know more about this.&lt;/p&gt;\n\n&lt;p&gt;I am specifically interested in the practical part : best practices, examples of scripts, detailed use cases with a lot of explaination, implementation frameworks, how to manage SCD, snapshoting, etc &lt;/p&gt;\n\n&lt;p&gt;Do you know what would be a good reference?&lt;/p&gt;\n\n&lt;p&gt;Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18cfxqv", "is_robot_indexable": true, "report_reasons": null, "author": "Ownards", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cfxqv/looking_for_a_good_book_on_virtual_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cfxqv/looking_for_a_good_book_on_virtual_dwh/", "subreddit_subscribers": 144362, "created_utc": 1701901141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is just me messing about - but I'm wondering what the 'best' (or most common) approach to this might be ? Basically I have a python module that will generate some fake data - and I want to write that to a storage bucket on a cloud scheduled job. So every X minutes it'll generate some synthetic data into some bucket (assume &lt;=1GB of data).\n\nIt seems that cloud run / cloud functions are the most obvious for this task, but which one and why (or is there something else more suited to running python code like this?)\n\nReading some of the documentation for cloud run [https://cloud.google.com/run/docs/developing](https://cloud.google.com/run/docs/developing) it seems to be geared towards web services (most of the examples seem to use flask...), and I'm not sure that's what I'm after here.\n\nI figured it'd be a fairly easy question for people with GCP experience.\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_tczfts4v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which GCP service/process to use to run a python script that generates data and writes it to a storage bucket", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cerir", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701898582.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701898122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is just me messing about - but I&amp;#39;m wondering what the &amp;#39;best&amp;#39; (or most common) approach to this might be ? Basically I have a python module that will generate some fake data - and I want to write that to a storage bucket on a cloud scheduled job. So every X minutes it&amp;#39;ll generate some synthetic data into some bucket (assume &amp;lt;=1GB of data).&lt;/p&gt;\n\n&lt;p&gt;It seems that cloud run / cloud functions are the most obvious for this task, but which one and why (or is there something else more suited to running python code like this?)&lt;/p&gt;\n\n&lt;p&gt;Reading some of the documentation for cloud run &lt;a href=\"https://cloud.google.com/run/docs/developing\"&gt;https://cloud.google.com/run/docs/developing&lt;/a&gt; it seems to be geared towards web services (most of the examples seem to use flask...), and I&amp;#39;m not sure that&amp;#39;s what I&amp;#39;m after here.&lt;/p&gt;\n\n&lt;p&gt;I figured it&amp;#39;d be a fairly easy question for people with GCP experience.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?auto=webp&amp;s=b3c1793ddfb0595cba1bbb23fba79360953beb8d", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0329d4207ada0345185e70a97a0ef1f27aec034", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8722bf8052baa4647e96ebeb0d22f50bf529b6ac", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6562c4a330763746058f2250630ec6d3854b2e3d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fff0deae054d2476ac870508887dbbee06d9387c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6a32be275833b4d47802b79f3345f568bd43a4d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hHuPbOBuX42Zu5_MT0rir3JO7cgRTfjn0ct4UWuqTu4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97e8d6d94b95697d64482f5fcda32d11814df7b8", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18cerir", "is_robot_indexable": true, "report_reasons": null, "author": "Subject_Fix2471", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cerir/which_gcp_serviceprocess_to_use_to_run_a_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cerir/which_gcp_serviceprocess_to_use_to_run_a_python/", "subreddit_subscribers": 144362, "created_utc": 1701898122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does a Data Product Strategy Impact the Day-to-Days of Your CMO, CDO, or CFO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18c3gdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/C9GcrRMwBxuDwCU1bdErjJXhJ_DYHOqrwf1MmdsjJM8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701867601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/how-does-a-data-product-strategy", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?auto=webp&amp;s=7a223862678a9cde54a90f4ed9d6bad68edd59fa", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4cab7a47d69ad9432d41b3f1b8b5ca92c7e96da", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23cbc75ce20c8a8501d97242d11a3da767ab0132", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0b0a52818eea2477c8bbd62453a787c34329517", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdd1a8bbf8324711b443be95e7684ff60515d28b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0c07dc4e279fa12e496b68ca3b4bfbb0312acf27", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4de8b0d0a5bec9c054c57af332df08e223edcaf", "width": 1080, "height": 540}], "variants": {}, "id": "kFmQZXSdF72mgYIKZgd9m7zyEeCYL5K-bgj5n2tyvVc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18c3gdx", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c3gdx/how_does_a_data_product_strategy_impact_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/how-does-a-data-product-strategy", "subreddit_subscribers": 144362, "created_utc": 1701867601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6f2li", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The State of SQL-based Observability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18cdd6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0CEa5ofz8oFCtvOUtG1LJADxh42-YMYQ3Cce8_k6g3Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701894551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "clickhouse.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://clickhouse.com/blog/the-state-of-sql-based-observability", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Vt1P86G2CVngkLHGSxvFer7KGBftGt5NX72YFDtxcT0.jpg?auto=webp&amp;s=b48ed7ae193fca5400d3f2a44484419e1aef6ec5", "width": 1600, "height": 840}, "resolutions": [{"url": "https://external-preview.redd.it/Vt1P86G2CVngkLHGSxvFer7KGBftGt5NX72YFDtxcT0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=706df4c1bd00ff6501177de47dc728970b44ac16", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Vt1P86G2CVngkLHGSxvFer7KGBftGt5NX72YFDtxcT0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40ebff7fd3f1d38d96a151dea7dfb1b50f547873", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Vt1P86G2CVngkLHGSxvFer7KGBftGt5NX72YFDtxcT0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c33de4e518f733675c2d5ec83ee074136851343d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Vt1P86G2CVngkLHGSxvFer7KGBftGt5NX72YFDtxcT0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c0210cba58847453371a3b65a72989072ed04be", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Vt1P86G2CVngkLHGSxvFer7KGBftGt5NX72YFDtxcT0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cf769c24e92921b36f408499be4c063db21ccf6a", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Vt1P86G2CVngkLHGSxvFer7KGBftGt5NX72YFDtxcT0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1821204ebd087a6c7dc701744b1d0ed67627dc93", "width": 1080, "height": 567}], "variants": {}, "id": "kxAh6sTfQwyh-2kLrT0N8brKnwqhkuZ5VqjQcgjtQAM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18cdd6i", "is_robot_indexable": true, "report_reasons": null, "author": "kadermo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cdd6i/the_state_of_sqlbased_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://clickhouse.com/blog/the-state-of-sql-based-observability", "subreddit_subscribers": 144362, "created_utc": 1701894551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need help finding tools for managing data in a weather/climate research setting. I am looking for a dashboard/management database that lets me track/manage my datasets, assign post-processing scripts to them, attach relevant model configuration files. My overall data size varies from several GB to ~100Tb. Most of the data are coming from m I work for a university so something that runs locally on linux is ideal. I am pretty motivated to get something like this implemented if push comes to shove on using a cloud service.  \n\n\n  \nCurrently I just use folders names to manage the data, but when one is generating lots of data using weather and climate models, it's easy to lose track of things. I am happy to answer questions if more information is needed. Thank you!", "author_fullname": "t2_bf1rl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data management/workflow for weather/climate/geospatial?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cglbu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701902917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help finding tools for managing data in a weather/climate research setting. I am looking for a dashboard/management database that lets me track/manage my datasets, assign post-processing scripts to them, attach relevant model configuration files. My overall data size varies from several GB to ~100Tb. Most of the data are coming from m I work for a university so something that runs locally on linux is ideal. I am pretty motivated to get something like this implemented if push comes to shove on using a cloud service.  &lt;/p&gt;\n\n&lt;p&gt;Currently I just use folders names to manage the data, but when one is generating lots of data using weather and climate models, it&amp;#39;s easy to lose track of things. I am happy to answer questions if more information is needed. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18cglbu", "is_robot_indexable": true, "report_reasons": null, "author": "gbromley", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cglbu/data_managementworkflow_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cglbu/data_managementworkflow_for/", "subreddit_subscribers": 144362, "created_utc": 1701902917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://reddit.com/link/18cae7h/video/s3e6yr3jvp4c1/player", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've definitely never received a snapchat from a girl, but I can auto-format my SQL queries to TitleCase!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"s3e6yr3jvp4c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18cae7h/asset/s3e6yr3jvp4c1/DASHPlaylist.mpd?a=1704541099%2CNjgyYzhkNWQ5MzdjM2UyY2E0ZTZkZTIxYjg0ZGQ5N2QxNWY5YmFlNWIxZGI4ZGUxODlhNDM0MDg1OTQxY2E5OA%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 720, "hlsUrl": "https://v.redd.it/link/18cae7h/asset/s3e6yr3jvp4c1/HLSPlaylist.m3u8?a=1704541099%2CMDNkZWYxNWQ0ODg2OTQzZDQ4ZTUyMmEyOWYzMTE0ZTJmNDk1NGIxOWI1OTRkNzI5NGZmNzgyZWQ5YjQ2MjM1MQ%3D%3D&amp;v=1&amp;f=sd", "id": "s3e6yr3jvp4c1", "isGif": false}}, "name": "t3_18cae7h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701886916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18cae7h/video/s3e6yr3jvp4c1/player\"&gt;https://reddit.com/link/18cae7h/video/s3e6yr3jvp4c1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18cae7h", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cae7h/ive_definitely_never_received_a_snapchat_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cae7h/ive_definitely_never_received_a_snapchat_from_a/", "subreddit_subscribers": 144362, "created_utc": 1701886916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context, I have just over 1.5 years of experience as a data analyst with an IT consulting firm and have been working with DBT, Snowflake and Azure during that time. I've learned a lot using this tech stack and I'm also studying for the Azure DP-203 exam with the goal of taking it this upcoming spring.\n\nWith how the job market is currently and my experience level is it even worth my time to be applying to Analytics Engineer and Data Engineer roles? Obviously having a cloud certification would be more beneficial but should I just wait until I hit the 2 year mark and have an Azure cert under my belt before even considering to apply elsewhere?", "author_fullname": "t2_71weyfry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it too soon for me to be applying to new roles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c9eo3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701884397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I have just over 1.5 years of experience as a data analyst with an IT consulting firm and have been working with DBT, Snowflake and Azure during that time. I&amp;#39;ve learned a lot using this tech stack and I&amp;#39;m also studying for the Azure DP-203 exam with the goal of taking it this upcoming spring.&lt;/p&gt;\n\n&lt;p&gt;With how the job market is currently and my experience level is it even worth my time to be applying to Analytics Engineer and Data Engineer roles? Obviously having a cloud certification would be more beneficial but should I just wait until I hit the 2 year mark and have an Azure cert under my belt before even considering to apply elsewhere?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18c9eo3", "is_robot_indexable": true, "report_reasons": null, "author": "DrunkenWhaler136", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c9eo3/is_it_too_soon_for_me_to_be_applying_to_new_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c9eo3/is_it_too_soon_for_me_to_be_applying_to_new_roles/", "subreddit_subscribers": 144362, "created_utc": 1701884397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have experience with reading directly from s3 (parquet/csv)?\n\nPandas uses s3fs via fsspect, which depends on  aiobotocore (which can cause dependency problems in many AWS services). AWS Data wrangler uses a custom s3fs which doesn't have this dependency.\n\n* [https://github.com/pandas-dev/pandas/issues/53712](https://github.com/pandas-dev/pandas/issues/53712)\n* [https://github.com/fsspec/s3fs/issues/815](https://github.com/fsspec/s3fs/issues/815)\n\nLooks like they recently relaxed the dependency version, so maybe not that big of a deal?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read files from s3 using Pandas/s3fs or AWS Data Wrangler?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c7p3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701879947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have experience with reading directly from s3 (parquet/csv)?&lt;/p&gt;\n\n&lt;p&gt;Pandas uses s3fs via fsspect, which depends on  aiobotocore (which can cause dependency problems in many AWS services). AWS Data wrangler uses a custom s3fs which doesn&amp;#39;t have this dependency.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/pandas-dev/pandas/issues/53712\"&gt;https://github.com/pandas-dev/pandas/issues/53712&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/fsspec/s3fs/issues/815\"&gt;https://github.com/fsspec/s3fs/issues/815&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Looks like they recently relaxed the dependency version, so maybe not that big of a deal?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?auto=webp&amp;s=27d795298b9e684a7da038bdcc98907cead4bc7d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a36a3909fa4192f6414e2ab6dba03099d7928c2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=43ad7d6ad95c597b4a7d6786098647b67d73cc28", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=787139355c4fc9a44101186ea68d1ce16d0a979f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=111501e0ea0c9270249754c13594bab3c8e7c1e6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ad84a9517be2f4cd585ccfa8e4971d3cb556e52", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a35e34c09a39e6cbc763d088c7ba7e499c5c65a", "width": 1080, "height": 540}], "variants": {}, "id": "QTNQeuXf5QeCcn9jEsbHOley3wu3l3gJcmCyM0NuerI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18c7p3u", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c7p3u/read_files_from_s3_using_pandass3fs_or_aws_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c7p3u/read_files_from_s3_using_pandass3fs_or_aws_data/", "subreddit_subscribers": 144362, "created_utc": 1701879947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nThanks for clicking on this post despite its non-catchy title. :) \n\nI'm a bit at loss on how to solve a snapshotting issue where rows from source disappear if they're deleted.\n\nI have a table that lists my suppliers per product that looks like below. There can only be one preferred supplier per product. \n\n**SuppliersPerProduct at 2023-12-03:**\n\n|Product|Vendor|PrefSupplier|CreateDate|LastMut|...|\n|:-|:-|:-|:-|:-|:-|\n|A|V1|N|2020-01-01|2021-05-08|...|\n|A|V2|Y|2020-01-01|2021-05-08|...|\n|A|V3|N|2021-01-01|2022-04-01|...|\n\n&amp;#x200B;\n\nOn 2023-12-04 vendor V2 is deleted from the system, vendor V3 takes over as the preferred supplier. \n\n**SuppliersPerProduct at 2023-12-04:**\n\n|Product|Vendor|PrefSupplier|CreateDate|LastMut|...|\n|:-|:-|:-|:-|:-|:-|\n|A|V1|N|2020-01-01|2021-05-08|...|\n|A|V3|Y|2021-01-01|2023-12-04|...|\n\n&amp;#x200B;\n\nI use DBT to snapshot the tables on 2023-12-05 with \\`invalidate\\_hard\\_deletes = True\\` and my result is:\n\n**Snapshot\\_SuppliersPerProduct at 2023-12-05:**\n\n|Product|Vendor|PrefSupplier|CreateDate|LastMut|...|dbt\\_valid\\_from|dbt\\_valid\\_to|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|A|V1|N|2020-01-01|2021-05-08|...|2021-05-08 |null|\n|A|V2|Y|2020-01-01|2021-05-08|...|2021-05-08|**2023-12-05**|\n|A|V3|N|2021-01-01|2022-04-01|...|2022-04-01|2023-12-05 |\n|A|V3|Y|2021-01-01|2023-12-04|...|**2023-12-04**|null|\n\nAccording to the snapshotted table there were two preferred suppliers on 2023-12-04, which is not possible according to our business logic. The problem originates from the 'dbt\\_valid\\_to' being set to the date (or rather, datetime) that dbt runs and dbt has no way of knowing two preferred suppliers is not possible.\n\nFor individual records I could of course update this manually, but at scale this would not be possible.\n\nI could write an additional step where I correct dbt\\_valid\\_to values in case of overlapping 'PrefSupplier' values, but I wonder if anyone has come across a similar scenario and solved it in a better way. \n\nDoes anyone have suggestions on how to best tackle this problem?", "author_fullname": "t2_ko2ybcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt snapshots - invalidate hard deletes dbt_valid_to overlaps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c4tgx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701871966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Thanks for clicking on this post despite its non-catchy title. :) &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a bit at loss on how to solve a snapshotting issue where rows from source disappear if they&amp;#39;re deleted.&lt;/p&gt;\n\n&lt;p&gt;I have a table that lists my suppliers per product that looks like below. There can only be one preferred supplier per product. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SuppliersPerProduct at 2023-12-03:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Vendor&lt;/th&gt;\n&lt;th align=\"left\"&gt;PrefSupplier&lt;/th&gt;\n&lt;th align=\"left\"&gt;CreateDate&lt;/th&gt;\n&lt;th align=\"left\"&gt;LastMut&lt;/th&gt;\n&lt;th align=\"left\"&gt;...&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V1&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-04-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;On 2023-12-04 vendor V2 is deleted from the system, vendor V3 takes over as the preferred supplier. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SuppliersPerProduct at 2023-12-04:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Vendor&lt;/th&gt;\n&lt;th align=\"left\"&gt;PrefSupplier&lt;/th&gt;\n&lt;th align=\"left\"&gt;CreateDate&lt;/th&gt;\n&lt;th align=\"left\"&gt;LastMut&lt;/th&gt;\n&lt;th align=\"left\"&gt;...&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V1&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2023-12-04&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I use DBT to snapshot the tables on 2023-12-05 with `invalidate_hard_deletes = True` and my result is:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Snapshot_SuppliersPerProduct at 2023-12-05:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Vendor&lt;/th&gt;\n&lt;th align=\"left\"&gt;PrefSupplier&lt;/th&gt;\n&lt;th align=\"left\"&gt;CreateDate&lt;/th&gt;\n&lt;th align=\"left\"&gt;LastMut&lt;/th&gt;\n&lt;th align=\"left\"&gt;...&lt;/th&gt;\n&lt;th align=\"left\"&gt;dbt_valid_from&lt;/th&gt;\n&lt;th align=\"left\"&gt;dbt_valid_to&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V1&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;null&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;2023-12-05&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-04-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-04-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2023-12-05&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2023-12-04&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;2023-12-04&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;null&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;According to the snapshotted table there were two preferred suppliers on 2023-12-04, which is not possible according to our business logic. The problem originates from the &amp;#39;dbt_valid_to&amp;#39; being set to the date (or rather, datetime) that dbt runs and dbt has no way of knowing two preferred suppliers is not possible.&lt;/p&gt;\n\n&lt;p&gt;For individual records I could of course update this manually, but at scale this would not be possible.&lt;/p&gt;\n\n&lt;p&gt;I could write an additional step where I correct dbt_valid_to values in case of overlapping &amp;#39;PrefSupplier&amp;#39; values, but I wonder if anyone has come across a similar scenario and solved it in a better way. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have suggestions on how to best tackle this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18c4tgx", "is_robot_indexable": true, "report_reasons": null, "author": "nl_dhh", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c4tgx/dbt_snapshots_invalidate_hard_deletes_dbt_valid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c4tgx/dbt_snapshots_invalidate_hard_deletes_dbt_valid/", "subreddit_subscribers": 144362, "created_utc": 1701871966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I'm new to data engineering. I have seen some jobs openings titled \"Data Engineer - AEP\". AEP is Adobe Experience Platform, something which I have no idea about. If there's an AEP Data Engineer here, can they provide a brief explanation about the job role?", "author_fullname": "t2_9daynfsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's an AEP data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18crrbp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701940492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m new to data engineering. I have seen some jobs openings titled &amp;quot;Data Engineer - AEP&amp;quot;. AEP is Adobe Experience Platform, something which I have no idea about. If there&amp;#39;s an AEP Data Engineer here, can they provide a brief explanation about the job role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18crrbp", "is_robot_indexable": true, "report_reasons": null, "author": "Captcodr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18crrbp/whats_an_aep_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18crrbp/whats_an_aep_data_engineer/", "subreddit_subscribers": 144362, "created_utc": 1701940492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Quant Finance background and currently work for a large US Bank, and I have only begun learning about data engineering concepts (through airflow and the orchestration of my team's financial models and data).\n\nRecently, a junior executive approached me asking how we can retain and access our analytics at the customer level (rather than sub-portfolio). He believes this would make us better at managing our balance sheet, and I agree with him. \n\nIn this particular space that's approximately 5 million customers. I've estimated that storing our modeled results at this precision would result in 15-20 terabytes of tabular data per year. \n\nAs a large US Bank, we are well acquainted with Microsoft, but what are my options for cloud storage?What should I recommend? I imagine using technology like Apache Spark to operate over the large data, querying information and running ML algorithms. \n\nWhat kind of pricing should I tell the junior executive to expect? \n\nAll advice is welcome and appreciated.", "author_fullname": "t2_2ngngbnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Storage Options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18clmri", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701917595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Quant Finance background and currently work for a large US Bank, and I have only begun learning about data engineering concepts (through airflow and the orchestration of my team&amp;#39;s financial models and data).&lt;/p&gt;\n\n&lt;p&gt;Recently, a junior executive approached me asking how we can retain and access our analytics at the customer level (rather than sub-portfolio). He believes this would make us better at managing our balance sheet, and I agree with him. &lt;/p&gt;\n\n&lt;p&gt;In this particular space that&amp;#39;s approximately 5 million customers. I&amp;#39;ve estimated that storing our modeled results at this precision would result in 15-20 terabytes of tabular data per year. &lt;/p&gt;\n\n&lt;p&gt;As a large US Bank, we are well acquainted with Microsoft, but what are my options for cloud storage?What should I recommend? I imagine using technology like Apache Spark to operate over the large data, querying information and running ML algorithms. &lt;/p&gt;\n\n&lt;p&gt;What kind of pricing should I tell the junior executive to expect? &lt;/p&gt;\n\n&lt;p&gt;All advice is welcome and appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18clmri", "is_robot_indexable": true, "report_reasons": null, "author": "mderst", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18clmri/cloud_storage_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18clmri/cloud_storage_options/", "subreddit_subscribers": 144362, "created_utc": 1701917595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wrote CPU/ memory intensive code which is using Python Processpool executor for parallel processing which running fine in local machine but when I dockerize it and running on Aws batch through ECR it\u2019s working for few iterations and then throw process pool broken error. I don\u2019t understand this behavior I replaced Processpool with Threadpool executor and it\u2019s working fine in Aws batch but slower compare to Processpool executor. \n\nI monitored compute environment and it\u2019s not even using half of the assigned memory when running Processpool executor. \n\nMay be someone shed some light on this behavior, any other things I may check or change ?", "author_fullname": "t2_a0ixumwh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processpool executor in Aws batch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ckfxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701913981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote CPU/ memory intensive code which is using Python Processpool executor for parallel processing which running fine in local machine but when I dockerize it and running on Aws batch through ECR it\u2019s working for few iterations and then throw process pool broken error. I don\u2019t understand this behavior I replaced Processpool with Threadpool executor and it\u2019s working fine in Aws batch but slower compare to Processpool executor. &lt;/p&gt;\n\n&lt;p&gt;I monitored compute environment and it\u2019s not even using half of the assigned memory when running Processpool executor. &lt;/p&gt;\n\n&lt;p&gt;May be someone shed some light on this behavior, any other things I may check or change ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ckfxn", "is_robot_indexable": true, "report_reasons": null, "author": "ExcitingAd7292", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ckfxn/processpool_executor_in_aws_batch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ckfxn/processpool_executor_in_aws_batch/", "subreddit_subscribers": 144362, "created_utc": 1701913981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nLooking for some book recommendatinos to learn more about CI/Cd both in theory and technical applications. I've been finding alot of videos but im more of reader type of guy.", "author_fullname": "t2_6o6sl8n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Recommendations for Ci/CD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ciotd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701908669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Looking for some book recommendatinos to learn more about CI/Cd both in theory and technical applications. I&amp;#39;ve been finding alot of videos but im more of reader type of guy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ciotd", "is_robot_indexable": true, "report_reasons": null, "author": "Tasty_Fold3012", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ciotd/book_recommendations_for_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ciotd/book_recommendations_for_cicd/", "subreddit_subscribers": 144362, "created_utc": 1701908669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you all instill more trust in data quality / etl pipelines in your end users?  This is a commonly occuring example of what I mean.\n\nA senior manager will be using our BI tool (Looker) to explore data, and see a 50% drop in customer usage of a specific feature in our product. In turn, they email me and say the data can't possibly be right, something's broken in the etl, can you look? We have set up pretty robust error handling, monitoring and alerting for all of our pipelines, so 99% of the time, the issue isn't in the etl layer. Either the data is correct and there actually was a 50% drop in usage for that feature, or the upstream modeling in the BI tool is defined incorrectly.  \n\nEither way, it takes time to debug the crappy BI modeling, or chase down business reasons for why that drop in feature usage occured (like deprecating that product feature in a new release, prod eng developing a new feature that does the same thing but better, etc).\n\nSo how do you all handle a general mistrust of the etl pipelines specifically, and the quality / integrity of data in general? TIA", "author_fullname": "t2_peqq109ch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My biggest issue in data engineering is end users trusting the integrity of the data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cgeoh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701902417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you all instill more trust in data quality / etl pipelines in your end users?  This is a commonly occuring example of what I mean.&lt;/p&gt;\n\n&lt;p&gt;A senior manager will be using our BI tool (Looker) to explore data, and see a 50% drop in customer usage of a specific feature in our product. In turn, they email me and say the data can&amp;#39;t possibly be right, something&amp;#39;s broken in the etl, can you look? We have set up pretty robust error handling, monitoring and alerting for all of our pipelines, so 99% of the time, the issue isn&amp;#39;t in the etl layer. Either the data is correct and there actually was a 50% drop in usage for that feature, or the upstream modeling in the BI tool is defined incorrectly.  &lt;/p&gt;\n\n&lt;p&gt;Either way, it takes time to debug the crappy BI modeling, or chase down business reasons for why that drop in feature usage occured (like deprecating that product feature in a new release, prod eng developing a new feature that does the same thing but better, etc).&lt;/p&gt;\n\n&lt;p&gt;So how do you all handle a general mistrust of the etl pipelines specifically, and the quality / integrity of data in general? TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18cgeoh", "is_robot_indexable": true, "report_reasons": null, "author": "No-Support4478", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cgeoh/my_biggest_issue_in_data_engineering_is_end_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cgeoh/my_biggest_issue_in_data_engineering_is_end_users/", "subreddit_subscribers": 144362, "created_utc": 1701902417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm a junior engineer currently focused on studying the migration of a database away from Cassandra. This database handles the storage of 10 million events daily, originally in the form of JSON files, distributed across several tables. These tables share the same information but have different primary keys. The primary requirement is to score these events in real time, aiming for a response time of less than 100ms for each event, based on approximately 50 queries per event on average. Additionally, the scores need to be stored and queried. The current database size is 10TB or more.\n\nConsidering these requirements, I'm seeking advice on available options and would appreciate guidance on creating an action plan. I'm particularly interested in understanding if achieving this level of performance is possible with PostgreSQL or Oracle. Alternatively, should we consider MongoDB, or are there other viable options that I should explore?", "author_fullname": "t2_4ff39qhj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Millions of events, real time scoring, what are my options ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cfbm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701899566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a junior engineer currently focused on studying the migration of a database away from Cassandra. This database handles the storage of 10 million events daily, originally in the form of JSON files, distributed across several tables. These tables share the same information but have different primary keys. The primary requirement is to score these events in real time, aiming for a response time of less than 100ms for each event, based on approximately 50 queries per event on average. Additionally, the scores need to be stored and queried. The current database size is 10TB or more.&lt;/p&gt;\n\n&lt;p&gt;Considering these requirements, I&amp;#39;m seeking advice on available options and would appreciate guidance on creating an action plan. I&amp;#39;m particularly interested in understanding if achieving this level of performance is possible with PostgreSQL or Oracle. Alternatively, should we consider MongoDB, or are there other viable options that I should explore?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18cfbm7", "is_robot_indexable": true, "report_reasons": null, "author": "elmoptimistic", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cfbm7/millions_of_events_real_time_scoring_what_are_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cfbm7/millions_of_events_real_time_scoring_what_are_my/", "subreddit_subscribers": 144362, "created_utc": 1701899566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello friends,\n\nI need a career advice. I am a 28 years old entry level data engineer with a masters degree in Business Analytics and undergraduate degree in Mechanical Engineering. I have done about 2 years of professional work in 3 companies and have fair amount expertise in SQL, SSIS, some coding in C# and python and about a year of cloud experience (AWS). I am by no means expert in this field but would like to continue on the data engineering path and ultimately become proficient enough to be able to earn at least $150,000 a year so my parents can retire, my wife can stay home and raise our kids (in about 3 years) and own a home.\n\nI have possible interest from a small company where I feel that the career advancement is pretty minimal, and wonder if I should explore other opportunities. My main goal here is to be skilled enough to demand at least $150,000/yr in the next three years. I need advice/recommendations regarding what industry to look out for (health care, finance, retail), tools to learn, good bootcamps, companies to apply, resume works et cetera.\n\nThank you in advance.", "author_fullname": "t2_9kaox1qv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need a career advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cak4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701887323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello friends,&lt;/p&gt;\n\n&lt;p&gt;I need a career advice. I am a 28 years old entry level data engineer with a masters degree in Business Analytics and undergraduate degree in Mechanical Engineering. I have done about 2 years of professional work in 3 companies and have fair amount expertise in SQL, SSIS, some coding in C# and python and about a year of cloud experience (AWS). I am by no means expert in this field but would like to continue on the data engineering path and ultimately become proficient enough to be able to earn at least $150,000 a year so my parents can retire, my wife can stay home and raise our kids (in about 3 years) and own a home.&lt;/p&gt;\n\n&lt;p&gt;I have possible interest from a small company where I feel that the career advancement is pretty minimal, and wonder if I should explore other opportunities. My main goal here is to be skilled enough to demand at least $150,000/yr in the next three years. I need advice/recommendations regarding what industry to look out for (health care, finance, retail), tools to learn, good bootcamps, companies to apply, resume works et cetera.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18cak4a", "is_robot_indexable": true, "report_reasons": null, "author": "Pure_Chest5539", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cak4a/need_a_career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cak4a/need_a_career_advice/", "subreddit_subscribers": 144362, "created_utc": 1701887323.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}