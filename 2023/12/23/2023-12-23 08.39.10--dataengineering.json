{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im a contractor and I was asked by someone at my client's to help out with another project I wasn't originally contracted for. It's an ERP migration project, so I really only have to write SQL statements and extract some data based on the new ERP system's template from the legacy ERP's backend (SQL Server).\n\nI've never done this kind of work before and largely unfamiliar with ERP systems, but since it's mostly just SQL work I thought I could help out. Project Manager gave me a list of fields I need to look for, but there are about 200 tables in the backend. I said I'd like to speak with the finance team managing this legacy ERP system to narrow down my search, and Project Manager says \"well I saw the table names, they all look pretty straightforward, I think you can just look through all those tables\". But I said \"I've never worked on this domain before so I would really appreciate some guidance\". And she didnt' respond.  I tried searching blindly but ended up finding a table with 10 addresses per customer and I don't know which of these are needed etc. \n\nWas I being unprofessional for wanting to ask for help to make sure I'm looking for the data in the right places? ", "author_fullname": "t2_7m2ues2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was I unprofessional when I said I needed help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18of52n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703252357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a contractor and I was asked by someone at my client&amp;#39;s to help out with another project I wasn&amp;#39;t originally contracted for. It&amp;#39;s an ERP migration project, so I really only have to write SQL statements and extract some data based on the new ERP system&amp;#39;s template from the legacy ERP&amp;#39;s backend (SQL Server).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never done this kind of work before and largely unfamiliar with ERP systems, but since it&amp;#39;s mostly just SQL work I thought I could help out. Project Manager gave me a list of fields I need to look for, but there are about 200 tables in the backend. I said I&amp;#39;d like to speak with the finance team managing this legacy ERP system to narrow down my search, and Project Manager says &amp;quot;well I saw the table names, they all look pretty straightforward, I think you can just look through all those tables&amp;quot;. But I said &amp;quot;I&amp;#39;ve never worked on this domain before so I would really appreciate some guidance&amp;quot;. And she didnt&amp;#39; respond.  I tried searching blindly but ended up finding a table with 10 addresses per customer and I don&amp;#39;t know which of these are needed etc. &lt;/p&gt;\n\n&lt;p&gt;Was I being unprofessional for wanting to ask for help to make sure I&amp;#39;m looking for the data in the right places? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18of52n", "is_robot_indexable": true, "report_reasons": null, "author": "DataScienceIsScience", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18of52n/was_i_unprofessional_when_i_said_i_needed_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18of52n/was_i_unprofessional_when_i_said_i_needed_help/", "subreddit_subscribers": 147802, "created_utc": 1703252357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody,\n\nI recently accepted a PhD position focusing on the development of a database solution for the laboratory of a university.\n\nThis lab is characterized by a high turnover of projects and students, with a rich diversity of data that holds the potential for significant insights. However, as of now, there's a notable absence of a structured data warehouse. I want to take the opportunity to set up a 21st century solution.\n\nGiven these circumstances, I'm reaching out to this you guys for advice. I'm interested in learning about:\n\n1. **Best Practices and Considerations:** What are the key factors I should consider when implementing a data warehouse from scratch? Are there specific best practices tailored for high-turnover, diverse data environments?\n2. **Appropriateness of a Data Warehouse:** Is a data warehouse the ideal solution for the lab? Or should we explore other data management systems?\n\n**About Me:** My background includes experience in Database QA and moderate programming skills. However, I'm relatively new to in-depth data management. I'm currently reading \"Fundamentals of Data Engineering\" and actively seeking resources to improve my core SQL skills and overall data handling expertise.\n\nI'd appreciate any insights, resources, or experiences you could share that could serve as a guide.\n\nThanks!", "author_fullname": "t2_ddwwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ground Up Database Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18obiz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703239979.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703239306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,&lt;/p&gt;\n\n&lt;p&gt;I recently accepted a PhD position focusing on the development of a database solution for the laboratory of a university.&lt;/p&gt;\n\n&lt;p&gt;This lab is characterized by a high turnover of projects and students, with a rich diversity of data that holds the potential for significant insights. However, as of now, there&amp;#39;s a notable absence of a structured data warehouse. I want to take the opportunity to set up a 21st century solution.&lt;/p&gt;\n\n&lt;p&gt;Given these circumstances, I&amp;#39;m reaching out to this you guys for advice. I&amp;#39;m interested in learning about:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Best Practices and Considerations:&lt;/strong&gt; What are the key factors I should consider when implementing a data warehouse from scratch? Are there specific best practices tailored for high-turnover, diverse data environments?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Appropriateness of a Data Warehouse:&lt;/strong&gt; Is a data warehouse the ideal solution for the lab? Or should we explore other data management systems?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;About Me:&lt;/strong&gt; My background includes experience in Database QA and moderate programming skills. However, I&amp;#39;m relatively new to in-depth data management. I&amp;#39;m currently reading &amp;quot;Fundamentals of Data Engineering&amp;quot; and actively seeking resources to improve my core SQL skills and overall data handling expertise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate any insights, resources, or experiences you could share that could serve as a guide.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18obiz9", "is_robot_indexable": true, "report_reasons": null, "author": "Jimmyfatz", "discussion_type": null, "num_comments": 17, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18obiz9/ground_up_database_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18obiz9/ground_up_database_design/", "subreddit_subscribers": 147802, "created_utc": 1703239306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my company we have some pipelines that do simple processing on raw data from S3 before storing them back as cleaned and partitioned (by day) datasets on S3, so users can effectively read clean data by ranges of days. Raw and partitioned data is stored as parquet.\n\nRight now this is done with Pandas on Airflow pipelines, running with KubernetesExecutor. This raw data only *occasionally* gets large: we do incremental syncs from data sources, so usually we have 1 larger dataset -- the first sync -- followed by small batches. These small batches tend to be really small on most sources: around 100 records. The larger datasets might grow up to millions of records. I would like to have a solution that scales out to this extraordinary cases, while still being used effectively for the small ones.\n\nFrom what I've seen, Spark has been losing popularity due to the hard work in maintaining the clusters and the need to study and learn it from scratch, while simpler tools rise. Polars, Dask or DuckDB seem to be good alternatives.\n\nWhat are the current low setup, easy to learn, cost effective solutions that scale to this wide range of sizes of data, and their pros and cons?", "author_fullname": "t2_c3hoz70b4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to scale pipelines to large datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oftc0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703254296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my company we have some pipelines that do simple processing on raw data from S3 before storing them back as cleaned and partitioned (by day) datasets on S3, so users can effectively read clean data by ranges of days. Raw and partitioned data is stored as parquet.&lt;/p&gt;\n\n&lt;p&gt;Right now this is done with Pandas on Airflow pipelines, running with KubernetesExecutor. This raw data only &lt;em&gt;occasionally&lt;/em&gt; gets large: we do incremental syncs from data sources, so usually we have 1 larger dataset -- the first sync -- followed by small batches. These small batches tend to be really small on most sources: around 100 records. The larger datasets might grow up to millions of records. I would like to have a solution that scales out to this extraordinary cases, while still being used effectively for the small ones.&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve seen, Spark has been losing popularity due to the hard work in maintaining the clusters and the need to study and learn it from scratch, while simpler tools rise. Polars, Dask or DuckDB seem to be good alternatives.&lt;/p&gt;\n\n&lt;p&gt;What are the current low setup, easy to learn, cost effective solutions that scale to this wide range of sizes of data, and their pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oftc0", "is_robot_indexable": true, "report_reasons": null, "author": "henriquemeloo_", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oftc0/best_way_to_scale_pipelines_to_large_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oftc0/best_way_to_scale_pipelines_to_large_datasets/", "subreddit_subscribers": 147802, "created_utc": 1703254296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nJust finished up a project I have been working on for the last few weeks.  Like the title says, it's a data pipeline to a Tableau dashboard containing world economic data.  \n\nHere's a link to the project:\n\nhttps://github.com/johnchandlerbaldwin/data-modeling-project\n\nHere's a link to the Tableau dashboard:\n\nhttps://public.tableau.com/app/profile/john.baldwin4618/viz/EconomicIndicators_17029599995600/FinalDashboard?publish=yes\n\nIt was great to get experience with Tableau and the Modern Data Stack.  I'm hoping this will help me transition into data engineering - career data analyst with a graduate degree in data science here trying to transition into the field.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Project: Data Pipeline from Wikipedia to BigQuery to Create a Tableau Dashboard of World Economic Indicators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18omyz7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703273615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;Just finished up a project I have been working on for the last few weeks.  Like the title says, it&amp;#39;s a data pipeline to a Tableau dashboard containing world economic data.  &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a link to the project:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/johnchandlerbaldwin/data-modeling-project\"&gt;https://github.com/johnchandlerbaldwin/data-modeling-project&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a link to the Tableau dashboard:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://public.tableau.com/app/profile/john.baldwin4618/viz/EconomicIndicators_17029599995600/FinalDashboard?publish=yes\"&gt;https://public.tableau.com/app/profile/john.baldwin4618/viz/EconomicIndicators_17029599995600/FinalDashboard?publish=yes&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It was great to get experience with Tableau and the Modern Data Stack.  I&amp;#39;m hoping this will help me transition into data engineering - career data analyst with a graduate degree in data science here trying to transition into the field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?auto=webp&amp;s=421642422a1e58da69fdf0dc300c050f6b141810", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2133b15c60695dc6f1b4d449193ff128e4190ae", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee9451e1c09607a592eac5807067544a170cdb3b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4183f52c1577942283e215f63de66f8a563129db", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc78d1c0e952e148090277a141f985ccdb2a1355", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc01910a5212eb1195da94a653086341beba14f3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/DvLsTi95n3GOxK65xsdyrAlVV2QuTdzr6LyzWWpy9Pg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90559908a68b7090de2eb6e24b69d7e55863d5ff", "width": 1080, "height": 540}], "variants": {}, "id": "TWDB43vlvDLUgVcdtOELJ74yCcMtQ5udPCeTdvIoslA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18omyz7", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18omyz7/personal_project_data_pipeline_from_wikipedia_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18omyz7/personal_project_data_pipeline_from_wikipedia_to/", "subreddit_subscribers": 147802, "created_utc": 1703273615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For a personal project/business idea, I'm working with the League of Legends API to create a database of players an their in-game match history.\n\nFor the pipeline itself, it creates/upserts on 3 different tables:\n\n1. player\\_account\\_info\n   1. stores account level info like profile picture, level, in-game name, and ID's used to make calls to other endpoints\n2. player\\_ranked\\_info\n   1. stores info about ranked status, like division, tier, LP, etc\n3. player\\_matches\n   1. stores info about the actual matches played. Each match typically has 10 records (1 for each player) and \\~300 columns of in-game stats\n\nThe thing that I'm unsure of about the design is that I have created this pipeline/DAG as a single class called \\`LeaguePipeline\\`. You supply the player's name and region, and then call a wrapper function called \\`process\\_user()\\`. Below is an example \\`main()\\` function that I use to run the program\n\n    def main():\n        pipeline = LeaguePipeline(\n        \triot_name='Annie Bot', \n        \triot_tagline='NA1',\n        \tregion='na1',\n        \tlog_level='DEBUG'\n        )\n        pipeline.MATCH_LIMIT = 20\n    \n        pipeline.process_user()\n        pipeline.database.conn.close()\n\nHere is what the \\`process\\_user()\\` function looks like\n\n    def process_user(self) -&gt; None:\n    \n        if self.played_already_registered():\n            self.update = True\n    \t\t\n        # for the accounts table\n        self.get_summoner_info()\n        # for the ranked table\n        self.get_ranked_info()\n    \t\t\n        # perform ETL on the match history\n        if not isinstance(self.generate_dataframes(), int):\n            self.clean_match_df()\n    \tself.clean_objectives_df()\n    \tself.export_data()\n    \n    \tlogger.success('Finished processing user')\n    \n        return None\n\n# The Problem\n\nThe one thing I'm having doubts about it its heavy reliance on class/instance variable. I'm trying to learn Airflow/Dagster, but running into issues when trying to apply it to this project. Airflow uses XComms to communicate between tasks, but my OOP pipeline uses so many variables that it would be hell to convert all of them explicitly. My problem with Dagster is similar, as it uses mainly functional programming. I would have to convert all the instance variables into regular python global variables, and all the methods into regular functions.\n\n# Is the way I've developed my pipeline \"wrong,\" are Airflow/Dagster \"too rigid,\" or is my pipeline just not adapted for these orchestrators?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Critique my data pipeline (personal project). Are Airflow/Dagster incompatible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ox7au", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703304338.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For a personal project/business idea, I&amp;#39;m working with the League of Legends API to create a database of players an their in-game match history.&lt;/p&gt;\n\n&lt;p&gt;For the pipeline itself, it creates/upserts on 3 different tables:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;player_account_info\n\n&lt;ol&gt;\n&lt;li&gt;stores account level info like profile picture, level, in-game name, and ID&amp;#39;s used to make calls to other endpoints&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;player_ranked_info\n\n&lt;ol&gt;\n&lt;li&gt;stores info about ranked status, like division, tier, LP, etc&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;player_matches\n\n&lt;ol&gt;\n&lt;li&gt;stores info about the actual matches played. Each match typically has 10 records (1 for each player) and ~300 columns of in-game stats&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The thing that I&amp;#39;m unsure of about the design is that I have created this pipeline/DAG as a single class called `LeaguePipeline`. You supply the player&amp;#39;s name and region, and then call a wrapper function called `process_user()`. Below is an example `main()` function that I use to run the program&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def main():\n    pipeline = LeaguePipeline(\n        riot_name=&amp;#39;Annie Bot&amp;#39;, \n        riot_tagline=&amp;#39;NA1&amp;#39;,\n        region=&amp;#39;na1&amp;#39;,\n        log_level=&amp;#39;DEBUG&amp;#39;\n    )\n    pipeline.MATCH_LIMIT = 20\n\n    pipeline.process_user()\n    pipeline.database.conn.close()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Here is what the `process_user()` function looks like&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def process_user(self) -&amp;gt; None:\n\n    if self.played_already_registered():\n        self.update = True\n\n    # for the accounts table\n    self.get_summoner_info()\n    # for the ranked table\n    self.get_ranked_info()\n\n    # perform ETL on the match history\n    if not isinstance(self.generate_dataframes(), int):\n        self.clean_match_df()\n    self.clean_objectives_df()\n    self.export_data()\n\n    logger.success(&amp;#39;Finished processing user&amp;#39;)\n\n    return None\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;The Problem&lt;/h1&gt;\n\n&lt;p&gt;The one thing I&amp;#39;m having doubts about it its heavy reliance on class/instance variable. I&amp;#39;m trying to learn Airflow/Dagster, but running into issues when trying to apply it to this project. Airflow uses XComms to communicate between tasks, but my OOP pipeline uses so many variables that it would be hell to convert all of them explicitly. My problem with Dagster is similar, as it uses mainly functional programming. I would have to convert all the instance variables into regular python global variables, and all the methods into regular functions.&lt;/p&gt;\n\n&lt;h1&gt;Is the way I&amp;#39;ve developed my pipeline &amp;quot;wrong,&amp;quot; are Airflow/Dagster &amp;quot;too rigid,&amp;quot; or is my pipeline just not adapted for these orchestrators?&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ox7au", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ox7au/critique_my_data_pipeline_personal_project_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ox7au/critique_my_data_pipeline_personal_project_are/", "subreddit_subscribers": 147802, "created_utc": 1703304338.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like the title says, I\u2019m curious how others are managing their development and production deployments of Meltano (specifically for its EL, not T utility). I\u2019ve generally used managed services for things like EL before, so I\u2019d like to get a feel for how others are handling this.\n\nAs an extra thought, I\u2019m also looking to add data versioning as a feature for our org down the line, and that seems like it will heavily influence how I handle the production deployment of Meltano.", "author_fullname": "t2_15uzwf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meltano: How do you handle CI/CD and devops on your team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18orom6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703286915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says, I\u2019m curious how others are managing their development and production deployments of Meltano (specifically for its EL, not T utility). I\u2019ve generally used managed services for things like EL before, so I\u2019d like to get a feel for how others are handling this.&lt;/p&gt;\n\n&lt;p&gt;As an extra thought, I\u2019m also looking to add data versioning as a feature for our org down the line, and that seems like it will heavily influence how I handle the production deployment of Meltano.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18orom6", "is_robot_indexable": true, "report_reasons": null, "author": "EngiNerd9000", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18orom6/meltano_how_do_you_handle_cicd_and_devops_on_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18orom6/meltano_how_do_you_handle_cicd_and_devops_on_your/", "subreddit_subscribers": 147802, "created_utc": 1703286915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used aws batch for orchestration?  Have you managed it out of a ci/cd?  Curious to know how others have set this up.", "author_fullname": "t2_5gzu4ur4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Aws batch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18or4ny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703285353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used aws batch for orchestration?  Have you managed it out of a ci/cd?  Curious to know how others have set this up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18or4ny", "is_robot_indexable": true, "report_reasons": null, "author": "bluezebra42", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18or4ny/aws_batch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18or4ny/aws_batch/", "subreddit_subscribers": 147802, "created_utc": 1703285353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\nI recently joined a company that has a Mac only policy. I only ever used windows+WSL2 for work (or at all, tbh). Never used a Mac in my life.\n\nWhat advice would you give me to make the transition smoother?\n\nFor instance, things that are a must-have for me are:\n- remap keys (e,g, Sharpkeys for windows)\n- edit system shortcuts\n- having multiple keyboard layouts installed (Italian, English, etc...)\n- install everything with package manager (does brew suffice? Is there a better alternative?)\n\nAny advice is more than welcome!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving from windows+WSL2 to Mac. Halp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ojz1f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703265564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,\nI recently joined a company that has a Mac only policy. I only ever used windows+WSL2 for work (or at all, tbh). Never used a Mac in my life.&lt;/p&gt;\n\n&lt;p&gt;What advice would you give me to make the transition smoother?&lt;/p&gt;\n\n&lt;p&gt;For instance, things that are a must-have for me are:\n- remap keys (e,g, Sharpkeys for windows)\n- edit system shortcuts\n- having multiple keyboard layouts installed (Italian, English, etc...)\n- install everything with package manager (does brew suffice? Is there a better alternative?)&lt;/p&gt;\n\n&lt;p&gt;Any advice is more than welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ojz1f", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ojz1f/moving_from_windowswsl2_to_mac_halp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ojz1f/moving_from_windowswsl2_to_mac_halp/", "subreddit_subscribers": 147802, "created_utc": 1703265564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently develop in Azure where we use ADF, ADLS and Databricks. Currently we have a few environment json files in a parameters container that map credentials to secrets in key vault across dev/uat/prd to reduce hardcoding.\n\nWhen creating a common pipeline I create a pipeline folder in our parameters container and store a config file with constants specific to the trigger. For example this file might could contain a list of tables that we want to extract from sql. This way we can easily add or remove tables to that trigger and reuse the pipeline for other sources and schedules by simply pointing a trigger to a different config file.\n\nMy question is, have I over complicated this? Is anyone else doing anything similar?", "author_fullname": "t2_qc6h84ym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Parameters &amp; Environment Variables in Azure to Enable CICD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oapzd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703235960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently develop in Azure where we use ADF, ADLS and Databricks. Currently we have a few environment json files in a parameters container that map credentials to secrets in key vault across dev/uat/prd to reduce hardcoding.&lt;/p&gt;\n\n&lt;p&gt;When creating a common pipeline I create a pipeline folder in our parameters container and store a config file with constants specific to the trigger. For example this file might could contain a list of tables that we want to extract from sql. This way we can easily add or remove tables to that trigger and reuse the pipeline for other sources and schedules by simply pointing a trigger to a different config file.&lt;/p&gt;\n\n&lt;p&gt;My question is, have I over complicated this? Is anyone else doing anything similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oapzd", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated-Western1788", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oapzd/managing_parameters_environment_variables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oapzd/managing_parameters_environment_variables_in/", "subreddit_subscribers": 147802, "created_utc": 1703235960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data engineers, I am at a dilemma.  \nHow do you organize your transformation code?\n\nFor airflow to run spark jobs, it needs access to the .py scripts  \nI would like to keep using Gitlab for managing the code base.\n\nMy initial thought was to build a custom operator that will pull the latest version of the code then use the command spark-submit to run the transformation.\n\nHow did you toggle this problem?", "author_fullname": "t2_j9pcxbcbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Apache Airflow + Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ojyhx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703265519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data engineers, I am at a dilemma.&lt;br/&gt;\nHow do you organize your transformation code?&lt;/p&gt;\n\n&lt;p&gt;For airflow to run spark jobs, it needs access to the .py scripts&lt;br/&gt;\nI would like to keep using Gitlab for managing the code base.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was to build a custom operator that will pull the latest version of the code then use the command spark-submit to run the transformation.&lt;/p&gt;\n\n&lt;p&gt;How did you toggle this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ojyhx", "is_robot_indexable": true, "report_reasons": null, "author": "Human-Failure-99", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ojyhx/help_with_apache_airflow_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ojyhx/help_with_apache_airflow_spark/", "subreddit_subscribers": 147802, "created_utc": 1703265519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Morning all,\n\nI need some out the box ideas on ways I can set up a process to send us notifications to a Teams channel that outline what pipeline failed and the error message. \n\nAny good ideas on how to do this are welcome. Originally I was thinking Power Automate through a web activity in a pipeline with the POST option however this is proving to be more difficult then I thought. \n\nPlatform I\u2019m using is Azure Synapse.\n\nCheers", "author_fullname": "t2_173udy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Notifications on Pipeline failures - best method?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oberj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703238860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Morning all,&lt;/p&gt;\n\n&lt;p&gt;I need some out the box ideas on ways I can set up a process to send us notifications to a Teams channel that outline what pipeline failed and the error message. &lt;/p&gt;\n\n&lt;p&gt;Any good ideas on how to do this are welcome. Originally I was thinking Power Automate through a web activity in a pipeline with the POST option however this is proving to be more difficult then I thought. &lt;/p&gt;\n\n&lt;p&gt;Platform I\u2019m using is Azure Synapse.&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oberj", "is_robot_indexable": true, "report_reasons": null, "author": "prodigypro", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oberj/notifications_on_pipeline_failures_best_method/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oberj/notifications_on_pipeline_failures_best_method/", "subreddit_subscribers": 147802, "created_utc": 1703238860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nThe work I currently do is more aligned with Analytics Engineer + light DevOps work than software engineer work.  The stack I work in is snowflake, dbt, databricks, and some cloud infra with terraform.\n\nI\u2019m thinking longer-term I\u2019d like to get more technical and pivot to a more software engineering focused team such as \u201cSoftware Engineer - Data\u201d or \u201cData Platform\u201d at a different company.\n\nWhat kind of skills should I drill down on over the next 6-12 months?", "author_fullname": "t2_nexbbb26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting onto a data platforming team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18p0idb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703316659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;The work I currently do is more aligned with Analytics Engineer + light DevOps work than software engineer work.  The stack I work in is snowflake, dbt, databricks, and some cloud infra with terraform.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking longer-term I\u2019d like to get more technical and pivot to a more software engineering focused team such as \u201cSoftware Engineer - Data\u201d or \u201cData Platform\u201d at a different company.&lt;/p&gt;\n\n&lt;p&gt;What kind of skills should I drill down on over the next 6-12 months?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18p0idb", "is_robot_indexable": true, "report_reasons": null, "author": "keep_it_professional", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18p0idb/getting_onto_a_data_platforming_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18p0idb/getting_onto_a_data_platforming_team/", "subreddit_subscribers": 147802, "created_utc": 1703316659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nWe have a built a data warehouse for our business analytics purposes, I need some help to optimise few things.\n\nOur metrics initially are stored in S3(partitioned by year/month/day/hour), the files are in csv format, we then run glue crawlers every hour to keep partition details updated. \n\nRedshift spectrum is then used to query this data from redshift. However this was slow for our end users as the data is huge (in range of 6-7 petabytes and increasing).\n\nSo we started aggregating data using aggregation queries in redshift(basically we run hourly scheduled group by sql queries over multiple columns and store the aggregated metrics and discard raw S3 metrics), all of this orchestrated using step funtions. We were able to achieve 90% compression.\n\nThe problem: We also need to run percentile aggregations as part of this process. So, instead of querying raw data, sort and get percentile for combinations of columns, we aggregate metrics for percentiles over some columns(~20 columns are present in each metric).\nThe percentile queries however are very slow, they take 20~hrs each and completly blocks other aggregation queries. So, two problems, its a cascading effect and I can't run all percentile queries, and other problem is that these queries also block normal hourly aggregation queries.\n\nAs we use provisioned redshift cluster, the cost is constant over month, what other approach can i use keeping cost to minimal, use emr? or spin up a hugh end redshift cluster which juat processes percentile queries?\n\nAslo, i found that even one percentile query blocks other queries as it's taking up cpu and network and disk io.", "author_fullname": "t2_4vwvhe5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large data SQL aggregations in Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oyrvy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703309861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;We have a built a data warehouse for our business analytics purposes, I need some help to optimise few things.&lt;/p&gt;\n\n&lt;p&gt;Our metrics initially are stored in S3(partitioned by year/month/day/hour), the files are in csv format, we then run glue crawlers every hour to keep partition details updated. &lt;/p&gt;\n\n&lt;p&gt;Redshift spectrum is then used to query this data from redshift. However this was slow for our end users as the data is huge (in range of 6-7 petabytes and increasing).&lt;/p&gt;\n\n&lt;p&gt;So we started aggregating data using aggregation queries in redshift(basically we run hourly scheduled group by sql queries over multiple columns and store the aggregated metrics and discard raw S3 metrics), all of this orchestrated using step funtions. We were able to achieve 90% compression.&lt;/p&gt;\n\n&lt;p&gt;The problem: We also need to run percentile aggregations as part of this process. So, instead of querying raw data, sort and get percentile for combinations of columns, we aggregate metrics for percentiles over some columns(~20 columns are present in each metric).\nThe percentile queries however are very slow, they take 20~hrs each and completly blocks other aggregation queries. So, two problems, its a cascading effect and I can&amp;#39;t run all percentile queries, and other problem is that these queries also block normal hourly aggregation queries.&lt;/p&gt;\n\n&lt;p&gt;As we use provisioned redshift cluster, the cost is constant over month, what other approach can i use keeping cost to minimal, use emr? or spin up a hugh end redshift cluster which juat processes percentile queries?&lt;/p&gt;\n\n&lt;p&gt;Aslo, i found that even one percentile query blocks other queries as it&amp;#39;s taking up cpu and network and disk io.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oyrvy", "is_robot_indexable": true, "report_reasons": null, "author": "_dEnOmInAtOr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oyrvy/large_data_sql_aggregations_in_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oyrvy/large_data_sql_aggregations_in_redshift/", "subreddit_subscribers": 147802, "created_utc": 1703309861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm pretty sure that cloud functions can be triggered by pubsub events, but I'm not sure about cloud run. Afaik cloud functions are basically the same as cloud run (since v2?) they just abstract the dockerfile part. \n\nBut, basically, I'm wondering if cloud run jobs can be started from a pub sub event, if not why not? I guess I don't understand the underlying reason (unless it's just \"Google says no\").\n\nEg - object added to bucket, cloud run job starts up and processes the object. I know this can be done with a cloud function - but this is partly academic/curiosity for me (hence asking).", "author_fullname": "t2_4atn0tqg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting a cloud run job from a pub sub message in gcp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oth5f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703292176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pretty sure that cloud functions can be triggered by pubsub events, but I&amp;#39;m not sure about cloud run. Afaik cloud functions are basically the same as cloud run (since v2?) they just abstract the dockerfile part. &lt;/p&gt;\n\n&lt;p&gt;But, basically, I&amp;#39;m wondering if cloud run jobs can be started from a pub sub event, if not why not? I guess I don&amp;#39;t understand the underlying reason (unless it&amp;#39;s just &amp;quot;Google says no&amp;quot;).&lt;/p&gt;\n\n&lt;p&gt;Eg - object added to bucket, cloud run job starts up and processes the object. I know this can be done with a cloud function - but this is partly academic/curiosity for me (hence asking).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oth5f", "is_robot_indexable": true, "report_reasons": null, "author": "onmyphoneinlondon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oth5f/starting_a_cloud_run_job_from_a_pub_sub_message/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oth5f/starting_a_cloud_run_job_from_a_pub_sub_message/", "subreddit_subscribers": 147802, "created_utc": 1703292176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am looking for resources for the  AWS Certified Data Engineer - Associate certification. I cannot see anything linked on the certification's page and at the moment I am going through the normal AWS documentation. Any suggestion for some extra content?", "author_fullname": "t2_utk7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best resources to study for AWS Certified Data Engineer - Associate - cert?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oma86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703271689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am looking for resources for the  AWS Certified Data Engineer - Associate certification. I cannot see anything linked on the certification&amp;#39;s page and at the moment I am going through the normal AWS documentation. Any suggestion for some extra content?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oma86", "is_robot_indexable": true, "report_reasons": null, "author": "df016", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oma86/what_are_the_best_resources_to_study_for_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oma86/what_are_the_best_resources_to_study_for_aws/", "subreddit_subscribers": 147802, "created_utc": 1703271689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I am working on a small project to create a simple real-time pipeline of posts on Reddit. I want to allow the user to query the subreddit name. But then the problem appears, **how should the new query subreddit change be written to the Kafka producer?**\n\n\\- Should I create a new topic for every new subreddit query, although that will *theoretically* create hundreds of thousands of topics (given that there are \\~100k subreddits) and would be a bad practice (although this is not going to happen in my use case)?    \n\\- Should I write everything to the same stream, but use some kind of ID to differentiate between topics? Would this aggregation of data make it harder to process the stream? Since it is relatively large now, my stream processor (e.g. Spark) will have to process everything just to select the rows with the intended subreddit name.\n\nI am quite stuck here and would appreciate any insight on this, thank you so much.", "author_fullname": "t2_38msj796", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategy to approach query-based Kafka producer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oefke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703250871.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703250152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I am working on a small project to create a simple real-time pipeline of posts on Reddit. I want to allow the user to query the subreddit name. But then the problem appears, &lt;strong&gt;how should the new query subreddit change be written to the Kafka producer?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Should I create a new topic for every new subreddit query, although that will &lt;em&gt;theoretically&lt;/em&gt; create hundreds of thousands of topics (given that there are ~100k subreddits) and would be a bad practice (although this is not going to happen in my use case)?&lt;br/&gt;\n- Should I write everything to the same stream, but use some kind of ID to differentiate between topics? Would this aggregation of data make it harder to process the stream? Since it is relatively large now, my stream processor (e.g. Spark) will have to process everything just to select the rows with the intended subreddit name.&lt;/p&gt;\n\n&lt;p&gt;I am quite stuck here and would appreciate any insight on this, thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oefke", "is_robot_indexable": true, "report_reasons": null, "author": "Babe_My_Name_Is_Hung", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oefke/strategy_to_approach_querybased_kafka_producer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oefke/strategy_to_approach_querybased_kafka_producer/", "subreddit_subscribers": 147802, "created_utc": 1703250152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know of any schools out there teaching Airflow? I've seen online training and USC has this 1 course -- [https://careers.usc.edu/classes/apache-airflow-essential-training/](https://careers.usc.edu/classes/apache-airflow-essential-training/)", "author_fullname": "t2_csqyrhw6p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any schools out there teaching Apache Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18or28c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703285161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of any schools out there teaching Airflow? I&amp;#39;ve seen online training and USC has this 1 course -- &lt;a href=\"https://careers.usc.edu/classes/apache-airflow-essential-training/\"&gt;https://careers.usc.edu/classes/apache-airflow-essential-training/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/r1jWZxgcwSQgxBYvl2xzAORAbT9UoQBZtO6OpvprrTM.jpg?auto=webp&amp;s=36d921c513ed886fd9033d528b30938dfe2269bf", "width": 512, "height": 288}, "resolutions": [{"url": "https://external-preview.redd.it/r1jWZxgcwSQgxBYvl2xzAORAbT9UoQBZtO6OpvprrTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0f04831f7c73fa94b851fc98bef86e590c78b0c", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/r1jWZxgcwSQgxBYvl2xzAORAbT9UoQBZtO6OpvprrTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb5710a83cadae5daecacbac88d05a73fa9241e8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/r1jWZxgcwSQgxBYvl2xzAORAbT9UoQBZtO6OpvprrTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d67cb83452ba9f26c616c0c624bfa6ee3968b358", "width": 320, "height": 180}], "variants": {}, "id": "mxvc6Jr0jf7Sm-AZ1vTsNDVUA7l2uNS8fT7SF32ibPA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18or28c", "is_robot_indexable": true, "report_reasons": null, "author": "AirFlordan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18or28c/are_there_any_schools_out_there_teaching_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18or28c/are_there_any_schools_out_there_teaching_apache/", "subreddit_subscribers": 147802, "created_utc": 1703285161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to copy files from an sftp server to ADLS via databricks and subsequently copy only new files from the sftp server to ADLS. I copied the file into a temp dbfs folder but I can't write it into ADLS. \n\nImport pysftp\n\n    files = sftp.listdir(\"/home/folder/my_files\")\ndbfs_path = 'my_temp_folder'\nos.makedirs(dbfs_path, exist_ok=True)\n\nFor f in files:\n    remote_file_path=\"home/folder/my_files\"\nLocal_file = dbfs_path + \"/\"  + f\nsftp.get(remote_file_path, Local_file)", "author_fullname": "t2_35f1rdk6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying files via Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oq8lf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703282812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to copy files from an sftp server to ADLS via databricks and subsequently copy only new files from the sftp server to ADLS. I copied the file into a temp dbfs folder but I can&amp;#39;t write it into ADLS. &lt;/p&gt;\n\n&lt;p&gt;Import pysftp&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;files = sftp.listdir(&amp;quot;/home/folder/my_files&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;dbfs_path = &amp;#39;my_temp_folder&amp;#39;\nos.makedirs(dbfs_path, exist_ok=True)&lt;/p&gt;\n\n&lt;p&gt;For f in files:\n    remote_file_path=&amp;quot;home/folder/my_files&amp;quot;\nLocal_file = dbfs_path + &amp;quot;/&amp;quot;  + f\nsftp.get(remote_file_path, Local_file)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oq8lf", "is_robot_indexable": true, "report_reasons": null, "author": "Rogie_88", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oq8lf/copying_files_via_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oq8lf/copying_files_via_databricks/", "subreddit_subscribers": 147802, "created_utc": 1703282812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently tackling a project to shift our operations from an outdated stack to a proper data engineering platform. Our current system, designed mainly by software engineers, is a mix of Java/Go microservices, overly dependent on manual heavy lifting  , and honestly, it's barely hanging on.\n\nHere's what I'm dealing with:\n\n1. I need to pull a list of stock prices that our clients are tracking, which comes from Cassandra. The twist is, both the client roster and the stock list are always in flux.\n2. For every customer, every five minutes or so, I've got to ping a bunch of REST APIs for each client to grab the latest stock info.\n3. Then, there's some basic data transformation before I shoot this data over to Kinesis for more heavy-lifting.\n\nBasically,\n\nEvery 5 minutes --&gt; Customer 1 --&gt; REST APIs A,B,C ---&gt; Transform and send to Kinesis\n\nEvery 5 minutes --&gt; Customer 2  --&gt; REST APIs D,E,F. --&gt; Transform and send to Kinesis\n\nI tried setting up the first step in a Master DAG in Airflow and then handling the next two steps in Nifi as part of Sub DAGs(corresponding to each customer). But, boy, am I hitting a wall trying to pass all the client-specific info through the Sub DAGs using the TriggerDagRunOperator. Trying to jam this all into a single DAG? Not gonna work \u2013 it just can't keep up with our five-minute deadline.\n\nNow, I'm starting to wonder if we even need an orchestrator in this puzzle, considering none seem up to snuff for our needs. Is it crazy to think about running all three stages directly in Nifi? My experience with Nifi has mostly been about data ingestion and routing, but here, it might have to take on a scheduler role too. Need some opinions on how to design this flow !", "author_fullname": "t2_8023gpp1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Options for migrating from a legacy pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18omla1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703272526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently tackling a project to shift our operations from an outdated stack to a proper data engineering platform. Our current system, designed mainly by software engineers, is a mix of Java/Go microservices, overly dependent on manual heavy lifting  , and honestly, it&amp;#39;s barely hanging on.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m dealing with:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I need to pull a list of stock prices that our clients are tracking, which comes from Cassandra. The twist is, both the client roster and the stock list are always in flux.&lt;/li&gt;\n&lt;li&gt;For every customer, every five minutes or so, I&amp;#39;ve got to ping a bunch of REST APIs for each client to grab the latest stock info.&lt;/li&gt;\n&lt;li&gt;Then, there&amp;#39;s some basic data transformation before I shoot this data over to Kinesis for more heavy-lifting.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Basically,&lt;/p&gt;\n\n&lt;p&gt;Every 5 minutes --&amp;gt; Customer 1 --&amp;gt; REST APIs A,B,C ---&amp;gt; Transform and send to Kinesis&lt;/p&gt;\n\n&lt;p&gt;Every 5 minutes --&amp;gt; Customer 2  --&amp;gt; REST APIs D,E,F. --&amp;gt; Transform and send to Kinesis&lt;/p&gt;\n\n&lt;p&gt;I tried setting up the first step in a Master DAG in Airflow and then handling the next two steps in Nifi as part of Sub DAGs(corresponding to each customer). But, boy, am I hitting a wall trying to pass all the client-specific info through the Sub DAGs using the TriggerDagRunOperator. Trying to jam this all into a single DAG? Not gonna work \u2013 it just can&amp;#39;t keep up with our five-minute deadline.&lt;/p&gt;\n\n&lt;p&gt;Now, I&amp;#39;m starting to wonder if we even need an orchestrator in this puzzle, considering none seem up to snuff for our needs. Is it crazy to think about running all three stages directly in Nifi? My experience with Nifi has mostly been about data ingestion and routing, but here, it might have to take on a scheduler role too. Need some opinions on how to design this flow !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18omla1", "is_robot_indexable": true, "report_reasons": null, "author": "keroomi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18omla1/options_for_migrating_from_a_legacy_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18omla1/options_for_migrating_from_a_legacy_pipeline/", "subreddit_subscribers": 147802, "created_utc": 1703272526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It would be helpful to see if anyone has been able to get meaningful discounts from vendors. \n\nIf you've received a discount on a vendor -- can you state vendor // annual spend and what discount you received? I'm particularly interested in Snowflake.", "author_fullname": "t2_j1vd6s00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vendor discounts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18okok3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703267444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It would be helpful to see if anyone has been able to get meaningful discounts from vendors. &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve received a discount on a vendor -- can you state vendor // annual spend and what discount you received? I&amp;#39;m particularly interested in Snowflake.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18okok3", "is_robot_indexable": true, "report_reasons": null, "author": "crhumble", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18okok3/vendor_discounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18okok3/vendor_discounts/", "subreddit_subscribers": 147802, "created_utc": 1703267444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just read an article says that building applications on top of a data warehouse is a very bad idea and data warehouses are \u201ca terrible application backend\u201d \n\nthen whats the best approach to build an application? \n\ni have an idea of using \u201cRetool\u201d to build an application just for internal use, and as i know this will be built on top of the data warehouse. \n\nnote: our developers are all engaged in a big project and we\u2019re planning to do it with the IT. trying to help each other. the idea is to build an application for us to monitor the vacations calendar. in certain period who will be on vacation and who will be delighted\u2026 etc we want to automate the process as much we can because this is not going right with us and my manager and I believe this is the solution \ud83e\udd37\ud83c\udffb\u200d\u2640\ufe0f", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building applications on DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ojz9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703265583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just read an article says that building applications on top of a data warehouse is a very bad idea and data warehouses are \u201ca terrible application backend\u201d &lt;/p&gt;\n\n&lt;p&gt;then whats the best approach to build an application? &lt;/p&gt;\n\n&lt;p&gt;i have an idea of using \u201cRetool\u201d to build an application just for internal use, and as i know this will be built on top of the data warehouse. &lt;/p&gt;\n\n&lt;p&gt;note: our developers are all engaged in a big project and we\u2019re planning to do it with the IT. trying to help each other. the idea is to build an application for us to monitor the vacations calendar. in certain period who will be on vacation and who will be delighted\u2026 etc we want to automate the process as much we can because this is not going right with us and my manager and I believe this is the solution \ud83e\udd37\ud83c\udffb\u200d\u2640\ufe0f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ojz9z", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ojz9z/building_applications_on_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ojz9z/building_applications_on_dwh/", "subreddit_subscribers": 147802, "created_utc": 1703265583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI only have about one year of experience in the corporate world. I worked as a Research Analyst, mainly using Google Sheets and Excel, with very little pay (2.16 LPA). I took this job to gain some experience.\n\nI have an interest in Data Engineering and have learned the basics of Big Data technologies like Hadoop, Hive, and Sqoop. I can handle SQL at a medium level, solving Leetcode problems and Data Lemur Problems. I can write loops and perform basic to intermediate ETL tasks on Azure, and I am currently learning Spark.\n\nThe problem is that I'm not getting any interview calls, especially for Azure roles. Some people suggested changing my previous work profile on my resume to make it look like I worked with Azure directly (essentially, faking it until I make it). However, would interviewers believe that I did Azure work for such a low salary?\n\nI'm confused about what to write in that regard. I have no experience in this field, and I know learning is one thing, but actual production work is different. Can someone suggest how to fake it effectively?\n\nThank you.\"\n\n&amp;#x200B;", "author_fullname": "t2_3rcj33dt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting Into DE with Littel Experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ozeih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703312180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I only have about one year of experience in the corporate world. I worked as a Research Analyst, mainly using Google Sheets and Excel, with very little pay (2.16 LPA). I took this job to gain some experience.&lt;/p&gt;\n\n&lt;p&gt;I have an interest in Data Engineering and have learned the basics of Big Data technologies like Hadoop, Hive, and Sqoop. I can handle SQL at a medium level, solving Leetcode problems and Data Lemur Problems. I can write loops and perform basic to intermediate ETL tasks on Azure, and I am currently learning Spark.&lt;/p&gt;\n\n&lt;p&gt;The problem is that I&amp;#39;m not getting any interview calls, especially for Azure roles. Some people suggested changing my previous work profile on my resume to make it look like I worked with Azure directly (essentially, faking it until I make it). However, would interviewers believe that I did Azure work for such a low salary?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m confused about what to write in that regard. I have no experience in this field, and I know learning is one thing, but actual production work is different. Can someone suggest how to fake it effectively?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18ozeih", "is_robot_indexable": true, "report_reasons": null, "author": "m_death", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ozeih/getting_into_de_with_littel_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ozeih/getting_into_de_with_littel_experience/", "subreddit_subscribers": 147802, "created_utc": 1703312180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Video \ud83e\udd73 Creating Pipelines without Airflow Knowledge \ud83d\ude33", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18oj17p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/uv-SZKjups8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18oj17p", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0YD4vBYIg6hW0KSK0YeJfmgVV0e-0de5UNZVlkRQlgs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703263076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/uv-SZKjups8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?auto=webp&amp;s=b8adcd0145fcb2d8cdb0aafd758c42ffb1ca3aa6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9e4e609b5416d661f1ef191e2b3d5d8c215a289", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed487119963730863ca04759c0cfd159528d036b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ReZ3NZA_BRknMGii7HheDnBavxXflRC9tZvmy7ly_tI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc690e5ec6feefab112123e44a213f99bf1d31db", "width": 320, "height": 240}], "variants": {}, "id": "B45PQTstzD0z7ab5oanv9VuTrBUi41DB_w7yZlDT-sA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18oj17p", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oj17p/new_video_creating_pipelines_without_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/uv-SZKjups8", "subreddit_subscribers": 147802, "created_utc": 1703263076.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uv-SZKjups8?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Create Data Pipelines with No Airflow Knowledge!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/uv-SZKjups8/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}], "before": null}}