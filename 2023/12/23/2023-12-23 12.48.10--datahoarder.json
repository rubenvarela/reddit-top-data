{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Bought a 5tb HDD from Amazon and when I connected it it read this as the error rate? No way that\u2019s possible? Or is this normal. Can\u2019t find anything about it\u2019s error read rate online. \n\nThanks in advance.", "author_fullname": "t2_5z3efa2s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read error rate on new 5tb HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18osnfw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/bZ1KU8gJ2t2YEw273hDhXtBvR5gfVKNG3ZmZdZEhot4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703289700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bought a 5tb HDD from Amazon and when I connected it it read this as the error rate? No way that\u2019s possible? Or is this normal. Can\u2019t find anything about it\u2019s error read rate online. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wbb5kj0sqx7c1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wbb5kj0sqx7c1.jpeg?auto=webp&amp;s=0429c071b169758978c1084964b8e0ca55e45950", "width": 668, "height": 690}, "resolutions": [{"url": "https://preview.redd.it/wbb5kj0sqx7c1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8944431c10c32d597902f6123b19329e0309b06", "width": 108, "height": 111}, {"url": "https://preview.redd.it/wbb5kj0sqx7c1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=722a02df72c3ef4fe0a5a9ef7c93577c0ef5b6cb", "width": 216, "height": 223}, {"url": "https://preview.redd.it/wbb5kj0sqx7c1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b382ae8b87ed2b0e23604840fe1e9e36638db3d8", "width": 320, "height": 330}, {"url": "https://preview.redd.it/wbb5kj0sqx7c1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=de28c7681a6b8558a5af46c664424025121f241a", "width": 640, "height": 661}], "variants": {}, "id": "ralXn8EaXwSlY80FGRwdR7-Rc2b24n_XpjcQBAPyNP8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18osnfw", "is_robot_indexable": true, "report_reasons": null, "author": "xRehve", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18osnfw/read_error_rate_on_new_5tb_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wbb5kj0sqx7c1.jpeg", "subreddit_subscribers": 719986, "created_utc": 1703289700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Wife got me an external HHD for christmas, 14TB Western Digital. Was able to create a partition, but not able to format it as NTFS (it was RAW format). Windows kept telling me the format failed and disk management would stop responding. Downloaded Disk Genius and I'm verifying the bad sectors... I'm only 6% done (it's already been 10 hours) and I currently have 9,806 bad sectors out of 112,300 scanned.\n\nDoes that mean this was used and possibly returned? ", "author_fullname": "t2_gqvx8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my brand new 14TB external HHD actually used?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ok074", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703265648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wife got me an external HHD for christmas, 14TB Western Digital. Was able to create a partition, but not able to format it as NTFS (it was RAW format). Windows kept telling me the format failed and disk management would stop responding. Downloaded Disk Genius and I&amp;#39;m verifying the bad sectors... I&amp;#39;m only 6% done (it&amp;#39;s already been 10 hours) and I currently have 9,806 bad sectors out of 112,300 scanned.&lt;/p&gt;\n\n&lt;p&gt;Does that mean this was used and possibly returned? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ok074", "is_robot_indexable": true, "report_reasons": null, "author": "CrisuKomie", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ok074/is_my_brand_new_14tb_external_hhd_actually_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ok074/is_my_brand_new_14tb_external_hhd_actually_used/", "subreddit_subscribers": 719986, "created_utc": 1703265648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Background for context:\n\nMy father passed away and had several different drives and backups. It was a bit of a \"rat's nest\" of data, with some drives having backups of the same information in general but not necessarily identical. I have since consolidated it all onto one USB drive. I used dupeguru to clear out all of the duplicate files, which was great for getting over 200k files down to 43k. The next challenge is trying to get what remains consolidated.\n\nFor example, he may have a \"2003 Photos\" folder in multiple places:  \nD:\\\\Photos\\\\2003 Photos  \nD:\\\\Backup\\\\2003 Photos  \nD:\\\\Home\\\\Photos\\\\2003 Photos  \n\u2026 and for whatever reason there are different/unique files in each location, no duplicates.\n\nIs there a relatively simple way to merge these folders automatically? Preferably for Windows. I'm fine with having to iterate the process as directories merge upward, but cannot realistically comb through it manually as currently there are still 3300 folders on the drive.", "author_fullname": "t2_7gj6me", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merging Duplicate Folders", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18osuwq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703290292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background for context:&lt;/p&gt;\n\n&lt;p&gt;My father passed away and had several different drives and backups. It was a bit of a &amp;quot;rat&amp;#39;s nest&amp;quot; of data, with some drives having backups of the same information in general but not necessarily identical. I have since consolidated it all onto one USB drive. I used dupeguru to clear out all of the duplicate files, which was great for getting over 200k files down to 43k. The next challenge is trying to get what remains consolidated.&lt;/p&gt;\n\n&lt;p&gt;For example, he may have a &amp;quot;2003 Photos&amp;quot; folder in multiple places:&lt;br/&gt;\nD:\\Photos\\2003 Photos&lt;br/&gt;\nD:\\Backup\\2003 Photos&lt;br/&gt;\nD:\\Home\\Photos\\2003 Photos&lt;br/&gt;\n\u2026 and for whatever reason there are different/unique files in each location, no duplicates.&lt;/p&gt;\n\n&lt;p&gt;Is there a relatively simple way to merge these folders automatically? Preferably for Windows. I&amp;#39;m fine with having to iterate the process as directories merge upward, but cannot realistically comb through it manually as currently there are still 3300 folders on the drive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18osuwq", "is_robot_indexable": true, "report_reasons": null, "author": "JoshMcMadMac", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18osuwq/merging_duplicate_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18osuwq/merging_duplicate_folders/", "subreddit_subscribers": 719986, "created_utc": 1703290292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So my current setup includes a RPi4 NAS with an external hard drive and a separate computer that handles my docker containers and serves out Plex.\n\nI\u2019ve heard the term DAS thrown around and as far as I can tell it\u2019s basically just a bunch of disks behaving like an external hard drive, is that more or less correct? If I attached a DAS to the computer with my containers (which is also serving files on the network) wouldn\u2019t it essentially just be a NAS?\n\nIs there a disadvantage to a DAS in this context? Does it have lower read/write speeds than if the disks were all in the server itself?", "author_fullname": "t2_1gyiwuuv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS vs DAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oum2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703295723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my current setup includes a RPi4 NAS with an external hard drive and a separate computer that handles my docker containers and serves out Plex.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve heard the term DAS thrown around and as far as I can tell it\u2019s basically just a bunch of disks behaving like an external hard drive, is that more or less correct? If I attached a DAS to the computer with my containers (which is also serving files on the network) wouldn\u2019t it essentially just be a NAS?&lt;/p&gt;\n\n&lt;p&gt;Is there a disadvantage to a DAS in this context? Does it have lower read/write speeds than if the disks were all in the server itself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oum2m", "is_robot_indexable": true, "report_reasons": null, "author": "trianglesteve", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oum2m/nas_vs_das/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oum2m/nas_vs_das/", "subreddit_subscribers": 719986, "created_utc": 1703295723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have to RMA an SSD and I would like to clone it back onto the new drive that I get from RMA. I do not have any free drive at the moment to which I can clone the SSD onto. I was wondering if there is a way to get like a full file dump kind of like an iso that incorporates the entirety of the drive and keep it as a file on my NAS and I can flash that onto the new SSD?", "author_fullname": "t2_stao3xx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Keep a SSD clone as a backup File?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oorcw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703278680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to RMA an SSD and I would like to clone it back onto the new drive that I get from RMA. I do not have any free drive at the moment to which I can clone the SSD onto. I was wondering if there is a way to get like a full file dump kind of like an iso that incorporates the entirety of the drive and keep it as a file on my NAS and I can flash that onto the new SSD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oorcw", "is_robot_indexable": true, "report_reasons": null, "author": "Yuimurasaki74", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oorcw/how_to_keep_a_ssd_clone_as_a_backup_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oorcw/how_to_keep_a_ssd_clone_as_a_backup_file/", "subreddit_subscribers": 719986, "created_utc": 1703278680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a physical media collection of about 400+ titles (DVDs, Blu-rays , and 4k). I want to begin the process of backing them up but I don't even know where to begin. What tech will I need? What obstacles can I expect? Does making digital copies damage the discs? Etc...", "author_fullname": "t2_kk2w4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice - backing up physical media collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oj288", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703263157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a physical media collection of about 400+ titles (DVDs, Blu-rays , and 4k). I want to begin the process of backing them up but I don&amp;#39;t even know where to begin. What tech will I need? What obstacles can I expect? Does making digital copies damage the discs? Etc...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oj288", "is_robot_indexable": true, "report_reasons": null, "author": "Jai-jo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oj288/need_advice_backing_up_physical_media_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oj288/need_advice_backing_up_physical_media_collection/", "subreddit_subscribers": 719986, "created_utc": 1703263157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_v2a9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do You Test If An EEPROM Can Hold Data For 100 Years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_18p18dz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JPf_BN0C8u-wAgTs5Uahp_3FBsQX6GOjSXVLdp07JS4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703319803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hackaday.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hackaday.com/2023/12/21/how-do-you-test-if-an-eeprom-can-hold-data-for-100-years/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?auto=webp&amp;s=6b166e25037e706ec1549aa3e8726c2311c397ed", "width": 3000, "height": 1815}, "resolutions": [{"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb0ece5921121e85bdde63d8da765dece806c328", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7dc81177220f6d70f11787425c0c47c2ac4e6652", "width": 216, "height": 130}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba67ee587c1e5ed3c838ded2258b3a6a8c8294ca", "width": 320, "height": 193}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30ecfa35fe2c0e0c4de929a878419ac1bc8ba208", "width": 640, "height": 387}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a087138ca695fc95c5511c52209204401306c9d4", "width": 960, "height": 580}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9434cc84ef02babaef568148af20263c93378576", "width": 1080, "height": 653}], "variants": {}, "id": "GufPJHoAEjOVkrn9vS8H26Lg5I8XMnIpEsBI-W8p7OQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p18dz", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy-Red-Fox", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p18dz/how_do_you_test_if_an_eeprom_can_hold_data_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hackaday.com/2023/12/21/how-do-you-test-if-an-eeprom-can-hold-data-for-100-years/", "subreddit_subscribers": 719986, "created_utc": 1703319803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nHey DataHoarder! \ud83d\udc4b \nI\u2019m looking for some insights on Seagate hard drives, specifically the Exos and Exos Enterprise series. Can anyone shed light on the differences between these two lines?\n\nAlso, I'm curious about the [various X generations](https://www.seagate.com/de/de/products/enterprise-drives/exos-x/) within these series and which one would be the best fit for data backup in a small business setup. Any recommendations or experiences to share? Thanks in advance! \n\nJulez", "author_fullname": "t2_5pnii6pu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Clarity: Difference Between Exos and Exos Enterprise, Understanding X Generation for Small Business Data Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ow5su", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703300762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DataHoarder! \ud83d\udc4b \nI\u2019m looking for some insights on Seagate hard drives, specifically the Exos and Exos Enterprise series. Can anyone shed light on the differences between these two lines?&lt;/p&gt;\n\n&lt;p&gt;Also, I&amp;#39;m curious about the &lt;a href=\"https://www.seagate.com/de/de/products/enterprise-drives/exos-x/\"&gt;various X generations&lt;/a&gt; within these series and which one would be the best fit for data backup in a small business setup. Any recommendations or experiences to share? Thanks in advance! &lt;/p&gt;\n\n&lt;p&gt;Julez&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?auto=webp&amp;s=b98f58f088fdf8fbeb225a485466816520892a66", "width": 1440, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=800aeac928e4d6917cdf3db161d37c173da92af5", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a9549b2c27b55813f6941439e0246cac408ff1d", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=708b52a8cc3161e776c554b1f4a488231d8bfce6", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=93eb129bea186541cdd32a88a0cb2d619f9a954c", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ad5e3bcc25ba7cdf8dd591fb8e247b839fe5aa7", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b52b8bf108499e8a6d3bd58f428c8f644af3085", "width": 1080, "height": 675}], "variants": {}, "id": "GJ9K1o7VNo6J4JXg3IrZBPizWfgWXRc6b6FmSaP5cNc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ow5su", "is_robot_indexable": true, "report_reasons": null, "author": "juIez_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ow5su/seeking_clarity_difference_between_exos_and_exos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ow5su/seeking_clarity_difference_between_exos_and_exos/", "subreddit_subscribers": 719986, "created_utc": 1703300762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a media server running Windows 10 Pro. i7-7700k, MSI Z270 board, 32gb ram, 5 x 16tb drives running in storage spaces with parity. I\u2019m going to be upgrading to Windows Server 2022 soon to break through the 63tb limit with Win10 Pro. I started to change my cluster size from 16kb to 64kb to prepare for the change. I read some good reviews on EaseUS software and started the cluster size change with it. It doesn\u2019t show a timeframe. Only shows a percentage complete. It\u2019s been on 95% for over a day. I can see the process is still running in task manager. My question is has anyone here had experience with changing cluster sizes with data retention and what kind of time frame should I expect? I\u2019ve heard mixed answers around the web. I wouldn\u2019t expect it to be done in a day or even two. I\u2019d expect at least a week for a collection of that size. Thanks in advance.", "author_fullname": "t2_25eskjyz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Timeframe to change cluster size on 63tb array Windows 10", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oxgf3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703305845.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703305217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a media server running Windows 10 Pro. i7-7700k, MSI Z270 board, 32gb ram, 5 x 16tb drives running in storage spaces with parity. I\u2019m going to be upgrading to Windows Server 2022 soon to break through the 63tb limit with Win10 Pro. I started to change my cluster size from 16kb to 64kb to prepare for the change. I read some good reviews on EaseUS software and started the cluster size change with it. It doesn\u2019t show a timeframe. Only shows a percentage complete. It\u2019s been on 95% for over a day. I can see the process is still running in task manager. My question is has anyone here had experience with changing cluster sizes with data retention and what kind of time frame should I expect? I\u2019ve heard mixed answers around the web. I wouldn\u2019t expect it to be done in a day or even two. I\u2019d expect at least a week for a collection of that size. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oxgf3", "is_robot_indexable": true, "report_reasons": null, "author": "SpcPewPew", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oxgf3/timeframe_to_change_cluster_size_on_63tb_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oxgf3/timeframe_to_change_cluster_size_on_63tb_array/", "subreddit_subscribers": 719986, "created_utc": 1703305217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI have been thinking about this, but this is one of those situations, where I don\u2019t even know what to search for. I would appreciate any ideas, thoughts, or comments.\n\n**The goal:** Have a server with hot-swap capable SAS / SATA bays. Find a NAS operating system that allows *easy, automatic* mounting and dismounting of single disks plugged into those bays, and sharing the files on them. I.e. the disks would be *single disks, not in a raid array*. Filesystem is not important. Can be ext4, btrfs, zfs, whatever.\n\nI know this sounds like a \u201cwhy is this even a question\u201d type of thing. But I haven\u2019t found any good solutions yet!\n\n**Solutions that are almost good, but not quite:**\n\n* Just use external USB drives, or USB adapters for the drives.\n   * It\u2019s not a bad idea. Using USB drives usually inherently means easy attach / detach. Works well on Windows, and Linux desktops. Synology (or Xpenology) handles them well too!\n   * However most other NAS software doesn\u2019t handle USB drives any different than SATA or SAS drives. You still need to *manually* mount or dismount the drive.\n   * Synology / Xpenology works well with external USB drives though! The problem is that I would prefer SAS / SATA.\n* Don't remove the drives, and spin them down when not used.\n   * Also a good idea. It\u2019s cold storage after all, sort of. Almost.\n   * The problem here is that I specifically want to remove the unused disks.\n* Use Synology / Xpenology, which supports hot-swapping SATA disks.\n   * Don\u2019t use any raid. Swap the disks whenever I like.\n   * This could actually work. But does it really work like that? Has anyone used Synology / Xpenology like this? Moving single disks in and out frequently?\n* Use TrueNAS, Unraid, OpenMediaVault, or whatever, and mount and unmount the disk manually every time.\n   * Not good enough. Manually doing everything every time is a hassle. Creating and stopping the shares too etc.\n* Use OpenMediaVault or TrueNAS, and create scripts for the things I want.\n   * Scripts for automatically mounting the drives and creating the shares.\n   * Since OpenMediaVault is based on Debian, it may be easy to do this.\n   * I haven\u2019t looked into this. It might work. But I would prefer out of the box solutions. One, because I\u2019m lazy, two because custom solutions can easily break with an update.\n\nDoes anyone have any better ideas?\n\nHas anyone tried to do something like this? How did you solve it? What did you end up doing?\n\nThanks for reading.", "author_fullname": "t2_hthgcyj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cold Storage with SAS / SATA Hot-Swap", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ojbq3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703263839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have been thinking about this, but this is one of those situations, where I don\u2019t even know what to search for. I would appreciate any ideas, thoughts, or comments.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The goal:&lt;/strong&gt; Have a server with hot-swap capable SAS / SATA bays. Find a NAS operating system that allows &lt;em&gt;easy, automatic&lt;/em&gt; mounting and dismounting of single disks plugged into those bays, and sharing the files on them. I.e. the disks would be &lt;em&gt;single disks, not in a raid array&lt;/em&gt;. Filesystem is not important. Can be ext4, btrfs, zfs, whatever.&lt;/p&gt;\n\n&lt;p&gt;I know this sounds like a \u201cwhy is this even a question\u201d type of thing. But I haven\u2019t found any good solutions yet!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solutions that are almost good, but not quite:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Just use external USB drives, or USB adapters for the drives.\n\n&lt;ul&gt;\n&lt;li&gt;It\u2019s not a bad idea. Using USB drives usually inherently means easy attach / detach. Works well on Windows, and Linux desktops. Synology (or Xpenology) handles them well too!&lt;/li&gt;\n&lt;li&gt;However most other NAS software doesn\u2019t handle USB drives any different than SATA or SAS drives. You still need to &lt;em&gt;manually&lt;/em&gt; mount or dismount the drive.&lt;/li&gt;\n&lt;li&gt;Synology / Xpenology works well with external USB drives though! The problem is that I would prefer SAS / SATA.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Don&amp;#39;t remove the drives, and spin them down when not used.\n\n&lt;ul&gt;\n&lt;li&gt;Also a good idea. It\u2019s cold storage after all, sort of. Almost.&lt;/li&gt;\n&lt;li&gt;The problem here is that I specifically want to remove the unused disks.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Use Synology / Xpenology, which supports hot-swapping SATA disks.\n\n&lt;ul&gt;\n&lt;li&gt;Don\u2019t use any raid. Swap the disks whenever I like.&lt;/li&gt;\n&lt;li&gt;This could actually work. But does it really work like that? Has anyone used Synology / Xpenology like this? Moving single disks in and out frequently?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Use TrueNAS, Unraid, OpenMediaVault, or whatever, and mount and unmount the disk manually every time.\n\n&lt;ul&gt;\n&lt;li&gt;Not good enough. Manually doing everything every time is a hassle. Creating and stopping the shares too etc.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Use OpenMediaVault or TrueNAS, and create scripts for the things I want.\n\n&lt;ul&gt;\n&lt;li&gt;Scripts for automatically mounting the drives and creating the shares.&lt;/li&gt;\n&lt;li&gt;Since OpenMediaVault is based on Debian, it may be easy to do this.&lt;/li&gt;\n&lt;li&gt;I haven\u2019t looked into this. It might work. But I would prefer out of the box solutions. One, because I\u2019m lazy, two because custom solutions can easily break with an update.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Does anyone have any better ideas?&lt;/p&gt;\n\n&lt;p&gt;Has anyone tried to do something like this? How did you solve it? What did you end up doing?&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ojbq3", "is_robot_indexable": true, "report_reasons": null, "author": "AbsurdMedia", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ojbq3/cold_storage_with_sas_sata_hotswap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ojbq3/cold_storage_with_sas_sata_hotswap/", "subreddit_subscribers": 719986, "created_utc": 1703263839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With drives getting up around 20TB or more, running a surface scan on the entire thing at once can take days. Is there a surface scan (preferably freeware) that lets you specify the start and end sectors you want to scan, so you can do it in chunks? Thanks.", "author_fullname": "t2_11tqbw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD surface scan that lets you select a region?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p26o3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703324003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With drives getting up around 20TB or more, running a surface scan on the entire thing at once can take days. Is there a surface scan (preferably freeware) that lets you specify the start and end sectors you want to scan, so you can do it in chunks? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p26o3", "is_robot_indexable": true, "report_reasons": null, "author": "alleyoopoop", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p26o3/hdd_surface_scan_that_lets_you_select_a_region/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p26o3/hdd_surface_scan_that_lets_you_select_a_region/", "subreddit_subscribers": 719986, "created_utc": 1703324003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to decide what to use to backup my data. I have a two 10TB hard drives in a Sabrent 3.5\" SATA Hard Drive Docking Station connected to a dell R620 server. Which has Proxmox installed running three VM's. A media server(emby), file server, Minecraft server(just family) and web-server. \n\nI add all this details just in case it helps. I am looking to backup movies (.mkv or .mp4) without losing quality when I need to retrieve this data. Also have .md, .txt, .cr3, .jpg and so on. Basically I have lot of different file types I don't think that matters much but maybe it does.  \n\nNeeds\n\n* easy retrieval of data even if the software i use stopped working or longer available for some reason. I can still get my data\n* easy to use would be nice\n* open source is nice but not required\n* protocols like FTP, SSH, WebDAV are good to have\n\nRight now none of my backups are going on any cloud server(trust issues LOL) and cost", "author_fullname": "t2_2tcxls6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deciding what to use to backup data/hard drive? Borg vs Duplicati", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p136k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703319161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to decide what to use to backup my data. I have a two 10TB hard drives in a Sabrent 3.5&amp;quot; SATA Hard Drive Docking Station connected to a dell R620 server. Which has Proxmox installed running three VM&amp;#39;s. A media server(emby), file server, Minecraft server(just family) and web-server. &lt;/p&gt;\n\n&lt;p&gt;I add all this details just in case it helps. I am looking to backup movies (.mkv or .mp4) without losing quality when I need to retrieve this data. Also have .md, .txt, .cr3, .jpg and so on. Basically I have lot of different file types I don&amp;#39;t think that matters much but maybe it does.  &lt;/p&gt;\n\n&lt;p&gt;Needs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;easy retrieval of data even if the software i use stopped working or longer available for some reason. I can still get my data&lt;/li&gt;\n&lt;li&gt;easy to use would be nice&lt;/li&gt;\n&lt;li&gt;open source is nice but not required&lt;/li&gt;\n&lt;li&gt;protocols like FTP, SSH, WebDAV are good to have&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now none of my backups are going on any cloud server(trust issues LOL) and cost&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p136k", "is_robot_indexable": true, "report_reasons": null, "author": "1michaelbrown", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p136k/deciding_what_to_use_to_backup_datahard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p136k/deciding_what_to_use_to_backup_datahard_drive/", "subreddit_subscribers": 719986, "created_utc": 1703319161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello everyone,\n\nI currently have over 200k photos and approximately 400GB of videos stored on Prime Photo/Drive. I'm eager to create a local backup of all these files. However, every method I've come across suggests manual downloads limited to a maximum of 5GB or 1k photos at a time. Aside from the significant effort involved, it seems prone to errors when manually downloading countless batches of files. Is there any way to download everything in a batch?\n\nI'm open to licensing software or renting an S3 server, among other options, to make this process smoother. With the impending end of \"Drive,\" I'm concerned that even more options might disappear, making the project even more challenging.\n\nThanks a lot for any assistance!", "author_fullname": "t2_iz00e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice: Efficient Way to Download 200k+ Photos and 400GB Videos from Prime Photo/Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18orbjo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703285883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I currently have over 200k photos and approximately 400GB of videos stored on Prime Photo/Drive. I&amp;#39;m eager to create a local backup of all these files. However, every method I&amp;#39;ve come across suggests manual downloads limited to a maximum of 5GB or 1k photos at a time. Aside from the significant effort involved, it seems prone to errors when manually downloading countless batches of files. Is there any way to download everything in a batch?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to licensing software or renting an S3 server, among other options, to make this process smoother. With the impending end of &amp;quot;Drive,&amp;quot; I&amp;#39;m concerned that even more options might disappear, making the project even more challenging.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for any assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18orbjo", "is_robot_indexable": true, "report_reasons": null, "author": "TSchiwek", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18orbjo/seeking_advice_efficient_way_to_download_200k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18orbjo/seeking_advice_efficient_way_to_download_200k/", "subreddit_subscribers": 719986, "created_utc": 1703285883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI bought Manufacturer Re-certified seagate drives from serverpartsdeal and when I tried checking the warranty on the drives on the seagate website a message appears that states \"Please Contact Place of Purchase.\"\n\nHow would I check the warranty on the drives like so many of you have done previously. Am I doing something wrong.", "author_fullname": "t2_174va3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unable to check warranty claim on serverpartsdeal hdds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oog2i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703277798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I bought Manufacturer Re-certified seagate drives from serverpartsdeal and when I tried checking the warranty on the drives on the seagate website a message appears that states &amp;quot;Please Contact Place of Purchase.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;How would I check the warranty on the drives like so many of you have done previously. Am I doing something wrong.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oog2i", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway49671", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oog2i/unable_to_check_warranty_claim_on_serverpartsdeal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oog2i/unable_to_check_warranty_claim_on_serverpartsdeal/", "subreddit_subscribers": 719986, "created_utc": 1703277798.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, I have been hoarding since ten years but I have used only external hard drives up to now. I lost data once, one drive was probably faulty and started making some screeching noises before. Now another external drive, after just two years, is having a ticking noise, and this is making me very anxious (I want to point out that I have just moved stuff once inside it, I know that moving back and forth damages drives but it's weird when it happens so soon). So, here I am, askingfor your help. I have a spare CPU and GPU (i3-6100 and GTX 1050 TI) but I still need a motherboard, a CPU fan, a PSU, RAM, a case and, of course, a drive. \n\n&amp;#x200B;\n\nThis is the drive I am interested in [https://www.amazon.it/Seagate-Interna-Enterprise-Hyperscale-ST16000NM000J/dp/B08JV7N6J1/ref=mp\\_s\\_a\\_1\\_1?crid=BE8EVKNLJNBL&amp;keywords=hard+disk+interno+16+tb&amp;qid=1703277027&amp;sprefix=hard+disk+interno+16+tb%2Caps%2C164&amp;sr=8-1](https://www.amazon.it/Seagate-Interna-Enterprise-Hyperscale-ST16000NM000J/dp/B08JV7N6J1/ref=mp_s_a_1_1?crid=BE8EVKNLJNBL&amp;keywords=hard+disk+interno+16+tb&amp;qid=1703277027&amp;sprefix=hard+disk+interno+16+tb%2Caps%2C164&amp;sr=8-1) Do you recommend it?\n\n&amp;#x200B;\n\nAnd about the other pieces, what do you suggest? Like, what PSU, and how many W? How much RAM?\n\n&amp;#x200B;\n\nWhat about the OS? Will Win10 work? My goal is to have this thing next to my PC and connect them with ethernet cable so I can access the data anytime.\n\n&amp;#x200B;\n\nThank you and sorry for the long post.\n\n&amp;#x200B;\n\nBy the way, are external drives trash or I am just unlucky?", "author_fullname": "t2_9sbr4jft", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help setting my first local storage \"server\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oo9h8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703277287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have been hoarding since ten years but I have used only external hard drives up to now. I lost data once, one drive was probably faulty and started making some screeching noises before. Now another external drive, after just two years, is having a ticking noise, and this is making me very anxious (I want to point out that I have just moved stuff once inside it, I know that moving back and forth damages drives but it&amp;#39;s weird when it happens so soon). So, here I am, askingfor your help. I have a spare CPU and GPU (i3-6100 and GTX 1050 TI) but I still need a motherboard, a CPU fan, a PSU, RAM, a case and, of course, a drive. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This is the drive I am interested in &lt;a href=\"https://www.amazon.it/Seagate-Interna-Enterprise-Hyperscale-ST16000NM000J/dp/B08JV7N6J1/ref=mp_s_a_1_1?crid=BE8EVKNLJNBL&amp;amp;keywords=hard+disk+interno+16+tb&amp;amp;qid=1703277027&amp;amp;sprefix=hard+disk+interno+16+tb%2Caps%2C164&amp;amp;sr=8-1\"&gt;https://www.amazon.it/Seagate-Interna-Enterprise-Hyperscale-ST16000NM000J/dp/B08JV7N6J1/ref=mp_s_a_1_1?crid=BE8EVKNLJNBL&amp;amp;keywords=hard+disk+interno+16+tb&amp;amp;qid=1703277027&amp;amp;sprefix=hard+disk+interno+16+tb%2Caps%2C164&amp;amp;sr=8-1&lt;/a&gt; Do you recommend it?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;And about the other pieces, what do you suggest? Like, what PSU, and how many W? How much RAM?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What about the OS? Will Win10 work? My goal is to have this thing next to my PC and connect them with ethernet cable so I can access the data anytime.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you and sorry for the long post.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;By the way, are external drives trash or I am just unlucky?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oo9h8", "is_robot_indexable": true, "report_reasons": null, "author": "Nepusona", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oo9h8/i_need_help_setting_my_first_local_storage_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oo9h8/i_need_help_setting_my_first_local_storage_server/", "subreddit_subscribers": 719986, "created_utc": 1703277287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there any available at a reasonable price? I\u2019m only finding a few older ones that want retail or more.", "author_fullname": "t2_ky5n5eca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cases w hdd cages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ohtz9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703259888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any available at a reasonable price? I\u2019m only finding a few older ones that want retail or more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ohtz9", "is_robot_indexable": true, "report_reasons": null, "author": "NoRepresentative5684", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ohtz9/cases_w_hdd_cages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ohtz9/cases_w_hdd_cages/", "subreddit_subscribers": 719986, "created_utc": 1703259888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am a member of a Telegram group that has over 150.000 files on it. I want to download all of them like I done on the others like save history and check files but there isn't any option to do that.\n\nWhat to do it?\n\nThank you in advance!", "author_fullname": "t2_6ljvypjch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download a lot of file from a Telegram group that has save history and files option disabled?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p0kvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703316951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a member of a Telegram group that has over 150.000 files on it. I want to download all of them like I done on the others like save history and check files but there isn&amp;#39;t any option to do that.&lt;/p&gt;\n\n&lt;p&gt;What to do it?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p0kvg", "is_robot_indexable": true, "report_reasons": null, "author": "SaseCaiFrumosi", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p0kvg/how_to_download_a_lot_of_file_from_a_telegram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p0kvg/how_to_download_a_lot_of_file_from_a_telegram/", "subreddit_subscribers": 719986, "created_utc": 1703316951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks,\n\nI'm trying to archive some local wrestling shows with yt-dlp and i'm running into an issue on Fite.tv.\n\nI've managed to scrape for the m3u8 links, and when pulling them into yt-dlp i'm getting a 403 error, which made me think I need to use my login credentials. So, I use this;\n    \n     yt-dlp --cookies cookies.txt -u username@gmail.com -p verysecurepassword69420 https://cdn-cf.fite.tv/fite-videos/feed_items/4543427/hls/4000/chunks.m3u8        \n\nThat doesn't work, so I then try another m3u8 that's scrapable;\n\n    yt-dlp --cookies cookies.txt -u username@gmail.com -p verysecurepassword69420 https://cdn-cf.fite.tv/fite-videos/feed_items/4543427/hls/playlist.m3u8?vsid=AN9hwkVTw-O420I&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tY2YuZml0ZS50di9maXRlLXZpZGVvcy9mZWVkX2l0ZW1zLzQ1NDM0MjcvaGxzLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MDMzMTQ3MDR9fX1dfQ__&amp;Signature=UDyjIi9mWfYaFU6aSBSnefjTcK0rwUy4uN8wJMqsRCotiS2xP2GcnE0IbjOF9I0Pn0EOzv0FERVVczT-CRO20fopMWFisVufPC036XMzsseSzMlqh8aLyGa14ohIMPVIHSa4OWjx60KhtqtUi2w-~u1COj69KUQHgwhOx2mq7phqe~Q1TOcWfpsjy~ebkCHLUPc0Nd6Vfaq~d40a2JUxX8oM-VG-RdiHILXj9JM01pmMEURNcfcrYyu4JG8IYXuiykKkW5zx7tJqZPsmpPBOqVbCgW6MmRWbc8CCKAFfhcQPpfJ5~Aq1~-RNVSi5FMhvHVwFP3sPzuzSVWZ0fR8tHw__&amp;Key-Pair-Id=APKAINQZDXTRMHG47U5A&amp;embed_site=fite   \n\nWhich gives me this error;\n    \n    &gt;       ERROR: [generic] Unable to download webpage: HTTP Error 403: Forbidden (caused by &lt;HTTPError 403: Forbidden&gt;); please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n    &gt;       'Policy' is not recognized as an internal or external command,\n    operable program or batch file.\n    &gt;       'Signature' is not recognized as an internal or external command,\n    operable program or batch file.\n    &gt;       'Key-Pair-Id' is not recognized as an internal or external command,\n    operable program or batch file. \n    &gt;     'embed_site' is not recognized as an internal or external command,\n    operable program or batch file.\n\nKind of stuck here, any ideas?\n\nedit:\n\nyt-dlp --cookies cookies.txt -u username@email.com -p password \"very long playlist.m3u8 url\" works just a treat.", "author_fullname": "t2_53vmy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "yt-dlp and fite.tv", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p08gg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703331064.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703315465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to archive some local wrestling shows with yt-dlp and i&amp;#39;m running into an issue on Fite.tv.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve managed to scrape for the m3u8 links, and when pulling them into yt-dlp i&amp;#39;m getting a 403 error, which made me think I need to use my login credentials. So, I use this;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; yt-dlp --cookies cookies.txt -u username@gmail.com -p verysecurepassword69420 https://cdn-cf.fite.tv/fite-videos/feed_items/4543427/hls/4000/chunks.m3u8        \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;That doesn&amp;#39;t work, so I then try another m3u8 that&amp;#39;s scrapable;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;yt-dlp --cookies cookies.txt -u username@gmail.com -p verysecurepassword69420 https://cdn-cf.fite.tv/fite-videos/feed_items/4543427/hls/playlist.m3u8?vsid=AN9hwkVTw-O420I&amp;amp;Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tY2YuZml0ZS50di9maXRlLXZpZGVvcy9mZWVkX2l0ZW1zLzQ1NDM0MjcvaGxzLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MDMzMTQ3MDR9fX1dfQ__&amp;amp;Signature=UDyjIi9mWfYaFU6aSBSnefjTcK0rwUy4uN8wJMqsRCotiS2xP2GcnE0IbjOF9I0Pn0EOzv0FERVVczT-CRO20fopMWFisVufPC036XMzsseSzMlqh8aLyGa14ohIMPVIHSa4OWjx60KhtqtUi2w-~u1COj69KUQHgwhOx2mq7phqe~Q1TOcWfpsjy~ebkCHLUPc0Nd6Vfaq~d40a2JUxX8oM-VG-RdiHILXj9JM01pmMEURNcfcrYyu4JG8IYXuiykKkW5zx7tJqZPsmpPBOqVbCgW6MmRWbc8CCKAFfhcQPpfJ5~Aq1~-RNVSi5FMhvHVwFP3sPzuzSVWZ0fR8tHw__&amp;amp;Key-Pair-Id=APKAINQZDXTRMHG47U5A&amp;amp;embed_site=fite   \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Which gives me this error;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;gt;       ERROR: [generic] Unable to download webpage: HTTP Error 403: Forbidden (caused by &amp;lt;HTTPError 403: Forbidden&amp;gt;); please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n&amp;gt;       &amp;#39;Policy&amp;#39; is not recognized as an internal or external command,\noperable program or batch file.\n&amp;gt;       &amp;#39;Signature&amp;#39; is not recognized as an internal or external command,\noperable program or batch file.\n&amp;gt;       &amp;#39;Key-Pair-Id&amp;#39; is not recognized as an internal or external command,\noperable program or batch file. \n&amp;gt;     &amp;#39;embed_site&amp;#39; is not recognized as an internal or external command,\noperable program or batch file.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Kind of stuck here, any ideas?&lt;/p&gt;\n\n&lt;p&gt;edit:&lt;/p&gt;\n\n&lt;p&gt;yt-dlp --cookies cookies.txt -u &lt;a href=\"mailto:username@email.com\"&gt;username@email.com&lt;/a&gt; -p password &amp;quot;very long playlist.m3u8 url&amp;quot; works just a treat.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "64TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p08gg", "is_robot_indexable": true, "report_reasons": null, "author": "jaredpaik", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18p08gg/ytdlp_and_fitetv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p08gg/ytdlp_and_fitetv/", "subreddit_subscribers": 719986, "created_utc": 1703315465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just got done changing from SHR-1 to SHR-2 and restored from backup. I am getting 2 different folder sizes for 1 of my folders - I probably have more, but this is first one I have found so far. Screenshot here - [https://i.imgur.com/LhcpHg9.png](https://i.imgur.com/LhcpHg9.png)\n\nI have a mirror profile using SyncBack, and SyncBack says source and destination are the same. Both Finder and File Manager in the Synology report about a 870GB difference, with the backup being larger. These are all movies, so I searched for .mkv files thinking I would find a piece count discrepancy, and there is not. The remainder of the small files is metadata for the movies, jpgs, nfo, etc. \n\nThe source is a Synolgy 1821+ in SHR-2 and the Backup is a WD EX2Ultra using JBOD. Any ideas why there is a such a discrepancy or can someone recommend a different software that can dig deeper on finding the discrepancy since Syncback doesn't see one? ", "author_fullname": "t2_9u63n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discrepancy in size between source and backup after restore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oy1zy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703307331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got done changing from SHR-1 to SHR-2 and restored from backup. I am getting 2 different folder sizes for 1 of my folders - I probably have more, but this is first one I have found so far. Screenshot here - &lt;a href=\"https://i.imgur.com/LhcpHg9.png\"&gt;https://i.imgur.com/LhcpHg9.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a mirror profile using SyncBack, and SyncBack says source and destination are the same. Both Finder and File Manager in the Synology report about a 870GB difference, with the backup being larger. These are all movies, so I searched for .mkv files thinking I would find a piece count discrepancy, and there is not. The remainder of the small files is metadata for the movies, jpgs, nfo, etc. &lt;/p&gt;\n\n&lt;p&gt;The source is a Synolgy 1821+ in SHR-2 and the Backup is a WD EX2Ultra using JBOD. Any ideas why there is a such a discrepancy or can someone recommend a different software that can dig deeper on finding the discrepancy since Syncback doesn&amp;#39;t see one? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i9sKQ2xo1-OBeZMRfnhjmxmTZ5ViSUQ40myHLA3b8FY.png?auto=webp&amp;s=f94f8f9d92202d986abb0c4b11249aebf3d78305", "width": 1052, "height": 594}, "resolutions": [{"url": "https://external-preview.redd.it/i9sKQ2xo1-OBeZMRfnhjmxmTZ5ViSUQ40myHLA3b8FY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9064ae902a823248cc979ee767a3dad0b1f22b4", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/i9sKQ2xo1-OBeZMRfnhjmxmTZ5ViSUQ40myHLA3b8FY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6fe1b23fabf6518cfdaf28df5f3e8153028832b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/i9sKQ2xo1-OBeZMRfnhjmxmTZ5ViSUQ40myHLA3b8FY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=49eec140cbeb2a2b50005167284b75eb242435f5", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/i9sKQ2xo1-OBeZMRfnhjmxmTZ5ViSUQ40myHLA3b8FY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fce3ab35e5aedfaff28ba4f8c3b53d8c4e98333d", "width": 640, "height": 361}, {"url": "https://external-preview.redd.it/i9sKQ2xo1-OBeZMRfnhjmxmTZ5ViSUQ40myHLA3b8FY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d3bdc5ca703279ff0ea4d9d26685f13441a3586", "width": 960, "height": 542}], "variants": {}, "id": "d7zqBrxlcg4OQMN5b_kP4OcYDjDCmqyRHjnYr7TynSY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oy1zy", "is_robot_indexable": true, "report_reasons": null, "author": "bee_ryan", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oy1zy/discrepancy_in_size_between_source_and_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oy1zy/discrepancy_in_size_between_source_and_backup/", "subreddit_subscribers": 719986, "created_utc": 1703307331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am cloning from 1 tb to a new 2tb drive (OG drive has 100gb left so that\u2019s why it takes 7 hours to complete) \n\nSo when 2tb drive completed, I made the rookie mistake of clicking fix drive errors and now the drive won\u2019t boot which okay \n\nBut now I am doing the process again which maybe less than 6 hours but how much of the life span am I killing by doing this again?", "author_fullname": "t2_2xvjbaio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rookie mistake or not so bad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ow0a0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703301597.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703300260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am cloning from 1 tb to a new 2tb drive (OG drive has 100gb left so that\u2019s why it takes 7 hours to complete) &lt;/p&gt;\n\n&lt;p&gt;So when 2tb drive completed, I made the rookie mistake of clicking fix drive errors and now the drive won\u2019t boot which okay &lt;/p&gt;\n\n&lt;p&gt;But now I am doing the process again which maybe less than 6 hours but how much of the life span am I killing by doing this again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ow0a0", "is_robot_indexable": true, "report_reasons": null, "author": "theguy_win", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ow0a0/rookie_mistake_or_not_so_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ow0a0/rookie_mistake_or_not_so_bad/", "subreddit_subscribers": 719986, "created_utc": 1703300260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have some files on an external drive formatted in NTFS. I primarily use linux now, so I'm open to the idea of switching it to ext4, but so far NTFS is working great so I don't know if I should bother, but will cshatag work? Or maybe I should read more about winshatag?", "author_fullname": "t2_2amctjzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does cshatag work with NTFS drives in linux?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18otirg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703292318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some files on an external drive formatted in NTFS. I primarily use linux now, so I&amp;#39;m open to the idea of switching it to ext4, but so far NTFS is working great so I don&amp;#39;t know if I should bother, but will cshatag work? Or maybe I should read more about winshatag?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18otirg", "is_robot_indexable": true, "report_reasons": null, "author": "forgotmypasswordsad", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18otirg/does_cshatag_work_with_ntfs_drives_in_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18otirg/does_cshatag_work_with_ntfs_drives_in_linux/", "subreddit_subscribers": 719986, "created_utc": 1703292318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! Does anybody know if this hard drive is cmr or smr? My guess is that its cmr but i cant seem to find anything on it\n\nSeagate ST12000NM0127: [https://www.ebay.com/itm/166349036307](https://www.ebay.com/itm/166349036307) ", "author_fullname": "t2_9xg2y7zd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate ST12000NM0127", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18osbcl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703288736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Does anybody know if this hard drive is cmr or smr? My guess is that its cmr but i cant seem to find anything on it&lt;/p&gt;\n\n&lt;p&gt;Seagate ST12000NM0127: &lt;a href=\"https://www.ebay.com/itm/166349036307\"&gt;https://www.ebay.com/itm/166349036307&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6HnqxRJQXGxxyiPT3L8mipBOkvPYpeYLq0JZF6ZyB20.jpg?auto=webp&amp;s=513ce8eb8c3da03224bda2c4a3c21af4b60522d7", "width": 400, "height": 304}, "resolutions": [{"url": "https://external-preview.redd.it/6HnqxRJQXGxxyiPT3L8mipBOkvPYpeYLq0JZF6ZyB20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5ba4c53e63a69b6f9b0341c6b2702a11c47e1bc", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/6HnqxRJQXGxxyiPT3L8mipBOkvPYpeYLq0JZF6ZyB20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd157f52e63e8822d6fc4aa4f21ed1fe62459ffe", "width": 216, "height": 164}, {"url": "https://external-preview.redd.it/6HnqxRJQXGxxyiPT3L8mipBOkvPYpeYLq0JZF6ZyB20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b9239655569c1f2b0fe89bf332c52bfd76b0ec9", "width": 320, "height": 243}], "variants": {}, "id": "W2vfxe7AXBCXumaAxqUEqBdCRt1ZNjRsFDVWIhhq2Wk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18osbcl", "is_robot_indexable": true, "report_reasons": null, "author": "IntelligentDesign776", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18osbcl/seagate_st12000nm0127/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18osbcl/seagate_st12000nm0127/", "subreddit_subscribers": 719986, "created_utc": 1703288736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know many of us are archivers and uploading to archive.org and similar sites.\n\nAnyone archiving on blockchain maybe images and screenshots?\n\nI guess NFTs can be lost but what about memo fields? Basically I'm asking is there a way to archive on blockchain so the content will never disappear?", "author_fullname": "t2_366izsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone archiving images on blockchain", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oyic4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703308928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know many of us are archivers and uploading to archive.org and similar sites.&lt;/p&gt;\n\n&lt;p&gt;Anyone archiving on blockchain maybe images and screenshots?&lt;/p&gt;\n\n&lt;p&gt;I guess NFTs can be lost but what about memo fields? Basically I&amp;#39;m asking is there a way to archive on blockchain so the content will never disappear?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oyic4", "is_robot_indexable": true, "report_reasons": null, "author": "gowithflow192", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oyic4/anyone_archiving_images_on_blockchain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oyic4/anyone_archiving_images_on_blockchain/", "subreddit_subscribers": 719986, "created_utc": 1703308928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, first of all, is that normal? I know an 18TB won't have **all** 18TB of usable storage, but I didn't know if it would be *that* low. \n\nAnd then second, this external was an open box from ebay. It was actually sealed and the device itself was still shrink-wrapped, so I'm inclined to believe this is unused (but of course there is the possibility it was used at some point). Is there anything I *should* do before loading it up with stuff?", "author_fullname": "t2_13d0ah", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "18TB external showing as 16.3TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ophfm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.31, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703280708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, first of all, is that normal? I know an 18TB won&amp;#39;t have &lt;strong&gt;all&lt;/strong&gt; 18TB of usable storage, but I didn&amp;#39;t know if it would be &lt;em&gt;that&lt;/em&gt; low. &lt;/p&gt;\n\n&lt;p&gt;And then second, this external was an open box from ebay. It was actually sealed and the device itself was still shrink-wrapped, so I&amp;#39;m inclined to believe this is unused (but of course there is the possibility it was used at some point). Is there anything I &lt;em&gt;should&lt;/em&gt; do before loading it up with stuff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ophfm", "is_robot_indexable": true, "report_reasons": null, "author": "quit_smoking1", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ophfm/18tb_external_showing_as_163tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ophfm/18tb_external_showing_as_163tb/", "subreddit_subscribers": 719986, "created_utc": 1703280708.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}