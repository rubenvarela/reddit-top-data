{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious to know how or even if you can learn DE concepts by reviewing project code bases?\n\nIf I were to find some project on GitHub and read through their documentation and (hopefully) well-commented code, would that help in the process of learning DE concepts?\n\nI'd like to create some projects myself, but I only know the basics of programming, and although I'll likely use GPT to help me through the whole process (please don't shoot me), my problem is finding real world use cases that can be \"projectized\" by simulating the scenario and building my own solution.\n\nSo, my intention is to find projects on GH so I can get a better grasp of how others are approaching similar situations. \n\nTL;DR: What's your views on learning DE by reviewing GH projects, and what are recommended best practices?", "author_fullname": "t2_3djr756e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learn DE by reviewing public projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p68zx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703339966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious to know how or even if you can learn DE concepts by reviewing project code bases?&lt;/p&gt;\n\n&lt;p&gt;If I were to find some project on GitHub and read through their documentation and (hopefully) well-commented code, would that help in the process of learning DE concepts?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to create some projects myself, but I only know the basics of programming, and although I&amp;#39;ll likely use GPT to help me through the whole process (please don&amp;#39;t shoot me), my problem is finding real world use cases that can be &amp;quot;projectized&amp;quot; by simulating the scenario and building my own solution.&lt;/p&gt;\n\n&lt;p&gt;So, my intention is to find projects on GH so I can get a better grasp of how others are approaching similar situations. &lt;/p&gt;\n\n&lt;p&gt;TL;DR: What&amp;#39;s your views on learning DE by reviewing GH projects, and what are recommended best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18p68zx", "is_robot_indexable": true, "report_reasons": null, "author": "Ablueblaze", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18p68zx/learn_de_by_reviewing_public_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18p68zx/learn_de_by_reviewing_public_projects/", "subreddit_subscribers": 147926, "created_utc": 1703339966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm learning about data lakehouses. Technologies like delta lake or iceberg allow to efficiently store and query tables as files in data lakes. However, their schemas seem to be rather fixed like in SQL databases.\n\nAre there also lakehouse technologies using flexible table formats like in noSQL database such as MongoDB?", "author_fullname": "t2_8o0lv2nz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse with flexible, noSQL-like schema?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p27jq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703324113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m learning about data lakehouses. Technologies like delta lake or iceberg allow to efficiently store and query tables as files in data lakes. However, their schemas seem to be rather fixed like in SQL databases.&lt;/p&gt;\n\n&lt;p&gt;Are there also lakehouse technologies using flexible table formats like in noSQL database such as MongoDB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18p27jq", "is_robot_indexable": true, "report_reasons": null, "author": "gebbissimo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18p27jq/lakehouse_with_flexible_nosqllike_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18p27jq/lakehouse_with_flexible_nosqllike_schema/", "subreddit_subscribers": 147926, "created_utc": 1703324113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning Airflow and I am having a hard time wrapping my head around what Airflow actually is?\n\nIll give you a very simple example that gets straight to my question, Imagine a DAG with one operator. A SqlToS3Operator which runs a select query on postgres database and inserts the result into an s3 bucket.\n\nNow, does the data flow from Postgres DB directly to my S3 Bucket? (In that case ill call it an orchestration tool) \n\nOr does it flow first to my airflow server and then to my S3 bucket? (In that case ill call it an ETL tool)\n\nCant find it in the docs how things get done under the hood.\n\nThanks,", "author_fullname": "t2_8bw9894u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow, Orchestration tool or ETL tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p5xvn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703338937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning Airflow and I am having a hard time wrapping my head around what Airflow actually is?&lt;/p&gt;\n\n&lt;p&gt;Ill give you a very simple example that gets straight to my question, Imagine a DAG with one operator. A SqlToS3Operator which runs a select query on postgres database and inserts the result into an s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;Now, does the data flow from Postgres DB directly to my S3 Bucket? (In that case ill call it an orchestration tool) &lt;/p&gt;\n\n&lt;p&gt;Or does it flow first to my airflow server and then to my S3 bucket? (In that case ill call it an ETL tool)&lt;/p&gt;\n\n&lt;p&gt;Cant find it in the docs how things get done under the hood.&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18p5xvn", "is_robot_indexable": true, "report_reasons": null, "author": "GameFitAverage", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18p5xvn/airflow_orchestration_tool_or_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18p5xvn/airflow_orchestration_tool_or_etl_tool/", "subreddit_subscribers": 147926, "created_utc": 1703338937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For a personal project/business idea, I'm working with the League of Legends API to create a database of players an their in-game match history.\n\nFor the pipeline itself, it creates/upserts on 3 different tables:\n\n1. player\\_account\\_info\n   1. stores account level info like profile picture, level, in-game name, and ID's used to make calls to other endpoints\n2. player\\_ranked\\_info\n   1. stores info about ranked status, like division, tier, LP, etc\n3. player\\_matches\n   1. stores info about the actual matches played. Each match typically has 10 records (1 for each player) and \\~300 columns of in-game stats\n\nThe thing that I'm unsure of about the design is that I have created this pipeline/DAG as a single class called \\`LeaguePipeline\\`. You supply the player's name and region, and then call a wrapper function called \\`process\\_user()\\`. Below is an example \\`main()\\` function that I use to run the program\n\n    def main():\n        pipeline = LeaguePipeline(\n        \triot_name='Annie Bot', \n        \triot_tagline='NA1',\n        \tregion='na1',\n        \tlog_level='DEBUG'\n        )\n        pipeline.MATCH_LIMIT = 20\n    \n        pipeline.process_user()\n        pipeline.database.conn.close()\n\nHere is what the \\`process\\_user()\\` function looks like\n\n    def process_user(self) -&gt; None:\n    \n        if self.played_already_registered():\n            self.update = True\n    \t\t\n        # for the accounts table\n        self.get_summoner_info()\n        # for the ranked table\n        self.get_ranked_info()\n    \t\t\n        # perform ETL on the match history\n        if not isinstance(self.generate_dataframes(), int):\n            self.clean_match_df()\n    \tself.clean_objectives_df()\n    \tself.export_data()\n    \n    \tlogger.success('Finished processing user')\n    \n        return None\n\n# The Problem\n\nThe one thing I'm having doubts about it its heavy reliance on class/instance variable. I'm trying to learn Airflow/Dagster, but running into issues when trying to apply it to this project. Airflow uses XComms to communicate between tasks, but my OOP pipeline uses so many variables that it would be hell to convert all of them explicitly. My problem with Dagster is similar, as it uses mainly functional programming. I would have to convert all the instance variables into regular python global variables, and all the methods into regular functions.\n\n# Is the way I've developed my pipeline \"wrong,\" are Airflow/Dagster \"too rigid,\" or is my pipeline just not adapted for these orchestrators?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Critique my data pipeline (personal project). Are Airflow/Dagster incompatible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ox7au", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703304338.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For a personal project/business idea, I&amp;#39;m working with the League of Legends API to create a database of players an their in-game match history.&lt;/p&gt;\n\n&lt;p&gt;For the pipeline itself, it creates/upserts on 3 different tables:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;player_account_info\n\n&lt;ol&gt;\n&lt;li&gt;stores account level info like profile picture, level, in-game name, and ID&amp;#39;s used to make calls to other endpoints&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;player_ranked_info\n\n&lt;ol&gt;\n&lt;li&gt;stores info about ranked status, like division, tier, LP, etc&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;player_matches\n\n&lt;ol&gt;\n&lt;li&gt;stores info about the actual matches played. Each match typically has 10 records (1 for each player) and ~300 columns of in-game stats&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The thing that I&amp;#39;m unsure of about the design is that I have created this pipeline/DAG as a single class called `LeaguePipeline`. You supply the player&amp;#39;s name and region, and then call a wrapper function called `process_user()`. Below is an example `main()` function that I use to run the program&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def main():\n    pipeline = LeaguePipeline(\n        riot_name=&amp;#39;Annie Bot&amp;#39;, \n        riot_tagline=&amp;#39;NA1&amp;#39;,\n        region=&amp;#39;na1&amp;#39;,\n        log_level=&amp;#39;DEBUG&amp;#39;\n    )\n    pipeline.MATCH_LIMIT = 20\n\n    pipeline.process_user()\n    pipeline.database.conn.close()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Here is what the `process_user()` function looks like&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def process_user(self) -&amp;gt; None:\n\n    if self.played_already_registered():\n        self.update = True\n\n    # for the accounts table\n    self.get_summoner_info()\n    # for the ranked table\n    self.get_ranked_info()\n\n    # perform ETL on the match history\n    if not isinstance(self.generate_dataframes(), int):\n        self.clean_match_df()\n    self.clean_objectives_df()\n    self.export_data()\n\n    logger.success(&amp;#39;Finished processing user&amp;#39;)\n\n    return None\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;The Problem&lt;/h1&gt;\n\n&lt;p&gt;The one thing I&amp;#39;m having doubts about it its heavy reliance on class/instance variable. I&amp;#39;m trying to learn Airflow/Dagster, but running into issues when trying to apply it to this project. Airflow uses XComms to communicate between tasks, but my OOP pipeline uses so many variables that it would be hell to convert all of them explicitly. My problem with Dagster is similar, as it uses mainly functional programming. I would have to convert all the instance variables into regular python global variables, and all the methods into regular functions.&lt;/p&gt;\n\n&lt;h1&gt;Is the way I&amp;#39;ve developed my pipeline &amp;quot;wrong,&amp;quot; are Airflow/Dagster &amp;quot;too rigid,&amp;quot; or is my pipeline just not adapted for these orchestrators?&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ox7au", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ox7au/critique_my_data_pipeline_personal_project_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ox7au/critique_my_data_pipeline_personal_project_are/", "subreddit_subscribers": 147926, "created_utc": 1703304338.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nThe work I currently do is more aligned with Analytics Engineer + light DevOps work than software engineer work.  The stack I work in is snowflake, dbt, databricks, and some cloud infra with terraform.\n\nI\u2019m thinking longer-term I\u2019d like to get more technical and pivot to a more software engineering focused team such as \u201cSoftware Engineer - Data\u201d or \u201cData Platform\u201d at a different company.\n\nWhat kind of skills should I drill down on over the next 6-12 months?", "author_fullname": "t2_nexbbb26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting onto a data platforming team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p0idb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703316659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;The work I currently do is more aligned with Analytics Engineer + light DevOps work than software engineer work.  The stack I work in is snowflake, dbt, databricks, and some cloud infra with terraform.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking longer-term I\u2019d like to get more technical and pivot to a more software engineering focused team such as \u201cSoftware Engineer - Data\u201d or \u201cData Platform\u201d at a different company.&lt;/p&gt;\n\n&lt;p&gt;What kind of skills should I drill down on over the next 6-12 months?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18p0idb", "is_robot_indexable": true, "report_reasons": null, "author": "keep_it_professional", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18p0idb/getting_onto_a_data_platforming_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18p0idb/getting_onto_a_data_platforming_team/", "subreddit_subscribers": 147926, "created_utc": 1703316659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like the title says, I\u2019m curious how others are managing their development and production deployments of Meltano (specifically for its EL, not T utility). I\u2019ve generally used managed services for things like EL before, so I\u2019d like to get a feel for how others are handling this.\n\nAs an extra thought, I\u2019m also looking to add data versioning as a feature for our org down the line, and that seems like it will heavily influence how I handle the production deployment of Meltano.", "author_fullname": "t2_15uzwf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meltano: How do you handle CI/CD and devops on your team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18orom6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703286915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says, I\u2019m curious how others are managing their development and production deployments of Meltano (specifically for its EL, not T utility). I\u2019ve generally used managed services for things like EL before, so I\u2019d like to get a feel for how others are handling this.&lt;/p&gt;\n\n&lt;p&gt;As an extra thought, I\u2019m also looking to add data versioning as a feature for our org down the line, and that seems like it will heavily influence how I handle the production deployment of Meltano.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18orom6", "is_robot_indexable": true, "report_reasons": null, "author": "EngiNerd9000", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18orom6/meltano_how_do_you_handle_cicd_and_devops_on_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18orom6/meltano_how_do_you_handle_cicd_and_devops_on_your/", "subreddit_subscribers": 147926, "created_utc": 1703286915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used aws batch for orchestration?  Have you managed it out of a ci/cd?  Curious to know how others have set this up.", "author_fullname": "t2_5gzu4ur4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Aws batch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18or4ny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703285353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used aws batch for orchestration?  Have you managed it out of a ci/cd?  Curious to know how others have set this up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18or4ny", "is_robot_indexable": true, "report_reasons": null, "author": "bluezebra42", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18or4ny/aws_batch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18or4ny/aws_batch/", "subreddit_subscribers": 147926, "created_utc": 1703285353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nWe have a built a data warehouse for our business analytics purposes, I need some help to optimise few things.\n\nOur metrics initially are stored in S3(partitioned by year/month/day/hour), the files are in csv format, we then run glue crawlers every hour to keep partition details updated. \n\nRedshift spectrum is then used to query this data from redshift. However this was slow for our end users as the data is huge (in range of 6-7 petabytes and increasing).\n\nSo we started aggregating data using aggregation queries in redshift(basically we run hourly scheduled group by sql queries over multiple columns and store the aggregated metrics and discard raw S3 metrics), all of this orchestrated using step funtions. We were able to achieve 90% compression.\n\nThe problem: We also need to run percentile aggregations as part of this process. So, instead of querying raw data, sort and get percentile for combinations of columns, we aggregate metrics for percentiles over some columns(~20 columns are present in each metric).\nThe percentile queries however are very slow, they take 20~hrs each and completly blocks other aggregation queries. So, two problems, its a cascading effect and I can't run all percentile queries, and other problem is that these queries also block normal hourly aggregation queries.\n\nAs we use provisioned redshift cluster, the cost is constant over month, what other approach can i use keeping cost to minimal, use emr? or spin up a hugh end redshift cluster which juat processes percentile queries?\n\nAslo, i found that even one percentile query blocks other queries as it's taking up cpu and network and disk io.", "author_fullname": "t2_4vwvhe5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large data SQL aggregations in Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oyrvy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703309861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;We have a built a data warehouse for our business analytics purposes, I need some help to optimise few things.&lt;/p&gt;\n\n&lt;p&gt;Our metrics initially are stored in S3(partitioned by year/month/day/hour), the files are in csv format, we then run glue crawlers every hour to keep partition details updated. &lt;/p&gt;\n\n&lt;p&gt;Redshift spectrum is then used to query this data from redshift. However this was slow for our end users as the data is huge (in range of 6-7 petabytes and increasing).&lt;/p&gt;\n\n&lt;p&gt;So we started aggregating data using aggregation queries in redshift(basically we run hourly scheduled group by sql queries over multiple columns and store the aggregated metrics and discard raw S3 metrics), all of this orchestrated using step funtions. We were able to achieve 90% compression.&lt;/p&gt;\n\n&lt;p&gt;The problem: We also need to run percentile aggregations as part of this process. So, instead of querying raw data, sort and get percentile for combinations of columns, we aggregate metrics for percentiles over some columns(~20 columns are present in each metric).\nThe percentile queries however are very slow, they take 20~hrs each and completly blocks other aggregation queries. So, two problems, its a cascading effect and I can&amp;#39;t run all percentile queries, and other problem is that these queries also block normal hourly aggregation queries.&lt;/p&gt;\n\n&lt;p&gt;As we use provisioned redshift cluster, the cost is constant over month, what other approach can i use keeping cost to minimal, use emr? or spin up a hugh end redshift cluster which juat processes percentile queries?&lt;/p&gt;\n\n&lt;p&gt;Aslo, i found that even one percentile query blocks other queries as it&amp;#39;s taking up cpu and network and disk io.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oyrvy", "is_robot_indexable": true, "report_reasons": null, "author": "_dEnOmInAtOr", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oyrvy/large_data_sql_aggregations_in_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oyrvy/large_data_sql_aggregations_in_redshift/", "subreddit_subscribers": 147926, "created_utc": 1703309861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I'm currently working as a data engineer and I'm interested in applying for a master's degree in data engineering in Europe or online, as I currently hold a bachelor's degree in commerce. I've been having trouble finding suitable data engineering master's programs; most are focused on data science or business analytics. I would greatly appreciate any suggestions or help regarding this matter.", "author_fullname": "t2_dm09aec1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering master degree in Europe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18pdzfi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703362466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m currently working as a data engineer and I&amp;#39;m interested in applying for a master&amp;#39;s degree in data engineering in Europe or online, as I currently hold a bachelor&amp;#39;s degree in commerce. I&amp;#39;ve been having trouble finding suitable data engineering master&amp;#39;s programs; most are focused on data science or business analytics. I would greatly appreciate any suggestions or help regarding this matter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18pdzfi", "is_robot_indexable": true, "report_reasons": null, "author": "Mountain-Luck7673", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18pdzfi/data_engineering_master_degree_in_europe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18pdzfi/data_engineering_master_degree_in_europe/", "subreddit_subscribers": 147926, "created_utc": 1703362466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have this use case where my boss is insisting using Alembic but I don't get the use of it for this. Has anyone used Alembic for this any tips?", "author_fullname": "t2_4c9picnu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone tried using Alembic to migrate data from RDS to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p2krc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703325765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have this use case where my boss is insisting using Alembic but I don&amp;#39;t get the use of it for this. Has anyone used Alembic for this any tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18p2krc", "is_robot_indexable": true, "report_reasons": null, "author": "Gora_HabshiYoYo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18p2krc/has_anyone_tried_using_alembic_to_migrate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18p2krc/has_anyone_tried_using_alembic_to_migrate_data/", "subreddit_subscribers": 147926, "created_utc": 1703325765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're planning to start an ETL project that involves many business logic as well based on our existing java application.\nSo should I create ETL pipelines with Talend and integrate with my application or create the pipeline in java in my application directly?\nWe have mostly Java developers and some Talend developers in other teams.\nMy main concern is that does Talend offer quick development and also does it offer the level of control that code development offers like exception handling, complex logic, etc.", "author_fullname": "t2_5bhequsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I use Talend or native software for a new project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p127w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703319042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re planning to start an ETL project that involves many business logic as well based on our existing java application.\nSo should I create ETL pipelines with Talend and integrate with my application or create the pipeline in java in my application directly?\nWe have mostly Java developers and some Talend developers in other teams.\nMy main concern is that does Talend offer quick development and also does it offer the level of control that code development offers like exception handling, complex logic, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18p127w", "is_robot_indexable": true, "report_reasons": null, "author": "jackie_119", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18p127w/should_i_use_talend_or_native_software_for_a_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18p127w/should_i_use_talend_or_native_software_for_a_new/", "subreddit_subscribers": 147926, "created_utc": 1703319042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm pretty sure that cloud functions can be triggered by pubsub events, but I'm not sure about cloud run. Afaik cloud functions are basically the same as cloud run (since v2?) they just abstract the dockerfile part. \n\nBut, basically, I'm wondering if cloud run jobs can be started from a pub sub event, if not why not? I guess I don't understand the underlying reason (unless it's just \"Google says no\").\n\nEg - object added to bucket, cloud run job starts up and processes the object. I know this can be done with a cloud function - but this is partly academic/curiosity for me (hence asking).", "author_fullname": "t2_4atn0tqg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting a cloud run job from a pub sub message in gcp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oth5f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703292176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pretty sure that cloud functions can be triggered by pubsub events, but I&amp;#39;m not sure about cloud run. Afaik cloud functions are basically the same as cloud run (since v2?) they just abstract the dockerfile part. &lt;/p&gt;\n\n&lt;p&gt;But, basically, I&amp;#39;m wondering if cloud run jobs can be started from a pub sub event, if not why not? I guess I don&amp;#39;t understand the underlying reason (unless it&amp;#39;s just &amp;quot;Google says no&amp;quot;).&lt;/p&gt;\n\n&lt;p&gt;Eg - object added to bucket, cloud run job starts up and processes the object. I know this can be done with a cloud function - but this is partly academic/curiosity for me (hence asking).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oth5f", "is_robot_indexable": true, "report_reasons": null, "author": "onmyphoneinlondon", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oth5f/starting_a_cloud_run_job_from_a_pub_sub_message/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oth5f/starting_a_cloud_run_job_from_a_pub_sub_message/", "subreddit_subscribers": 147926, "created_utc": 1703292176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a new release of our beta product for making it easier to use Rook and Ceph for data storage. Anyone can use this for free. We are happy to help you with the setup.\n\n[https://koor.tech/blog/2023/release\\_0\\_5\\_2/](https://koor.tech/blog/2023/release_0_5_2/)", "author_fullname": "t2_d321jhgdp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Koor Data Control Center - release 0.5.2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18pbtvw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703356297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a new release of our beta product for making it easier to use Rook and Ceph for data storage. Anyone can use this for free. We are happy to help you with the setup.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://koor.tech/blog/2023/release_0_5_2/\"&gt;https://koor.tech/blog/2023/release_0_5_2/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uezLE7OJJ5kuWVoyIDDYBLdHmUi4S-yDHW3RBnJSv0A.jpg?auto=webp&amp;s=9d2513955ee2b43169481b1d797f291f1f898238", "width": 1000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/uezLE7OJJ5kuWVoyIDDYBLdHmUi4S-yDHW3RBnJSv0A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d19d0a93dec90233b236061f8dc6ad5eedd4f0f7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/uezLE7OJJ5kuWVoyIDDYBLdHmUi4S-yDHW3RBnJSv0A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=320e9c2acee2be3d777773eac459f81aa07dd435", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/uezLE7OJJ5kuWVoyIDDYBLdHmUi4S-yDHW3RBnJSv0A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=346a10d9ad2551c941a42fa09df51183d6c68453", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/uezLE7OJJ5kuWVoyIDDYBLdHmUi4S-yDHW3RBnJSv0A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f5c1213e2db9b9d983ba5404ad88bf7b937a862", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/uezLE7OJJ5kuWVoyIDDYBLdHmUi4S-yDHW3RBnJSv0A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=81bc7e653cce9d4d816221dcbe9cfd43f0c5771b", "width": 960, "height": 960}], "variants": {}, "id": "otOfF_O1h3ZTGFfnZqS9EMMC1nB0gPNVAriMp7PsGWI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18pbtvw", "is_robot_indexable": true, "report_reasons": null, "author": "Dave-at-Koor", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18pbtvw/koor_data_control_center_release_052/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18pbtvw/koor_data_control_center_release_052/", "subreddit_subscribers": 147926, "created_utc": 1703356297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to create a public API as a source in Airbyte. The API has to be called 4 times to get the actual data. I have Airbyte setup in local using Docker. I created a custom connector and added initial stream. How do I create remaining streams as continuous streams of previous one to get the actual data ?\n\nHow can I achieve this?", "author_fullname": "t2_5dxvcityu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-step API source custom connector for Airbyte", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p1q5m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703321985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to create a public API as a source in Airbyte. The API has to be called 4 times to get the actual data. I have Airbyte setup in local using Docker. I created a custom connector and added initial stream. How do I create remaining streams as continuous streams of previous one to get the actual data ?&lt;/p&gt;\n\n&lt;p&gt;How can I achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18p1q5m", "is_robot_indexable": true, "report_reasons": null, "author": "Money_Maker_69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18p1q5m/multistep_api_source_custom_connector_for_airbyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18p1q5m/multistep_api_source_custom_connector_for_airbyte/", "subreddit_subscribers": 147926, "created_utc": 1703321985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know of any schools out there teaching Airflow? I've seen online training and USC has this 1 course -- [https://careers.usc.edu/classes/apache-airflow-essential-training/](https://careers.usc.edu/classes/apache-airflow-essential-training/)", "author_fullname": "t2_csqyrhw6p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any schools out there teaching Apache Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18or28c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703285161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of any schools out there teaching Airflow? I&amp;#39;ve seen online training and USC has this 1 course -- &lt;a href=\"https://careers.usc.edu/classes/apache-airflow-essential-training/\"&gt;https://careers.usc.edu/classes/apache-airflow-essential-training/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/r1jWZxgcwSQgxBYvl2xzAORAbT9UoQBZtO6OpvprrTM.jpg?auto=webp&amp;s=36d921c513ed886fd9033d528b30938dfe2269bf", "width": 512, "height": 288}, "resolutions": [{"url": "https://external-preview.redd.it/r1jWZxgcwSQgxBYvl2xzAORAbT9UoQBZtO6OpvprrTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0f04831f7c73fa94b851fc98bef86e590c78b0c", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/r1jWZxgcwSQgxBYvl2xzAORAbT9UoQBZtO6OpvprrTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb5710a83cadae5daecacbac88d05a73fa9241e8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/r1jWZxgcwSQgxBYvl2xzAORAbT9UoQBZtO6OpvprrTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d67cb83452ba9f26c616c0c624bfa6ee3968b358", "width": 320, "height": 180}], "variants": {}, "id": "mxvc6Jr0jf7Sm-AZ1vTsNDVUA7l2uNS8fT7SF32ibPA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18or28c", "is_robot_indexable": true, "report_reasons": null, "author": "AirFlordan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18or28c/are_there_any_schools_out_there_teaching_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18or28c/are_there_any_schools_out_there_teaching_apache/", "subreddit_subscribers": 147926, "created_utc": 1703285161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to copy files from an sftp server to ADLS via databricks and subsequently copy only new files from the sftp server to ADLS. I copied the file into a temp dbfs folder but I can't write it into ADLS. \n\nImport pysftp\n\n    files = sftp.listdir(\"/home/folder/my_files\")\ndbfs_path = 'my_temp_folder'\nos.makedirs(dbfs_path, exist_ok=True)\n\nFor f in files:\n    remote_file_path=\"home/folder/my_files\"\nLocal_file = dbfs_path + \"/\"  + f\nsftp.get(remote_file_path, Local_file)", "author_fullname": "t2_35f1rdk6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying files via Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oq8lf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703282812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to copy files from an sftp server to ADLS via databricks and subsequently copy only new files from the sftp server to ADLS. I copied the file into a temp dbfs folder but I can&amp;#39;t write it into ADLS. &lt;/p&gt;\n\n&lt;p&gt;Import pysftp&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;files = sftp.listdir(&amp;quot;/home/folder/my_files&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;dbfs_path = &amp;#39;my_temp_folder&amp;#39;\nos.makedirs(dbfs_path, exist_ok=True)&lt;/p&gt;\n\n&lt;p&gt;For f in files:\n    remote_file_path=&amp;quot;home/folder/my_files&amp;quot;\nLocal_file = dbfs_path + &amp;quot;/&amp;quot;  + f\nsftp.get(remote_file_path, Local_file)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18oq8lf", "is_robot_indexable": true, "report_reasons": null, "author": "Rogie_88", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18oq8lf/copying_files_via_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18oq8lf/copying_files_via_databricks/", "subreddit_subscribers": 147926, "created_utc": 1703282812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have use case where I would like create a DLT pipeline from existing source Delta table. To run this pipeline incrementally, would like to read CDC from the source delta table. Documentation I found uses file system as a source for triggering DLT pipeline.\n\nThanks in advance!", "author_fullname": "t2_8gmrp70f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Delta Live Tables pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18pciap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703358231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have use case where I would like create a DLT pipeline from existing source Delta table. To run this pipeline incrementally, would like to read CDC from the source delta table. Documentation I found uses file system as a source for triggering DLT pipeline.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18pciap", "is_robot_indexable": true, "report_reasons": null, "author": "Confident_Opening331", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18pciap/databricks_delta_live_tables_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18pciap/databricks_delta_live_tables_pipeline/", "subreddit_subscribers": 147926, "created_utc": 1703358231.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}