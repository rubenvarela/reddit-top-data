{"kind": "Listing", "data": {"after": "t3_18p4f0d", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_v2a9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do You Test If An EEPROM Can Hold Data For 100 Years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_18p18dz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JPf_BN0C8u-wAgTs5Uahp_3FBsQX6GOjSXVLdp07JS4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703319803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hackaday.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hackaday.com/2023/12/21/how-do-you-test-if-an-eeprom-can-hold-data-for-100-years/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?auto=webp&amp;s=6b166e25037e706ec1549aa3e8726c2311c397ed", "width": 3000, "height": 1815}, "resolutions": [{"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb0ece5921121e85bdde63d8da765dece806c328", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7dc81177220f6d70f11787425c0c47c2ac4e6652", "width": 216, "height": 130}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba67ee587c1e5ed3c838ded2258b3a6a8c8294ca", "width": 320, "height": 193}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30ecfa35fe2c0e0c4de929a878419ac1bc8ba208", "width": 640, "height": 387}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a087138ca695fc95c5511c52209204401306c9d4", "width": 960, "height": 580}, {"url": "https://external-preview.redd.it/m0IgQy2_7CGpYk568yuRzmn_28Xo512tq_xpgpyY-8c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9434cc84ef02babaef568148af20263c93378576", "width": 1080, "height": 653}], "variants": {}, "id": "GufPJHoAEjOVkrn9vS8H26Lg5I8XMnIpEsBI-W8p7OQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p18dz", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy-Red-Fox", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p18dz/how_do_you_test_if_an_eeprom_can_hold_data_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hackaday.com/2023/12/21/how-do-you-test-if-an-eeprom-can-hold-data-for-100-years/", "subreddit_subscribers": 720062, "created_utc": 1703319803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a brand new 2TB Samsung S7 SDD which I purchased to transfer a folders that contain around 200GB of data.   \n\nWhen I try to copy a whole folder across I'll get \"interuppted action\" and a message that my hard drive needs around 11GB or so. However, when I copy the subfolders individually in batches it's fine.   \n\nAny idea?\n\nEdit: is there any chance you need space on your original drive to cache the folder as it moves to your external ssd? My if I\u2019m copying over from a drive that\u2019s full perhaps that\u2019s the issue? ", "author_fullname": "t2_ey91h2hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung S7 ssd - can't do transfer folders larger than ~ 10GB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p6vqy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703348198.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703341954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a brand new 2TB Samsung S7 SDD which I purchased to transfer a folders that contain around 200GB of data.   &lt;/p&gt;\n\n&lt;p&gt;When I try to copy a whole folder across I&amp;#39;ll get &amp;quot;interuppted action&amp;quot; and a message that my hard drive needs around 11GB or so. However, when I copy the subfolders individually in batches it&amp;#39;s fine.   &lt;/p&gt;\n\n&lt;p&gt;Any idea?&lt;/p&gt;\n\n&lt;p&gt;Edit: is there any chance you need space on your original drive to cache the folder as it moves to your external ssd? My if I\u2019m copying over from a drive that\u2019s full perhaps that\u2019s the issue? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p6vqy", "is_robot_indexable": true, "report_reasons": null, "author": "Royal_Difficulty_678", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p6vqy/samsung_s7_ssd_cant_do_transfer_folders_larger/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p6vqy/samsung_s7_ssd_cant_do_transfer_folders_larger/", "subreddit_subscribers": 720062, "created_utc": 1703341954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So my current setup includes a RPi4 NAS with an external hard drive and a separate computer that handles my docker containers and serves out Plex.\n\nI\u2019ve heard the term DAS thrown around and as far as I can tell it\u2019s basically just a bunch of disks behaving like an external hard drive, is that more or less correct? If I attached a DAS to the computer with my containers (which is also serving files on the network) wouldn\u2019t it essentially just be a NAS?\n\nIs there a disadvantage to a DAS in this context? Does it have lower read/write speeds than if the disks were all in the server itself?", "author_fullname": "t2_1gyiwuuv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS vs DAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oum2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703295723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my current setup includes a RPi4 NAS with an external hard drive and a separate computer that handles my docker containers and serves out Plex.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve heard the term DAS thrown around and as far as I can tell it\u2019s basically just a bunch of disks behaving like an external hard drive, is that more or less correct? If I attached a DAS to the computer with my containers (which is also serving files on the network) wouldn\u2019t it essentially just be a NAS?&lt;/p&gt;\n\n&lt;p&gt;Is there a disadvantage to a DAS in this context? Does it have lower read/write speeds than if the disks were all in the server itself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oum2m", "is_robot_indexable": true, "report_reasons": null, "author": "trianglesteve", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oum2m/nas_vs_das/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oum2m/nas_vs_das/", "subreddit_subscribers": 720062, "created_utc": 1703295723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Background for context:\n\nMy father passed away and had several different drives and backups. It was a bit of a \"rat's nest\" of data, with some drives having backups of the same information in general but not necessarily identical. I have since consolidated it all onto one USB drive. I used dupeguru to clear out all of the duplicate files, which was great for getting over 200k files down to 43k. The next challenge is trying to get what remains consolidated.\n\nFor example, he may have a \"2003 Photos\" folder in multiple places:  \nD:\\\\Photos\\\\2003 Photos  \nD:\\\\Backup\\\\2003 Photos  \nD:\\\\Home\\\\Photos\\\\2003 Photos  \n\u2026 and for whatever reason there are different/unique files in each location, no duplicates.\n\nIs there a relatively simple way to merge these folders automatically? Preferably for Windows. I'm fine with having to iterate the process as directories merge upward, but cannot realistically comb through it manually as currently there are still 3300 folders on the drive.", "author_fullname": "t2_7gj6me", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merging Duplicate Folders", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18osuwq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703290292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background for context:&lt;/p&gt;\n\n&lt;p&gt;My father passed away and had several different drives and backups. It was a bit of a &amp;quot;rat&amp;#39;s nest&amp;quot; of data, with some drives having backups of the same information in general but not necessarily identical. I have since consolidated it all onto one USB drive. I used dupeguru to clear out all of the duplicate files, which was great for getting over 200k files down to 43k. The next challenge is trying to get what remains consolidated.&lt;/p&gt;\n\n&lt;p&gt;For example, he may have a &amp;quot;2003 Photos&amp;quot; folder in multiple places:&lt;br/&gt;\nD:\\Photos\\2003 Photos&lt;br/&gt;\nD:\\Backup\\2003 Photos&lt;br/&gt;\nD:\\Home\\Photos\\2003 Photos&lt;br/&gt;\n\u2026 and for whatever reason there are different/unique files in each location, no duplicates.&lt;/p&gt;\n\n&lt;p&gt;Is there a relatively simple way to merge these folders automatically? Preferably for Windows. I&amp;#39;m fine with having to iterate the process as directories merge upward, but cannot realistically comb through it manually as currently there are still 3300 folders on the drive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18osuwq", "is_robot_indexable": true, "report_reasons": null, "author": "JoshMcMadMac", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18osuwq/merging_duplicate_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18osuwq/merging_duplicate_folders/", "subreddit_subscribers": 720062, "created_utc": 1703290292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Was wondering if there is a plex-like solution that let's you host your own ebooks (pdf and or epubs) and serves it easily on most devices. A bonus would be if it could also serve it or push it to an e-reader (Kobo, Kindle or any other available e-reader)?", "author_fullname": "t2_4cg63eip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Question] Ebooks with a plex-like experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p7t4d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703344793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was wondering if there is a plex-like solution that let&amp;#39;s you host your own ebooks (pdf and or epubs) and serves it easily on most devices. A bonus would be if it could also serve it or push it to an e-reader (Kobo, Kindle or any other available e-reader)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p7t4d", "is_robot_indexable": true, "report_reasons": null, "author": "TheBlackKey2000", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p7t4d/question_ebooks_with_a_plexlike_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p7t4d/question_ebooks_with_a_plexlike_experience/", "subreddit_subscribers": 720062, "created_utc": 1703344793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a media server running Windows 10 Pro. i7-7700k, MSI Z270 board, 32gb ram, 5 x 16tb drives running in storage spaces with parity. I\u2019m going to be upgrading to Windows Server 2022 soon to break through the 63tb limit with Win10 Pro. I started to change my cluster size from 16kb to 64kb to prepare for the change. I read some good reviews on EaseUS software and started the cluster size change with it. It doesn\u2019t show a timeframe. Only shows a percentage complete. It\u2019s been on 95% for over a day. I can see the process is still running in task manager. My question is has anyone here had experience with changing cluster sizes with data retention and what kind of time frame should I expect? I\u2019ve heard mixed answers around the web. I wouldn\u2019t expect it to be done in a day or even two. I\u2019d expect at least a week for a collection of that size. Thanks in advance.", "author_fullname": "t2_25eskjyz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Timeframe to change cluster size on 63tb array Windows 10", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18oxgf3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703305845.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703305217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a media server running Windows 10 Pro. i7-7700k, MSI Z270 board, 32gb ram, 5 x 16tb drives running in storage spaces with parity. I\u2019m going to be upgrading to Windows Server 2022 soon to break through the 63tb limit with Win10 Pro. I started to change my cluster size from 16kb to 64kb to prepare for the change. I read some good reviews on EaseUS software and started the cluster size change with it. It doesn\u2019t show a timeframe. Only shows a percentage complete. It\u2019s been on 95% for over a day. I can see the process is still running in task manager. My question is has anyone here had experience with changing cluster sizes with data retention and what kind of time frame should I expect? I\u2019ve heard mixed answers around the web. I wouldn\u2019t expect it to be done in a day or even two. I\u2019d expect at least a week for a collection of that size. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18oxgf3", "is_robot_indexable": true, "report_reasons": null, "author": "SpcPewPew", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18oxgf3/timeframe_to_change_cluster_size_on_63tb_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18oxgf3/timeframe_to_change_cluster_size_on_63tb_array/", "subreddit_subscribers": 720062, "created_utc": 1703305217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nHey DataHoarder! \ud83d\udc4b \nI\u2019m looking for some insights on Seagate hard drives, specifically the Exos and Exos Enterprise series. Can anyone shed light on the differences between these two lines?\n\nAlso, I'm curious about the [various X generations](https://www.seagate.com/de/de/products/enterprise-drives/exos-x/) within these series and which one would be the best fit for data backup in a small business setup. Any recommendations or experiences to share? Thanks in advance! \n\nJulez", "author_fullname": "t2_5pnii6pu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Clarity: Difference Between Exos and Exos Enterprise, Understanding X Generation for Small Business Data Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ow5su", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703300762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DataHoarder! \ud83d\udc4b \nI\u2019m looking for some insights on Seagate hard drives, specifically the Exos and Exos Enterprise series. Can anyone shed light on the differences between these two lines?&lt;/p&gt;\n\n&lt;p&gt;Also, I&amp;#39;m curious about the &lt;a href=\"https://www.seagate.com/de/de/products/enterprise-drives/exos-x/\"&gt;various X generations&lt;/a&gt; within these series and which one would be the best fit for data backup in a small business setup. Any recommendations or experiences to share? Thanks in advance! &lt;/p&gt;\n\n&lt;p&gt;Julez&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?auto=webp&amp;s=b98f58f088fdf8fbeb225a485466816520892a66", "width": 1440, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=800aeac928e4d6917cdf3db161d37c173da92af5", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a9549b2c27b55813f6941439e0246cac408ff1d", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=708b52a8cc3161e776c554b1f4a488231d8bfce6", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=93eb129bea186541cdd32a88a0cb2d619f9a954c", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ad5e3bcc25ba7cdf8dd591fb8e247b839fe5aa7", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b52b8bf108499e8a6d3bd58f428c8f644af3085", "width": 1080, "height": 675}], "variants": {}, "id": "GJ9K1o7VNo6J4JXg3IrZBPizWfgWXRc6b6FmSaP5cNc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ow5su", "is_robot_indexable": true, "report_reasons": null, "author": "juIez_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ow5su/seeking_clarity_difference_between_exos_and_exos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ow5su/seeking_clarity_difference_between_exos_and_exos/", "subreddit_subscribers": 720062, "created_utc": 1703300762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello everyone,\n\nI currently have over 200k photos and approximately 400GB of videos stored on Prime Photo/Drive. I'm eager to create a local backup of all these files. However, every method I've come across suggests manual downloads limited to a maximum of 5GB or 1k photos at a time. Aside from the significant effort involved, it seems prone to errors when manually downloading countless batches of files. Is there any way to download everything in a batch?\n\nI'm open to licensing software or renting an S3 server, among other options, to make this process smoother. With the impending end of \"Drive,\" I'm concerned that even more options might disappear, making the project even more challenging.\n\nThanks a lot for any assistance!", "author_fullname": "t2_iz00e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice: Efficient Way to Download 200k+ Photos and 400GB Videos from Prime Photo/Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18orbjo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703285883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I currently have over 200k photos and approximately 400GB of videos stored on Prime Photo/Drive. I&amp;#39;m eager to create a local backup of all these files. However, every method I&amp;#39;ve come across suggests manual downloads limited to a maximum of 5GB or 1k photos at a time. Aside from the significant effort involved, it seems prone to errors when manually downloading countless batches of files. Is there any way to download everything in a batch?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to licensing software or renting an S3 server, among other options, to make this process smoother. With the impending end of &amp;quot;Drive,&amp;quot; I&amp;#39;m concerned that even more options might disappear, making the project even more challenging.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for any assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18orbjo", "is_robot_indexable": true, "report_reasons": null, "author": "TSchiwek", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18orbjo/seeking_advice_efficient_way_to_download_200k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18orbjo/seeking_advice_efficient_way_to_download_200k/", "subreddit_subscribers": 720062, "created_utc": 1703285883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys!\n\nI'm looking to see if you guys here know a good software or something that will help automatically backup a certain folder on my pc to an external hard-drive, scheduled like once a week or something. \n\nJust looking to backup my personal art projects to a hard drive im receiving for xmas. Thank you", "author_fullname": "t2_w480d9p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically Back Up A Specific Folder to External Hard-drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18pevo2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703365060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to see if you guys here know a good software or something that will help automatically backup a certain folder on my pc to an external hard-drive, scheduled like once a week or something. &lt;/p&gt;\n\n&lt;p&gt;Just looking to backup my personal art projects to a hard drive im receiving for xmas. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18pevo2", "is_robot_indexable": true, "report_reasons": null, "author": "xamiaxo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18pevo2/automatically_back_up_a_specific_folder_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18pevo2/automatically_back_up_a_specific_folder_to/", "subreddit_subscribers": 720062, "created_utc": 1703365060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Starting to get low on hdd space. I'm thinking about buying a used 14tb drive from FB. Price is little lower than I would expect so maybe someone is selling it because it's about to die. I know there are software out t here that can scan bad sectors and what not but the few free ones I've found sound like it takes hours if not days to run and complete. Is there any software that can do a quick scan vs hours or days?..\n\n&amp;#x200B;", "author_fullname": "t2_3t7jqq72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External hdd scan software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18pbz49", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703356710.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Starting to get low on hdd space. I&amp;#39;m thinking about buying a used 14tb drive from FB. Price is little lower than I would expect so maybe someone is selling it because it&amp;#39;s about to die. I know there are software out t here that can scan bad sectors and what not but the few free ones I&amp;#39;ve found sound like it takes hours if not days to run and complete. Is there any software that can do a quick scan vs hours or days?..&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18pbz49", "is_robot_indexable": true, "report_reasons": null, "author": "xracerboy66", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18pbz49/external_hdd_scan_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18pbz49/external_hdd_scan_software/", "subreddit_subscribers": 720062, "created_utc": 1703356710.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to decide what to use to backup my data. I have a two 10TB hard drives in a Sabrent 3.5\" SATA Hard Drive Docking Station connected to a dell R620 server. Which has Proxmox installed running three VM's. A media server(emby), file server, Minecraft server(just family) and web-server. \n\nI add all this details just in case it helps. I am looking to backup movies (.mkv or .mp4) without losing quality when I need to retrieve this data. Also have .md, .txt, .cr3, .jpg and so on. Basically I have lot of different file types I don't think that matters much but maybe it does.  \n\nNeeds\n\n* easy retrieval of data even if the software i use stopped working or longer available for some reason. I can still get my data\n* easy to use would be nice\n* open source is nice but not required\n* protocols like FTP, SSH, WebDAV are good to have\n\nRight now none of my backups are going on any cloud server(trust issues LOL) and cost", "author_fullname": "t2_2tcxls6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deciding what to use to backup data/hard drive? Borg vs Duplicati", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p136k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703319161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to decide what to use to backup my data. I have a two 10TB hard drives in a Sabrent 3.5&amp;quot; SATA Hard Drive Docking Station connected to a dell R620 server. Which has Proxmox installed running three VM&amp;#39;s. A media server(emby), file server, Minecraft server(just family) and web-server. &lt;/p&gt;\n\n&lt;p&gt;I add all this details just in case it helps. I am looking to backup movies (.mkv or .mp4) without losing quality when I need to retrieve this data. Also have .md, .txt, .cr3, .jpg and so on. Basically I have lot of different file types I don&amp;#39;t think that matters much but maybe it does.  &lt;/p&gt;\n\n&lt;p&gt;Needs&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;easy retrieval of data even if the software i use stopped working or longer available for some reason. I can still get my data&lt;/li&gt;\n&lt;li&gt;easy to use would be nice&lt;/li&gt;\n&lt;li&gt;open source is nice but not required&lt;/li&gt;\n&lt;li&gt;protocols like FTP, SSH, WebDAV are good to have&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now none of my backups are going on any cloud server(trust issues LOL) and cost&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p136k", "is_robot_indexable": true, "report_reasons": null, "author": "1michaelbrown", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p136k/deciding_what_to_use_to_backup_datahard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p136k/deciding_what_to_use_to_backup_datahard_drive/", "subreddit_subscribers": 720062, "created_utc": 1703319161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a DS220j with a 4TB WD Red (WD40EFAX) and I'm looking for a new HDD.\n\nI've recently discovered SMR mess and am wondering which HDD I should buy.\n\nNow I haven't any redundancy (Synology Hybrid RAID without data protection) and I'm only doing backups and I cannot use any types of filesystems but ext4.\n\nSo I need your help: in your opinion, what should I buy?  \n", "author_fullname": "t2_4qv7px88", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DS220j HDD Upgrade", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18pbf3k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703355126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a DS220j with a 4TB WD Red (WD40EFAX) and I&amp;#39;m looking for a new HDD.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently discovered SMR mess and am wondering which HDD I should buy.&lt;/p&gt;\n\n&lt;p&gt;Now I haven&amp;#39;t any redundancy (Synology Hybrid RAID without data protection) and I&amp;#39;m only doing backups and I cannot use any types of filesystems but ext4.&lt;/p&gt;\n\n&lt;p&gt;So I need your help: in your opinion, what should I buy?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18pbf3k", "is_robot_indexable": true, "report_reasons": null, "author": "Theviki20110", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18pbf3k/ds220j_hdd_upgrade/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18pbf3k/ds220j_hdd_upgrade/", "subreddit_subscribers": 720062, "created_utc": 1703355126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So TL:DR, is there a way to add a power source or convert my 5TB portable drive so I can use it without worry?\n\nI got a 5TB removable hard drive about a year ago and at the time I saw it as a great buy. Get some movies on it, put my steam games on it, because primarily I use my laptop since I'm not at my desktop for very long due to work and family. When I started it up, I loaded it with a bunch of games and noticed it was freezing a lot. Eventually I found out that it was drawing so much power that unless it's the only thing plugged in, the other peripherals and such of my computer will cause it to blink in and out, along with everything in a USB port.\n\nSo I instead started storing my movies and shows on it, because with that and my headphones, it seemed to work perfectly. FF to yesterday and I was putting more shows and movies on it when it started doing the same, even without much else going on in terms of power. I got worried because It's only half full of movies and shows right now and I didn't want to lose all that.\n\nA buddy of mine told me it may just need external power, and I would love to convert it if at all possible, but I cannot for the life of me find how to convert it, I see a lot of converting internal storage to external, but nothing like what my friend described, and he can't even find it either. \n\nFood for thought: I have tried different USB ports, with usb 3.0 causing the least amount of issues, my laptop does have a usb-c port but I'm not sure if that would be the best solution or not. It's a 5TB seagate that came with a USB 3 cable and that's it. \n\nAny and all help is appreciated, thank you", "author_fullname": "t2_cvlyv82s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Question] 5TB causes power issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p952n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703348575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So TL:DR, is there a way to add a power source or convert my 5TB portable drive so I can use it without worry?&lt;/p&gt;\n\n&lt;p&gt;I got a 5TB removable hard drive about a year ago and at the time I saw it as a great buy. Get some movies on it, put my steam games on it, because primarily I use my laptop since I&amp;#39;m not at my desktop for very long due to work and family. When I started it up, I loaded it with a bunch of games and noticed it was freezing a lot. Eventually I found out that it was drawing so much power that unless it&amp;#39;s the only thing plugged in, the other peripherals and such of my computer will cause it to blink in and out, along with everything in a USB port.&lt;/p&gt;\n\n&lt;p&gt;So I instead started storing my movies and shows on it, because with that and my headphones, it seemed to work perfectly. FF to yesterday and I was putting more shows and movies on it when it started doing the same, even without much else going on in terms of power. I got worried because It&amp;#39;s only half full of movies and shows right now and I didn&amp;#39;t want to lose all that.&lt;/p&gt;\n\n&lt;p&gt;A buddy of mine told me it may just need external power, and I would love to convert it if at all possible, but I cannot for the life of me find how to convert it, I see a lot of converting internal storage to external, but nothing like what my friend described, and he can&amp;#39;t even find it either. &lt;/p&gt;\n\n&lt;p&gt;Food for thought: I have tried different USB ports, with usb 3.0 causing the least amount of issues, my laptop does have a usb-c port but I&amp;#39;m not sure if that would be the best solution or not. It&amp;#39;s a 5TB seagate that came with a USB 3 cable and that&amp;#39;s it. &lt;/p&gt;\n\n&lt;p&gt;Any and all help is appreciated, thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p952n", "is_robot_indexable": true, "report_reasons": null, "author": "Trajjed", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p952n/question_5tb_causes_power_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p952n/question_5tb_causes_power_issues/", "subreddit_subscribers": 720062, "created_utc": 1703348575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a batch of used drives - some 14 TBs and some 16 TBs. They have been honest in their power on hours and excellent in service for almost 2 years now, so I decided to chance it with a new model - some HUH728080ALE601s.\n\nInitial testing found this discrepancy in power on hours and the SMART test log. Could this be the work of a rather dim refurbisher, or a firmware / smartctl snag? Online searches suggested that both are possible. Thanks!\n\n[https://imgur.com/a/DQQ7VKf](https://imgur.com/a/DQQ7VKf)", "author_fullname": "t2_utzj5t9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power On Hours vs Lifetime Hours in test log. Unscrupulous \"refurbisher\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p3uwl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703331716.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703331288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a batch of used drives - some 14 TBs and some 16 TBs. They have been honest in their power on hours and excellent in service for almost 2 years now, so I decided to chance it with a new model - some HUH728080ALE601s.&lt;/p&gt;\n\n&lt;p&gt;Initial testing found this discrepancy in power on hours and the SMART test log. Could this be the work of a rather dim refurbisher, or a firmware / smartctl snag? Online searches suggested that both are possible. Thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/DQQ7VKf\"&gt;https://imgur.com/a/DQQ7VKf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p3uwl", "is_robot_indexable": true, "report_reasons": null, "author": "hc530", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p3uwl/power_on_hours_vs_lifetime_hours_in_test_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p3uwl/power_on_hours_vs_lifetime_hours_in_test_log/", "subreddit_subscribers": 720062, "created_utc": 1703331288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, I have a QNAP Plex server currently in RAID 6 with 8x18TB hard drives and \\~90TB of data that I am looking to upgrade to 8x22TB RAID 6\n\nI remember when I upgraded from 8x12TB to 8x18TB a few years ago switching the drives one-by-one and then expanding the storage space that it took over a week and just about disabled my system in that timespan. With this much data and these size drives I know this will be even significantly longer\n\nA thought I had to speed up the process was to backup the \\~90TB of media I have to two separate locations and then delete all of that data on the server reducing the data on it to close to 0. Without any media, will the rebuild per 22TB drive be just a matter of minutes due to the lack of data or is there a minimal amount of time each 22TB drive will take even without the data?\n\nIn answer to the inevitable question of why if I am already restoring the \\~90TB to begin with why I don't just install all 8 22TB drives at once and start fresh, with QNAP the Plex folder is a bit of a pain in the ass to manipulate and while backing up my media is fairly straightforward, backing up that folder is a bit trickier and data loss on the Plex side of things would be far worse than the data loss on the media side of things so I'd like to avoid risking that\n\nThanks for the help", "author_fullname": "t2_4539ugppm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID 6 Upgrade/Rebuild Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p3kg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703330082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I have a QNAP Plex server currently in RAID 6 with 8x18TB hard drives and ~90TB of data that I am looking to upgrade to 8x22TB RAID 6&lt;/p&gt;\n\n&lt;p&gt;I remember when I upgraded from 8x12TB to 8x18TB a few years ago switching the drives one-by-one and then expanding the storage space that it took over a week and just about disabled my system in that timespan. With this much data and these size drives I know this will be even significantly longer&lt;/p&gt;\n\n&lt;p&gt;A thought I had to speed up the process was to backup the ~90TB of media I have to two separate locations and then delete all of that data on the server reducing the data on it to close to 0. Without any media, will the rebuild per 22TB drive be just a matter of minutes due to the lack of data or is there a minimal amount of time each 22TB drive will take even without the data?&lt;/p&gt;\n\n&lt;p&gt;In answer to the inevitable question of why if I am already restoring the ~90TB to begin with why I don&amp;#39;t just install all 8 22TB drives at once and start fresh, with QNAP the Plex folder is a bit of a pain in the ass to manipulate and while backing up my media is fairly straightforward, backing up that folder is a bit trickier and data loss on the Plex side of things would be far worse than the data loss on the media side of things so I&amp;#39;d like to avoid risking that&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p3kg9", "is_robot_indexable": true, "report_reasons": null, "author": "SidneyFalco1313", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p3kg9/raid_6_upgraderebuild_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p3kg9/raid_6_upgraderebuild_question/", "subreddit_subscribers": 720062, "created_utc": 1703330082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "G'day to all of you\n\nI need some help archiving all comments on a YT channel.\n\nI would like to archive all comments on all videos on a particular YT channel. It is an old channel that is part of my childhood, it has been active since 2007 and has about 3.1k videos and an average of 10-20 comments per video.\n\nI already have the video part archived, but I can't figure out how to dump the comments from each video to a file.\n\nI have been doing some research on my own, but google only returns paid solutions, I have tried with [https://github.com/terorie/ytpriv](https://github.com/terorie/ytpriv) but I have not been able to get it to work properly.\n\nHas anyone managed to do this?\n\nThanks :)", "author_fullname": "t2_513k5k95c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive all comments on a YT channel", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p1wa3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703322707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;G&amp;#39;day to all of you&lt;/p&gt;\n\n&lt;p&gt;I need some help archiving all comments on a YT channel.&lt;/p&gt;\n\n&lt;p&gt;I would like to archive all comments on all videos on a particular YT channel. It is an old channel that is part of my childhood, it has been active since 2007 and has about 3.1k videos and an average of 10-20 comments per video.&lt;/p&gt;\n\n&lt;p&gt;I already have the video part archived, but I can&amp;#39;t figure out how to dump the comments from each video to a file.&lt;/p&gt;\n\n&lt;p&gt;I have been doing some research on my own, but google only returns paid solutions, I have tried with &lt;a href=\"https://github.com/terorie/ytpriv\"&gt;https://github.com/terorie/ytpriv&lt;/a&gt; but I have not been able to get it to work properly.&lt;/p&gt;\n\n&lt;p&gt;Has anyone managed to do this?&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tR2I07wTbNfPqhty2dI07Ide9MHGJJZF0yQx1Qk-IaU.jpg?auto=webp&amp;s=2e9eb72aa39a0a5e5bc47df20ffbfb42d04fc242", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/tR2I07wTbNfPqhty2dI07Ide9MHGJJZF0yQx1Qk-IaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4dbefcedcc3cd442d8867785dee91c592a8f5c04", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/tR2I07wTbNfPqhty2dI07Ide9MHGJJZF0yQx1Qk-IaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe2486f9671222a1632cd920669cf8f091bebcab", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/tR2I07wTbNfPqhty2dI07Ide9MHGJJZF0yQx1Qk-IaU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f2a110bc746c6e7f1fc37fa0387359b8220f626", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/tR2I07wTbNfPqhty2dI07Ide9MHGJJZF0yQx1Qk-IaU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=99b17964b233487b9a186340bc9fcf87b5e4342a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/tR2I07wTbNfPqhty2dI07Ide9MHGJJZF0yQx1Qk-IaU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c966bf625db0ad11fd519998de6b4d7f9e7387a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/tR2I07wTbNfPqhty2dI07Ide9MHGJJZF0yQx1Qk-IaU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba8bcafb68ea49318ece26007ed88b938f6308e0", "width": 1080, "height": 540}], "variants": {}, "id": "5nw6JO3jNtM9Zu0_v_RTgaOCg99GP-te76ohS9NWPoY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p1wa3", "is_robot_indexable": true, "report_reasons": null, "author": "Southern_Tip4955", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p1wa3/archive_all_comments_on_a_yt_channel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p1wa3/archive_all_comments_on_a_yt_channel/", "subreddit_subscribers": 720062, "created_utc": 1703322707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I backup terabytes of data to cloud providers like Google and Dropbox. And from time to time, I delete files that I don't need anymore or I feel are taking up more storage than I'd like to use. I use tools like FolderSizes or WizTree to scan drives. I'm hoping someone knows of a disk analyzer similar to DaisyDisk for Windows that also has a feature to scan cloud providers like Dropbox - [https://daisydiskapp.com/guide/cloud-scan](https://daisydiskapp.com/guide/cloud-scan). ", "author_fullname": "t2_54og1flc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-platform disk analyzer including cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p0bu3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703315881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I backup terabytes of data to cloud providers like Google and Dropbox. And from time to time, I delete files that I don&amp;#39;t need anymore or I feel are taking up more storage than I&amp;#39;d like to use. I use tools like FolderSizes or WizTree to scan drives. I&amp;#39;m hoping someone knows of a disk analyzer similar to DaisyDisk for Windows that also has a feature to scan cloud providers like Dropbox - &lt;a href=\"https://daisydiskapp.com/guide/cloud-scan\"&gt;https://daisydiskapp.com/guide/cloud-scan&lt;/a&gt;. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uw-l5RJMB6hijFJCMqrI5ElyXU8OetJI6DxhXT3jOKQ.jpg?auto=webp&amp;s=690c8038dda5cf42bd7fcfaf90e955c1c96552ee", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/uw-l5RJMB6hijFJCMqrI5ElyXU8OetJI6DxhXT3jOKQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a28735e1595b8b388a6269b3762e42bc7c8347d3", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/uw-l5RJMB6hijFJCMqrI5ElyXU8OetJI6DxhXT3jOKQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23d97dc406347e837fec6c5b62327a3b52081897", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/uw-l5RJMB6hijFJCMqrI5ElyXU8OetJI6DxhXT3jOKQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cda091a9edb5a25a5d4b1e249bb5c28823f43673", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/uw-l5RJMB6hijFJCMqrI5ElyXU8OetJI6DxhXT3jOKQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72aaa8a2fcefcbf125dae9ee2f445585af09c17c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/uw-l5RJMB6hijFJCMqrI5ElyXU8OetJI6DxhXT3jOKQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30b396879a050b2afa13bcb6cdd02b5af919a0d6", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/uw-l5RJMB6hijFJCMqrI5ElyXU8OetJI6DxhXT3jOKQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6dfb4c359cd3a7e10b55ac21bfcfa53aab8f4868", "width": 1080, "height": 567}], "variants": {}, "id": "yLC3zkt6g4swal6-aE6TV2qYLbJeCb2LAR36vpF4pms"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p0bu3", "is_robot_indexable": true, "report_reasons": null, "author": "ryanhallinger", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p0bu3/multiplatform_disk_analyzer_including_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p0bu3/multiplatform_disk_analyzer_including_cloud/", "subreddit_subscribers": 720062, "created_utc": 1703315881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I posted about this on r/datarescue recently but there's really not much activity there and I didn't get any helpful responses.  Hoping maybe someone here has experience and can steer me right.\n\nI am currently working on a project to recover data from a bunch of ~25 year old CD-Rs.  I started with a first batch of about 300 discs.  I am using Carbon Copy Cloner on a mac, and it managed to copy about 275 of them without any errors at all.  The other 25 would mount okay on the desktop but either gave read errors or timed out during the copy.  So I took those stubborn ones and have been trying to use ddrescue to recover them, but am running into some problems.  Initially I was using a sector size of 2048 because that's what the examples I googled were using, but at the end of the operation ddrescue would say \"Unaligned read error. Is sector size correct?\" and left me with an unreadable iso.  After some research it seemed like maybe 2352 was actually the magic number, so I tried again with that and it appeared to work fine, ended successfully, but still left me with an unreadable iso (when I try to mount it, Finder tells me \"The disk image could not be mounted.  The disk image is corrupted\").  Any ideas what I might be doing wrong here?  Any help is appreciated.\n\nIf it helps, the Terminal command I'm using is:\n\n    ddrescue -b 2352 -n -v /dev/disk2 cd173.iso cd173.log\n\nThanks!", "author_fullname": "t2_nw2t2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any ddrescue gurus can help me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ox4qz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703304102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted about this on &lt;a href=\"/r/datarescue\"&gt;r/datarescue&lt;/a&gt; recently but there&amp;#39;s really not much activity there and I didn&amp;#39;t get any helpful responses.  Hoping maybe someone here has experience and can steer me right.&lt;/p&gt;\n\n&lt;p&gt;I am currently working on a project to recover data from a bunch of ~25 year old CD-Rs.  I started with a first batch of about 300 discs.  I am using Carbon Copy Cloner on a mac, and it managed to copy about 275 of them without any errors at all.  The other 25 would mount okay on the desktop but either gave read errors or timed out during the copy.  So I took those stubborn ones and have been trying to use ddrescue to recover them, but am running into some problems.  Initially I was using a sector size of 2048 because that&amp;#39;s what the examples I googled were using, but at the end of the operation ddrescue would say &amp;quot;Unaligned read error. Is sector size correct?&amp;quot; and left me with an unreadable iso.  After some research it seemed like maybe 2352 was actually the magic number, so I tried again with that and it appeared to work fine, ended successfully, but still left me with an unreadable iso (when I try to mount it, Finder tells me &amp;quot;The disk image could not be mounted.  The disk image is corrupted&amp;quot;).  Any ideas what I might be doing wrong here?  Any help is appreciated.&lt;/p&gt;\n\n&lt;p&gt;If it helps, the Terminal command I&amp;#39;m using is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ddrescue -b 2352 -n -v /dev/disk2 cd173.iso cd173.log\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ox4qz", "is_robot_indexable": true, "report_reasons": null, "author": "MadDogFargo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ox4qz/any_ddrescue_gurus_can_help_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ox4qz/any_ddrescue_gurus_can_help_me/", "subreddit_subscribers": 720062, "created_utc": 1703304102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "To be more specific, there is the usb 3.0 cable for data transfer, but there is the dc adapter for power. Is there an adapter for the dc adapter to somehow be converted for usb?   \n\n\nReason I ask is that I have a PS5 that I take on the road with me in my semi, and I am limited on plugs inside since I don't have an auxillary power unit (apu). So my thought process was that I can power it directly off of the PS5.  \n\n\nThoughts?", "author_fullname": "t2_nglak", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a Western Digital D10 - 8TB and need an adapter .....", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18pafgj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703352295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To be more specific, there is the usb 3.0 cable for data transfer, but there is the dc adapter for power. Is there an adapter for the dc adapter to somehow be converted for usb?   &lt;/p&gt;\n\n&lt;p&gt;Reason I ask is that I have a PS5 that I take on the road with me in my semi, and I am limited on plugs inside since I don&amp;#39;t have an auxillary power unit (apu). So my thought process was that I can power it directly off of the PS5.  &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18pafgj", "is_robot_indexable": true, "report_reasons": null, "author": "wesker1213", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18pafgj/i_have_a_western_digital_d10_8tb_and_need_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18pafgj/i_have_a_western_digital_d10_8tb_and_need_an/", "subreddit_subscribers": 720062, "created_utc": 1703352295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\nI recently decommissioned my old Synology NAS in favor of connecting the four 3TB drives directly to my computer, using DrivePool to merge them into one drive. I enabled file duplication for the whole pool, and toggled the setting for equal file distribution across drives. I was very pleased with the increased speed and the attractive UI of DrivePool. I then replaced three of the four drives with 4TB ones to increase the capacity of the pool. I replaced them one by one, making sure DrivePool finished duplicating to the new drives before adding the next one. Everything seemed to go fine and I didn't get any errors.\n\nHowever, today I went to watch a movie on my Plex server and got the error \"cannot locate media.\" I checked my DrivePool folder, and to my horror about 15% of my movie files have vanished! The folders are still there; sometimes the folders are empty, and sometimes they contain SRTs that were included with the movie. It seems only the movie files themselves (avi, mp4, mkv, etc.) have been affected.\n\nI'm frantically trying to recover my lost files from the old NAS drives and have sent a support inquiry to DrivePool. Has anyone else had this happen? Thank you in advance!", "author_fullname": "t2_w776kpv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DrivePool mysteriously deleting files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18pabff", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703351969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I recently decommissioned my old Synology NAS in favor of connecting the four 3TB drives directly to my computer, using DrivePool to merge them into one drive. I enabled file duplication for the whole pool, and toggled the setting for equal file distribution across drives. I was very pleased with the increased speed and the attractive UI of DrivePool. I then replaced three of the four drives with 4TB ones to increase the capacity of the pool. I replaced them one by one, making sure DrivePool finished duplicating to the new drives before adding the next one. Everything seemed to go fine and I didn&amp;#39;t get any errors.&lt;/p&gt;\n\n&lt;p&gt;However, today I went to watch a movie on my Plex server and got the error &amp;quot;cannot locate media.&amp;quot; I checked my DrivePool folder, and to my horror about 15% of my movie files have vanished! The folders are still there; sometimes the folders are empty, and sometimes they contain SRTs that were included with the movie. It seems only the movie files themselves (avi, mp4, mkv, etc.) have been affected.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m frantically trying to recover my lost files from the old NAS drives and have sent a support inquiry to DrivePool. Has anyone else had this happen? Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18pabff", "is_robot_indexable": true, "report_reasons": null, "author": "greatsonne", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18pabff/drivepool_mysteriously_deleting_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18pabff/drivepool_mysteriously_deleting_files/", "subreddit_subscribers": 720062, "created_utc": 1703351969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm new at this stuff and struggling PS: sorry english not my first langage ", "author_fullname": "t2_nsok0d3re", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I download a video in swf file (adobe flash) from a link alone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p9e9g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703349330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new at this stuff and struggling PS: sorry english not my first langage &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p9e9g", "is_robot_indexable": true, "report_reasons": null, "author": "No-Bug-3887", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p9e9g/how_do_i_download_a_video_in_swf_file_adobe_flash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p9e9g/how_do_i_download_a_video_in_swf_file_adobe_flash/", "subreddit_subscribers": 720062, "created_utc": 1703349330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a nvgs300 (this is the pal one, I think. PVGS is NTSC) and I am pretty sure my Sony miniDV tapes are also PAL. I had previously got an PVGS500, and it gave \"Wrong Tape Format\" error and now with the NVGS300 I am also getting wrong tape format. So what is happening here?\n\nI chose the Panasonic gs line because you can use USB to get video off the tapes in source quality.\nI have already done it with a NTSC tape that came with the pvgs, but I don't have that tape or the pvgs500 any more.\n\nI've already spent 200 bucks buying these cameras trying to get into these tapes. help\n\nEdit: They are all DMV60me that were recorded by a sony DCR-HC32E.", "author_fullname": "t2_8204l5f8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Retro tech support: Sony MiniDV \"wrong format\" in both PVGS500 and NVGS300", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p86ge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703352073.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703345829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a nvgs300 (this is the pal one, I think. PVGS is NTSC) and I am pretty sure my Sony miniDV tapes are also PAL. I had previously got an PVGS500, and it gave &amp;quot;Wrong Tape Format&amp;quot; error and now with the NVGS300 I am also getting wrong tape format. So what is happening here?&lt;/p&gt;\n\n&lt;p&gt;I chose the Panasonic gs line because you can use USB to get video off the tapes in source quality.\nI have already done it with a NTSC tape that came with the pvgs, but I don&amp;#39;t have that tape or the pvgs500 any more.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve already spent 200 bucks buying these cameras trying to get into these tapes. help&lt;/p&gt;\n\n&lt;p&gt;Edit: They are all DMV60me that were recorded by a sony DCR-HC32E.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p86ge", "is_robot_indexable": true, "report_reasons": null, "author": "notasheepl", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p86ge/retro_tech_support_sony_minidv_wrong_format_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p86ge/retro_tech_support_sony_minidv_wrong_format_in/", "subreddit_subscribers": 720062, "created_utc": 1703345829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For the longest time I've read that the WD Black HDDs are the best for gaming purposes and the Exos line is best for enterprise uses. I have been using the Exos line (18-22TB; 18x-20x) for a long time for both gaming and mass media storage. \n\nI'm here to ask the community why is it that WD Black drives are always given the gaming HDD crown? Both lines have 256mb cache, 7200rpm, ~260mb/s sustained write, 550tb/year rating, and a few other stats that are identical. \n\nThe only real difference I can see is that the Exos line offers higher capacities and (from my experience) is alot more reliable and FAR cheaper as well as more readily available.\n\nLet me know what you think, I'd love to hear what my fellow HDD people have to say about this.", "author_fullname": "t2_ti0nfdys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wd Black vs SG Exos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p4vkt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703335210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the longest time I&amp;#39;ve read that the WD Black HDDs are the best for gaming purposes and the Exos line is best for enterprise uses. I have been using the Exos line (18-22TB; 18x-20x) for a long time for both gaming and mass media storage. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m here to ask the community why is it that WD Black drives are always given the gaming HDD crown? Both lines have 256mb cache, 7200rpm, ~260mb/s sustained write, 550tb/year rating, and a few other stats that are identical. &lt;/p&gt;\n\n&lt;p&gt;The only real difference I can see is that the Exos line offers higher capacities and (from my experience) is alot more reliable and FAR cheaper as well as more readily available.&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think, I&amp;#39;d love to hear what my fellow HDD people have to say about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p4vkt", "is_robot_indexable": true, "report_reasons": null, "author": "Breeze23412", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p4vkt/wd_black_vs_sg_exos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p4vkt/wd_black_vs_sg_exos/", "subreddit_subscribers": 720062, "created_utc": 1703335210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm trying to download a website for med school since I have premium for only 1 month and that it's expensive.\n\nI tried httracks, sitesucker, teleport pro and others but I have errors maybe because it's logging me off or because of the robot.txt rules i really struggle so maybe someone in this sub already did a silimar project and is willing to help me ?\n\nHuge thanks !!\n\n&amp;#x200B;", "author_fullname": "t2_2zwdels4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Options to Download this website that as a login page ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p4uht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703335100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to download a website for med school since I have premium for only 1 month and that it&amp;#39;s expensive.&lt;/p&gt;\n\n&lt;p&gt;I tried httracks, sitesucker, teleport pro and others but I have errors maybe because it&amp;#39;s logging me off or because of the robot.txt rules i really struggle so maybe someone in this sub already did a silimar project and is willing to help me ?&lt;/p&gt;\n\n&lt;p&gt;Huge thanks !!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p4uht", "is_robot_indexable": true, "report_reasons": null, "author": "Kindaboii", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p4uht/best_options_to_download_this_website_that_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p4uht/best_options_to_download_this_website_that_as_a/", "subreddit_subscribers": 720062, "created_utc": 1703335100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used to use a WD MyHome device for my father's sign shop, but after having my hardware effectively bricked TWICE by WD, I'm kind of done with them. Looking at replacing it with something that has some sustaining power. We moved to a new state recently-ish and I'm getting things set up in the new place. My father still dabbles in sign stuff even though he's retired (and I help him), and I'm an architect getting ready to launch my own practice. He uses Macs exclusively, and I use both, but live most of my life in Windows. My architectural project files can be fairly big (1 project can be 50+GB) and I need to access them from both in the home office and remotely. I'm a handy person but not an elite haxxor by any stretch. I can configure IPs if needed, but I'd prefer a \"works out of the box\" kind of solution. Thanks for considering.", "author_fullname": "t2_4ii3nxvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended NAS for small business/mixed OS environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18p4f0d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703333434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to use a WD MyHome device for my father&amp;#39;s sign shop, but after having my hardware effectively bricked TWICE by WD, I&amp;#39;m kind of done with them. Looking at replacing it with something that has some sustaining power. We moved to a new state recently-ish and I&amp;#39;m getting things set up in the new place. My father still dabbles in sign stuff even though he&amp;#39;s retired (and I help him), and I&amp;#39;m an architect getting ready to launch my own practice. He uses Macs exclusively, and I use both, but live most of my life in Windows. My architectural project files can be fairly big (1 project can be 50+GB) and I need to access them from both in the home office and remotely. I&amp;#39;m a handy person but not an elite haxxor by any stretch. I can configure IPs if needed, but I&amp;#39;d prefer a &amp;quot;works out of the box&amp;quot; kind of solution. Thanks for considering.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18p4f0d", "is_robot_indexable": true, "report_reasons": null, "author": "moistmarbles", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18p4f0d/recommended_nas_for_small_businessmixed_os/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18p4f0d/recommended_nas_for_small_businessmixed_os/", "subreddit_subscribers": 720062, "created_utc": 1703333434.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}