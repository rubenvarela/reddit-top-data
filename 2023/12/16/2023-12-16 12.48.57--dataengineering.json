{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j7j4f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"We have so many challenging projects!\" ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18j0ygk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 123, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/qgx75hdxug6c1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 1280, "width": 714, "scrubber_media_url": "https://v.redd.it/qgx75hdxug6c1/DASH_96.mp4", "dash_url": "https://v.redd.it/qgx75hdxug6c1/DASHPlaylist.mpd?a=1705322937%2CODVkYjkxY2Y0Zjk3OWNiZTliNjQ4MjliOGNlYzgxMjgzYmJhODhhMzVkMjE1MWE5NTM0YjBkNmVmY2U2OGVlOA%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/qgx75hdxug6c1/HLSPlaylist.m3u8?a=1705322937%2CNDY1YTQ0OGQ4YTIyMzkwNDkwZWNkNDg4YTc0YzViODhiOTRhYmFjMDk0NWI1N2MxMDcxYjQwZTQ3ZGI5ZTUwOA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 123, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=227036178b5d344290414f523618603d36a0186e", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702649432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/qgx75hdxug6c1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?format=pjpg&amp;auto=webp&amp;s=69523afcc11a24b54037c8b3cb719b6ec403c640", "width": 983, "height": 1764}, "resolutions": [{"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2028b47397af20b16906282fe6cc7cfc2fed329f", "width": 108, "height": 193}, {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=06eca739f46c7645390bc07092b7143ccada48ee", "width": 216, "height": 387}, {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b9500c3eb62e1d6c22ea1e7a14f358c8d892b60e", "width": 320, "height": 574}, {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0aa541b0d69aff973a1d9be1d34c4d7bd12bd9fb", "width": 640, "height": 1148}, {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d0478df9f02dcc92699a762226bf241906adaf69", "width": 960, "height": 1722}], "variants": {}, "id": "OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18j0ygk", "is_robot_indexable": true, "report_reasons": null, "author": "Marawishka", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j0ygk/we_have_so_many_challenging_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/qgx75hdxug6c1", "subreddit_subscribers": 146295, "created_utc": 1702649432.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/qgx75hdxug6c1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 1280, "width": 714, "scrubber_media_url": "https://v.redd.it/qgx75hdxug6c1/DASH_96.mp4", "dash_url": "https://v.redd.it/qgx75hdxug6c1/DASHPlaylist.mpd?a=1705322937%2CODVkYjkxY2Y0Zjk3OWNiZTliNjQ4MjliOGNlYzgxMjgzYmJhODhhMzVkMjE1MWE5NTM0YjBkNmVmY2U2OGVlOA%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/qgx75hdxug6c1/HLSPlaylist.m3u8?a=1705322937%2CNDY1YTQ0OGQ4YTIyMzkwNDkwZWNkNDg4YTc0YzViODhiOTRhYmFjMDk0NWI1N2MxMDcxYjQwZTQ3ZGI5ZTUwOA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everybody,\n\nThis is a bit of a self-promotion, and I don't usually do that (I have never done it here), but I figured many of you may find it helpful.\n\nFor context, I am a Head of data (&amp; analytics) engineering at a Fintech company and have interviewed hundreds of candidates.\n\nWhat I have outlined in my blog post would, obviously, not apply to every interview you may have, but I believe there are many things people don't usually discuss.\n\nPlease go wild with any questions you may have.\n\nhttps://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I interview data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ja9hq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 117, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 117, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702674334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody,&lt;/p&gt;\n\n&lt;p&gt;This is a bit of a self-promotion, and I don&amp;#39;t usually do that (I have never done it here), but I figured many of you may find it helpful.&lt;/p&gt;\n\n&lt;p&gt;For context, I am a Head of data (&amp;amp; analytics) engineering at a Fintech company and have interviewed hundreds of candidates.&lt;/p&gt;\n\n&lt;p&gt;What I have outlined in my blog post would, obviously, not apply to every interview you may have, but I believe there are many things people don&amp;#39;t usually discuss.&lt;/p&gt;\n\n&lt;p&gt;Please go wild with any questions you may have.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcome=true\"&gt;https://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcome=true&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?auto=webp&amp;s=d62d1df611eb5fd3b6c201a2ca1198764c34576e", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d233877d035936e4cdd051c8308d2ed8687ea4b4", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=658a8bd49e51df3ec03cb2ee2e11353625e67188", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6fa1c75aa9279837a1bc754209d43a4ecc494bf", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e81e57fe8694136946f0e0b90cd174e447289020", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c2d765d29649bb8542956c04e415c72a8794e9e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b00fa2cb6c57cffff61869c14eb6b4e6cbb86d4", "width": 1080, "height": 720}], "variants": {}, "id": "5zTWOZ4g6_ZEJv5W5FoiMbIfg0hblAMLhiLqgJKuoFg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ja9hq", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ja9hq/how_i_interview_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ja9hq/how_i_interview_data_engineers/", "subreddit_subscribers": 146295, "created_utc": 1702674334.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It was on this day that Data Engineering was born", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_18j25eu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3IwlOr3WzZlLtJ6JCuLBD2j46D0QTOyxN8FxgikyptM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702652766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ad8gxeos4h6c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?auto=webp&amp;s=25faaf445c6d3315d4710fee34a974144be3b0b2", "width": 1792, "height": 1024}, "resolutions": [{"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f3d28a2ca30fe5246337857d3e6ba043cb2b831", "width": 108, "height": 61}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=459573272534a9db2d52e6eb1cb2abf827ee3cee", "width": 216, "height": 123}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=571d0c21cb3a0ac43702e07e8f079c333f9ee893", "width": 320, "height": 182}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=593ff544d465402452d7309ad4cae72d0cc90dd7", "width": 640, "height": 365}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7529979eb3706e099419035507d99d13912efe9", "width": 960, "height": 548}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67155114bf2f671a7a7b6e45437bec502a16f2cf", "width": 1080, "height": 617}], "variants": {}, "id": "AEf51p2KV45AN5i0u4jdzl_w_LWvhR69DlaAlf1CvCc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18j25eu", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j25eu/it_was_on_this_day_that_data_engineering_was_born/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ad8gxeos4h6c1.png", "subreddit_subscribers": 146295, "created_utc": 1702652766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi there,\n\nI'm currently kinda having a hard time understanding the DevOps process for databases. Up until now, my experience has been limited to on-premises databases, where any modifications made in the development database were manually transferred to the production database.\n\nI'm now transitioning to setting up my DB in Azure, where I plan to establish both a development and production environment. I've delved into various tutorials on configuring build and release pipelines, as well as setting up database projects in Visual Studio Code. However, I'm still uncertain whether these processes exclusively apply to changes in the schema, views, procedures, etc., or if there should also be a component in the pipeline responsible for transferring the entire (new) data from the development (or QA) environment to production.\n\nThis entire process seems a bit like a overkill to me, especially since the data model I'm using is already well-established. The only potential changes I might make occasionally are related to views. Also the entire ETL process happens outside of Azure.\n\nI would greatly appreciate hearing examples of how you implement DevOps with SQL Databases. Additionally, any advice or recommendations for videos or literature would be highly appreciated.\n\nP.S.: I am no data engineer by training, just a data scientist/chemist, who is forced to do all this stuff with very limited support at work. So please excuse any stupid questions. \n\n&amp;#x200B;", "author_fullname": "t2_4ed3khrv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure DevOps Pipelines for SQL Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j23or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702652637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently kinda having a hard time understanding the DevOps process for databases. Up until now, my experience has been limited to on-premises databases, where any modifications made in the development database were manually transferred to the production database.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now transitioning to setting up my DB in Azure, where I plan to establish both a development and production environment. I&amp;#39;ve delved into various tutorials on configuring build and release pipelines, as well as setting up database projects in Visual Studio Code. However, I&amp;#39;m still uncertain whether these processes exclusively apply to changes in the schema, views, procedures, etc., or if there should also be a component in the pipeline responsible for transferring the entire (new) data from the development (or QA) environment to production.&lt;/p&gt;\n\n&lt;p&gt;This entire process seems a bit like a overkill to me, especially since the data model I&amp;#39;m using is already well-established. The only potential changes I might make occasionally are related to views. Also the entire ETL process happens outside of Azure.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate hearing examples of how you implement DevOps with SQL Databases. Additionally, any advice or recommendations for videos or literature would be highly appreciated.&lt;/p&gt;\n\n&lt;p&gt;P.S.: I am no data engineer by training, just a data scientist/chemist, who is forced to do all this stuff with very limited support at work. So please excuse any stupid questions. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18j23or", "is_robot_indexable": true, "report_reasons": null, "author": "stelo55", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j23or/azure_devops_pipelines_for_sql_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j23or/azure_devops_pipelines_for_sql_database/", "subreddit_subscribers": 146295, "created_utc": 1702652637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This may seem like a dumb question, I am working on pulling data from an  RDBMS,  with a Python script and placing it into a data warehouse for visualization purposes. My question is, in this instance is it best practice to make as many transformations in my  RDBMS via SQL, create the table extract and load it. Or should this be done only in my data ware house. The data in my in my table I would be creating would be me averaging data, also not sure how that would effect me when using PowerBI.\n\nTIA ALL!", "author_fullname": "t2_8txv38ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BEST ETL TRANSFORMING PRACTICE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jb8hr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702677553.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702676932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This may seem like a dumb question, I am working on pulling data from an  RDBMS,  with a Python script and placing it into a data warehouse for visualization purposes. My question is, in this instance is it best practice to make as many transformations in my  RDBMS via SQL, create the table extract and load it. Or should this be done only in my data ware house. The data in my in my table I would be creating would be me averaging data, also not sure how that would effect me when using PowerBI.&lt;/p&gt;\n\n&lt;p&gt;TIA ALL!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jb8hr", "is_robot_indexable": true, "report_reasons": null, "author": "Fraiz24", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jb8hr/best_etl_transforming_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jb8hr/best_etl_transforming_practice/", "subreddit_subscribers": 146295, "created_utc": 1702676932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nhow do you do your continious testing? Which test tools do you use? Do you have tips for literature, concepts?\n\nThank you in advance for your response.", "author_fullname": "t2_9umujqax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataOps: how do you do your continious testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jkmrf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702706195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;how do you do your continious testing? Which test tools do you use? Do you have tips for literature, concepts?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your response.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jkmrf", "is_robot_indexable": true, "report_reasons": null, "author": "ButterscotchBulky320", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jkmrf/dataops_how_do_you_do_your_continious_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jkmrf/dataops_how_do_you_do_your_continious_testing/", "subreddit_subscribers": 146295, "created_utc": 1702706195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, my company is currently storing a ton of data in databricks and i would like to do \u201cstuff\u201d with this data. Like analysis, maybe some graphs, maybe some code to get some specific metrics. Not being a dataengineer (but im a developer) what are some things i can learn in this space to join meetings and not be 100% lost. Is this one of pandas use cases? ", "author_fullname": "t2_2gjyr66m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks for non dataengineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jb2ze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702676793.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702676517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, my company is currently storing a ton of data in databricks and i would like to do \u201cstuff\u201d with this data. Like analysis, maybe some graphs, maybe some code to get some specific metrics. Not being a dataengineer (but im a developer) what are some things i can learn in this space to join meetings and not be 100% lost. Is this one of pandas use cases? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jb2ze", "is_robot_indexable": true, "report_reasons": null, "author": "SuperLucas2000", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jb2ze/databricks_for_non_dataengineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jb2ze/databricks_for_non_dataengineers/", "subreddit_subscribers": 146295, "created_utc": 1702676517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This week, I created a dbt model that pinpoints the NBA's top \"one-hit wonders.\"\n\n\"One hit wonder\" = Players who had 1 season that's dramatically better than the avg. of all their other seasons.\n\nTo find these players, I used a formula called Player Efficiency Rating (PER) across seasons. The PER formula condenses a player's contributions into a single, comprehensive metric. By weighing 12 distinct stats, each with its unique importance, PER offers a all-in-one metric to identify a players performance.\n\nDisclaimer: PER isn't the end-all and be-all of player metrics, it points me in the right direction.\n\nTools used:\n\n\\- \ud835\udc08\ud835\udc27\ud835\udc20\ud835\udc1e\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27: public NBA API + Python\n\n\\- \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1a\ud835\udc20\ud835\udc1e: DuckDB (development) &amp; Snowflake (Production)\n\n\\- \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c (dbt): Paradime\n\n\\- \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc27\ud835\udc20 (\ud835\udc01\ud835\udc08) -Lightdash\n\nIf you're curious, here's the repo:  \nhttps://github.com/jpooksy/NBA\\_Data\\_Modeling  \n\n\nhttps://preview.redd.it/69709ezuwi6c1.png?width=978&amp;format=png&amp;auto=webp&amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590\n\n&amp;#x200B;", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analyzing \"One hit wonder\" NBA Players", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 110, "top_awarded_type": null, "hide_score": false, "media_metadata": {"69709ezuwi6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3190b528ffc53e48e3fc3191fcc8e57763ae961c"}, {"y": 170, "x": 216, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6f5c589ea0dc15a126060d837e4e4dc82803a0f"}, {"y": 252, "x": 320, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0dc820f9448ed220606e4ffc0a0673867b069ba"}, {"y": 504, "x": 640, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=66ea372b61096ac6e87f289064776282d14b7eaa"}, {"y": 756, "x": 960, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd3fa91bf0d7504759d0c1e22337ecd4d71c9da7"}], "s": {"y": 771, "x": 978, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;format=png&amp;auto=webp&amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590"}, "id": "69709ezuwi6c1"}}, "name": "t3_18ja96u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/73whFpo_IHCGK7dqQzOmjYuICSsjOFhke4-OP5fs-Yo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702674311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This week, I created a dbt model that pinpoints the NBA&amp;#39;s top &amp;quot;one-hit wonders.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;One hit wonder&amp;quot; = Players who had 1 season that&amp;#39;s dramatically better than the avg. of all their other seasons.&lt;/p&gt;\n\n&lt;p&gt;To find these players, I used a formula called Player Efficiency Rating (PER) across seasons. The PER formula condenses a player&amp;#39;s contributions into a single, comprehensive metric. By weighing 12 distinct stats, each with its unique importance, PER offers a all-in-one metric to identify a players performance.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: PER isn&amp;#39;t the end-all and be-all of player metrics, it points me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;Tools used:&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc08\ud835\udc27\ud835\udc20\ud835\udc1e\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27: public NBA API + Python&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1a\ud835\udc20\ud835\udc1e: DuckDB (development) &amp;amp; Snowflake (Production)&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c (dbt): Paradime&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc27\ud835\udc20 (\ud835\udc01\ud835\udc08) -Lightdash&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, here&amp;#39;s the repo:&lt;br/&gt;\n&lt;a href=\"https://github.com/jpooksy/NBA%5C_Data%5C_Modeling\"&gt;https://github.com/jpooksy/NBA\\_Data\\_Modeling&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590\"&gt;https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18ja96u", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ja96u/analyzing_one_hit_wonder_nba_players/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ja96u/analyzing_one_hit_wonder_nba_players/", "subreddit_subscribers": 146295, "created_utc": 1702674311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm heading into an interview next week for a Business Intelligence Engineer role requiring 2+ years experience. It's a technical revolving around SQL and data visualization.\n\nDoes anyone have insight into the types of questions asked on these interviews? I've reviewed the cheat sheet and SQL is honestly a weakness for me so I'd like to be as prepared as possible.", "author_fullname": "t2_1323iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon BIE Interview Process (SQL)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j8s9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702670328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m heading into an interview next week for a Business Intelligence Engineer role requiring 2+ years experience. It&amp;#39;s a technical revolving around SQL and data visualization.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have insight into the types of questions asked on these interviews? I&amp;#39;ve reviewed the cheat sheet and SQL is honestly a weakness for me so I&amp;#39;d like to be as prepared as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18j8s9p", "is_robot_indexable": true, "report_reasons": null, "author": "bopper1826", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j8s9p/amazon_bie_interview_process_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j8s9p/amazon_bie_interview_process_sql/", "subreddit_subscribers": 146295, "created_utc": 1702670328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys\n\nData engineer here. Only ever worked in back end engineering.\n\nSide hobby of mine is collecting interesting data from around the web which I transform and aggregate etc. I'd like to start offering it back to end users for free.\n\n* What I want to do is pipe it from my cloud sql server to some kind of front end website, simple CSV etc.\n   * ..nothing fancy just something like the below where users can get a static preview of the data (this could be a png no issues)\n* And they can click download etc to download the actual full data set.\n\nI'm thinking nightly snapshots get pushed to the front ends server, overwriting previous version present allowing people to have access to a regularly updated set.\n\nBut I want it secure, I dont want any chance of the front end interrupting or allowing easier attacks to my cloud sql server?? (forgive ignorance, netsec not my specialty)\n\nYour thoughts/suggestions on avenues to achieve above? Literally NO experience in front end so completely oblivious to libraries, frameworks, saas offerings on F/E. \n\nTIA\n\n&amp;#x200B;\n\nRandom image of what I've got in my head for front end\n\nhttps://preview.redd.it/vjah84unam6c1.png?width=1314&amp;format=png&amp;auto=webp&amp;s=8e6e8abb303fda33a2410ca94393207631cff069", "author_fullname": "t2_6mnzdcoh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to push data safely from sql to front end for end users to download?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vjah84unam6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/vjah84unam6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=358157563a7b31d6e088858c72474eee94f487b2"}, {"y": 77, "x": 216, "u": "https://preview.redd.it/vjah84unam6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f3567d25536b3e97fa08b745fbf53a7f89203c1"}, {"y": 114, "x": 320, "u": "https://preview.redd.it/vjah84unam6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b47720b4287509bc7d71446bbb92bd32b50bf7e"}, {"y": 229, "x": 640, "u": "https://preview.redd.it/vjah84unam6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dfeef35eece48ab5a2b7ac9a55837e59327c88c6"}, {"y": 344, "x": 960, "u": "https://preview.redd.it/vjah84unam6c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b04ff4b519b8fa44b05fc354f5e85c7e89c61c29"}, {"y": 387, "x": 1080, "u": "https://preview.redd.it/vjah84unam6c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce39fce51825f46e65b7191964ae0768f8f8754b"}], "s": {"y": 472, "x": 1314, "u": "https://preview.redd.it/vjah84unam6c1.png?width=1314&amp;format=png&amp;auto=webp&amp;s=8e6e8abb303fda33a2410ca94393207631cff069"}, "id": "vjah84unam6c1"}}, "name": "t3_18jmumu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NH6nekM3IB9DFNi3DGzoOLTLqFOnuI08G65HpU1Y9ns.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702715285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys&lt;/p&gt;\n\n&lt;p&gt;Data engineer here. Only ever worked in back end engineering.&lt;/p&gt;\n\n&lt;p&gt;Side hobby of mine is collecting interesting data from around the web which I transform and aggregate etc. I&amp;#39;d like to start offering it back to end users for free.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What I want to do is pipe it from my cloud sql server to some kind of front end website, simple CSV etc.\n\n&lt;ul&gt;\n&lt;li&gt;..nothing fancy just something like the below where users can get a static preview of the data (this could be a png no issues)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;And they can click download etc to download the actual full data set.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m thinking nightly snapshots get pushed to the front ends server, overwriting previous version present allowing people to have access to a regularly updated set.&lt;/p&gt;\n\n&lt;p&gt;But I want it secure, I dont want any chance of the front end interrupting or allowing easier attacks to my cloud sql server?? (forgive ignorance, netsec not my specialty)&lt;/p&gt;\n\n&lt;p&gt;Your thoughts/suggestions on avenues to achieve above? Literally NO experience in front end so completely oblivious to libraries, frameworks, saas offerings on F/E. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Random image of what I&amp;#39;ve got in my head for front end&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vjah84unam6c1.png?width=1314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e6e8abb303fda33a2410ca94393207631cff069\"&gt;https://preview.redd.it/vjah84unam6c1.png?width=1314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e6e8abb303fda33a2410ca94393207631cff069&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jmumu", "is_robot_indexable": true, "report_reasons": null, "author": "_DESTRUCTION", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jmumu/how_to_push_data_safely_from_sql_to_front_end_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jmumu/how_to_push_data_safely_from_sql_to_front_end_for/", "subreddit_subscribers": 146295, "created_utc": 1702715285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any helpful guide on spark, which goes deeper into working of spark e.g how to deal with datasets larger than memory of spark cluster. And how to effectively work with huge datasets, something like that. Video, blog, course anything will be helpful.", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Course, blog or resources for guide on dealing with large datasets in spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jlv80", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702711043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any helpful guide on spark, which goes deeper into working of spark e.g how to deal with datasets larger than memory of spark cluster. And how to effectively work with huge datasets, something like that. Video, blog, course anything will be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jlv80", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jlv80/course_blog_or_resources_for_guide_on_dealing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jlv80/course_blog_or_resources_for_guide_on_dealing/", "subreddit_subscribers": 146295, "created_utc": 1702711043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some credits from my company that I didn't use for this year. Can I pass the dbt certification exam with 0 knowledge of **dbt cloud**? I have used **dbt core** extensively in my work.", "author_fullname": "t2_ncq2v4j0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much knowledge of dbt Cloud is required for the dbt certification exam?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j8ar8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702668988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some credits from my company that I didn&amp;#39;t use for this year. Can I pass the dbt certification exam with 0 knowledge of &lt;strong&gt;dbt cloud&lt;/strong&gt;? I have used &lt;strong&gt;dbt core&lt;/strong&gt; extensively in my work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18j8ar8", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable_Buyer_4487", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j8ar8/how_much_knowledge_of_dbt_cloud_is_required_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j8ar8/how_much_knowledge_of_dbt_cloud_is_required_for/", "subreddit_subscribers": 146295, "created_utc": 1702668988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been applying and feel like I'm over qualified since going from coding to report making is an easy switch. My current role is basically a data engineer even though my title is software developer. I can write sql in my sleep, I understand relational databases, I understand and work with big data. I have created reports in everything from power Bi, tableau, ssrs, etc...But I think because my resume lacks any history of a data title I've been getting rejected.... Do I lie and fudge my title or what?", "author_fullname": "t2_5hg1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to pivot from software dev to data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j87m2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702668744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been applying and feel like I&amp;#39;m over qualified since going from coding to report making is an easy switch. My current role is basically a data engineer even though my title is software developer. I can write sql in my sleep, I understand relational databases, I understand and work with big data. I have created reports in everything from power Bi, tableau, ssrs, etc...But I think because my resume lacks any history of a data title I&amp;#39;ve been getting rejected.... Do I lie and fudge my title or what?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18j87m2", "is_robot_indexable": true, "report_reasons": null, "author": "skydreamer303", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j87m2/how_to_pivot_from_software_dev_to_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j87m2/how_to_pivot_from_software_dev_to_data_engineer/", "subreddit_subscribers": 146295, "created_utc": 1702668744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Question:**\n\nIs it possible to do so? I want either of \n\n1. run with latest commited code\n2. straight up fail this pipeline without running this operation/anything\n\n**Clarification:**\n\nA colleague used a production notebook, the kind that runs automatically on schedules, as a scratchpad to showcase code to a junior member. Kinda junior move from him, if you ask me, but not the point I'm trying to make. At the end of this teaching session, he kept an extra cmd on the notebook, a very unfortunate one: it contained a `dbutils.fs.rm(CONTAINER_URI)` line. Long story short, this is going to be a long weekend babysitting some jobs.  \nOur pull review fail safes were bypassed by this user editing the production notebook. Nothing like that would pass by a peer's review. It wouldn't happen if uncommited code were somehow blocked from running in jobs in that cluster.\n\nA lot has to change at my current job, but this is priority 0, I want it implemented today. Anyone has done it before and can point me in the right direction? Thanks ", "author_fullname": "t2_lpkh5lbv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prevent Data Factory from running Databricks notebooks with uncommited code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j27to", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702652948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is it possible to do so? I want either of &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;run with latest commited code&lt;/li&gt;\n&lt;li&gt;straight up fail this pipeline without running this operation/anything&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Clarification:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A colleague used a production notebook, the kind that runs automatically on schedules, as a scratchpad to showcase code to a junior member. Kinda junior move from him, if you ask me, but not the point I&amp;#39;m trying to make. At the end of this teaching session, he kept an extra cmd on the notebook, a very unfortunate one: it contained a &lt;code&gt;dbutils.fs.rm(CONTAINER_URI)&lt;/code&gt; line. Long story short, this is going to be a long weekend babysitting some jobs.&lt;br/&gt;\nOur pull review fail safes were bypassed by this user editing the production notebook. Nothing like that would pass by a peer&amp;#39;s review. It wouldn&amp;#39;t happen if uncommited code were somehow blocked from running in jobs in that cluster.&lt;/p&gt;\n\n&lt;p&gt;A lot has to change at my current job, but this is priority 0, I want it implemented today. Anyone has done it before and can point me in the right direction? Thanks &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18j27to", "is_robot_indexable": true, "report_reasons": null, "author": "recruta54", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j27to/prevent_data_factory_from_running_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j27to/prevent_data_factory_from_running_databricks/", "subreddit_subscribers": 146295, "created_utc": 1702652948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've found a lot of inspiration from Maxime Beauchemin's [article](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a) on Functional Data Engineering.The concepts of immutable partitions and dimension snapshots, in particular, have caught my attention. However, I'm trying to figure out how to handle updates in fact events, which may change over their lifecycle.\n\nTake, for example, a payment transaction. It might be AUTHORISED on DAY 1, then SETTLED on DAY 2, and possibly REFUNDED on DAY 3.According to the principles laid out in the article, I would avoid applying UPDATE operations. But how should I manage these kinds of status changes? Taking a daily snapshot of all the facts is unbearable. And if I decide to update the rows I lose most of the benefits tracked down in the article.And if the creation date is still the same for all the statuses, appending three rows would break the idea of single unit of work, that is, every job is responsible for inserting overwriting only one partition. (the table would be ideally partitioned by creation date)\n\nThank you :)", "author_fullname": "t2_8x1e3w17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Functional DE: how to deal with changing facts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jnsql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702719476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found a lot of inspiration from Maxime Beauchemin&amp;#39;s &lt;a href=\"https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a\"&gt;article&lt;/a&gt; on Functional Data Engineering.The concepts of immutable partitions and dimension snapshots, in particular, have caught my attention. However, I&amp;#39;m trying to figure out how to handle updates in fact events, which may change over their lifecycle.&lt;/p&gt;\n\n&lt;p&gt;Take, for example, a payment transaction. It might be AUTHORISED on DAY 1, then SETTLED on DAY 2, and possibly REFUNDED on DAY 3.According to the principles laid out in the article, I would avoid applying UPDATE operations. But how should I manage these kinds of status changes? Taking a daily snapshot of all the facts is unbearable. And if I decide to update the rows I lose most of the benefits tracked down in the article.And if the creation date is still the same for all the statuses, appending three rows would break the idea of single unit of work, that is, every job is responsible for inserting overwriting only one partition. (the table would be ideally partitioned by creation date)&lt;/p&gt;\n\n&lt;p&gt;Thank you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?auto=webp&amp;s=5b900fdde8f52cea2c5e7a59744fa9d6807d42ca", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a538992e7d8f748ddda7e006ef2ce7ac3d8fbe25", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=04e9ae89fc8101af35cb6a66ed19ba9aee6d7c0f", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=602a102be62cb3c7faa868d8e9f49677f92ecee3", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6cc284b82dece4fb5b4e1e4540d5e1b94cc9612b", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40a2b87bf29a3f9fefc6ac748fefe5424748fbb1", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=292e9d0f71a6c8996194259b487f51ff1ba877c2", "width": 1080, "height": 720}], "variants": {}, "id": "QnHDt9HlR913WRkisNYJkFlw9Lr3Bt7UcxRxAQAkPdY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jnsql", "is_robot_indexable": true, "report_reasons": null, "author": "Anselboero", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jnsql/functional_de_how_to_deal_with_changing_facts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jnsql/functional_de_how_to_deal_with_changing_facts/", "subreddit_subscribers": 146295, "created_utc": 1702719476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you all manage your _source.yaml file in a dbt project? The way we implement those can get unruly really quick. \n\nSo far we have four staging tables in sources.yaml and its up to 1,500 lines.\n\nIm not really sold into the whole \u201cinfrastructure as yaml files\u201d deal yet because of unruly data like this, but I am super open to ideas on how to manage it in a cleaner way.\n\nThanks!", "author_fullname": "t2_u59p0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing _source.yaml in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jedfl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702685610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you all manage your _source.yaml file in a dbt project? The way we implement those can get unruly really quick. &lt;/p&gt;\n\n&lt;p&gt;So far we have four staging tables in sources.yaml and its up to 1,500 lines.&lt;/p&gt;\n\n&lt;p&gt;Im not really sold into the whole \u201cinfrastructure as yaml files\u201d deal yet because of unruly data like this, but I am super open to ideas on how to manage it in a cleaner way.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jedfl", "is_robot_indexable": true, "report_reasons": null, "author": "fleegz2007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jedfl/managing_sourceyaml_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jedfl/managing_sourceyaml_in_dbt/", "subreddit_subscribers": 146295, "created_utc": 1702685610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data warehouse is structured like this: \n\n1) we have a number of integrations in stitch that E&amp;L data from various sources like our ERP, CRM, other databases into our data warehouse. Each of these integrations has its own schedule. \n\n2) we have a \"first layer\" transform dbt project that takes the \"raw\" data landed by these stitch integrations and does some light transformy stuff like friendly renaming, timezone standardization, even denormalization in some rare cases. \n\n3) then we have some downstream dbt projects that prepare analytics reports for the business. The first layer project and analytics projects are directly coupled. Meaning we have a singular data warehouse schedule that runs the first layer project and then immediately starts execution of the analytics projects after the first task finishes. I believe my predecessors were planning to decouple these but never got around to it and we have been operating like this for a couple of years now. \n\nI am exploring ways to decouple these projects for a handful of reasons including shortening the duration of the main data warehouse refresh job so that we can increase its frequency, moving to a sort of event-based architecture where only do something when it is needed and not any more often (we are currently running models in this first layer more frequently than stitch loads data in some cases), etc. \n\nStitch has [post-load hooks](https://www.stitchdata.com/docs/developers/webhooks/post-load-webhooks) where an API call is made after a successful load, we could build a service that takes these calls and fires tasks to run the specific model corresponding to the data that was just loaded. \n\nIssues I see are: \n\n1) naming conventions. It's easy for a human with a basic understanding of our codebase to determine what data model corresponds with what source table, but harder to do this programmatically. I have not found anything in dbt's manifest documents that would allow us to connect say our `customers.sql` data model with the `customer` table in our erp. \n\n2) I mentioned we do some denormalization, meaning some data models have more than one source. We would either need to build a way to determine if all the sources have loaded before we start the downstream model or we would need to run a model two times if we simply said \"x table load finished -&gt; start model y\" and this might violate referential integrity between the two tables. Timing between this process and our main analytics refresh might sometimes result in \"missing\" data because we were only able to load one source before we started building our downstream reports. \n\n3) stitch would, for better or worse, become our orchestration platform at this point. We could leave the analytics projects running on a fixed schedule, but stitch would trigger the extract, load, and now the transform for these models. Having this all in one place would offer some convenience but I am leaning towards the idea that stitch as our orchestrator is not a good thing since this is not really what stitch is designed to do. \n\n4) complexity. Our system as it is is simple. Moving to an architecture like this would introduce complexity, it would make processes more granular rather than broad \"ok now run everything\" like we have today. More complexity introduces risk from various angles. \n\nMost of this is not specific to stitch. Any tool that we would consider for this purpose would bring the same challenges. But we use stitch today.\n\nAnyway, I am wondering if anyone has any experience with this part of the platform that would be willing to weigh in.", "author_fullname": "t2_6nf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using Stitch's post-load hooks to invoke dbt models in their data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jcohf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702680840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data warehouse is structured like this: &lt;/p&gt;\n\n&lt;p&gt;1) we have a number of integrations in stitch that E&amp;amp;L data from various sources like our ERP, CRM, other databases into our data warehouse. Each of these integrations has its own schedule. &lt;/p&gt;\n\n&lt;p&gt;2) we have a &amp;quot;first layer&amp;quot; transform dbt project that takes the &amp;quot;raw&amp;quot; data landed by these stitch integrations and does some light transformy stuff like friendly renaming, timezone standardization, even denormalization in some rare cases. &lt;/p&gt;\n\n&lt;p&gt;3) then we have some downstream dbt projects that prepare analytics reports for the business. The first layer project and analytics projects are directly coupled. Meaning we have a singular data warehouse schedule that runs the first layer project and then immediately starts execution of the analytics projects after the first task finishes. I believe my predecessors were planning to decouple these but never got around to it and we have been operating like this for a couple of years now. &lt;/p&gt;\n\n&lt;p&gt;I am exploring ways to decouple these projects for a handful of reasons including shortening the duration of the main data warehouse refresh job so that we can increase its frequency, moving to a sort of event-based architecture where only do something when it is needed and not any more often (we are currently running models in this first layer more frequently than stitch loads data in some cases), etc. &lt;/p&gt;\n\n&lt;p&gt;Stitch has &lt;a href=\"https://www.stitchdata.com/docs/developers/webhooks/post-load-webhooks\"&gt;post-load hooks&lt;/a&gt; where an API call is made after a successful load, we could build a service that takes these calls and fires tasks to run the specific model corresponding to the data that was just loaded. &lt;/p&gt;\n\n&lt;p&gt;Issues I see are: &lt;/p&gt;\n\n&lt;p&gt;1) naming conventions. It&amp;#39;s easy for a human with a basic understanding of our codebase to determine what data model corresponds with what source table, but harder to do this programmatically. I have not found anything in dbt&amp;#39;s manifest documents that would allow us to connect say our &lt;code&gt;customers.sql&lt;/code&gt; data model with the &lt;code&gt;customer&lt;/code&gt; table in our erp. &lt;/p&gt;\n\n&lt;p&gt;2) I mentioned we do some denormalization, meaning some data models have more than one source. We would either need to build a way to determine if all the sources have loaded before we start the downstream model or we would need to run a model two times if we simply said &amp;quot;x table load finished -&amp;gt; start model y&amp;quot; and this might violate referential integrity between the two tables. Timing between this process and our main analytics refresh might sometimes result in &amp;quot;missing&amp;quot; data because we were only able to load one source before we started building our downstream reports. &lt;/p&gt;\n\n&lt;p&gt;3) stitch would, for better or worse, become our orchestration platform at this point. We could leave the analytics projects running on a fixed schedule, but stitch would trigger the extract, load, and now the transform for these models. Having this all in one place would offer some convenience but I am leaning towards the idea that stitch as our orchestrator is not a good thing since this is not really what stitch is designed to do. &lt;/p&gt;\n\n&lt;p&gt;4) complexity. Our system as it is is simple. Moving to an architecture like this would introduce complexity, it would make processes more granular rather than broad &amp;quot;ok now run everything&amp;quot; like we have today. More complexity introduces risk from various angles. &lt;/p&gt;\n\n&lt;p&gt;Most of this is not specific to stitch. Any tool that we would consider for this purpose would bring the same challenges. But we use stitch today.&lt;/p&gt;\n\n&lt;p&gt;Anyway, I am wondering if anyone has any experience with this part of the platform that would be willing to weigh in.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jcohf", "is_robot_indexable": true, "report_reasons": null, "author": "radil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jcohf/is_anyone_using_stitchs_postload_hooks_to_invoke/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jcohf/is_anyone_using_stitchs_postload_hooks_to_invoke/", "subreddit_subscribers": 146295, "created_utc": 1702680840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, this might be a basic question but I'm having difficulty filling a new derived column.\n\nI have a column of datatype date I want to populate, let's call this \"combodates\", this is a combination of a two columns, one is full of old dates called OGdates of datatype int, and one is half full of new dates called Newdates datatype date.\n\nI've been trying to build an expression in adf to populate this combodates column with old dates, unless there's a new date in which case populate it using that.\n\nSo far the best I've come up with is changing the datatype of the OGdates column through a derived column to date and putting it into an expression in another derived column:\n\niif(isNull(Newdates), OGdates, Newdates)\n\nI used a derived column to change the datatype because cast was absolutely not playing ball.\n\nThe script I have so far is filling the new combodates column only when there is data in the Newdates column. It's completely ignoring the request to fill the column with OGdates when the Newdates column has no data.\n\nI'm not sure what to do I've been wracking my brain, Google, forums and chatgpt for hours. \n\nFor context this is part of a data flow which upserts into a table if 1==1 \n\nThank you in advance.", "author_fullname": "t2_lovh9cgne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trouble with derived column", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j7386", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702665819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, this might be a basic question but I&amp;#39;m having difficulty filling a new derived column.&lt;/p&gt;\n\n&lt;p&gt;I have a column of datatype date I want to populate, let&amp;#39;s call this &amp;quot;combodates&amp;quot;, this is a combination of a two columns, one is full of old dates called OGdates of datatype int, and one is half full of new dates called Newdates datatype date.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to build an expression in adf to populate this combodates column with old dates, unless there&amp;#39;s a new date in which case populate it using that.&lt;/p&gt;\n\n&lt;p&gt;So far the best I&amp;#39;ve come up with is changing the datatype of the OGdates column through a derived column to date and putting it into an expression in another derived column:&lt;/p&gt;\n\n&lt;p&gt;iif(isNull(Newdates), OGdates, Newdates)&lt;/p&gt;\n\n&lt;p&gt;I used a derived column to change the datatype because cast was absolutely not playing ball.&lt;/p&gt;\n\n&lt;p&gt;The script I have so far is filling the new combodates column only when there is data in the Newdates column. It&amp;#39;s completely ignoring the request to fill the column with OGdates when the Newdates column has no data.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure what to do I&amp;#39;ve been wracking my brain, Google, forums and chatgpt for hours. &lt;/p&gt;\n\n&lt;p&gt;For context this is part of a data flow which upserts into a table if 1==1 &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18j7386", "is_robot_indexable": true, "report_reasons": null, "author": "Two_5536", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j7386/trouble_with_derived_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j7386/trouble_with_derived_column/", "subreddit_subscribers": 146295, "created_utc": 1702665819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently in my final year of undergrad I want to look for a mentor(especially from India) who can help me out in guidance. I want to make my career in data engineer. \nPlease say 'sure' in the comments.", "author_fullname": "t2_9oxqckqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Mentorship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jnc74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702717445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently in my final year of undergrad I want to look for a mentor(especially from India) who can help me out in guidance. I want to make my career in data engineer. \nPlease say &amp;#39;sure&amp;#39; in the comments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18jnc74", "is_robot_indexable": true, "report_reasons": null, "author": "Mean-Pin-8271", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jnc74/data_engineering_mentorship/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jnc74/data_engineering_mentorship/", "subreddit_subscribers": 146295, "created_utc": 1702717445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I've been in DE role for almost a year now. This is my first job after college. Im making 90k base and my monthly check is around $5200. Im paying over $1500 in tax every month. My wife dont work and I have a baby as dependent. Currently living paycheck to paycheck.\nI want to know is this how much I should be taking home in my situation?\nI would love to know if someone been in the same situation.", "author_fullname": "t2_ga1qh5zl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A general question regarding taxes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jmb4m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702712940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;ve been in DE role for almost a year now. This is my first job after college. Im making 90k base and my monthly check is around $5200. Im paying over $1500 in tax every month. My wife dont work and I have a baby as dependent. Currently living paycheck to paycheck.\nI want to know is this how much I should be taking home in my situation?\nI would love to know if someone been in the same situation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jmb4m", "is_robot_indexable": true, "report_reasons": null, "author": "Flat-Departure-7922", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jmb4m/a_general_question_regarding_taxes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jmb4m/a_general_question_regarding_taxes/", "subreddit_subscribers": 146295, "created_utc": 1702712940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, we are trying to ingest loads of data from different data sources like SAP, salesforce,  on-prem SQL DB via direct connection,  onprem DB via API calls etc.. data destination would be Snowflake. Data includes either full or delta load. Our environment is on AWS. \nLet us discuss about tools like Fivetran, airbyte etc from the subject experts. Thanks.", "author_fullname": "t2_eozceps7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j9s97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702673042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, we are trying to ingest loads of data from different data sources like SAP, salesforce,  on-prem SQL DB via direct connection,  onprem DB via API calls etc.. data destination would be Snowflake. Data includes either full or delta load. Our environment is on AWS. \nLet us discuss about tools like Fivetran, airbyte etc from the subject experts. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18j9s97", "is_robot_indexable": true, "report_reasons": null, "author": "Liily_07", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j9s97/data_ingestion_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j9s97/data_ingestion_tools/", "subreddit_subscribers": 146295, "created_utc": 1702673042.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}