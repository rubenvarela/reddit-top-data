{"kind": "Listing", "data": {"after": "t3_18j0v0g", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ev065", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Come on Kingston... Do Better!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18j43eo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 514, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 514, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c0NwfLMauQ7HGfy9jt-wWmeZ_oR2ULC4_qLvazjGWAA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702658017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8taijqobkh6c1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?auto=webp&amp;s=c559e876d037d78e2827367a07269a4edef22942", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71f68c29717332a2a186d09ea132d80b400145a3", "width": 108, "height": 81}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ce2df53814e50b9650050ce6268eddb3f287b52", "width": 216, "height": 162}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9246c0e08e429aee0122accdfbe4fcedee6fa03e", "width": 320, "height": 240}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=56cefba73e05779a5e51f6dc104bf7e0da91817c", "width": 640, "height": 480}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e3464a282d0e36342aa9d5fb3a78d7e51d10c6b", "width": 960, "height": 720}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=969a77d6c9ca3371a70627016739b5f5596849f4", "width": 1080, "height": 810}], "variants": {}, "id": "_CJouJmRgGzuxL0A-uh26EL2IKXlNQyA6ymyZOk2Z8c"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j43eo", "is_robot_indexable": true, "report_reasons": null, "author": "zaca21", "discussion_type": null, "num_comments": 184, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j43eo/come_on_kingston_do_better/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8taijqobkh6c1.jpeg", "subreddit_subscribers": 718392, "created_utc": 1702658017.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My psu killed everything connected to the sata port. I had 5 hdds and 2 of them burned and look like this. Other 2 visually look fine but I haven't tested them. Is there any hope on recovering the data on something this damaged if I can find a replacement pcb?", "author_fullname": "t2_102rqwcb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the chances that the drive will work if I replace the PCB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18jayeq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6dWAGNS7PhqdWmdl9WLQc_OPLgilOomz9yyRMFrZ4jY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702676173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My psu killed everything connected to the sata port. I had 5 hdds and 2 of them burned and look like this. Other 2 visually look fine but I haven&amp;#39;t tested them. Is there any hope on recovering the data on something this damaged if I can find a replacement pcb?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0b74iguf2j6c1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0b74iguf2j6c1.jpeg?auto=webp&amp;s=d915fce3e481ff358db96da13a90ee1bb7199f91", "width": 3468, "height": 4624}, "resolutions": [{"url": "https://preview.redd.it/0b74iguf2j6c1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=046fd803368df8b5a0be192a9772d2b6ed46ea9c", "width": 108, "height": 144}, {"url": "https://preview.redd.it/0b74iguf2j6c1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ffab71c16412fb38a35bca33f13a8a5b3c0e590", "width": 216, "height": 288}, {"url": "https://preview.redd.it/0b74iguf2j6c1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b265c3bf211ecbf26fd159eb314e8d9c2ba2df7d", "width": 320, "height": 426}, {"url": "https://preview.redd.it/0b74iguf2j6c1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=50a773565ddd6b7fe799571be53d01ec7be86c8a", "width": 640, "height": 853}, {"url": "https://preview.redd.it/0b74iguf2j6c1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c87fb4e6e89a5a34aa0e4c3feb4ebb982a0a6017", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/0b74iguf2j6c1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fdd8d315713487d81fa066d57cd803213776eb0", "width": 1080, "height": 1440}], "variants": {}, "id": "jF70aAzOtoGf405iScup3msPshyjGSr6meUbE-dDeEM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jayeq", "is_robot_indexable": true, "report_reasons": null, "author": "EpicSqueaker", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jayeq/what_are_the_chances_that_the_drive_will_work_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0b74iguf2j6c1.jpeg", "subreddit_subscribers": 718392, "created_utc": 1702676173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I give you the [TNASR1](https://imgur.com/a/fduKt1N) \"Tiny NAS with RAID1\" - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5\" SATA HDDs in  RAID1\n\n\n##Table of content\n\n1. Intro\n\n2. Why\n\n3. Goals\n\n4. BOM\n\n5. My Setup \n\n6. Cost\n\n7. Performance\n\n8. Power consumption\n\n9. Setup\n\n10. Scripting with scheduled wakeup to save power\n\n###Intro\n\nSo I have an Unraid Server wich is great and I am really happy with it. It stores all my files including media but also my very important documents as well as my maybe even more important pictures and videos of my family. So I always felt uneasy thinking about its safety. It's a consumer grade tower PC located in my garage. I use two parity drives so on a system level I have some redundancy. Nothing wrong with that but if something happens to my server as a whole (fire/flooding/kids) all my data is gone.\n\nOf course there is the possibility if using a cloud storage provider. Google drive is $12 per month or $144 per year. First of all I would be trusting a third party with the safety of my data and secondly I am cheap bastard. \n\nThis is the genesis of the TNASR1.\n\nIt lives in my garden shed in a watertight container and does everything a google drive would.\n\n\n###Why\n\n- Data Backup needs - common sense i.e. the [3 2 1 Rule](https://en.wikipedia.org/wiki/Glossary_of_backup_terms#Terms_and_definitions) demands a remote copy.\n\n- Control - cloud providers are a third party that require a level of trust and cost money above certain storage needs\n\n- Cost - cheaper in the long run\n\n\n###Goals\n\n- Cheap\n\n- Low Power\n\n- enough storage for the most important files. At least 500gb+\n\n- RAID1 for some fault resilience\n\n###BOM\n\n- Raspberry Pi Zero 2 W\n\n- a small heat sink\n\n- thermal paste\n\n- microSD card with at least 2gb\n\n- USB OTG Hub Host Cable\n\n- A 2A/**5.3V** - Power supply\n\n- USB A to Micro USB B cable\n\n- 2x 2.5inch SATA Case \n\n- 2x SATA drives\n\n###My Setup\n\n\n- Raspberry Pi Zero 2 W  - \u20ac19.80\n\n- The heat sink and thermal paste i had laying around. But I guess \u20ac1 is fair.\n\n- INTENSO 3413460 - MicroSDHC-Card with 8GB, Intenso Class 10 - \u20ac3.40\n\n- USB OTG Hub Host Cable. Ali express item:3256805033322631 \u20ac1.40\n\n- Samsung EP-TA10EWE Power supply.\n\n- 2x 2.5inch SATA Case. Ali express item: 3256805261700001  - \u20ac1.00\n\n- For storage I used  2x 2TB Toshiba L200 bulk HDWL120UZSVA - \u20ac65.00 each\n\n###Cost\n\nNAS cost before storage: \u20ac34.60 or around $37.20\n\nIn total ~ \u20ac164 or $178 for 2 Terabyte of remote RAID1 storage. Not bad if I say so myself. ROI vs. google Drive ($144 per year for 2 terabyte) in 15 months. \n\nCost for electricity (more down below) is about $/\u20ac20 a year if you let it run continuously with $/\u20ac0.3 per kWh. ROI vs Google Drive under 17 months.\n\n###Performance\n\nSo the two SATA drives in RAID1 are connected over a single USB 2.0 interface. Suffice to say you won't get SSD speeds. But I am Happy to report that it is quite fast enough for our needs. I get around 2MB/s write and 4MB/s of read. That means I can sync 1 gigabyte of data in under 9 Minutes. Now this is nothing to write home about but we have to consider context. This for a remote backup that uses less than 10w. Also after initial setup this is for syncing the diff only. I tend to sync less than 5 gigabyte of new data a week. That means backup takes under an hour.\n\nFor my initial sync of my files it took 8 hours for 48 Gigabytes with a transfer speed of about  1.8 MB/s\n\n    Transferred:   \t   51.312 GiB / 51.312 GiB, 100%, 1,852.5 KiB/s, ETA 0s\n    Checks:             71697 / 71697, 100%\n    Deleted:               11 (files), 0 (dirs)\n    Transferred:        70846 / 70846, 100%\n    Elapsed time:    8h4m34.3s\n\nMy initial sync of my pictures and videos of 435 Gigabytes took 2 days and 9 hours with an average transfer speed of 2.2 MB/s\n\n    Transferred:   \t  455.358 GiB / 455.358 GiB, 100%, 2.316 MiB/s, ETA 0s\n    Checks:            111442 / 111442, 100%\n    Deleted:               11 (files), 0 (dirs)\n    Transferred:       111478 / 111478, 100%\n    Elapsed time:  2d9h13m13.9s\n\nA sync run without anything to sync takes under 2 Minutes.\n\n    Rclone sync completed in 0 hours, 1 minutes, and 45 seconds 390 milliseconds.\n\n###Power Consumption\n\nThe star of the show is undoubtedly the 5.3V power supply. Any power supply with just 5V, even ones that can deliver 100W, failed the boot up during the high ramp up power spike from the HDDs. Te whole system is teetering on being power starved. But it was up and running and syncing my 500 gigabytes without any issues.\n\n[The whole setup draws continuous 5.3V 1.5A or 8W max during a sync.](https://imgur.com/PANKpo5) \n\nI am sure when the drives spin up the power draw spikes up above 1.5A but as the supply has more than 5V it does not cause any issues.\n\n[At Idle the power draw is about 0.6A or about 3W.](https://imgur.com/c1CnIv7)\n\nThere are some Samsung Power Supplies that have 5.3V. \n\n[Alternatively there are AC/DC adapters like these.](https://www.voc-electronics.com/a-42425080/power-batteries-adapters/5-3v-2a-power-supply-eu/#description)\n\n\n\n###Setup\n\n1. Setup the MicroSD with Raspberry Pi Imager. \n\n  1.1 Choose Raspberry Pi OS (other)\n\n  1.2 Chose Raspberry Pi OS lite (64-bit)\n\n  1.3 Set Up WiFi and turn on SSH\n\n  1.4 Burn Image\n\n2. Check if everything is running\n\n  2.1 Insert microSD \n\n  2.2 Connect Power Supply\n\n  2.3 Ping device\n\n  2.4 If successful try to SSH into the pi with : SSH user@nameOfPi or: SSH user@IPaddrOfPi\n\n  2.5 Power down and disconnect power supply\n\n3. [Setup the drives](https://www.computernetworkingnotes.com/linux-tutorials/how-to-configure-raid-in-linux-step-by-step-guide.html)\n\n  3.1 Connect drives via the USB hub dongle\n\n  3.2 Reconnect Power Supply\n\n  3.3 SSH back into the Pi\n\n  3.4 Type in:  \n\n        lsblk  \n\n  This should confirm two things. The drives are connected and have the right size. You should have an output like this:\n\n\n    \n        sda           8:16   0  1.8T  0 disk \n        sdb           8:16   0  1.8T  0 disk\n        mmcblk0     179:0    0  7.5G  0 disk \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n\n  3.5 Type in: \n\n        sudo fdisk /dev/sda\n\n  new drive\n\n        n\n\n  primary\n\n        p\n  \n  list\n\n        l\n\n  type\n\n        t\n\n  Linux raid auto\n\n        fd\n\n\n  write\n\n        w\n\n  3.6 repeat for sdb\n\n  3.7 [Partprobe](https://www.computerhope.com/unix/partprob.htm)\n\n        partprobe\n\n  3.8 Check if sda1 and sdb1 are listed\n\n        lsblk\n\n  output should look like this\n\n        NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n        sda           8:0    0  1.8T  0 disk \n        \u2514\u2500sda1        8:1    0  1.8T  0 part \n        sdb           8:16   0  1.8T  0 disk \n        \u2514\u2500sdb1        8:17   0  1.8T  0 part \n        mmcblk0     179:0    0  7.5G  0 disk \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n        \n  3.9 Check if the file system is correct\n\n        sudo fdisk -l\n\n  the output should include the type \"Linux raid auto\"\n\n        Device     Boot Start        End    Sectors  Size Id Type\n        /dev/sda1        2048 3907029167 3907027120  1.8T fd Linux raid autodetect\n \n\n4. [Setup raid](https://www.thetechedvocate.org/how-to-set-up-raid-1-on-the-raspberry-pi-the-easy-way/)\n\n  4.1 Install mdadm\n\n        sudo apt-get install mdadm\n\n  4.2 create raid array\n\n        sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1\n\n  Continue creating array?\n\n        y\n\n  4.3 check if md0 was created\n\n        lsblk\n\n  the output should look like this\n\n        NAME        MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT\n        sda           8:0    0  1.8T  0 disk  \n        \u2514\u2500sda1        8:1    0  1.8T  0 part  \n          \u2514\u2500md0       9:0    0  1.8T  0 raid1 \n        sdb           8:16   0  1.8T  0 disk  \n        \u2514\u2500sdb1        8:17   0  1.8T  0 part  \n          \u2514\u2500md0       9:0    0  1.8T  0 raid1 \n        mmcblk0     179:0    0  7.5G  0 disk  \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part  /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part  /\n\n5. Format the raid array / make filesystem\n\n  5.1 Type in\n\n        sudo mkfs.ext4 /dev/md0\n\n6. [Mount to folder](https://www.youtube.com/watch?v=Ff96FPJHq5o)\n\n  6.1 Make folder under mnt\n\n        sudo mkdir /mnt/storage\n\n  6.2 Mount md0 to storage\n\n        sudo mount /dev/md0 /mnt/storage\n\n\n  6.6. mount on startup\n\n        sudo nano /etc/fstab\n\n  6.7 add line\n\n        /dev/md0 /mnt/storage ext4 defaults 0 0\n\n  6.8 Save and exit\n\n  6.9 reboot\n\n        reboot\n\n\n  6.4 check if mount is successfull\n        \n        df -alh\n\n        \n  output should look like this\n\n        Filesystem      Size  Used Avail Use% Mounted on\n        /dev/root       7.1G  1.7G  5.2G  25% /\n        devtmpfs         80M     0   80M   0% /dev\n        proc               0     0     0    - /proc\n        sysfs              0     0     0    - /sys\n        securityfs         0     0     0    - /sys/kernel/security\n        tmpfs           210M     0  210M   0% /dev/shm\n        devpts             0     0     0    - /dev/pts\n        tmpfs            84M  3.0M   81M   4% /run\n        tmpfs           5.0M  4.0K  5.0M   1% /run/lock\n        cgroup2            0     0     0    - /sys/fs/cgroup\n        pstore             0     0     0    - /sys/fs/pstore\n        bpf                0     0     0    - /sys/fs/bpf\n        systemd-1          0     0     0    - /proc/sys/fs/binfmt_misc\n        mqueue             0     0     0    - /dev/mqueue\n        debugfs            0     0     0    - /sys/kernel/debug\n        sunrpc             0     0     0    - /run/rpc_pipefs\n        tracefs            0     0     0    - /sys/kernel/tracing\n        configfs           0     0     0    - /sys/kernel/config\n        fusectl            0     0     0    - /sys/fs/fuse/connections\n        /dev/mmcblk0p1  255M   31M  225M  13% /boot\n        tmpfs            42M     0   42M   0% /run/user/1000\n        /dev/md0        1.8T   28K  1.7T   1% /mnt/storage\n        \n7. enable SMB sharing\n\n  7.1 install samba\n\n        sudo apt-get samba\n\n  7.2 edit samba config\n\n        sudo nano /etc/samba/smb.conf\n\n  7.3 add this at the end\n\n        [storage]\n        path=/mnt/storage\n        writeable=yes\n        reate mask=0666\n        directorty mask=0666\n        public=yes\n\n  7.4 Save end exit\n\n  7.5 restart samba service\n\n        sudo systemctl restart smbd\n\n  7.6 ad a user to samba\n\n        sudo smbpasswd -a yourDesiredUsername\n\n  7.7 Set a user password\n\nThat is it you are done. See if you can find your folder in the network on your windows machine.\n\n\n###Scripting\n\nOn my unraid server I have this neat little script where I use mosquitto and nodered to turn on and off a shelly socket to save power. The script is on a weekly schedule. In Nodered I also send myself a telegram message once the script starts, When the PI is found, and when its done including the time elapsed and the amount of data synced.\n\n\n\n    #!/bin/bash\n    \n    \n    \n    # Record the start time\n    start_timeTotal=$(date +%s%3N)\n    \n    # Set your Raspberry Pi's IP address\n    RASPBERRY_PI_IP=\"192.168.8.107\"\n    \n    \n    # Set your Mosquitto container name or ID\n    CONTAINER_NAME=\"mosquitto\"\n    \n    \n    \n    # Set MQTT details\n    HOST=\"localhost\"  # Use localhost because we're inside the container\n    PORT=1883         # Specify the Mosquitto broker port\n    TOPIC=\"Backup\"    # Specify the topic you want to publish to\n    \n    \n    # Execute mosquitto_pub inside the container\n    #docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Starting\"\n    \n    \n    \n    \n    for _ in {1..12}; do\n        if ping -c 1 \"$RASPBERRY_PI_IP\" &amp;&gt; /dev/null; then\n            echo \"Raspberry Pi is up!\"\n            break\n        else\n            sleep 5  # Wait 5 seconds before checking again\n        fi\n    done\n    \n    # If Raspberry Pi is still not up after 1 minute, exit\n    if ! ping -c 1 \"$RASPBERRY_PI_IP\" &amp;&gt; /dev/null; then\n        docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Raspberry Pi did not start up within 1 minute. Exiting.\"\n        exit 1\n    fi\n    \n    \n    # Record the start time\n    start_timeFiles=$(date +%s%3N)\n    \n    # Start Rclone sync (adjust paths and remote as needed)\n    rclone sync /mnt/user/files backupPI:storage/files -v\n    \n    # Record the end time of files\n    end_timeFiles=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timeFiles - start_timeFiles))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Rclone sync files completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    \n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Sync Files done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n    # Record the start time pictures\n    start_timePictures=$(date +%s%3N)\n    \n    \n    # Start Rclone sync (adjust paths and remote as needed)\n    rclone sync /mnt/user/pictures backupPI:storage/pictures -v\n    \n    \n    \n    # Record the end time\n    end_timePictures=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timePictures - start_timePictures))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Rclone sync Pictures completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    \n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Sync Pictures done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n    # Record the end time\n    end_timeTotal=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timeTotal - start_timeTotal))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Complete sync completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Complete sync completed and was done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n\n\n\nI hope this is useful to some of you. I will amend and edit this post with your feedback and keep it alive as long as I can. Have a great weekend!", "author_fullname": "t2_fbw2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[TNASR1] - Tiny NAS with RAID1 - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5\" SATA HDDs in RAID1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j130b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup | Guide/How-to | Scripts/Software", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702649808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I give you the &lt;a href=\"https://imgur.com/a/fduKt1N\"&gt;TNASR1&lt;/a&gt; &amp;quot;Tiny NAS with RAID1&amp;quot; - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5&amp;quot; SATA HDDs in  RAID1&lt;/p&gt;\n\n&lt;h2&gt;Table of content&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Intro&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Why&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Goals&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;BOM&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;My Setup &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cost&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Performance&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Power consumption&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Setup&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scripting with scheduled wakeup to save power&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Intro&lt;/h3&gt;\n\n&lt;p&gt;So I have an Unraid Server wich is great and I am really happy with it. It stores all my files including media but also my very important documents as well as my maybe even more important pictures and videos of my family. So I always felt uneasy thinking about its safety. It&amp;#39;s a consumer grade tower PC located in my garage. I use two parity drives so on a system level I have some redundancy. Nothing wrong with that but if something happens to my server as a whole (fire/flooding/kids) all my data is gone.&lt;/p&gt;\n\n&lt;p&gt;Of course there is the possibility if using a cloud storage provider. Google drive is $12 per month or $144 per year. First of all I would be trusting a third party with the safety of my data and secondly I am cheap bastard. &lt;/p&gt;\n\n&lt;p&gt;This is the genesis of the TNASR1.&lt;/p&gt;\n\n&lt;p&gt;It lives in my garden shed in a watertight container and does everything a google drive would.&lt;/p&gt;\n\n&lt;h3&gt;Why&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Data Backup needs - common sense i.e. the &lt;a href=\"https://en.wikipedia.org/wiki/Glossary_of_backup_terms#Terms_and_definitions\"&gt;3 2 1 Rule&lt;/a&gt; demands a remote copy.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Control - cloud providers are a third party that require a level of trust and cost money above certain storage needs&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cost - cheaper in the long run&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Goals&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Cheap&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Low Power&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;enough storage for the most important files. At least 500gb+&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;RAID1 for some fault resilience&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;BOM&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Raspberry Pi Zero 2 W&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;a small heat sink&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;thermal paste&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;microSD card with at least 2gb&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB OTG Hub Host Cable&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A 2A/&lt;strong&gt;5.3V&lt;/strong&gt; - Power supply&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB A to Micro USB B cable&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x 2.5inch SATA Case &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x SATA drives&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;My Setup&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Raspberry Pi Zero 2 W  - \u20ac19.80&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The heat sink and thermal paste i had laying around. But I guess \u20ac1 is fair.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;INTENSO 3413460 - MicroSDHC-Card with 8GB, Intenso Class 10 - \u20ac3.40&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB OTG Hub Host Cable. Ali express item:3256805033322631 \u20ac1.40&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Samsung EP-TA10EWE Power supply.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x 2.5inch SATA Case. Ali express item: 3256805261700001  - \u20ac1.00&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For storage I used  2x 2TB Toshiba L200 bulk HDWL120UZSVA - \u20ac65.00 each&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Cost&lt;/h3&gt;\n\n&lt;p&gt;NAS cost before storage: \u20ac34.60 or around $37.20&lt;/p&gt;\n\n&lt;p&gt;In total ~ \u20ac164 or $178 for 2 Terabyte of remote RAID1 storage. Not bad if I say so myself. ROI vs. google Drive ($144 per year for 2 terabyte) in 15 months. &lt;/p&gt;\n\n&lt;p&gt;Cost for electricity (more down below) is about $/\u20ac20 a year if you let it run continuously with $/\u20ac0.3 per kWh. ROI vs Google Drive under 17 months.&lt;/p&gt;\n\n&lt;h3&gt;Performance&lt;/h3&gt;\n\n&lt;p&gt;So the two SATA drives in RAID1 are connected over a single USB 2.0 interface. Suffice to say you won&amp;#39;t get SSD speeds. But I am Happy to report that it is quite fast enough for our needs. I get around 2MB/s write and 4MB/s of read. That means I can sync 1 gigabyte of data in under 9 Minutes. Now this is nothing to write home about but we have to consider context. This for a remote backup that uses less than 10w. Also after initial setup this is for syncing the diff only. I tend to sync less than 5 gigabyte of new data a week. That means backup takes under an hour.&lt;/p&gt;\n\n&lt;p&gt;For my initial sync of my files it took 8 hours for 48 Gigabytes with a transfer speed of about  1.8 MB/s&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Transferred:       51.312 GiB / 51.312 GiB, 100%, 1,852.5 KiB/s, ETA 0s\nChecks:             71697 / 71697, 100%\nDeleted:               11 (files), 0 (dirs)\nTransferred:        70846 / 70846, 100%\nElapsed time:    8h4m34.3s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My initial sync of my pictures and videos of 435 Gigabytes took 2 days and 9 hours with an average transfer speed of 2.2 MB/s&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Transferred:      455.358 GiB / 455.358 GiB, 100%, 2.316 MiB/s, ETA 0s\nChecks:            111442 / 111442, 100%\nDeleted:               11 (files), 0 (dirs)\nTransferred:       111478 / 111478, 100%\nElapsed time:  2d9h13m13.9s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;A sync run without anything to sync takes under 2 Minutes.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Rclone sync completed in 0 hours, 1 minutes, and 45 seconds 390 milliseconds.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Power Consumption&lt;/h3&gt;\n\n&lt;p&gt;The star of the show is undoubtedly the 5.3V power supply. Any power supply with just 5V, even ones that can deliver 100W, failed the boot up during the high ramp up power spike from the HDDs. Te whole system is teetering on being power starved. But it was up and running and syncing my 500 gigabytes without any issues.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/PANKpo5\"&gt;The whole setup draws continuous 5.3V 1.5A or 8W max during a sync.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I am sure when the drives spin up the power draw spikes up above 1.5A but as the supply has more than 5V it does not cause any issues.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/c1CnIv7\"&gt;At Idle the power draw is about 0.6A or about 3W.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are some Samsung Power Supplies that have 5.3V. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.voc-electronics.com/a-42425080/power-batteries-adapters/5-3v-2a-power-supply-eu/#description\"&gt;Alternatively there are AC/DC adapters like these.&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Setup&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Setup the MicroSD with Raspberry Pi Imager. &lt;/p&gt;\n\n&lt;p&gt;1.1 Choose Raspberry Pi OS (other)&lt;/p&gt;\n\n&lt;p&gt;1.2 Chose Raspberry Pi OS lite (64-bit)&lt;/p&gt;\n\n&lt;p&gt;1.3 Set Up WiFi and turn on SSH&lt;/p&gt;\n\n&lt;p&gt;1.4 Burn Image&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Check if everything is running&lt;/p&gt;\n\n&lt;p&gt;2.1 Insert microSD &lt;/p&gt;\n\n&lt;p&gt;2.2 Connect Power Supply&lt;/p&gt;\n\n&lt;p&gt;2.3 Ping device&lt;/p&gt;\n\n&lt;p&gt;2.4 If successful try to SSH into the pi with : SSH user@nameOfPi or: SSH user@IPaddrOfPi&lt;/p&gt;\n\n&lt;p&gt;2.5 Power down and disconnect power supply&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.computernetworkingnotes.com/linux-tutorials/how-to-configure-raid-in-linux-step-by-step-guide.html\"&gt;Setup the drives&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;3.1 Connect drives via the USB hub dongle&lt;/p&gt;\n\n&lt;p&gt;3.2 Reconnect Power Supply&lt;/p&gt;\n\n&lt;p&gt;3.3 SSH back into the Pi&lt;/p&gt;\n\n&lt;p&gt;3.4 Type in:  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This should confirm two things. The drives are connected and have the right size. You should have an output like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sda           8:16   0  1.8T  0 disk \nsdb           8:16   0  1.8T  0 disk\nmmcblk0     179:0    0  7.5G  0 disk \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.5 Type in: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo fdisk /dev/sda\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;new drive&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;n\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;primary&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;p\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;list&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;l\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;type&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;t\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Linux raid auto&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;fd\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;write&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;w\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.6 repeat for sdb&lt;/p&gt;\n\n&lt;p&gt;3.7 &lt;a href=\"https://www.computerhope.com/unix/partprob.htm\"&gt;Partprobe&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;partprobe\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.8 Check if sda1 and sdb1 are listed&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsda           8:0    0  1.8T  0 disk \n\u2514\u2500sda1        8:1    0  1.8T  0 part \nsdb           8:16   0  1.8T  0 disk \n\u2514\u2500sdb1        8:17   0  1.8T  0 part \nmmcblk0     179:0    0  7.5G  0 disk \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.9 Check if the file system is correct&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo fdisk -l\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the output should include the type &amp;quot;Linux raid auto&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Device     Boot Start        End    Sectors  Size Id Type\n/dev/sda1        2048 3907029167 3907027120  1.8T fd Linux raid autodetect\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.thetechedvocate.org/how-to-set-up-raid-1-on-the-raspberry-pi-the-easy-way/\"&gt;Setup raid&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;4.1 Install mdadm&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo apt-get install mdadm\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;4.2 create raid array&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Continue creating array?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;y\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;4.3 check if md0 was created&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;NAME        MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT\nsda           8:0    0  1.8T  0 disk  \n\u2514\u2500sda1        8:1    0  1.8T  0 part  \n  \u2514\u2500md0       9:0    0  1.8T  0 raid1 \nsdb           8:16   0  1.8T  0 disk  \n\u2514\u2500sdb1        8:17   0  1.8T  0 part  \n  \u2514\u2500md0       9:0    0  1.8T  0 raid1 \nmmcblk0     179:0    0  7.5G  0 disk  \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part  /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part  /\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Format the raid array / make filesystem&lt;/p&gt;\n\n&lt;p&gt;5.1 Type in&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mkfs.ext4 /dev/md0\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=Ff96FPJHq5o\"&gt;Mount to folder&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;6.1 Make folder under mnt&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mkdir /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.2 Mount md0 to storage&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mount /dev/md0 /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.6. mount on startup&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo nano /etc/fstab\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.7 add line&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/dev/md0 /mnt/storage ext4 defaults 0 0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.8 Save and exit&lt;/p&gt;\n\n&lt;p&gt;6.9 reboot&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;reboot\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.4 check if mount is successfull&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df -alh\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Filesystem      Size  Used Avail Use% Mounted on\n/dev/root       7.1G  1.7G  5.2G  25% /\ndevtmpfs         80M     0   80M   0% /dev\nproc               0     0     0    - /proc\nsysfs              0     0     0    - /sys\nsecurityfs         0     0     0    - /sys/kernel/security\ntmpfs           210M     0  210M   0% /dev/shm\ndevpts             0     0     0    - /dev/pts\ntmpfs            84M  3.0M   81M   4% /run\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ncgroup2            0     0     0    - /sys/fs/cgroup\npstore             0     0     0    - /sys/fs/pstore\nbpf                0     0     0    - /sys/fs/bpf\nsystemd-1          0     0     0    - /proc/sys/fs/binfmt_misc\nmqueue             0     0     0    - /dev/mqueue\ndebugfs            0     0     0    - /sys/kernel/debug\nsunrpc             0     0     0    - /run/rpc_pipefs\ntracefs            0     0     0    - /sys/kernel/tracing\nconfigfs           0     0     0    - /sys/kernel/config\nfusectl            0     0     0    - /sys/fs/fuse/connections\n/dev/mmcblk0p1  255M   31M  225M  13% /boot\ntmpfs            42M     0   42M   0% /run/user/1000\n/dev/md0        1.8T   28K  1.7T   1% /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;enable SMB sharing&lt;/p&gt;\n\n&lt;p&gt;7.1 install samba&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo apt-get samba\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.2 edit samba config&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo nano /etc/samba/smb.conf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.3 add this at the end&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[storage]\npath=/mnt/storage\nwriteable=yes\nreate mask=0666\ndirectorty mask=0666\npublic=yes\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.4 Save end exit&lt;/p&gt;\n\n&lt;p&gt;7.5 restart samba service&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo systemctl restart smbd\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.6 ad a user to samba&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo smbpasswd -a yourDesiredUsername\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.7 Set a user password&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That is it you are done. See if you can find your folder in the network on your windows machine.&lt;/p&gt;\n\n&lt;h3&gt;Scripting&lt;/h3&gt;\n\n&lt;p&gt;On my unraid server I have this neat little script where I use mosquitto and nodered to turn on and off a shelly socket to save power. The script is on a weekly schedule. In Nodered I also send myself a telegram message once the script starts, When the PI is found, and when its done including the time elapsed and the amount of data synced.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\n\n\n\n# Record the start time\nstart_timeTotal=$(date +%s%3N)\n\n# Set your Raspberry Pi&amp;#39;s IP address\nRASPBERRY_PI_IP=&amp;quot;192.168.8.107&amp;quot;\n\n\n# Set your Mosquitto container name or ID\nCONTAINER_NAME=&amp;quot;mosquitto&amp;quot;\n\n\n\n# Set MQTT details\nHOST=&amp;quot;localhost&amp;quot;  # Use localhost because we&amp;#39;re inside the container\nPORT=1883         # Specify the Mosquitto broker port\nTOPIC=&amp;quot;Backup&amp;quot;    # Specify the topic you want to publish to\n\n\n# Execute mosquitto_pub inside the container\n#docker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Starting&amp;quot;\n\n\n\n\nfor _ in {1..12}; do\n    if ping -c 1 &amp;quot;$RASPBERRY_PI_IP&amp;quot; &amp;amp;&amp;gt; /dev/null; then\n        echo &amp;quot;Raspberry Pi is up!&amp;quot;\n        break\n    else\n        sleep 5  # Wait 5 seconds before checking again\n    fi\ndone\n\n# If Raspberry Pi is still not up after 1 minute, exit\nif ! ping -c 1 &amp;quot;$RASPBERRY_PI_IP&amp;quot; &amp;amp;&amp;gt; /dev/null; then\n    docker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Raspberry Pi did not start up within 1 minute. Exiting.&amp;quot;\n    exit 1\nfi\n\n\n# Record the start time\nstart_timeFiles=$(date +%s%3N)\n\n# Start Rclone sync (adjust paths and remote as needed)\nrclone sync /mnt/user/files backupPI:storage/files -v\n\n# Record the end time of files\nend_timeFiles=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timeFiles - start_timeFiles))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Rclone sync files completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Sync Files done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n\n# Record the start time pictures\nstart_timePictures=$(date +%s%3N)\n\n\n# Start Rclone sync (adjust paths and remote as needed)\nrclone sync /mnt/user/pictures backupPI:storage/pictures -v\n\n\n\n# Record the end time\nend_timePictures=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timePictures - start_timePictures))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Rclone sync Pictures completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Sync Pictures done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n\n# Record the end time\nend_timeTotal=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timeTotal - start_timeTotal))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Complete sync completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Complete sync completed and was done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I hope this is useful to some of you. I will amend and edit this post with your feedback and keep it alive as long as I can. Have a great weekend!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?auto=webp&amp;s=1b8a86e76338b6c56926fad15933014e95c3842f", "width": 1500, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89bdfc939cafb8391d5e61a9567b4d4824284fcc", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86e77b7a1e8adf26f55b29177d43a792eaf05079", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=79eb0d4e1427b28536b1113953431a110410e80f", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=815fb17aab37a32dcd7821a0b443af7e57d6e5e9", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=240c63ed78cf374d220221b681b49faf1cf9a1e2", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2df389b2db307451f66469500dbffb7465972360", "width": 1080, "height": 1440}], "variants": {}, "id": "bMThnKntPA0wspAl37RZB5eagDAmk4PAnyYetytPxlU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j130b", "is_robot_indexable": true, "report_reasons": null, "author": "ElementII5", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j130b/tnasr1_tiny_nas_with_raid1_the_cheapest_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j130b/tnasr1_tiny_nas_with_raid1_the_cheapest_and/", "subreddit_subscribers": 718392, "created_utc": 1702649808.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a bunch of old VHS videos I'm wanting to digitise. I purchased this used [VHS player](https://www.ebay.com.au/itm/266552319384) (an LG V181 as the price was good) and [this capture card](https://www.ebay.com.au/itm/175982263794). I understand the card is cheap and may not be the best quality (the same could be said for the VHS player). I thought I'd trial first and see how we go. \n\nI've made a rookie error, because even with the card I have no additional cord to connect the capture card to the VHS player. The VHS player also doesn't have a scart plug (I don't know if that's already ended my chances of this not working). \n\nI'm hoping to get some advice to rectify the situation. In order to connect the capture card to the VHS player correctly I'm looking at the cord option below. It might connect to the two, but don't know if that will technically do anything. \n\n [3 x RCA Piggyback Plugs to 3 RCA Plugs - 1.5m | Jaycar Electronics](https://www.jaycar.com.au/3-x-rca-piggyback-plugs-to-3-rca-plugs-1-5m/p/WV7324) \n\nIf a cable to connect to the two is all I need and the above one is wrong, if there's any other cables [in their store](https://www.jaycar.com.au/cables-connectors/audio-video-cables/rca-av-cables/c/1AD?q=%3Apopularity-desc%3AsshomeAVLEAD-avleadTYPESIG%3AComposite%2BVideo%2Band%2BAudio&amp;text=#) that would work, please let me know. If this is the wrong approach and I need another piece of equipment, please also let me know.\n\nAny help would be greatly appreciated. \n\n&amp;#x200B;", "author_fullname": "t2_ezv1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Purchased used VCR and VHS converter card - Need advice on next steps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iwyx0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702635496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of old VHS videos I&amp;#39;m wanting to digitise. I purchased this used &lt;a href=\"https://www.ebay.com.au/itm/266552319384\"&gt;VHS player&lt;/a&gt; (an LG V181 as the price was good) and &lt;a href=\"https://www.ebay.com.au/itm/175982263794\"&gt;this capture card&lt;/a&gt;. I understand the card is cheap and may not be the best quality (the same could be said for the VHS player). I thought I&amp;#39;d trial first and see how we go. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made a rookie error, because even with the card I have no additional cord to connect the capture card to the VHS player. The VHS player also doesn&amp;#39;t have a scart plug (I don&amp;#39;t know if that&amp;#39;s already ended my chances of this not working). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping to get some advice to rectify the situation. In order to connect the capture card to the VHS player correctly I&amp;#39;m looking at the cord option below. It might connect to the two, but don&amp;#39;t know if that will technically do anything. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.jaycar.com.au/3-x-rca-piggyback-plugs-to-3-rca-plugs-1-5m/p/WV7324\"&gt;3 x RCA Piggyback Plugs to 3 RCA Plugs - 1.5m | Jaycar Electronics&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If a cable to connect to the two is all I need and the above one is wrong, if there&amp;#39;s any other cables &lt;a href=\"https://www.jaycar.com.au/cables-connectors/audio-video-cables/rca-av-cables/c/1AD?q=%3Apopularity-desc%3AsshomeAVLEAD-avleadTYPESIG%3AComposite%2BVideo%2Band%2BAudio&amp;amp;text=#\"&gt;in their store&lt;/a&gt; that would work, please let me know. If this is the wrong approach and I need another piece of equipment, please also let me know.&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dK-vPSUxnCDa9AQ3CJrlLRyGaKeHHI_zrKRuGo6GBIA.jpg?auto=webp&amp;s=5057ef2f3a93cc0fa1374b2daff196167248ae9e", "width": 160, "height": 159}, "resolutions": [{"url": "https://external-preview.redd.it/dK-vPSUxnCDa9AQ3CJrlLRyGaKeHHI_zrKRuGo6GBIA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e032bf4c46820210d06c55050551f54ecf7dfb7a", "width": 108, "height": 107}], "variants": {}, "id": "sTudcUIITL1ED_TEpskZf2L45uLN0C0Oir49ptIzTrE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iwyx0", "is_robot_indexable": true, "report_reasons": null, "author": "jtoml3", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iwyx0/purchased_used_vcr_and_vhs_converter_card_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iwyx0/purchased_used_vcr_and_vhs_converter_card_need/", "subreddit_subscribers": 718392, "created_utc": 1702635496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My company currently has a rather large (maybe not too large by some of your standards) dataset of images of handwritten documents totalling just over 10TB, with more to come in future. These images are very high resolution, but we don't want to reduce their size as we want to retain all of the information. Due to the nature of the images there is a large reduction in file size when converting from BMP to PNG, and so my question is whether there are any drawbacks to using PNG over BMP in this scenario, or more generally why would one want to use BMP over PNG for anything? ", "author_fullname": "t2_65dpfzvg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reason to use BMP over PNG?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j3gmq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702656294.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company currently has a rather large (maybe not too large by some of your standards) dataset of images of handwritten documents totalling just over 10TB, with more to come in future. These images are very high resolution, but we don&amp;#39;t want to reduce their size as we want to retain all of the information. Due to the nature of the images there is a large reduction in file size when converting from BMP to PNG, and so my question is whether there are any drawbacks to using PNG over BMP in this scenario, or more generally why would one want to use BMP over PNG for anything? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j3gmq", "is_robot_indexable": true, "report_reasons": null, "author": "ClearlyCylindrical", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j3gmq/any_reason_to_use_bmp_over_png/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j3gmq/any_reason_to_use_bmp_over_png/", "subreddit_subscribers": 718392, "created_utc": 1702656294.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My drives get fanned 24/7, and since it's winter they get pretty chill. At what temp should I worry and do something? under 30 degrees C? Under 25? 20?\n\nMy drives are hovering around 25, some drives even lower at times, like 21/22.", "author_fullname": "t2_9ymyrd1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How cold can you keep the hard drives running at?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18jmer8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702713380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My drives get fanned 24/7, and since it&amp;#39;s winter they get pretty chill. At what temp should I worry and do something? under 30 degrees C? Under 25? 20?&lt;/p&gt;\n\n&lt;p&gt;My drives are hovering around 25, some drives even lower at times, like 21/22.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "400TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jmer8", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Arugula-1592", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18jmer8/how_cold_can_you_keep_the_hard_drives_running_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jmer8/how_cold_can_you_keep_the_hard_drives_running_at/", "subreddit_subscribers": 718392, "created_utc": 1702713380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello there, r/DataHoarder, I am looking to get a SFF PC with a m.2 slot, either SATA or nvme, 2 3.5 bays, and something that is energy efficient. I will install freenas on it, and might install a 10 gigabit card down the line.", "author_fullname": "t2_7mak8foc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would be the best SFF PC to use as a ghetto NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jl1ix", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702707758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there, &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt;, I am looking to get a SFF PC with a m.2 slot, either SATA or nvme, 2 3.5 bays, and something that is energy efficient. I will install freenas on it, and might install a 10 gigabit card down the line.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18jl1ix", "is_robot_indexable": true, "report_reasons": null, "author": "Fragrant_Hour987", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jl1ix/what_would_be_the_best_sff_pc_to_use_as_a_ghetto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jl1ix/what_would_be_the_best_sff_pc_to_use_as_a_ghetto/", "subreddit_subscribers": 718392, "created_utc": 1702707758.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have 2TB of videos and pictures that I want to store on the cloud as a backup, these files are separated into folders each folder will be encrypted with a password using 7zip because I don't want the cloud provider to access them.  I'm not planning to download these files but I will be deleting some files to upload updated ones if necessary and uploading new files sometimes but I won't exceed 2TB.\n\nThanks\n\n&amp;#x200B;", "author_fullname": "t2_2rnc4qdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the cheapest cloud storage for my situation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ji1o6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702697217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have 2TB of videos and pictures that I want to store on the cloud as a backup, these files are separated into folders each folder will be encrypted with a password using 7zip because I don&amp;#39;t want the cloud provider to access them.  I&amp;#39;m not planning to download these files but I will be deleting some files to upload updated ones if necessary and uploading new files sometimes but I won&amp;#39;t exceed 2TB.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ji1o6", "is_robot_indexable": true, "report_reasons": null, "author": "AngryAndCrestfallen", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ji1o6/what_is_the_cheapest_cloud_storage_for_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ji1o6/what_is_the_cheapest_cloud_storage_for_my/", "subreddit_subscribers": 718392, "created_utc": 1702697217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\\*I've read all of the related posts I could find, some of which are great alternate routes if I can't get this route working.\n\n\\*If there is a better sub you think this should be in I'll gladly post it there instead/as well.\n\nSo I have all of my family's home video VHS tapes from the early 90's into the mid 00's. Trying to start the process of VHS&gt;DVD&gt;MP4 (cool christmas gift?). And I recently acquired a DVD + VHS player that also dubs VHS&gt;DVD \"at the press of a button\", the JVC DRMV5S.. So far I've successfully converted 2 tapes to DVD R, since the machine won't read DVD RW even though it says it does. And the 2 DVD Rs I did convert are only watchable on that machine, I've tried my Mac Books, Xbox One, and a separate DVD player, and all of them see it as a blank disk, however I can physically see the written and unwritten parts of the DVD in the light.\n\nThe manual has half of a page dedicated to this function, it's as simple as loading the tape, a blank DVD, and pressing the dub button for 3 seconds. And it really is that easy when using the DVD R discs, but they're unreadable to anything besides the machine..\n\nThoughts?\n\ntl;dr - JVC DRMV5S VHS to DVD player/dubbing machine. Writes to DVD R but not DVD RW. Converted DVD R discs are only playable on the machine that burned them.", "author_fullname": "t2_1v6asba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Another VHS to DVD question... What can you tell me about the \"JVC DRMV5S\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jdech", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702682814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;*I&amp;#39;ve read all of the related posts I could find, some of which are great alternate routes if I can&amp;#39;t get this route working.&lt;/p&gt;\n\n&lt;p&gt;*If there is a better sub you think this should be in I&amp;#39;ll gladly post it there instead/as well.&lt;/p&gt;\n\n&lt;p&gt;So I have all of my family&amp;#39;s home video VHS tapes from the early 90&amp;#39;s into the mid 00&amp;#39;s. Trying to start the process of VHS&amp;gt;DVD&amp;gt;MP4 (cool christmas gift?). And I recently acquired a DVD + VHS player that also dubs VHS&amp;gt;DVD &amp;quot;at the press of a button&amp;quot;, the JVC DRMV5S.. So far I&amp;#39;ve successfully converted 2 tapes to DVD R, since the machine won&amp;#39;t read DVD RW even though it says it does. And the 2 DVD Rs I did convert are only watchable on that machine, I&amp;#39;ve tried my Mac Books, Xbox One, and a separate DVD player, and all of them see it as a blank disk, however I can physically see the written and unwritten parts of the DVD in the light.&lt;/p&gt;\n\n&lt;p&gt;The manual has half of a page dedicated to this function, it&amp;#39;s as simple as loading the tape, a blank DVD, and pressing the dub button for 3 seconds. And it really is that easy when using the DVD R discs, but they&amp;#39;re unreadable to anything besides the machine..&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n\n&lt;p&gt;tl;dr - JVC DRMV5S VHS to DVD player/dubbing machine. Writes to DVD R but not DVD RW. Converted DVD R discs are only playable on the machine that burned them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jdech", "is_robot_indexable": true, "report_reasons": null, "author": "thekylegonzalez", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jdech/another_vhs_to_dvd_question_what_can_you_tell_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jdech/another_vhs_to_dvd_question_what_can_you_tell_me/", "subreddit_subscribers": 718392, "created_utc": 1702682814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've recently acquired access to a huge number of smaller (2-3TB) HDDs. My question is, how do I connect such a massive number of HDDs (think \"we need a scalable solution that connects 100\\~ of these\") in an economically feasible way? (the main pro of the drives is they're free/extremely cheap)\n\nI've thought about USB -&gt; 8x sata breakouts, but it seems they don't really exist. Bandwidth isn't the limitation, because its very unlikely all 8 drives in a cluster would be active at once. # of ports is what I don't know how to get past, because even with 8x PCIE sata cards, it's infeasible to connect enough of them.\n\nIdeally there's USB in the link somewhere, because I don't need them all drawing power when idle. \n\nWhat's the best solution here? Is it feasible at all, or should I give up and relegate these to cold storage?", "author_fullname": "t2_aw8fvxxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feasible way to connect massive amounts of HDDs to one computer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18jmli8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702714179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently acquired access to a huge number of smaller (2-3TB) HDDs. My question is, how do I connect such a massive number of HDDs (think &amp;quot;we need a scalable solution that connects 100~ of these&amp;quot;) in an economically feasible way? (the main pro of the drives is they&amp;#39;re free/extremely cheap)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve thought about USB -&amp;gt; 8x sata breakouts, but it seems they don&amp;#39;t really exist. Bandwidth isn&amp;#39;t the limitation, because its very unlikely all 8 drives in a cluster would be active at once. # of ports is what I don&amp;#39;t know how to get past, because even with 8x PCIE sata cards, it&amp;#39;s infeasible to connect enough of them.&lt;/p&gt;\n\n&lt;p&gt;Ideally there&amp;#39;s USB in the link somewhere, because I don&amp;#39;t need them all drawing power when idle. &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best solution here? Is it feasible at all, or should I give up and relegate these to cold storage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "48TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jmli8", "is_robot_indexable": true, "report_reasons": null, "author": "DrDrago-4", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18jmli8/feasible_way_to_connect_massive_amounts_of_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jmli8/feasible_way_to_connect_massive_amounts_of_hdds/", "subreddit_subscribers": 718392, "created_utc": 1702714179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought a SABRENT Rocket Q 8TB NVMe M.2 with the hopes of using it as a Time Machine backup drive for mac. I also use backblaze for online backups, so quiet operation and speed is more important to me than data persistance.\n\nI found that upon the initial back up of my 2tb drive, the speed was initially fast (maxing out the usb 3.2 speeds of the enclosure), but then after about 15 mins of operation, the transfer speed grinds to a halt. The enclosure is very hot to the touch. I'm thinking its overheating, as I don't think caching is a problem in this circumstance.\n\nAny ideas for a better enclosure to use for this purpose - maybe something with a fan, and heat pads for both sides of the SSD rather than just one, and maybe a large heat sink?\n\nI've searched around and can't find anything that looks promising, and that claims it will work with an 8tb drive.", "author_fullname": "t2_czx3tdrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best enclosure for 8tb NVMe M.2 SSD for large file transfer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18jmi5t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702713769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought a SABRENT Rocket Q 8TB NVMe M.2 with the hopes of using it as a Time Machine backup drive for mac. I also use backblaze for online backups, so quiet operation and speed is more important to me than data persistance.&lt;/p&gt;\n\n&lt;p&gt;I found that upon the initial back up of my 2tb drive, the speed was initially fast (maxing out the usb 3.2 speeds of the enclosure), but then after about 15 mins of operation, the transfer speed grinds to a halt. The enclosure is very hot to the touch. I&amp;#39;m thinking its overheating, as I don&amp;#39;t think caching is a problem in this circumstance.&lt;/p&gt;\n\n&lt;p&gt;Any ideas for a better enclosure to use for this purpose - maybe something with a fan, and heat pads for both sides of the SSD rather than just one, and maybe a large heat sink?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve searched around and can&amp;#39;t find anything that looks promising, and that claims it will work with an 8tb drive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jmi5t", "is_robot_indexable": true, "report_reasons": null, "author": "Many_Slices_Of_Bread", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jmi5t/best_enclosure_for_8tb_nvme_m2_ssd_for_large_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jmi5t/best_enclosure_for_8tb_nvme_m2_ssd_for_large_file/", "subreddit_subscribers": 718392, "created_utc": 1702713769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "`httm` now directly supports Time Machine backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jgsg8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_hokp5z5a", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "zfs", "selftext": "You can have it all -- the convenience of Time Machine backups and the speed and flexibility of `httm`.\n\nOf course, one could always use `httm` to browse all files in a folder backed up via `rsync` to a remote ZFS or btrfs share, if that share has a hidden snapshot directory:\n\n```\n# mount the share\n\u279c open smb://&lt;your name&gt;@&lt;your remote share&gt;.local/Home\n# execute httm \n\u279c httm -b -R /Volumes/Home\n```\n\nOr one could use `httm` to browse all files in your local MacOS home directory, if you mapped that directory to a remote dataset. Note: The difference from above is, here, you're browsing files from a \"live\" directory:\n\n```\n# mount the share\n\u279c open smb://&lt;your name&gt;@&lt;your remote share&gt;.local/Home\n# execute httm\n\u279c httm -b -R --map-aliases /Users/&lt;your name&gt;:/Volumes/Home\n```\n\nNow, `httm` supports your Time Machine backups directly, [when those backups are mounted](https://github.com/kimono-koans/httm/blob/master/scripts/equine.bash):\n\n```\n\u279c httm .zshrc\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTue May 09 22:57:09 2023  6.7 KiB  \"/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-11-08-212757.backup/2023-11-08-212757.backup/Data/Users/kimono/.zshrc\"\nSun Nov 12 20:29:57 2023  6.7 KiB  \"/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-11-18-011056.backup/2023-11-18-011056.backup/Data/Users/kimono/.zshrc\"\nSun Nov 26 02:14:56 2023  6.7 KiB  \"/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-12-13-054342.backup/2023-12-13-054342.backup/Data/Users/kimono/.zshrc\"\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSun Nov 26 02:14:56 2023  6.7 KiB  \"/Users/kimono/.zshrc\"\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n\n[httm](https://github.com/kimono-koans/httm) prints the size, date and corresponding locations of available unique versions (deduplicated by modify time and size) of files residing on snapshots, but can also be used interactively to select and restore files, even snapshot mounts by file! httm might change the way you use snapshots (because ZFS/BTRFS/NILFS2 aren't designed for finding for unique file versions) or the Time Machine concept (because httm is very fast!).  \n\nFor more info, see the [README](https://github.com/kimono-koans/httm/blob/master/README.md).", "author_fullname": "t2_hokp5z5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "`httm` now directly supports Time Machine backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/zfs", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jgcv8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702693342.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702691673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.zfs", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can have it all -- the convenience of Time Machine backups and the speed and flexibility of &lt;code&gt;httm&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;Of course, one could always use &lt;code&gt;httm&lt;/code&gt; to browse all files in a folder backed up via &lt;code&gt;rsync&lt;/code&gt; to a remote ZFS or btrfs share, if that share has a hidden snapshot directory:&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h1&gt;mount the share&lt;/h1&gt;\n\n&lt;p&gt;\u279c open smb://&amp;lt;your name&amp;gt;@&amp;lt;your remote share&amp;gt;.local/Home&lt;/p&gt;\n\n&lt;h1&gt;execute httm&lt;/h1&gt;\n\n&lt;p&gt;\u279c httm -b -R /Volumes/Home\n```&lt;/p&gt;\n\n&lt;p&gt;Or one could use &lt;code&gt;httm&lt;/code&gt; to browse all files in your local MacOS home directory, if you mapped that directory to a remote dataset. Note: The difference from above is, here, you&amp;#39;re browsing files from a &amp;quot;live&amp;quot; directory:&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h1&gt;mount the share&lt;/h1&gt;\n\n&lt;p&gt;\u279c open smb://&amp;lt;your name&amp;gt;@&amp;lt;your remote share&amp;gt;.local/Home&lt;/p&gt;\n\n&lt;h1&gt;execute httm&lt;/h1&gt;\n\n&lt;p&gt;\u279c httm -b -R --map-aliases /Users/&amp;lt;your name&amp;gt;:/Volumes/Home\n```&lt;/p&gt;\n\n&lt;p&gt;Now, &lt;code&gt;httm&lt;/code&gt; supports your Time Machine backups directly, &lt;a href=\"https://github.com/kimono-koans/httm/blob/master/scripts/equine.bash\"&gt;when those backups are mounted&lt;/a&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n\u279c httm .zshrc\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTue May 09 22:57:09 2023  6.7 KiB  &amp;quot;/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-11-08-212757.backup/2023-11-08-212757.backup/Data/Users/kimono/.zshrc&amp;quot;\nSun Nov 12 20:29:57 2023  6.7 KiB  &amp;quot;/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-11-18-011056.backup/2023-11-18-011056.backup/Data/Users/kimono/.zshrc&amp;quot;\nSun Nov 26 02:14:56 2023  6.7 KiB  &amp;quot;/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-12-13-054342.backup/2023-12-13-054342.backup/Data/Users/kimono/.zshrc&amp;quot;\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSun Nov 26 02:14:56 2023  6.7 KiB  &amp;quot;/Users/kimono/.zshrc&amp;quot;\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/kimono-koans/httm\"&gt;httm&lt;/a&gt; prints the size, date and corresponding locations of available unique versions (deduplicated by modify time and size) of files residing on snapshots, but can also be used interactively to select and restore files, even snapshot mounts by file! httm might change the way you use snapshots (because ZFS/BTRFS/NILFS2 aren&amp;#39;t designed for finding for unique file versions) or the Time Machine concept (because httm is very fast!).  &lt;/p&gt;\n\n&lt;p&gt;For more info, see the &lt;a href=\"https://github.com/kimono-koans/httm/blob/master/README.md\"&gt;README&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?auto=webp&amp;s=289ae7bd911636bd23ad7b0471e03d0338afc69f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=72d6e44d66b51c9708547e518ab2ecd08b74d294", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4bb531ce82a16032f4b57a0e091cb765ee005a1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f2232cce32ad3e34bd35cc0f1c936d05f321500", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a994e185edcff2ef789a0edaca551c4259242a4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ae3a0051e16d201e98f9716ff78e2906b12f3a5a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8382e573d51bb691efc81f59ac110ffc4d867671", "width": 1080, "height": 540}], "variants": {}, "id": "ls_jgHNP-aKIiMW8JRNP0at33ieMFcWHSWWlcWs9tVY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ruui", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "18jgcv8", "is_robot_indexable": true, "report_reasons": null, "author": "small_kimono", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/zfs/comments/18jgcv8/httm_now_directly_supports_time_machine_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/zfs/comments/18jgcv8/httm_now_directly_supports_time_machine_backups/", "subreddit_subscribers": 28936, "created_utc": 1702691673.0, "num_crossposts": 4, "media": null, "is_video": false}], "created": 1702693078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.zfs", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/zfs/comments/18jgcv8/httm_now_directly_supports_time_machine_backups/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?auto=webp&amp;s=289ae7bd911636bd23ad7b0471e03d0338afc69f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=72d6e44d66b51c9708547e518ab2ecd08b74d294", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4bb531ce82a16032f4b57a0e091cb765ee005a1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f2232cce32ad3e34bd35cc0f1c936d05f321500", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a994e185edcff2ef789a0edaca551c4259242a4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ae3a0051e16d201e98f9716ff78e2906b12f3a5a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8382e573d51bb691efc81f59ac110ffc4d867671", "width": 1080, "height": 540}], "variants": {}, "id": "ls_jgHNP-aKIiMW8JRNP0at33ieMFcWHSWWlcWs9tVY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jgsg8", "is_robot_indexable": true, "report_reasons": null, "author": "small_kimono", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_18jgcv8", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jgsg8/httm_now_directly_supports_time_machine_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/zfs/comments/18jgcv8/httm_now_directly_supports_time_machine_backups/", "subreddit_subscribers": 718392, "created_utc": 1702693078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm living in a temporary spot. I wanted to recreate my storage availability from the 4x3.5\" drives I had, but didn't want to invest in a system I may leave behind or build anything too large to take with me. Sure, usb can reach \"Max throughput\" of 10Gbps these days, but I have enough to worry about with my filesytem of choice. I don't need usb making things worse. \n\nI came dangerously close to buying a pi with a sata hat... then I slapped myself and looked for a real solution. \n\nEnter esata. \n\nFor less than $300 (minus drives), I have 4x6TB drives in a btrfs Raid10 (raid1c4 metadata) array that won't die over a flaky usb connection.\n\nThis Lenovo m729q takes a low profile expansion card. Putting in a Startech esata card with FIS was cheap (ya know since esata is \"dead\"). The Mediasonic probox was around $120. The lenovo itself came from eBay and has an 8th Gen i3, 8GB DDR4 (accepts 32GB), wifi, bt, nvme, and generous 5/10Gbps usb (for transfers). It ran me $75 plus $30 for the pcie riser. It sits on my desk and I barely hear it.\n\nIt's not production, but for a home seedbox/jellyfin server on a budget, it's stable as hell.. Oh, did I mention it's not usb, but sata... portable zfs for the next one.\n\nhttps://imgur.com/gallery/5mzITkh\n\nThanks u/joeB for the heads up about the Tiny", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Poor-man's SAS enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jer5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": "", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702686706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m living in a temporary spot. I wanted to recreate my storage availability from the 4x3.5&amp;quot; drives I had, but didn&amp;#39;t want to invest in a system I may leave behind or build anything too large to take with me. Sure, usb can reach &amp;quot;Max throughput&amp;quot; of 10Gbps these days, but I have enough to worry about with my filesytem of choice. I don&amp;#39;t need usb making things worse. &lt;/p&gt;\n\n&lt;p&gt;I came dangerously close to buying a pi with a sata hat... then I slapped myself and looked for a real solution. &lt;/p&gt;\n\n&lt;p&gt;Enter esata. &lt;/p&gt;\n\n&lt;p&gt;For less than $300 (minus drives), I have 4x6TB drives in a btrfs Raid10 (raid1c4 metadata) array that won&amp;#39;t die over a flaky usb connection.&lt;/p&gt;\n\n&lt;p&gt;This Lenovo m729q takes a low profile expansion card. Putting in a Startech esata card with FIS was cheap (ya know since esata is &amp;quot;dead&amp;quot;). The Mediasonic probox was around $120. The lenovo itself came from eBay and has an 8th Gen i3, 8GB DDR4 (accepts 32GB), wifi, bt, nvme, and generous 5/10Gbps usb (for transfers). It ran me $75 plus $30 for the pcie riser. It sits on my desk and I barely hear it.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not production, but for a home seedbox/jellyfin server on a budget, it&amp;#39;s stable as hell.. Oh, did I mention it&amp;#39;s not usb, but sata... portable zfs for the next one.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/gallery/5mzITkh\"&gt;https://imgur.com/gallery/5mzITkh&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks &lt;a href=\"/u/joeB\"&gt;u/joeB&lt;/a&gt; for the heads up about the Tiny&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/f4K2Bcg4LoAgsI9v-_EM_b742LvGp0p5sTuNrrjFOjA.jpg?auto=webp&amp;s=57d9c40abdb745c8fe7a36bd29576b5e2179d71d", "width": 4000, "height": 3000}, "resolutions": [{"url": "https://external-preview.redd.it/f4K2Bcg4LoAgsI9v-_EM_b742LvGp0p5sTuNrrjFOjA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9f2c84dc2dede46b12c76c65447f4fc40cb5c28", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/f4K2Bcg4LoAgsI9v-_EM_b742LvGp0p5sTuNrrjFOjA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=513c7dbf9bcb460138c762b712633373d3b455d9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/f4K2Bcg4LoAgsI9v-_EM_b742LvGp0p5sTuNrrjFOjA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0ac1d75101878457bec5284a093541ddffd0f32", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/f4K2Bcg4LoAgsI9v-_EM_b742LvGp0p5sTuNrrjFOjA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e4a838807e3483852528cfa955d62de5f2fd4fa", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/f4K2Bcg4LoAgsI9v-_EM_b742LvGp0p5sTuNrrjFOjA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c6879c91da995e93f9a56eff2533ce5267df319", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/f4K2Bcg4LoAgsI9v-_EM_b742LvGp0p5sTuNrrjFOjA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db5a418869e8108dc93bef8d2929b2c6e15fb7b0", "width": 1080, "height": 810}], "variants": {}, "id": "ord1N7SM-PJwqLhfc51xnt4g_MkCLTGv8CvwZhwOp5g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jer5f", "is_robot_indexable": true, "report_reasons": null, "author": "[deleted]", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18jer5f/my_poormans_sas_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jer5f/my_poormans_sas_enclosure/", "subreddit_subscribers": 718392, "created_utc": 1702686706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello data hoarders,\n\nCan you please give me some advice about my DIY NAS/Server project?  I did some searching, but came up with generic info, and am hoping for feedback on my specific hardware.\n\nI currently have a 6-year old desktop I'd like to turn into a NAS or Server (and then build a new desktop computer).\n\nCurrent hardware is:\n\n* i3-7100\n* ASRock H270M Pro4 Micro ATX LGA1151 Motherboard\n* 16 GB (2 x 8 GB) DDR4-2400 RAM\n* Intel 600p 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive\n* 450 W 80+ Bronze Power Supply\n* I will buy some large storage (looking at 2x 12GB drives), and have some sort of RAID system running\n* My router has a second network port that I would connect to\n* I've heard that TrueNAS would be the right OS for this - any advice there?  I use Ubuntu on my home PC, so I have a little experience, but I've never dealt NASes or servers.\n\nI'd like to use it as a \"cloud\" for family sharing of pictures (myself, wife, three kids), videos, music, etc.  Goal is to save cellphone pictures to it, and be able to access our music collection remotely, etc.\n\nIs this a good plan?\n\nI'm open to changing some of the hardware if it makes a difference.\n\nThanks in advance.", "author_fullname": "t2_pjqstcyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for DIY NAS/Server from existing desktop PC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jd5al", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702682094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello data hoarders,&lt;/p&gt;\n\n&lt;p&gt;Can you please give me some advice about my DIY NAS/Server project?  I did some searching, but came up with generic info, and am hoping for feedback on my specific hardware.&lt;/p&gt;\n\n&lt;p&gt;I currently have a 6-year old desktop I&amp;#39;d like to turn into a NAS or Server (and then build a new desktop computer).&lt;/p&gt;\n\n&lt;p&gt;Current hardware is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;i3-7100&lt;/li&gt;\n&lt;li&gt;ASRock H270M Pro4 Micro ATX LGA1151 Motherboard&lt;/li&gt;\n&lt;li&gt;16 GB (2 x 8 GB) DDR4-2400 RAM&lt;/li&gt;\n&lt;li&gt;Intel 600p 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive&lt;/li&gt;\n&lt;li&gt;450 W 80+ Bronze Power Supply&lt;/li&gt;\n&lt;li&gt;I will buy some large storage (looking at 2x 12GB drives), and have some sort of RAID system running&lt;/li&gt;\n&lt;li&gt;My router has a second network port that I would connect to&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve heard that TrueNAS would be the right OS for this - any advice there?  I use Ubuntu on my home PC, so I have a little experience, but I&amp;#39;ve never dealt NASes or servers.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d like to use it as a &amp;quot;cloud&amp;quot; for family sharing of pictures (myself, wife, three kids), videos, music, etc.  Goal is to save cellphone pictures to it, and be able to access our music collection remotely, etc.&lt;/p&gt;\n\n&lt;p&gt;Is this a good plan?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to changing some of the hardware if it makes a difference.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jd5al", "is_robot_indexable": true, "report_reasons": null, "author": "Setter4Hire", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jd5al/advice_for_diy_nasserver_from_existing_desktop_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jd5al/advice_for_diy_nasserver_from_existing_desktop_pc/", "subreddit_subscribers": 718392, "created_utc": 1702682094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all \ud83d\udc4b \u2014 I bought the SAS version of a hard drive  instead of the SATA version. \n\n- WD Ultrastar DC HC650 (WSH722020AL4204) 20TB\n\nPC specs:\n- OS \u2014 Windows 11 Enterprise\n- CPU \u2014 AMD Ryzen 9 3900xt (12-Core)\n- MB \u2014 MSI MEG X570S ACE MAX (AM4)\n- RAM \u2014 32 GB (16gb x2) Dual\u2010Channel DDR4\n- GPU \u2014 (EVGA) NVIDIA RTX 3090 Ti Hybrid\n- STORAGE:\n\n1 \u2014 2TB Sabrent M.2 NVMe drive (OS)\n\n2 \u2014 750gb Crucial SSD (miscellaneous)\n\n3 \u2014 4TB WD Gold drive (\"datacenter A\")\n\n4 \u2014 4TB WD Gold drive (\"datacenter B\")\n\n5 \u2014 1TB hdd (Torrents)\n\n\nI (think) I need a PCI Express SATA / SAS Controller Card, but I have never used an SAS Drive before so I was wondering what would you all recommend I get in order to just use the drive for storage?\n\nI'd like it to be as performant as possible, but I understand I did not adequately prepare or research before buying this type of drive. All advice &amp; help is appreciated \ud83d\ude4f", "author_fullname": "t2_169fjhiu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using a SAS Drive in a Windows Desktop PC (non-server)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jcnfe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702681018.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702680756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all \ud83d\udc4b \u2014 I bought the SAS version of a hard drive  instead of the SATA version. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;WD Ultrastar DC HC650 (WSH722020AL4204) 20TB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;PC specs:\n- OS \u2014 Windows 11 Enterprise\n- CPU \u2014 AMD Ryzen 9 3900xt (12-Core)\n- MB \u2014 MSI MEG X570S ACE MAX (AM4)\n- RAM \u2014 32 GB (16gb x2) Dual\u2010Channel DDR4\n- GPU \u2014 (EVGA) NVIDIA RTX 3090 Ti Hybrid\n- STORAGE:&lt;/p&gt;\n\n&lt;p&gt;1 \u2014 2TB Sabrent M.2 NVMe drive (OS)&lt;/p&gt;\n\n&lt;p&gt;2 \u2014 750gb Crucial SSD (miscellaneous)&lt;/p&gt;\n\n&lt;p&gt;3 \u2014 4TB WD Gold drive (&amp;quot;datacenter A&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;4 \u2014 4TB WD Gold drive (&amp;quot;datacenter B&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;5 \u2014 1TB hdd (Torrents)&lt;/p&gt;\n\n&lt;p&gt;I (think) I need a PCI Express SATA / SAS Controller Card, but I have never used an SAS Drive before so I was wondering what would you all recommend I get in order to just use the drive for storage?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like it to be as performant as possible, but I understand I did not adequately prepare or research before buying this type of drive. All advice &amp;amp; help is appreciated \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jcnfe", "is_robot_indexable": true, "report_reasons": null, "author": "Silencer711", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jcnfe/using_a_sas_drive_in_a_windows_desktop_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jcnfe/using_a_sas_drive_in_a_windows_desktop_pc/", "subreddit_subscribers": 718392, "created_utc": 1702680756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I got Photosync to work with my Ubuntu/Samba server. I configured two android phones to use this.\n\n* Issue when configuring SMB in PhotoSync\n   * For user1's phone , I had to select '/' as the destination folder\n   * For user2's phone I had to select the destination folder as '/sambashare/'\n* Both these locations point to the same directory on the drive\n* Below is my modification to the smb.conf file\n\n\\[sambashare\\]\n\ncomment = Samba on Ubuntu\n\npath = /media/Backup/Photosync\n\nbrowsable = yes\n\nread only = no\n\nnetbios name = PHOTOS\n\nvalid users = user1, user2\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_3yp1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photosync/Samba configuration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jb1u4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702676431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I got Photosync to work with my Ubuntu/Samba server. I configured two android phones to use this.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Issue when configuring SMB in PhotoSync\n\n&lt;ul&gt;\n&lt;li&gt;For user1&amp;#39;s phone , I had to select &amp;#39;/&amp;#39; as the destination folder&lt;/li&gt;\n&lt;li&gt;For user2&amp;#39;s phone I had to select the destination folder as &amp;#39;/sambashare/&amp;#39;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Both these locations point to the same directory on the drive&lt;/li&gt;\n&lt;li&gt;Below is my modification to the smb.conf file&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;[sambashare]&lt;/p&gt;\n\n&lt;p&gt;comment = Samba on Ubuntu&lt;/p&gt;\n\n&lt;p&gt;path = /media/Backup/Photosync&lt;/p&gt;\n\n&lt;p&gt;browsable = yes&lt;/p&gt;\n\n&lt;p&gt;read only = no&lt;/p&gt;\n\n&lt;p&gt;netbios name = PHOTOS&lt;/p&gt;\n\n&lt;p&gt;valid users = user1, user2&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jb1u4", "is_robot_indexable": true, "report_reasons": null, "author": "rdu01", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jb1u4/photosyncsamba_configuration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jb1u4/photosyncsamba_configuration/", "subreddit_subscribers": 718392, "created_utc": 1702676431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download images from a blogspot blog but the files are saved with \"file\" extension under the images file. As I know, this means that the file type is unrecognised but how can I download images as jpg/jpeg, png or anything easy to view ?\n\nThank you", "author_fullname": "t2_np3vaof1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using wget to download images of a blogspot blog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j72ue", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702665789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download images from a blogspot blog but the files are saved with &amp;quot;file&amp;quot; extension under the images file. As I know, this means that the file type is unrecognised but how can I download images as jpg/jpeg, png or anything easy to view ?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j72ue", "is_robot_indexable": true, "report_reasons": null, "author": "moonau9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j72ue/using_wget_to_download_images_of_a_blogspot_blog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j72ue/using_wget_to_download_images_of_a_blogspot_blog/", "subreddit_subscribers": 718392, "created_utc": 1702665789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im currently backing up a ~50TB zfs array to a mergerfs target with 7 data disks and 2 snapraid parity disks.\n\nThe idea here is that I can pop all the data and parity disks out after the data sync is complete. Store the parity disks with the data disks somewhere else and then test, sync changes on intervals such as 6 months to a year. \n\nI was thinking this would be the most sane as each disk will have its own filesystem and the backup storage doesn't need to be high performance. \n\nThe other option I was considering is borg so have incrementals but this doesn't seem useful in this case as it's only media files in this backup.", "author_fullname": "t2_j85nt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sanity check - mergerfs + snapraid as a backup target", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j6s04", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702665017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im currently backing up a ~50TB zfs array to a mergerfs target with 7 data disks and 2 snapraid parity disks.&lt;/p&gt;\n\n&lt;p&gt;The idea here is that I can pop all the data and parity disks out after the data sync is complete. Store the parity disks with the data disks somewhere else and then test, sync changes on intervals such as 6 months to a year. &lt;/p&gt;\n\n&lt;p&gt;I was thinking this would be the most sane as each disk will have its own filesystem and the backup storage doesn&amp;#39;t need to be high performance. &lt;/p&gt;\n\n&lt;p&gt;The other option I was considering is borg so have incrementals but this doesn&amp;#39;t seem useful in this case as it&amp;#39;s only media files in this backup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j6s04", "is_robot_indexable": true, "report_reasons": null, "author": "unsafetypin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j6s04/sanity_check_mergerfs_snapraid_as_a_backup_target/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j6s04/sanity_check_mergerfs_snapraid_as_a_backup_target/", "subreddit_subscribers": 718392, "created_utc": 1702665017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Apologies in advance for how long this is, but I'm hoping someone here can help me because I've really had trouble finding a solution on my own after many hours of research.\n\nI'm looking for a way to tame our current photo management system at work. I am the communications specialist in a very small (5 admin/staff and 5 faculty, \\~15-20 students per semester) higher ed program that collects a LOT of photos. They bring a program camera on each trip, out in the field, and to things like project presentations. Google Drive makes it hard to see folder size, but I determined that one semester's folder was 24 GB. We have had about 80 semesters now. I'm guessing the last \\~40 semesters are similar in size and earlier semesters are likely smaller.\n\nThe current system needs an update and we're just not sure where to go from here. Photos from the Fall 2017 class to present are each in their own Google Drive folder owned by a staff member. Within each class folder, subfolders organize them by trip/location, subject/course, etc. These folders are then shared with the relevant people (the students from that class, myself, the Executive and Deputy Directors, the Admissions Director).  Between what's currently on Google Drive and what's on various hard drives, there are probably several TB of photos.\n\nThe reason we're not happy with our current system is that the searching, tagging, metadata, etc. options are very limited with Google and it has become near impossible to find things. We end up creating extra folders (\"best promotional photos 2023\" for example), which then just creates duplicates and takes up more space. We each get 150 GB of storage in Google Drive, which isn't enough for the photos we have. We also don't want photos tied to one specific person's Google account because when people leave their job, the college shuts down their account pretty immediately and it has always been an issue getting ownership transferred.\n\nWe have Adobe CC accounts through the college we're affiliated with, with many TB of space, so Lightroom would've been perfect if I weren't needing to share these photos with several people. I'm looking into Adobe Bridge but I quite honestly find it hard to navigate, and I'm by far the most tech savvy in the office so I think the transition would be impossible. [Pics.io](https://Pics.io) sitting on top of Google Drive would be perfect if we had enough storage space in Google Drive. I'd consider [Pics.io](https://Pics.io) \\+ a different cloud service, but that seems like it would get pricey. Canto seems a little bit overkill and pricey.\n\nPrimary needs:\n\n* Metadata and/or tagging\n* Searching based on metadata/tags\n* Cloud access - the main goal we're looking at right now is making photos accessible to myself and 2 or 3 other employees who often need photos for communications/promotional materials/etc. 1 of the employees works in a separate building, and in order to improve workflow, photos really need to be accessible via a cloud or other shared network.\n* Size: we don't necessarily need ALL our photos available on the cloud; we can certainly archive older photos that are rarely used onto a hard drive. But we still probably need at least 1 TB, maybe more.\n* Either a single sign in not tied to a pre-existing account (Google, Apple ID, etc) or an ability to have multiple \"users\"\n\nIdeal but negotiable:\n\n* People/face tagging (could use regular tags with their name if needed)\n* Batch actions (renaming, tagging, etc)\n* Being able to store ALL of our photos (I think somewhere around 2-4 TB) on a shared network\n* Price - being a small program, we can't be spending $500+ on something like this. Not saying it has to be like $10, but keeping it reasonably low is ideal.", "author_fullname": "t2_4ffbgt6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a cloud-based photo management/organization system for a large photo collection in my office", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j39ux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702655809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies in advance for how long this is, but I&amp;#39;m hoping someone here can help me because I&amp;#39;ve really had trouble finding a solution on my own after many hours of research.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a way to tame our current photo management system at work. I am the communications specialist in a very small (5 admin/staff and 5 faculty, ~15-20 students per semester) higher ed program that collects a LOT of photos. They bring a program camera on each trip, out in the field, and to things like project presentations. Google Drive makes it hard to see folder size, but I determined that one semester&amp;#39;s folder was 24 GB. We have had about 80 semesters now. I&amp;#39;m guessing the last ~40 semesters are similar in size and earlier semesters are likely smaller.&lt;/p&gt;\n\n&lt;p&gt;The current system needs an update and we&amp;#39;re just not sure where to go from here. Photos from the Fall 2017 class to present are each in their own Google Drive folder owned by a staff member. Within each class folder, subfolders organize them by trip/location, subject/course, etc. These folders are then shared with the relevant people (the students from that class, myself, the Executive and Deputy Directors, the Admissions Director).  Between what&amp;#39;s currently on Google Drive and what&amp;#39;s on various hard drives, there are probably several TB of photos.&lt;/p&gt;\n\n&lt;p&gt;The reason we&amp;#39;re not happy with our current system is that the searching, tagging, metadata, etc. options are very limited with Google and it has become near impossible to find things. We end up creating extra folders (&amp;quot;best promotional photos 2023&amp;quot; for example), which then just creates duplicates and takes up more space. We each get 150 GB of storage in Google Drive, which isn&amp;#39;t enough for the photos we have. We also don&amp;#39;t want photos tied to one specific person&amp;#39;s Google account because when people leave their job, the college shuts down their account pretty immediately and it has always been an issue getting ownership transferred.&lt;/p&gt;\n\n&lt;p&gt;We have Adobe CC accounts through the college we&amp;#39;re affiliated with, with many TB of space, so Lightroom would&amp;#39;ve been perfect if I weren&amp;#39;t needing to share these photos with several people. I&amp;#39;m looking into Adobe Bridge but I quite honestly find it hard to navigate, and I&amp;#39;m by far the most tech savvy in the office so I think the transition would be impossible. &lt;a href=\"https://Pics.io\"&gt;Pics.io&lt;/a&gt; sitting on top of Google Drive would be perfect if we had enough storage space in Google Drive. I&amp;#39;d consider &lt;a href=\"https://Pics.io\"&gt;Pics.io&lt;/a&gt; + a different cloud service, but that seems like it would get pricey. Canto seems a little bit overkill and pricey.&lt;/p&gt;\n\n&lt;p&gt;Primary needs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Metadata and/or tagging&lt;/li&gt;\n&lt;li&gt;Searching based on metadata/tags&lt;/li&gt;\n&lt;li&gt;Cloud access - the main goal we&amp;#39;re looking at right now is making photos accessible to myself and 2 or 3 other employees who often need photos for communications/promotional materials/etc. 1 of the employees works in a separate building, and in order to improve workflow, photos really need to be accessible via a cloud or other shared network.&lt;/li&gt;\n&lt;li&gt;Size: we don&amp;#39;t necessarily need ALL our photos available on the cloud; we can certainly archive older photos that are rarely used onto a hard drive. But we still probably need at least 1 TB, maybe more.&lt;/li&gt;\n&lt;li&gt;Either a single sign in not tied to a pre-existing account (Google, Apple ID, etc) or an ability to have multiple &amp;quot;users&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ideal but negotiable:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;People/face tagging (could use regular tags with their name if needed)&lt;/li&gt;\n&lt;li&gt;Batch actions (renaming, tagging, etc)&lt;/li&gt;\n&lt;li&gt;Being able to store ALL of our photos (I think somewhere around 2-4 TB) on a shared network&lt;/li&gt;\n&lt;li&gt;Price - being a small program, we can&amp;#39;t be spending $500+ on something like this. Not saying it has to be like $10, but keeping it reasonably low is ideal.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/D-aa1kSA9vfCaUZsaTtRXMlx8g-89XGBRCp3C1u57fQ.jpg?auto=webp&amp;s=8c5d361dc86f7afd900fb54ca7aed8f299914b2e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/D-aa1kSA9vfCaUZsaTtRXMlx8g-89XGBRCp3C1u57fQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bc0f1e9cee958a75512c61626978358b3ba59c4", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/D-aa1kSA9vfCaUZsaTtRXMlx8g-89XGBRCp3C1u57fQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1914c16a372da17184137542448e0a1866ec3b8f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/D-aa1kSA9vfCaUZsaTtRXMlx8g-89XGBRCp3C1u57fQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aae7bc5676e461613bf32f6163c4a53cc292f7f1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/D-aa1kSA9vfCaUZsaTtRXMlx8g-89XGBRCp3C1u57fQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=25f301c3c2f952fa7a1b747851aa03b541fcc2db", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/D-aa1kSA9vfCaUZsaTtRXMlx8g-89XGBRCp3C1u57fQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5846f0e7f94fa11b4c3cd651bb6f70a4461cb5b9", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/D-aa1kSA9vfCaUZsaTtRXMlx8g-89XGBRCp3C1u57fQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6db93b037ebefe516ec703e0216bccdc3b088ed6", "width": 1080, "height": 567}], "variants": {}, "id": "UxMnzC4VnFgMPUmD6ss0uhMB3XkEfmOuezt7t4NYPGY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j39ux", "is_robot_indexable": true, "report_reasons": null, "author": "Feisty-Sherbert", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j39ux/looking_for_a_cloudbased_photo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j39ux/looking_for_a_cloudbased_photo/", "subreddit_subscribers": 718392, "created_utc": 1702655809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My dad is gonna pay for a service to write 40+ DVDs onto flash drives and is going to pay 1k for it all. This seems like quite a bit of money and flash drives don't seem like the most reliable storage from what I've perused. I've been on this sub before and know redundancy is key. Since some of these videos are at least as old as the 1940s, I really want to be sure.\n\nHowever should he just get flash drives and a DVD drive (USB) to rip them himself? That process seems pretty easy as there won't be any copy protection. The hardware itself would be under $250 so 1k seems a bit unreasonable, but I'm not sure.\n\nQuestions are:\n\n1. Will a USB DVD drive cause a loss of quality when the data is transferred?\n\n2. What storage hardware is recommended, I store my personal stuff on a WD 1tb hard drive.\n\n3. Is MakeMKV still the software to use for these personally made DVDs?\n\nThanks!", "author_fullname": "t2_7km3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive DVDs of family videos from 4+ generations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jjmkt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702702577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My dad is gonna pay for a service to write 40+ DVDs onto flash drives and is going to pay 1k for it all. This seems like quite a bit of money and flash drives don&amp;#39;t seem like the most reliable storage from what I&amp;#39;ve perused. I&amp;#39;ve been on this sub before and know redundancy is key. Since some of these videos are at least as old as the 1940s, I really want to be sure.&lt;/p&gt;\n\n&lt;p&gt;However should he just get flash drives and a DVD drive (USB) to rip them himself? That process seems pretty easy as there won&amp;#39;t be any copy protection. The hardware itself would be under $250 so 1k seems a bit unreasonable, but I&amp;#39;m not sure.&lt;/p&gt;\n\n&lt;p&gt;Questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Will a USB DVD drive cause a loss of quality when the data is transferred?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What storage hardware is recommended, I store my personal stuff on a WD 1tb hard drive.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is MakeMKV still the software to use for these personally made DVDs?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jjmkt", "is_robot_indexable": true, "report_reasons": null, "author": "gumbo100", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jjmkt/archive_dvds_of_family_videos_from_4_generations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jjmkt/archive_dvds_of_family_videos_from_4_generations/", "subreddit_subscribers": 718392, "created_utc": 1702702577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title. I can't find anything to pull these video files onto my hard disk. Everyone raves about jdownloader and I can't even get it installed right because I have no idea how linux terminal works. Someone please ELI5. ", "author_fullname": "t2_1ffvv4x3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Newb with Chromebook - I tried to install Jdownloader through linux and its not working. How do I uninstall to try again? I have no idea what I'm doing, but I need to download non-youtube web videos quite desperately before the new year.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jb3z5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702676594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. I can&amp;#39;t find anything to pull these video files onto my hard disk. Everyone raves about jdownloader and I can&amp;#39;t even get it installed right because I have no idea how linux terminal works. Someone please ELI5. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jb3z5", "is_robot_indexable": true, "report_reasons": null, "author": "cpt_raymondholt", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jb3z5/newb_with_chromebook_i_tried_to_install/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jb3z5/newb_with_chromebook_i_tried_to_install/", "subreddit_subscribers": 718392, "created_utc": 1702676594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have around 100$ budget and want external HDD. I am thinking to go either one of those ways.\n\nWhat do you recommend and what drives to go for?\n\nI will be storing media data on it and will not be using frequently.", "author_fullname": "t2_xdxarw1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use desktop HDD as external drive with the enclosure? or just buy stock external HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ja88n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702674239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have around 100$ budget and want external HDD. I am thinking to go either one of those ways.&lt;/p&gt;\n\n&lt;p&gt;What do you recommend and what drives to go for?&lt;/p&gt;\n\n&lt;p&gt;I will be storing media data on it and will not be using frequently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ja88n", "is_robot_indexable": true, "report_reasons": null, "author": "Franka035", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ja88n/use_desktop_hdd_as_external_drive_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ja88n/use_desktop_hdd_as_external_drive_with_the/", "subreddit_subscribers": 718392, "created_utc": 1702674239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,   \n\n\nI currently own 2 4bay Synology nas machines. And i'm looking to get something with a bit more performance.   \n\n\nIt looks like unraid is a good match for me, but i need to decide on the hardware.  I'm thinking something rack-mounted would be nice with capacity of 8+ drives at least.   \nCan you recommend a rack mounted case that can hold normal atx motherboard and psu + many drives?\n\nThanks!  \n\n\n&amp;#x200B;", "author_fullname": "t2_1192kd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendation NAS Hardware", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j66yf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702663478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,   &lt;/p&gt;\n\n&lt;p&gt;I currently own 2 4bay Synology nas machines. And i&amp;#39;m looking to get something with a bit more performance.   &lt;/p&gt;\n\n&lt;p&gt;It looks like unraid is a good match for me, but i need to decide on the hardware.  I&amp;#39;m thinking something rack-mounted would be nice with capacity of 8+ drives at least.&lt;br/&gt;\nCan you recommend a rack mounted case that can hold normal atx motherboard and psu + many drives?&lt;/p&gt;\n\n&lt;p&gt;Thanks!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j66yf", "is_robot_indexable": true, "report_reasons": null, "author": "frikandeloorlog", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j66yf/recommendation_nas_hardware/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j66yf/recommendation_nas_hardware/", "subreddit_subscribers": 718392, "created_utc": 1702663478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings fellow hoarders.  \n\nSeveral years ago I had a drive with about 3-4k movies on it.  At some point I screwed up and \"broke\" the file system or wiped the partition table.  Can't remember exactly what happened but I was able to use photorec to carve out all the files and they're all on a new drive with generic file names f12345678 and so on.  The files are all playable but I can't tell what is what.  They have just been sitting on that drive since I recovered everything, waiting for me to make time to deal with the mess of identifying the files and renaming them.  Occasionally I'll open up a few movies, check for a title credit or recognizable scene and manually rename them but it's a painful process.  \n\nThis may be a long shot but I'm looking for a tool that I can point to the directory of files that will \"view\" each one and identify the name.  I don't really care how long it takes.  I have hardware available to dedicate to this job if needed and can easily spin up a VM to work on this in the background. \n\nDon't worry, I've since gotten smarter about backups, setting up offsite copies, etc.  This is essentially the last big effort to get things fully recovered.   Thanks in advance!", "author_fullname": "t2_58o36", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for determining movie title with broken file names.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j36h8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702655547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings fellow hoarders.  &lt;/p&gt;\n\n&lt;p&gt;Several years ago I had a drive with about 3-4k movies on it.  At some point I screwed up and &amp;quot;broke&amp;quot; the file system or wiped the partition table.  Can&amp;#39;t remember exactly what happened but I was able to use photorec to carve out all the files and they&amp;#39;re all on a new drive with generic file names f12345678 and so on.  The files are all playable but I can&amp;#39;t tell what is what.  They have just been sitting on that drive since I recovered everything, waiting for me to make time to deal with the mess of identifying the files and renaming them.  Occasionally I&amp;#39;ll open up a few movies, check for a title credit or recognizable scene and manually rename them but it&amp;#39;s a painful process.  &lt;/p&gt;\n\n&lt;p&gt;This may be a long shot but I&amp;#39;m looking for a tool that I can point to the directory of files that will &amp;quot;view&amp;quot; each one and identify the name.  I don&amp;#39;t really care how long it takes.  I have hardware available to dedicate to this job if needed and can easily spin up a VM to work on this in the background. &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t worry, I&amp;#39;ve since gotten smarter about backups, setting up offsite copies, etc.  This is essentially the last big effort to get things fully recovered.   Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j36h8", "is_robot_indexable": true, "report_reasons": null, "author": "lobstahcookah", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j36h8/software_for_determining_movie_title_with_broken/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j36h8/software_for_determining_movie_title_with_broken/", "subreddit_subscribers": 718392, "created_utc": 1702655547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many videos collections and i have been looking since a while for a way on windows to have an animated preview thumbnail of parts of the video when you are hovering the mouse on it, does anyone knows if it is somehow possible? I haven't found anything good, i tried one paid software recommended on reddit but i forgot the name and it was bad anyways, it did not had that specific feature", "author_fullname": "t2_eneyhc4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Animated thumbnails for video collections?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j0v0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702649150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many videos collections and i have been looking since a while for a way on windows to have an animated preview thumbnail of parts of the video when you are hovering the mouse on it, does anyone knows if it is somehow possible? I haven&amp;#39;t found anything good, i tried one paid software recommended on reddit but i forgot the name and it was bad anyways, it did not had that specific feature&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j0v0g", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Gate6899", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j0v0g/animated_thumbnails_for_video_collections/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j0v0g/animated_thumbnails_for_video_collections/", "subreddit_subscribers": 718392, "created_utc": 1702649150.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}