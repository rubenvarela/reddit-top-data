{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everybody,\n\nThis is a bit of a self-promotion, and I don't usually do that (I have never done it here), but I figured many of you may find it helpful.\n\nFor context, I am a Head of data (&amp; analytics) engineering at a Fintech company and have interviewed hundreds of candidates.\n\nWhat I have outlined in my blog post would, obviously, not apply to every interview you may have, but I believe there are many things people don't usually discuss.\n\nPlease go wild with any questions you may have.\n\nhttps://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I interview data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ja9hq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 174, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 174, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702674334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody,&lt;/p&gt;\n\n&lt;p&gt;This is a bit of a self-promotion, and I don&amp;#39;t usually do that (I have never done it here), but I figured many of you may find it helpful.&lt;/p&gt;\n\n&lt;p&gt;For context, I am a Head of data (&amp;amp; analytics) engineering at a Fintech company and have interviewed hundreds of candidates.&lt;/p&gt;\n\n&lt;p&gt;What I have outlined in my blog post would, obviously, not apply to every interview you may have, but I believe there are many things people don&amp;#39;t usually discuss.&lt;/p&gt;\n\n&lt;p&gt;Please go wild with any questions you may have.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcome=true\"&gt;https://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcome=true&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?auto=webp&amp;s=d62d1df611eb5fd3b6c201a2ca1198764c34576e", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d233877d035936e4cdd051c8308d2ed8687ea4b4", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=658a8bd49e51df3ec03cb2ee2e11353625e67188", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6fa1c75aa9279837a1bc754209d43a4ecc494bf", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e81e57fe8694136946f0e0b90cd174e447289020", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c2d765d29649bb8542956c04e415c72a8794e9e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b00fa2cb6c57cffff61869c14eb6b4e6cbb86d4", "width": 1080, "height": 720}], "variants": {}, "id": "5zTWOZ4g6_ZEJv5W5FoiMbIfg0hblAMLhiLqgJKuoFg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ja9hq", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 67, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ja9hq/how_i_interview_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ja9hq/how_i_interview_data_engineers/", "subreddit_subscribers": 146394, "created_utc": 1702674334.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've found a lot of inspiration from Maxime Beauchemin's [article](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a) on Functional Data Engineering.The concepts of immutable partitions and dimension snapshots, in particular, have caught my attention. However, I'm trying to figure out how to handle updates in fact events, which may change over their lifecycle.\n\nTake, for example, a payment transaction. It might be AUTHORISED on DAY 1, then SETTLED on DAY 2, and possibly REFUNDED on DAY 3.According to the principles laid out in the article, I would avoid applying UPDATE operations. But how should I manage these kinds of status changes? Taking a daily snapshot of all the facts is unbearable. And if I decide to update the rows I lose most of the benefits tracked down in the article.And if the creation date is still the same for all the statuses, appending three rows would break the idea of single unit of work, that is, every job is responsible for inserting overwriting only one partition. (the table would be ideally partitioned by creation date)\n\nThank you :)", "author_fullname": "t2_8x1e3w17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Functional DE: how to deal with changing facts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jnsql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702719476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found a lot of inspiration from Maxime Beauchemin&amp;#39;s &lt;a href=\"https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a\"&gt;article&lt;/a&gt; on Functional Data Engineering.The concepts of immutable partitions and dimension snapshots, in particular, have caught my attention. However, I&amp;#39;m trying to figure out how to handle updates in fact events, which may change over their lifecycle.&lt;/p&gt;\n\n&lt;p&gt;Take, for example, a payment transaction. It might be AUTHORISED on DAY 1, then SETTLED on DAY 2, and possibly REFUNDED on DAY 3.According to the principles laid out in the article, I would avoid applying UPDATE operations. But how should I manage these kinds of status changes? Taking a daily snapshot of all the facts is unbearable. And if I decide to update the rows I lose most of the benefits tracked down in the article.And if the creation date is still the same for all the statuses, appending three rows would break the idea of single unit of work, that is, every job is responsible for inserting overwriting only one partition. (the table would be ideally partitioned by creation date)&lt;/p&gt;\n\n&lt;p&gt;Thank you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?auto=webp&amp;s=5b900fdde8f52cea2c5e7a59744fa9d6807d42ca", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a538992e7d8f748ddda7e006ef2ce7ac3d8fbe25", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=04e9ae89fc8101af35cb6a66ed19ba9aee6d7c0f", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=602a102be62cb3c7faa868d8e9f49677f92ecee3", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6cc284b82dece4fb5b4e1e4540d5e1b94cc9612b", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40a2b87bf29a3f9fefc6ac748fefe5424748fbb1", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/TJgRgV2zcr5WsBUssA1HWxNQKpgaykb--RtsLvQSO2c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=292e9d0f71a6c8996194259b487f51ff1ba877c2", "width": 1080, "height": 720}], "variants": {}, "id": "QnHDt9HlR913WRkisNYJkFlw9Lr3Bt7UcxRxAQAkPdY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jnsql", "is_robot_indexable": true, "report_reasons": null, "author": "Anselboero", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jnsql/functional_de_how_to_deal_with_changing_facts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jnsql/functional_de_how_to_deal_with_changing_facts/", "subreddit_subscribers": 146394, "created_utc": 1702719476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Many teams in my current org build services that produce data that they'd like ingested into our data platform (databricks on AWS) for analytics/DS use cases. Many of these teams seem to love CSV! \n\nAll our ETL pipelines are written using our internal pyspark based ETL library which provides modularized ETL building blocks (e.g. readers, writers, processors etc).\n\nOur standard practice is to get producers to write their data to an S3 bucket and configure an AWS Glue Crawler to produce a schema for us to use to ingest said data (lack of published schemas are a chronic issue at my org - and yes it's annoying). This is all fine until someone deploys a change somewhere which results in the output file schema changing. \n\nSuch schema changes are fine if they add a new column to the end of the file, but more often than not it's a new column in the middle of the file or a deleted column that's removed instead of being set to null. Our spark code uses the schema from glue to ingest this data but it seems that spark matches columns on position rather than name. This means that the ingested data is screwed with mixed up columns.\n\nBecause of this I'm recommending that producers who do not wish to publish a schema only export data as JSON, but we still have a number of pipelines to maintain with historical data that needs ingesting that use CSV.\n\nDoes anyone know of a convenient way to get spark to match on column name when reading CSV using a schema, or any other advice on how best to handle these scenarios on the interim until we can get the org to move on schemas.", "author_fullname": "t2_p4x73mvn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you ingesting raw CSV data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jsihr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702737394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Many teams in my current org build services that produce data that they&amp;#39;d like ingested into our data platform (databricks on AWS) for analytics/DS use cases. Many of these teams seem to love CSV! &lt;/p&gt;\n\n&lt;p&gt;All our ETL pipelines are written using our internal pyspark based ETL library which provides modularized ETL building blocks (e.g. readers, writers, processors etc).&lt;/p&gt;\n\n&lt;p&gt;Our standard practice is to get producers to write their data to an S3 bucket and configure an AWS Glue Crawler to produce a schema for us to use to ingest said data (lack of published schemas are a chronic issue at my org - and yes it&amp;#39;s annoying). This is all fine until someone deploys a change somewhere which results in the output file schema changing. &lt;/p&gt;\n\n&lt;p&gt;Such schema changes are fine if they add a new column to the end of the file, but more often than not it&amp;#39;s a new column in the middle of the file or a deleted column that&amp;#39;s removed instead of being set to null. Our spark code uses the schema from glue to ingest this data but it seems that spark matches columns on position rather than name. This means that the ingested data is screwed with mixed up columns.&lt;/p&gt;\n\n&lt;p&gt;Because of this I&amp;#39;m recommending that producers who do not wish to publish a schema only export data as JSON, but we still have a number of pipelines to maintain with historical data that needs ingesting that use CSV.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a convenient way to get spark to match on column name when reading CSV using a schema, or any other advice on how best to handle these scenarios on the interim until we can get the org to move on schemas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jsihr", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Raspberry5383", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jsihr/how_are_you_ingesting_raw_csv_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jsihr/how_are_you_ingesting_raw_csv_data/", "subreddit_subscribers": 146394, "created_utc": 1702737394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nhow do you do your continious testing? Which test tools do you use? Do you have tips for literature, concepts?\n\nThank you in advance for your response.", "author_fullname": "t2_9umujqax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataOps: how do you do your continious testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jkmrf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702706195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;how do you do your continious testing? Which test tools do you use? Do you have tips for literature, concepts?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your response.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jkmrf", "is_robot_indexable": true, "report_reasons": null, "author": "ButterscotchBulky320", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jkmrf/dataops_how_do_you_do_your_continious_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jkmrf/dataops_how_do_you_do_your_continious_testing/", "subreddit_subscribers": 146394, "created_utc": 1702706195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys\n\nData engineer here. Only ever worked in back end engineering.\n\nSide hobby of mine is collecting interesting data from around the web which I transform and aggregate etc. I'd like to start offering it back to end users for free.\n\n* What I want to do is pipe it from my cloud sql server to some kind of front end website, simple CSV etc.\n   * ..nothing fancy just something like the below where users can get a static preview of the data (this could be a png no issues)\n* And they can click download etc to download the actual full data set.\n\nI'm thinking nightly snapshots get pushed to the front ends server, overwriting previous version present allowing people to have access to a regularly updated set.\n\nBut I want it secure, I dont want any chance of the front end interrupting or allowing easier attacks to my cloud sql server?? (forgive ignorance, netsec not my specialty)\n\nYour thoughts/suggestions on avenues to achieve above? Literally NO experience in front end so completely oblivious to libraries, frameworks, saas offerings on F/E. \n\nTIA\n\n&amp;#x200B;\n\nRandom image of what I've got in my head for front end\n\nhttps://preview.redd.it/vjah84unam6c1.png?width=1314&amp;format=png&amp;auto=webp&amp;s=8e6e8abb303fda33a2410ca94393207631cff069", "author_fullname": "t2_6mnzdcoh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to push data safely from sql to front end for end users to download?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vjah84unam6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/vjah84unam6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=358157563a7b31d6e088858c72474eee94f487b2"}, {"y": 77, "x": 216, "u": "https://preview.redd.it/vjah84unam6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f3567d25536b3e97fa08b745fbf53a7f89203c1"}, {"y": 114, "x": 320, "u": "https://preview.redd.it/vjah84unam6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b47720b4287509bc7d71446bbb92bd32b50bf7e"}, {"y": 229, "x": 640, "u": "https://preview.redd.it/vjah84unam6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dfeef35eece48ab5a2b7ac9a55837e59327c88c6"}, {"y": 344, "x": 960, "u": "https://preview.redd.it/vjah84unam6c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b04ff4b519b8fa44b05fc354f5e85c7e89c61c29"}, {"y": 387, "x": 1080, "u": "https://preview.redd.it/vjah84unam6c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce39fce51825f46e65b7191964ae0768f8f8754b"}], "s": {"y": 472, "x": 1314, "u": "https://preview.redd.it/vjah84unam6c1.png?width=1314&amp;format=png&amp;auto=webp&amp;s=8e6e8abb303fda33a2410ca94393207631cff069"}, "id": "vjah84unam6c1"}}, "name": "t3_18jmumu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NH6nekM3IB9DFNi3DGzoOLTLqFOnuI08G65HpU1Y9ns.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702715285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys&lt;/p&gt;\n\n&lt;p&gt;Data engineer here. Only ever worked in back end engineering.&lt;/p&gt;\n\n&lt;p&gt;Side hobby of mine is collecting interesting data from around the web which I transform and aggregate etc. I&amp;#39;d like to start offering it back to end users for free.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What I want to do is pipe it from my cloud sql server to some kind of front end website, simple CSV etc.\n\n&lt;ul&gt;\n&lt;li&gt;..nothing fancy just something like the below where users can get a static preview of the data (this could be a png no issues)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;And they can click download etc to download the actual full data set.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m thinking nightly snapshots get pushed to the front ends server, overwriting previous version present allowing people to have access to a regularly updated set.&lt;/p&gt;\n\n&lt;p&gt;But I want it secure, I dont want any chance of the front end interrupting or allowing easier attacks to my cloud sql server?? (forgive ignorance, netsec not my specialty)&lt;/p&gt;\n\n&lt;p&gt;Your thoughts/suggestions on avenues to achieve above? Literally NO experience in front end so completely oblivious to libraries, frameworks, saas offerings on F/E. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Random image of what I&amp;#39;ve got in my head for front end&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vjah84unam6c1.png?width=1314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e6e8abb303fda33a2410ca94393207631cff069\"&gt;https://preview.redd.it/vjah84unam6c1.png?width=1314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e6e8abb303fda33a2410ca94393207631cff069&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jmumu", "is_robot_indexable": true, "report_reasons": null, "author": "_DESTRUCTION", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jmumu/how_to_push_data_safely_from_sql_to_front_end_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jmumu/how_to_push_data_safely_from_sql_to_front_end_for/", "subreddit_subscribers": 146394, "created_utc": 1702715285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nGreetings fellow data enthusiasts! I'm a Data Analyst with 2 years of experience, specializing in ETL tools such as Abinitio and Informatica. Currently, I'm diving into the exciting realm of the modern data stack and seeking advice on the skills I should acquire in the next six months.\n\n**Current Skillset:**\n\n* ETL tools: Abinitio, Informatica\n* Languages: SQL, Java, UNIX\n* Basic understanding of cloud and data warehousing\n\n**Career Transition Plan:** In the upcoming six months, I plan to make a switch in the modern data stack. Given my quick learning ability, I'm eager to know:\n\n**Python vs. Scala:**  \n\n\n* Is Scala still relevant, or is it considered outdated? (Referencing a post by Zach Wilson)\n* Recommendations on whether to focus on Python or Scala for the modern data stack.\n\n**SQL Proficiency:**  \n\n\n* What level of SQL knowledge is typically expected for someone transitioning into the modern data stack?\n\n**Software Engineering Knowledge:**  \n\n\n* Essential software engineering concepts and practices relevant to a data engineer.\n\n**Data Structures and Algorithms (DSA):**  \n\n\n* The importance of DSA in data engineering roles.\n* A list of DSA topics commonly asked during interviews.\n\n**Additional Tech Stack:**  \n\n\n* Other technologies or tools that complement a data engineer's skill set.\n\n**Cloud Knowledge:**  \n\n\n* The level of proficiency in cloud technologies (e.g., AWS, Azure, GCP) required for a data engineer in the current job market.\n\nIndian Data engineer folks need your help also here for salary expectation (\ud83d\ude05).\n\n**Salary Expectations:** Currently earning 7 LPA, I'm curious about the salary expectations in the market for someone with +2 years of experience in data engineering. Is aiming for 15 LPA too ambitious?\n\n**Closing Thoughts:** If you have additional insights or points to consider in my career transition journey, please share your suggestions. Your help is greatly appreciated!\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_ijlf61xv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Guidance on Career Transition and Salary Expectations for Indian Data Engineers.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jusv1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702744217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings fellow data enthusiasts! I&amp;#39;m a Data Analyst with 2 years of experience, specializing in ETL tools such as Abinitio and Informatica. Currently, I&amp;#39;m diving into the exciting realm of the modern data stack and seeking advice on the skills I should acquire in the next six months.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Skillset:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ETL tools: Abinitio, Informatica&lt;/li&gt;\n&lt;li&gt;Languages: SQL, Java, UNIX&lt;/li&gt;\n&lt;li&gt;Basic understanding of cloud and data warehousing&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Career Transition Plan:&lt;/strong&gt; In the upcoming six months, I plan to make a switch in the modern data stack. Given my quick learning ability, I&amp;#39;m eager to know:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Python vs. Scala:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is Scala still relevant, or is it considered outdated? (Referencing a post by Zach Wilson)&lt;/li&gt;\n&lt;li&gt;Recommendations on whether to focus on Python or Scala for the modern data stack.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;SQL Proficiency:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What level of SQL knowledge is typically expected for someone transitioning into the modern data stack?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Software Engineering Knowledge:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Essential software engineering concepts and practices relevant to a data engineer.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Structures and Algorithms (DSA):&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The importance of DSA in data engineering roles.&lt;/li&gt;\n&lt;li&gt;A list of DSA topics commonly asked during interviews.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Additional Tech Stack:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Other technologies or tools that complement a data engineer&amp;#39;s skill set.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Cloud Knowledge:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The level of proficiency in cloud technologies (e.g., AWS, Azure, GCP) required for a data engineer in the current job market.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Indian Data engineer folks need your help also here for salary expectation (\ud83d\ude05).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Salary Expectations:&lt;/strong&gt; Currently earning 7 LPA, I&amp;#39;m curious about the salary expectations in the market for someone with +2 years of experience in data engineering. Is aiming for 15 LPA too ambitious?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Closing Thoughts:&lt;/strong&gt; If you have additional insights or points to consider in my career transition journey, please share your suggestions. Your help is greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18jusv1", "is_robot_indexable": true, "report_reasons": null, "author": "solouchiha64", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jusv1/seeking_guidance_on_career_transition_and_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jusv1/seeking_guidance_on_career_transition_and_salary/", "subreddit_subscribers": 146394, "created_utc": 1702744217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This may seem like a dumb question, I am working on pulling data from an  RDBMS,  with a Python script and placing it into a data warehouse for visualization purposes. My question is, in this instance is it best practice to make as many transformations in my  RDBMS via SQL, create the table extract and load it. Or should this be done only in my data ware house. The data in my in my table I would be creating would be me averaging data, also not sure how that would effect me when using PowerBI.\n\nTIA ALL!", "author_fullname": "t2_8txv38ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BEST ETL TRANSFORMING PRACTICE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jb8hr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702677553.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702676932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This may seem like a dumb question, I am working on pulling data from an  RDBMS,  with a Python script and placing it into a data warehouse for visualization purposes. My question is, in this instance is it best practice to make as many transformations in my  RDBMS via SQL, create the table extract and load it. Or should this be done only in my data ware house. The data in my in my table I would be creating would be me averaging data, also not sure how that would effect me when using PowerBI.&lt;/p&gt;\n\n&lt;p&gt;TIA ALL!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jb8hr", "is_robot_indexable": true, "report_reasons": null, "author": "Fraiz24", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jb8hr/best_etl_transforming_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jb8hr/best_etl_transforming_practice/", "subreddit_subscribers": 146394, "created_utc": 1702676932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, my company is currently storing a ton of data in databricks and i would like to do \u201cstuff\u201d with this data. Like analysis, maybe some graphs, maybe some code to get some specific metrics. Not being a dataengineer (but im a developer) what are some things i can learn in this space to join meetings and not be 100% lost. Is this one of pandas use cases? ", "author_fullname": "t2_2gjyr66m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks for non dataengineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jb2ze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702676793.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702676517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, my company is currently storing a ton of data in databricks and i would like to do \u201cstuff\u201d with this data. Like analysis, maybe some graphs, maybe some code to get some specific metrics. Not being a dataengineer (but im a developer) what are some things i can learn in this space to join meetings and not be 100% lost. Is this one of pandas use cases? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jb2ze", "is_robot_indexable": true, "report_reasons": null, "author": "SuperLucas2000", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jb2ze/databricks_for_non_dataengineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jb2ze/databricks_for_non_dataengineers/", "subreddit_subscribers": 146394, "created_utc": 1702676517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This week, I created a dbt model that pinpoints the NBA's top \"one-hit wonders.\"\n\n\"One hit wonder\" = Players who had 1 season that's dramatically better than the avg. of all their other seasons.\n\nTo find these players, I used a formula called Player Efficiency Rating (PER) across seasons. The PER formula condenses a player's contributions into a single, comprehensive metric. By weighing 12 distinct stats, each with its unique importance, PER offers a all-in-one metric to identify a players performance.\n\nDisclaimer: PER isn't the end-all and be-all of player metrics, it points me in the right direction.\n\nTools used:\n\n\\- \ud835\udc08\ud835\udc27\ud835\udc20\ud835\udc1e\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27: public NBA API + Python\n\n\\- \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1a\ud835\udc20\ud835\udc1e: DuckDB (development) &amp; Snowflake (Production)\n\n\\- \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c (dbt): Paradime\n\n\\- \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc27\ud835\udc20 (\ud835\udc01\ud835\udc08) -Lightdash\n\nIf you're curious, here's the repo:  \nhttps://github.com/jpooksy/NBA\\_Data\\_Modeling  \n\n\nhttps://preview.redd.it/69709ezuwi6c1.png?width=978&amp;format=png&amp;auto=webp&amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590\n\n&amp;#x200B;", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analyzing \"One hit wonder\" NBA Players", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 110, "top_awarded_type": null, "hide_score": false, "media_metadata": {"69709ezuwi6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3190b528ffc53e48e3fc3191fcc8e57763ae961c"}, {"y": 170, "x": 216, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6f5c589ea0dc15a126060d837e4e4dc82803a0f"}, {"y": 252, "x": 320, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0dc820f9448ed220606e4ffc0a0673867b069ba"}, {"y": 504, "x": 640, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=66ea372b61096ac6e87f289064776282d14b7eaa"}, {"y": 756, "x": 960, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd3fa91bf0d7504759d0c1e22337ecd4d71c9da7"}], "s": {"y": 771, "x": 978, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;format=png&amp;auto=webp&amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590"}, "id": "69709ezuwi6c1"}}, "name": "t3_18ja96u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/73whFpo_IHCGK7dqQzOmjYuICSsjOFhke4-OP5fs-Yo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702674311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This week, I created a dbt model that pinpoints the NBA&amp;#39;s top &amp;quot;one-hit wonders.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;One hit wonder&amp;quot; = Players who had 1 season that&amp;#39;s dramatically better than the avg. of all their other seasons.&lt;/p&gt;\n\n&lt;p&gt;To find these players, I used a formula called Player Efficiency Rating (PER) across seasons. The PER formula condenses a player&amp;#39;s contributions into a single, comprehensive metric. By weighing 12 distinct stats, each with its unique importance, PER offers a all-in-one metric to identify a players performance.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: PER isn&amp;#39;t the end-all and be-all of player metrics, it points me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;Tools used:&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc08\ud835\udc27\ud835\udc20\ud835\udc1e\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27: public NBA API + Python&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1a\ud835\udc20\ud835\udc1e: DuckDB (development) &amp;amp; Snowflake (Production)&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c (dbt): Paradime&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc27\ud835\udc20 (\ud835\udc01\ud835\udc08) -Lightdash&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, here&amp;#39;s the repo:&lt;br/&gt;\n&lt;a href=\"https://github.com/jpooksy/NBA%5C_Data%5C_Modeling\"&gt;https://github.com/jpooksy/NBA\\_Data\\_Modeling&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590\"&gt;https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18ja96u", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ja96u/analyzing_one_hit_wonder_nba_players/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ja96u/analyzing_one_hit_wonder_nba_players/", "subreddit_subscribers": 146394, "created_utc": 1702674311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "HI,\n\nIM new in Dagster and i come more from DevOps than from DataEngineering. My job will be basically to setup dagster to be able to run ETLs on dev/test/production.\n\nMy data team is creating a PoC. All data projects should be part of our SDLC, and im looking into how to deploy Dagster to different stages. I will deploy Dagster on K8s, that, i guess it will be the source of truth of running jobs on production. And again, i guess, an instance per environment.\n\nI went to the documentation, building some pipes on local computer. But,\n\ncan we use git to host all the source code?\n\ncan i connect Dagster (the instace deployed on k8s) to a git repo? (the concept repositories on dagster is not clear to me).\n\nis it Dagster deployment GitOps oriented (a prod instance check the master code and setup the assets, jobs, etc)?\n\ndoes it Dagsters has the concept \"environment\", where i use different variables, secrets per environment?\n\nThanks\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_7uozpvtp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster DevOps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jx1cw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702750445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;HI,&lt;/p&gt;\n\n&lt;p&gt;IM new in Dagster and i come more from DevOps than from DataEngineering. My job will be basically to setup dagster to be able to run ETLs on dev/test/production.&lt;/p&gt;\n\n&lt;p&gt;My data team is creating a PoC. All data projects should be part of our SDLC, and im looking into how to deploy Dagster to different stages. I will deploy Dagster on K8s, that, i guess it will be the source of truth of running jobs on production. And again, i guess, an instance per environment.&lt;/p&gt;\n\n&lt;p&gt;I went to the documentation, building some pipes on local computer. But,&lt;/p&gt;\n\n&lt;p&gt;can we use git to host all the source code?&lt;/p&gt;\n\n&lt;p&gt;can i connect Dagster (the instace deployed on k8s) to a git repo? (the concept repositories on dagster is not clear to me).&lt;/p&gt;\n\n&lt;p&gt;is it Dagster deployment GitOps oriented (a prod instance check the master code and setup the assets, jobs, etc)?&lt;/p&gt;\n\n&lt;p&gt;does it Dagsters has the concept &amp;quot;environment&amp;quot;, where i use different variables, secrets per environment?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jx1cw", "is_robot_indexable": true, "report_reasons": null, "author": "Ancient_Canary1148", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jx1cw/dagster_devops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jx1cw/dagster_devops/", "subreddit_subscribers": 146394, "created_utc": 1702750445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any helpful guide on spark, which goes deeper into working of spark e.g how to deal with datasets larger than memory of spark cluster. And how to effectively work with huge datasets, something like that. Video, blog, course anything will be helpful.", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Course, blog or resources for guide on dealing with large datasets in spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jlv80", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702711043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any helpful guide on spark, which goes deeper into working of spark e.g how to deal with datasets larger than memory of spark cluster. And how to effectively work with huge datasets, something like that. Video, blog, course anything will be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jlv80", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jlv80/course_blog_or_resources_for_guide_on_dealing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jlv80/course_blog_or_resources_for_guide_on_dealing/", "subreddit_subscribers": 146394, "created_utc": 1702711043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nAt my job I've set up an iot data load in databricks, which follow the folling steps:\n\n1. Load of json files via autoloader into a delta table with some metadata added and append only\n\n2. Read raw delta via stream and cdc, some transformations like jsonparsing, data cleansing etc. and then merge into the qualified table.\n\nLike you can see the amount of being done on the data is not that crazy and the amount of data being processed is maybe max 100 gb per day. The load job is run daily and loads data from a s3 bucket where new data lands every hour.\n\nMy issue now is that the amount of tasks being done and the runtime remains the same. No difference if i run the job daily or hourly even though the amount of data being processed should be different. And it makes me, to put it mildly, crazy.\n\nAny ideas? What can i check to alleviate the issue?", "author_fullname": "t2_bk0fqe9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark speed up batch job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jq3c7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702729203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;At my job I&amp;#39;ve set up an iot data load in databricks, which follow the folling steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Load of json files via autoloader into a delta table with some metadata added and append only&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Read raw delta via stream and cdc, some transformations like jsonparsing, data cleansing etc. and then merge into the qualified table.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Like you can see the amount of being done on the data is not that crazy and the amount of data being processed is maybe max 100 gb per day. The load job is run daily and loads data from a s3 bucket where new data lands every hour.&lt;/p&gt;\n\n&lt;p&gt;My issue now is that the amount of tasks being done and the runtime remains the same. No difference if i run the job daily or hourly even though the amount of data being processed should be different. And it makes me, to put it mildly, crazy.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? What can i check to alleviate the issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jq3c7", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable_Bake_783", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jq3c7/spark_speed_up_batch_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jq3c7/spark_speed_up_batch_job/", "subreddit_subscribers": 146394, "created_utc": 1702729203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nSo, in about two months I'm expected to start a 6 month internship followed by fulltime contract from a Business Consulting company in EU full remote.\n\nI do have a bachelor degree in computer science\nand been working for a year as a full-stack dev junior using Python's micro framework Flask, Jinja2, MySQL, PostgreSql, gitlab, html,css,jscript,JSON,bootstrap, ect to build CRM platforms for our clients from scratch.\n\nI've also used in the past Datacamp's Data Scientist path so have a basic understanding of libraries like pandas,numpy,matplot.\n\nAlso do have Azure AZ900 certificate if it's worth anything.\n\nNow, the company has told me that I'm going to work with (at least):\n\n1. Pyspark\n2. Elastic search\n3. Amazon AWS Athena\n4. Git\n\nTherefore I was hoping someone could/would recommend me any resource or course that brings these arguments altogether. Obviously I do understand that in a 2 month timeframe it's impossible to reach a good level but it's still worth than nothing.\n\nAny tips,hints,suggestions,opinions will be very much accepted and definitely help me a lot!\n\nP.S: To spare you some time, Have I ever worked with Big data or huge datasets? No, but I'm a database collector. I do have my own rack at home and a total of more than 2 TB of data. Doni know other programming languages? Yes, C and a little PHP and Java. Have I ever used Unix environments? Yes, d'Istria like Fedora, Manjaro and Ubuntu for person fun or school projects.", "author_fullname": "t2_6fzzcr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A soon-to-be DE asking for suggestions and resources!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jsem5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702737313.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702737063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;So, in about two months I&amp;#39;m expected to start a 6 month internship followed by fulltime contract from a Business Consulting company in EU full remote.&lt;/p&gt;\n\n&lt;p&gt;I do have a bachelor degree in computer science\nand been working for a year as a full-stack dev junior using Python&amp;#39;s micro framework Flask, Jinja2, MySQL, PostgreSql, gitlab, html,css,jscript,JSON,bootstrap, ect to build CRM platforms for our clients from scratch.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also used in the past Datacamp&amp;#39;s Data Scientist path so have a basic understanding of libraries like pandas,numpy,matplot.&lt;/p&gt;\n\n&lt;p&gt;Also do have Azure AZ900 certificate if it&amp;#39;s worth anything.&lt;/p&gt;\n\n&lt;p&gt;Now, the company has told me that I&amp;#39;m going to work with (at least):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Pyspark&lt;/li&gt;\n&lt;li&gt;Elastic search&lt;/li&gt;\n&lt;li&gt;Amazon AWS Athena&lt;/li&gt;\n&lt;li&gt;Git&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Therefore I was hoping someone could/would recommend me any resource or course that brings these arguments altogether. Obviously I do understand that in a 2 month timeframe it&amp;#39;s impossible to reach a good level but it&amp;#39;s still worth than nothing.&lt;/p&gt;\n\n&lt;p&gt;Any tips,hints,suggestions,opinions will be very much accepted and definitely help me a lot!&lt;/p&gt;\n\n&lt;p&gt;P.S: To spare you some time, Have I ever worked with Big data or huge datasets? No, but I&amp;#39;m a database collector. I do have my own rack at home and a total of more than 2 TB of data. Doni know other programming languages? Yes, C and a little PHP and Java. Have I ever used Unix environments? Yes, d&amp;#39;Istria like Fedora, Manjaro and Ubuntu for person fun or school projects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jsem5", "is_robot_indexable": true, "report_reasons": null, "author": "lolcol1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jsem5/a_soontobe_de_asking_for_suggestions_and_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jsem5/a_soontobe_de_asking_for_suggestions_and_resources/", "subreddit_subscribers": 146394, "created_utc": 1702737063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you all manage your _source.yaml file in a dbt project? The way we implement those can get unruly really quick. \n\nSo far we have four staging tables in sources.yaml and its up to 1,500 lines.\n\nIm not really sold into the whole \u201cinfrastructure as yaml files\u201d deal yet because of unruly data like this, but I am super open to ideas on how to manage it in a cleaner way.\n\nThanks!", "author_fullname": "t2_u59p0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing _source.yaml in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jedfl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702685610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you all manage your _source.yaml file in a dbt project? The way we implement those can get unruly really quick. &lt;/p&gt;\n\n&lt;p&gt;So far we have four staging tables in sources.yaml and its up to 1,500 lines.&lt;/p&gt;\n\n&lt;p&gt;Im not really sold into the whole \u201cinfrastructure as yaml files\u201d deal yet because of unruly data like this, but I am super open to ideas on how to manage it in a cleaner way.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jedfl", "is_robot_indexable": true, "report_reasons": null, "author": "fleegz2007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jedfl/managing_sourceyaml_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jedfl/managing_sourceyaml_in_dbt/", "subreddit_subscribers": 146394, "created_utc": 1702685610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data warehouse is structured like this: \n\n1) we have a number of integrations in stitch that E&amp;L data from various sources like our ERP, CRM, other databases into our data warehouse. Each of these integrations has its own schedule. \n\n2) we have a \"first layer\" transform dbt project that takes the \"raw\" data landed by these stitch integrations and does some light transformy stuff like friendly renaming, timezone standardization, even denormalization in some rare cases. \n\n3) then we have some downstream dbt projects that prepare analytics reports for the business. The first layer project and analytics projects are directly coupled. Meaning we have a singular data warehouse schedule that runs the first layer project and then immediately starts execution of the analytics projects after the first task finishes. I believe my predecessors were planning to decouple these but never got around to it and we have been operating like this for a couple of years now. \n\nI am exploring ways to decouple these projects for a handful of reasons including shortening the duration of the main data warehouse refresh job so that we can increase its frequency, moving to a sort of event-based architecture where only do something when it is needed and not any more often (we are currently running models in this first layer more frequently than stitch loads data in some cases), etc. \n\nStitch has [post-load hooks](https://www.stitchdata.com/docs/developers/webhooks/post-load-webhooks) where an API call is made after a successful load, we could build a service that takes these calls and fires tasks to run the specific model corresponding to the data that was just loaded. \n\nIssues I see are: \n\n1) naming conventions. It's easy for a human with a basic understanding of our codebase to determine what data model corresponds with what source table, but harder to do this programmatically. I have not found anything in dbt's manifest documents that would allow us to connect say our `customers.sql` data model with the `customer` table in our erp. \n\n2) I mentioned we do some denormalization, meaning some data models have more than one source. We would either need to build a way to determine if all the sources have loaded before we start the downstream model or we would need to run a model two times if we simply said \"x table load finished -&gt; start model y\" and this might violate referential integrity between the two tables. Timing between this process and our main analytics refresh might sometimes result in \"missing\" data because we were only able to load one source before we started building our downstream reports. \n\n3) stitch would, for better or worse, become our orchestration platform at this point. We could leave the analytics projects running on a fixed schedule, but stitch would trigger the extract, load, and now the transform for these models. Having this all in one place would offer some convenience but I am leaning towards the idea that stitch as our orchestrator is not a good thing since this is not really what stitch is designed to do. \n\n4) complexity. Our system as it is is simple. Moving to an architecture like this would introduce complexity, it would make processes more granular rather than broad \"ok now run everything\" like we have today. More complexity introduces risk from various angles. \n\nMost of this is not specific to stitch. Any tool that we would consider for this purpose would bring the same challenges. But we use stitch today.\n\nAnyway, I am wondering if anyone has any experience with this part of the platform that would be willing to weigh in.", "author_fullname": "t2_6nf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using Stitch's post-load hooks to invoke dbt models in their data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jcohf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702680840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data warehouse is structured like this: &lt;/p&gt;\n\n&lt;p&gt;1) we have a number of integrations in stitch that E&amp;amp;L data from various sources like our ERP, CRM, other databases into our data warehouse. Each of these integrations has its own schedule. &lt;/p&gt;\n\n&lt;p&gt;2) we have a &amp;quot;first layer&amp;quot; transform dbt project that takes the &amp;quot;raw&amp;quot; data landed by these stitch integrations and does some light transformy stuff like friendly renaming, timezone standardization, even denormalization in some rare cases. &lt;/p&gt;\n\n&lt;p&gt;3) then we have some downstream dbt projects that prepare analytics reports for the business. The first layer project and analytics projects are directly coupled. Meaning we have a singular data warehouse schedule that runs the first layer project and then immediately starts execution of the analytics projects after the first task finishes. I believe my predecessors were planning to decouple these but never got around to it and we have been operating like this for a couple of years now. &lt;/p&gt;\n\n&lt;p&gt;I am exploring ways to decouple these projects for a handful of reasons including shortening the duration of the main data warehouse refresh job so that we can increase its frequency, moving to a sort of event-based architecture where only do something when it is needed and not any more often (we are currently running models in this first layer more frequently than stitch loads data in some cases), etc. &lt;/p&gt;\n\n&lt;p&gt;Stitch has &lt;a href=\"https://www.stitchdata.com/docs/developers/webhooks/post-load-webhooks\"&gt;post-load hooks&lt;/a&gt; where an API call is made after a successful load, we could build a service that takes these calls and fires tasks to run the specific model corresponding to the data that was just loaded. &lt;/p&gt;\n\n&lt;p&gt;Issues I see are: &lt;/p&gt;\n\n&lt;p&gt;1) naming conventions. It&amp;#39;s easy for a human with a basic understanding of our codebase to determine what data model corresponds with what source table, but harder to do this programmatically. I have not found anything in dbt&amp;#39;s manifest documents that would allow us to connect say our &lt;code&gt;customers.sql&lt;/code&gt; data model with the &lt;code&gt;customer&lt;/code&gt; table in our erp. &lt;/p&gt;\n\n&lt;p&gt;2) I mentioned we do some denormalization, meaning some data models have more than one source. We would either need to build a way to determine if all the sources have loaded before we start the downstream model or we would need to run a model two times if we simply said &amp;quot;x table load finished -&amp;gt; start model y&amp;quot; and this might violate referential integrity between the two tables. Timing between this process and our main analytics refresh might sometimes result in &amp;quot;missing&amp;quot; data because we were only able to load one source before we started building our downstream reports. &lt;/p&gt;\n\n&lt;p&gt;3) stitch would, for better or worse, become our orchestration platform at this point. We could leave the analytics projects running on a fixed schedule, but stitch would trigger the extract, load, and now the transform for these models. Having this all in one place would offer some convenience but I am leaning towards the idea that stitch as our orchestrator is not a good thing since this is not really what stitch is designed to do. &lt;/p&gt;\n\n&lt;p&gt;4) complexity. Our system as it is is simple. Moving to an architecture like this would introduce complexity, it would make processes more granular rather than broad &amp;quot;ok now run everything&amp;quot; like we have today. More complexity introduces risk from various angles. &lt;/p&gt;\n\n&lt;p&gt;Most of this is not specific to stitch. Any tool that we would consider for this purpose would bring the same challenges. But we use stitch today.&lt;/p&gt;\n\n&lt;p&gt;Anyway, I am wondering if anyone has any experience with this part of the platform that would be willing to weigh in.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jcohf", "is_robot_indexable": true, "report_reasons": null, "author": "radil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jcohf/is_anyone_using_stitchs_postload_hooks_to_invoke/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jcohf/is_anyone_using_stitchs_postload_hooks_to_invoke/", "subreddit_subscribers": 146394, "created_utc": 1702680840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please a road map for data engineer", "author_fullname": "t2_aonc7pf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Road map for Data Engineer please", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jxeuy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.14, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702751506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please a road map for data engineer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jxeuy", "is_robot_indexable": true, "report_reasons": null, "author": "tycheff", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jxeuy/road_map_for_data_engineer_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jxeuy/road_map_for_data_engineer_please/", "subreddit_subscribers": 146394, "created_utc": 1702751506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "NordVPN coupons on Reddit are an excellent way to save money while protecting online privacy. With the rising cost of internet service, it is essential to take advantage of every opportunity available to reduce costs without compromising security. NordVPN offers a wide range of discounts and deals on their services that can be found through their subreddit page, making them an ideal choice for anyone looking for a reliable VPN provider at an affordable price. \n\nOne great benefit of using NordVPN coupons on Reddit is the ability to customize your package based on what you need from the service. By browsing through different subreddits related to this topic, you can find information about specific packages and promo codes that could help you get more bang for your buck when signing up with NordVPN. They also offer special deals such as monthly or yearly plans, which provide additional savings depending on how long you commit yourself to them - perfect if budgeting is essential in helping secure online connections! \n\nFinally, another great reason why people should consider using Nordvpn coupon Reddit over other providers' offeriproviders'ause they have a strong commitment towards user safety and security by providing military-grade encryption protocols along with various features like kill switch protection so no one will ever know who's behind thoswho'saddresses accessing sensitive data or websites! Plus, they have 24/7 customer support ready at any time should any issues arise during the setup process - all these ensure customers always feel safe &amp; protected while surfing the web securely, even when away from a home network connection (e.g., public wifi). All in all, whether someone needs primary VPN access or wants higher levels of anonymity &amp; protection, choosing the right deal via NordVPN coupon, Reddit could prove very useful!\n\n* '2YCOUPON' (click [here](https://2h.ae/DQur)), 2-year plan +3 extra months, **$3.19/month**, 70% discount\n* '1YCOUPON' (click [here](https://2h.ae/DQur)), 1-year plan, **$4.99/month**, 50% discount\n\n**Or get up to 68% on NordVPN's Complete plan, including NordPass and NordLocker!**\n\n* 2-year plan + 3 extra months, **$5.19/month**, 70% discount\n* 1-year plan, **$6.99/month**, 57% discount\n\n**You can apply the coupon code manually to get a discount:**\n\n1. Go to\u00a0[https://2h.ae/DQur](https://2h.ae/DQur)\n2. Select the wanted plan and in the order summary, juts below the total price, click on \"Got coupon?\"\n3. Copy the coupon or write it down into the box and click \"Apply\"\n4. Now with the coupon applied, you will see the discounted price and it's new total.", "author_fullname": "t2_kbmwqnui2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Find NordVPN Coupons Reddit Users Love!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jxmxh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.06, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702752153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NordVPN coupons on Reddit are an excellent way to save money while protecting online privacy. With the rising cost of internet service, it is essential to take advantage of every opportunity available to reduce costs without compromising security. NordVPN offers a wide range of discounts and deals on their services that can be found through their subreddit page, making them an ideal choice for anyone looking for a reliable VPN provider at an affordable price. &lt;/p&gt;\n\n&lt;p&gt;One great benefit of using NordVPN coupons on Reddit is the ability to customize your package based on what you need from the service. By browsing through different subreddits related to this topic, you can find information about specific packages and promo codes that could help you get more bang for your buck when signing up with NordVPN. They also offer special deals such as monthly or yearly plans, which provide additional savings depending on how long you commit yourself to them - perfect if budgeting is essential in helping secure online connections! &lt;/p&gt;\n\n&lt;p&gt;Finally, another great reason why people should consider using Nordvpn coupon Reddit over other providers&amp;#39; offeriproviders&amp;#39;ause they have a strong commitment towards user safety and security by providing military-grade encryption protocols along with various features like kill switch protection so no one will ever know who&amp;#39;s behind thoswho&amp;#39;saddresses accessing sensitive data or websites! Plus, they have 24/7 customer support ready at any time should any issues arise during the setup process - all these ensure customers always feel safe &amp;amp; protected while surfing the web securely, even when away from a home network connection (e.g., public wifi). All in all, whether someone needs primary VPN access or wants higher levels of anonymity &amp;amp; protection, choosing the right deal via NordVPN coupon, Reddit could prove very useful!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;#39;2YCOUPON&amp;#39; (click &lt;a href=\"https://2h.ae/DQur\"&gt;here&lt;/a&gt;), 2-year plan +3 extra months, &lt;strong&gt;$3.19/month&lt;/strong&gt;, 70% discount&lt;/li&gt;\n&lt;li&gt;&amp;#39;1YCOUPON&amp;#39; (click &lt;a href=\"https://2h.ae/DQur\"&gt;here&lt;/a&gt;), 1-year plan, &lt;strong&gt;$4.99/month&lt;/strong&gt;, 50% discount&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Or get up to 68% on NordVPN&amp;#39;s Complete plan, including NordPass and NordLocker!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2-year plan + 3 extra months, &lt;strong&gt;$5.19/month&lt;/strong&gt;, 70% discount&lt;/li&gt;\n&lt;li&gt;1-year plan, &lt;strong&gt;$6.99/month&lt;/strong&gt;, 57% discount&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;You can apply the coupon code manually to get a discount:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to\u00a0&lt;a href=\"https://2h.ae/DQur\"&gt;https://2h.ae/DQur&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Select the wanted plan and in the order summary, juts below the total price, click on &amp;quot;Got coupon?&amp;quot;&lt;/li&gt;\n&lt;li&gt;Copy the coupon or write it down into the box and click &amp;quot;Apply&amp;quot;&lt;/li&gt;\n&lt;li&gt;Now with the coupon applied, you will see the discounted price and it&amp;#39;s new total.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/a7K4iW88vBhpiBOyed4P0qMKPalb1KP47MnGxvLuAj8.jpg?auto=webp&amp;s=dabaf81b32e3a2a0185d3638db0bcbe8994c38dd", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/a7K4iW88vBhpiBOyed4P0qMKPalb1KP47MnGxvLuAj8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=93c56b2da603fde097a6d9df8442a51d9a0ea1f3", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/a7K4iW88vBhpiBOyed4P0qMKPalb1KP47MnGxvLuAj8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=078d4930d7fca0ddce7c13911e5e385606bbecda", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/a7K4iW88vBhpiBOyed4P0qMKPalb1KP47MnGxvLuAj8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bab915cdc409dbaaf73601e7e931889bd1e9a2a", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/a7K4iW88vBhpiBOyed4P0qMKPalb1KP47MnGxvLuAj8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=01192a9804eb99c83539f04be1fd19f78ec84d28", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/a7K4iW88vBhpiBOyed4P0qMKPalb1KP47MnGxvLuAj8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca338678d1e5592f574db74c8c99b7288b0bc5ff", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/a7K4iW88vBhpiBOyed4P0qMKPalb1KP47MnGxvLuAj8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=beee52c178725f2e6ff1ec5893bb5930a216c68a", "width": 1080, "height": 567}], "variants": {}, "id": "2ov6fCpqznRNdB1VfZtiN02u-giSii9UKGYx8chkGwY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18jxmxh", "is_robot_indexable": true, "report_reasons": null, "author": "FrederickJones7", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jxmxh/find_nordvpn_coupons_reddit_users_love/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jxmxh/find_nordvpn_coupons_reddit_users_love/", "subreddit_subscribers": 146394, "created_utc": 1702752153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi everyone I'm currently working in a service-based company in india with 1.5 yrs exp as a DE but the work i did here is just building flask apis for a mlops platform using existing sdks of popular cloud platforms in python and since last two months there is not much work  and there was nothing much to learn so i asked my manager about team change for wich he said yes but from one of my seniors told me that he is putting me in a maintenance project which lasts for 3 years in which the learning is nil and my manager didn't even admit that to me (I feel he is screwing my career just for billability from the client) so it is going to waste my time again with no data engineering skills so what is best possible option for me to switch the company as quickly as possible because I am not learning much here and if I talk to any of the seniors about upskilling what they tell me is to do some certifications (Honestly speaking most of the things i learned for clearing certifications don't stick in my mind and it is not giving me the fundamentals required for Data engineering)\n\nNeed your thoughts and suggestions on how to get out of this", "author_fullname": "t2_r4eilh96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice regarding my career (Please don't ignore my career is in jeopardy)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jqv5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.11, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702732075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone I&amp;#39;m currently working in a service-based company in india with 1.5 yrs exp as a DE but the work i did here is just building flask apis for a mlops platform using existing sdks of popular cloud platforms in python and since last two months there is not much work  and there was nothing much to learn so i asked my manager about team change for wich he said yes but from one of my seniors told me that he is putting me in a maintenance project which lasts for 3 years in which the learning is nil and my manager didn&amp;#39;t even admit that to me (I feel he is screwing my career just for billability from the client) so it is going to waste my time again with no data engineering skills so what is best possible option for me to switch the company as quickly as possible because I am not learning much here and if I talk to any of the seniors about upskilling what they tell me is to do some certifications (Honestly speaking most of the things i learned for clearing certifications don&amp;#39;t stick in my mind and it is not giving me the fundamentals required for Data engineering)&lt;/p&gt;\n\n&lt;p&gt;Need your thoughts and suggestions on how to get out of this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jqv5o", "is_robot_indexable": true, "report_reasons": null, "author": "Left_Tip_7300", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jqv5o/need_advice_regarding_my_career_please_dont/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jqv5o/need_advice_regarding_my_career_please_dont/", "subreddit_subscribers": 146394, "created_utc": 1702732075.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}