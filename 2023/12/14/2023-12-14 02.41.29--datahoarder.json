{"kind": "Listing", "data": {"after": "t3_18hw3v6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After adding all the drivers I got during BF, I now have an unRAID server of 200TB and growing fast. I have a problem with hoarding. But I bet it\u2019s not as bad as some of you. \n\nAnyone cares to confess?", "author_fullname": "t2_exuc50e6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "200TB and growing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hqtr1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702500644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After adding all the drivers I got during BF, I now have an unRAID server of 200TB and growing fast. I have a problem with hoarding. But I bet it\u2019s not as bad as some of you. &lt;/p&gt;\n\n&lt;p&gt;Anyone cares to confess?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18hqtr1", "is_robot_indexable": true, "report_reasons": null, "author": "NewportB", "discussion_type": null, "num_comments": 84, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hqtr1/200tb_and_growing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hqtr1/200tb_and_growing/", "subreddit_subscribers": 717967, "created_utc": 1702500644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've backed numerous pieces of otherwise nonexistent software up to the Internet Archive, and it's saved me when I needed a specific thing numerous times.   \nBut given worst case it may be shut down and taken totally off the web, what's going to happen to the otherwise totally lost software? Does anyone have a way to manage this?   \nThe closest to a coherent full backup style plan I have that's not \\*totally\\* impossible is to spread hundreds of partial backups across dozens of the servers people in this community run, with a central hub to all of the server sites containing each chunk. This way at least even if fragmented between many sites, the entire software collection is present, and even if a chunk is lost, the entire thing doesn't go (the eggs aren't in one basket, so to speak). But I don't have the skills nor supplies and hardware to coordinate such a huge thing, and it's unlikely enough would help out and host such a huge amount of junk. It's really possible only on paper.  \nIs there any (good) plan B here or is the best realistic plan to grab what you want now and keep it safe and host anything especially rare on your own sites? Because if so, I've gotta get working on that. ", "author_fullname": "t2_718mpex4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happens to software if the internet archive gets taken offline, and how can we prevent losing all of it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18harob", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702449783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve backed numerous pieces of otherwise nonexistent software up to the Internet Archive, and it&amp;#39;s saved me when I needed a specific thing numerous times.&lt;br/&gt;\nBut given worst case it may be shut down and taken totally off the web, what&amp;#39;s going to happen to the otherwise totally lost software? Does anyone have a way to manage this?&lt;br/&gt;\nThe closest to a coherent full backup style plan I have that&amp;#39;s not *totally* impossible is to spread hundreds of partial backups across dozens of the servers people in this community run, with a central hub to all of the server sites containing each chunk. This way at least even if fragmented between many sites, the entire software collection is present, and even if a chunk is lost, the entire thing doesn&amp;#39;t go (the eggs aren&amp;#39;t in one basket, so to speak). But I don&amp;#39;t have the skills nor supplies and hardware to coordinate such a huge thing, and it&amp;#39;s unlikely enough would help out and host such a huge amount of junk. It&amp;#39;s really possible only on paper.&lt;br/&gt;\nIs there any (good) plan B here or is the best realistic plan to grab what you want now and keep it safe and host anything especially rare on your own sites? Because if so, I&amp;#39;ve gotta get working on that. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "6TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18harob", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_McGuggins", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18harob/what_happens_to_software_if_the_internet_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18harob/what_happens_to_software_if_the_internet_archive/", "subreddit_subscribers": 717967, "created_utc": 1702449783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In 2021 I [posted here in this sub](https://www.reddit.com/r/DataHoarder/comments/qrkpak/a_tiktok_downloader_named_myfavett_to_backup_your/) about a TikTok archiving tool I built. Last week a user [replied to an old comment](https://www.reddit.com/r/DataHoarder/comments/f33h92/comment/kc8j06r/?utm_source=share&amp;utm_medium=web2x&amp;context=3) saying \"still working great to this day\", which reminded me to write this 2-year report - indeed I've been quietly maintaining it all this time.\n\n## What it is:\n\nIt's a tool to download TikTok videos and manage them in a local archive offline. It's called `myfaveTT`, can be found on google.\n\n## I love TikTok:\n\nSome people despise TikTok, but I'm a fan. If you intentionally train it (click the \u2764 when seeing something you like), the algorithm quickly understands your taste, and your feed becomes very likable.\n\n## What this tool can do:\n\n1. Download all videos in your `Favorite` list.\n2. Download all videos in your `Liked` (hearts) list.\n3. Download all videos from accounts you `Follow`.\n4. MP4s are put into your target folder, alongside an \"Archive.html\" file which can be opened by a browser. It displays all your local videos, just like on TikTok. From there you can browse, play, search, sort, see statistics, etc.\n5. When you have new Favorites or Likes, or when people you follow uploaded new videos, the local archive can sync the change.\n6. When a video disappears from tiktok (either taken down or deleted by creator), they'll be locally tagged as \"no longer available online\". This happens extremely often - in my calculation, things on tiktok have a half life of 1.5 years.\n\n## How it works:\n\nIt's a chrome extension. You login on [www.tiktok.com](http://www.tiktok.com), then the extension retrieves videos on your behalf. [(Screenshots)](https://myfavett.com/#screenshots)\n\n## Why I built it:\n\nTo use it myself - every feature originated from my own need.\n\n## How many users I have:\n\nAs of today, my developer dashboard reports 9617 users. There are constantly installs and uninstalls everyday but 9617 is the number of people who have it in their browser today.\n\n## \"In-app purchases\":\n\nI wouldn't say it's for profit, but to prevent user abuse which may get me into trouble, I created some obstacles by money:\n\n* I set `Favorites` to be free to download.\n* I set `Likes` to be free up to 10000 videos, then $10 per 5000 additional videos.\n* I set `Followings` to be free up to 50 accounts, then $10 per 50 additional accounts. (Each of them could have thousands of videos)\n\nMost people don't hit the threshold; scrapers won't pay; hoarders will pay only if they care enough about these videos. That's what I think.\n\n## Money I made:\n\nFrom November 2021 to November 2023, these \"abuse prevention mechanisms\" made me a total of $5760. It's equivalent of 1-2 weeks of my day job as a programmer.\n\n## How much time I spent:\n\nMy estimate is 1000 hours, which is why I say it's not for profit.\n\nI don't mind spending 1000 hours because it's really a passion project, but it would've cost only 5% time if I only made it functional for myself. The rest 95% time were spent making it usable for others. The repo currently has 3604 commits at version v1.10.34 with an amazing UI, while I myself could've used v0.1 to achieve the same goal, just with no UI.\n\nIn the future I probably won't do this kind of projects again.\n\n## Did I promote it:\n\nNot much - 2 years ago I posted in this sub, got 2-digit upvotes; also posted on Hacker News once, got 2-digit upvotes; last year I made a TikTok video once, got 2-digit likes. That's all.\n\nFundamentally I hate doing these stuff - I could code for 8 hours straight, but can't do 1 hour of \"marketing\" chores without procrastinating 7 hours, so I just don't do.\n\nBut the only [one TikTok video](https://www.tiktok.com/@myfavett/video/7172934453331889454) I made about it was quite good, well summarized the gist of the app, highly recommend watching.\n\n## What I learned:\n\n\"People are different. \" - we all know it, but it can never be overstated.\n\nFolks here think hoarding data is so important, but 99% of the population probably don't care.\n\nEach hoarder is different too. To me what's worth saving the most are the things I've personally loved before, e.g. my favorite videos, favorite movies, favorite songs, etc, but many people prefer hoarding things they haven't consumed yet (and may not consume in the future). Perhaps I should be called a collector more than a hoarder?\n\nBut if every person is different, surely they can each find their own likings on TikTok, so this app is immune to people's differences, right? Wrong, because they don't hoard. People don't care if 10 videos vanish daily from their \"Liked\", that's what I learned.", "author_fullname": "t2_1zols3gr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "the TikTok Archiver I built - Status report after 2 years, lessons learned, a little money made, etc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hp3io", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702508953.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702496161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In 2021 I &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/qrkpak/a_tiktok_downloader_named_myfavett_to_backup_your/\"&gt;posted here in this sub&lt;/a&gt; about a TikTok archiving tool I built. Last week a user &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/f33h92/comment/kc8j06r/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;replied to an old comment&lt;/a&gt; saying &amp;quot;still working great to this day&amp;quot;, which reminded me to write this 2-year report - indeed I&amp;#39;ve been quietly maintaining it all this time.&lt;/p&gt;\n\n&lt;h2&gt;What it is:&lt;/h2&gt;\n\n&lt;p&gt;It&amp;#39;s a tool to download TikTok videos and manage them in a local archive offline. It&amp;#39;s called &lt;code&gt;myfaveTT&lt;/code&gt;, can be found on google.&lt;/p&gt;\n\n&lt;h2&gt;I love TikTok:&lt;/h2&gt;\n\n&lt;p&gt;Some people despise TikTok, but I&amp;#39;m a fan. If you intentionally train it (click the \u2764 when seeing something you like), the algorithm quickly understands your taste, and your feed becomes very likable.&lt;/p&gt;\n\n&lt;h2&gt;What this tool can do:&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download all videos in your &lt;code&gt;Favorite&lt;/code&gt; list.&lt;/li&gt;\n&lt;li&gt;Download all videos in your &lt;code&gt;Liked&lt;/code&gt; (hearts) list.&lt;/li&gt;\n&lt;li&gt;Download all videos from accounts you &lt;code&gt;Follow&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;MP4s are put into your target folder, alongside an &amp;quot;Archive.html&amp;quot; file which can be opened by a browser. It displays all your local videos, just like on TikTok. From there you can browse, play, search, sort, see statistics, etc.&lt;/li&gt;\n&lt;li&gt;When you have new Favorites or Likes, or when people you follow uploaded new videos, the local archive can sync the change.&lt;/li&gt;\n&lt;li&gt;When a video disappears from tiktok (either taken down or deleted by creator), they&amp;#39;ll be locally tagged as &amp;quot;no longer available online&amp;quot;. This happens extremely often - in my calculation, things on tiktok have a half life of 1.5 years.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h2&gt;How it works:&lt;/h2&gt;\n\n&lt;p&gt;It&amp;#39;s a chrome extension. You login on &lt;a href=\"http://www.tiktok.com\"&gt;www.tiktok.com&lt;/a&gt;, then the extension retrieves videos on your behalf. &lt;a href=\"https://myfavett.com/#screenshots\"&gt;(Screenshots)&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Why I built it:&lt;/h2&gt;\n\n&lt;p&gt;To use it myself - every feature originated from my own need.&lt;/p&gt;\n\n&lt;h2&gt;How many users I have:&lt;/h2&gt;\n\n&lt;p&gt;As of today, my developer dashboard reports 9617 users. There are constantly installs and uninstalls everyday but 9617 is the number of people who have it in their browser today.&lt;/p&gt;\n\n&lt;h2&gt;&amp;quot;In-app purchases&amp;quot;:&lt;/h2&gt;\n\n&lt;p&gt;I wouldn&amp;#39;t say it&amp;#39;s for profit, but to prevent user abuse which may get me into trouble, I created some obstacles by money:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I set &lt;code&gt;Favorites&lt;/code&gt; to be free to download.&lt;/li&gt;\n&lt;li&gt;I set &lt;code&gt;Likes&lt;/code&gt; to be free up to 10000 videos, then $10 per 5000 additional videos.&lt;/li&gt;\n&lt;li&gt;I set &lt;code&gt;Followings&lt;/code&gt; to be free up to 50 accounts, then $10 per 50 additional accounts. (Each of them could have thousands of videos)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Most people don&amp;#39;t hit the threshold; scrapers won&amp;#39;t pay; hoarders will pay only if they care enough about these videos. That&amp;#39;s what I think.&lt;/p&gt;\n\n&lt;h2&gt;Money I made:&lt;/h2&gt;\n\n&lt;p&gt;From November 2021 to November 2023, these &amp;quot;abuse prevention mechanisms&amp;quot; made me a total of $5760. It&amp;#39;s equivalent of 1-2 weeks of my day job as a programmer.&lt;/p&gt;\n\n&lt;h2&gt;How much time I spent:&lt;/h2&gt;\n\n&lt;p&gt;My estimate is 1000 hours, which is why I say it&amp;#39;s not for profit.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind spending 1000 hours because it&amp;#39;s really a passion project, but it would&amp;#39;ve cost only 5% time if I only made it functional for myself. The rest 95% time were spent making it usable for others. The repo currently has 3604 commits at version v1.10.34 with an amazing UI, while I myself could&amp;#39;ve used v0.1 to achieve the same goal, just with no UI.&lt;/p&gt;\n\n&lt;p&gt;In the future I probably won&amp;#39;t do this kind of projects again.&lt;/p&gt;\n\n&lt;h2&gt;Did I promote it:&lt;/h2&gt;\n\n&lt;p&gt;Not much - 2 years ago I posted in this sub, got 2-digit upvotes; also posted on Hacker News once, got 2-digit upvotes; last year I made a TikTok video once, got 2-digit likes. That&amp;#39;s all.&lt;/p&gt;\n\n&lt;p&gt;Fundamentally I hate doing these stuff - I could code for 8 hours straight, but can&amp;#39;t do 1 hour of &amp;quot;marketing&amp;quot; chores without procrastinating 7 hours, so I just don&amp;#39;t do.&lt;/p&gt;\n\n&lt;p&gt;But the only &lt;a href=\"https://www.tiktok.com/@myfavett/video/7172934453331889454\"&gt;one TikTok video&lt;/a&gt; I made about it was quite good, well summarized the gist of the app, highly recommend watching.&lt;/p&gt;\n\n&lt;h2&gt;What I learned:&lt;/h2&gt;\n\n&lt;p&gt;&amp;quot;People are different. &amp;quot; - we all know it, but it can never be overstated.&lt;/p&gt;\n\n&lt;p&gt;Folks here think hoarding data is so important, but 99% of the population probably don&amp;#39;t care.&lt;/p&gt;\n\n&lt;p&gt;Each hoarder is different too. To me what&amp;#39;s worth saving the most are the things I&amp;#39;ve personally loved before, e.g. my favorite videos, favorite movies, favorite songs, etc, but many people prefer hoarding things they haven&amp;#39;t consumed yet (and may not consume in the future). Perhaps I should be called a collector more than a hoarder?&lt;/p&gt;\n\n&lt;p&gt;But if every person is different, surely they can each find their own likings on TikTok, so this app is immune to people&amp;#39;s differences, right? Wrong, because they don&amp;#39;t hoard. People don&amp;#39;t care if 10 videos vanish daily from their &amp;quot;Liked&amp;quot;, that&amp;#39;s what I learned.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hp3io", "is_robot_indexable": true, "report_reasons": null, "author": "ZYinMD", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hp3io/the_tiktok_archiver_i_built_status_report_after_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hp3io/the_tiktok_archiver_i_built_status_report_after_2/", "subreddit_subscribers": 717967, "created_utc": 1702496161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/jp5bp9yjbz5c1.jpg?width=1212&amp;format=pjpg&amp;auto=webp&amp;s=4f46a2f6de66c1b0a001923434164094a4db843a\n\nI use a grand total of 1.4MB out of my 2GB. The email is clearly implying that my Dropbox is full, without actually saying it. All they say is I have \"limited space\", which is true. But the carefulness of the wording really reveals the underhanded intent. And older people who don't even know where or how their files are stored might see an email like this and purchase immediately, thinking all their files everywhere might be jeopardized.\n\nThis is really fucking low.", "author_fullname": "t2_b07ma11b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got This Scummy Email From Dropbox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"jp5bp9yjbz5c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=568f100798d1fd534200ac75cbcf1c926f86c926"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbb2d1b0291fc683aed611aeba3f87c71eb9b028"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a6c0bff7b9483969946fa9896701b445a2b65eb"}, {"y": 337, "x": 640, "u": "https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=76ea2b68633e99e7b8adc0480494e91ae9a1d0c4"}, {"y": 506, "x": 960, "u": "https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d88d780865e362d7e0d217846ceb97a9874f3b4"}, {"y": 569, "x": 1080, "u": "https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d7d8c806e033e8008d6ae2c050e8e416ba96872"}], "s": {"y": 639, "x": 1212, "u": "https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=1212&amp;format=pjpg&amp;auto=webp&amp;s=4f46a2f6de66c1b0a001923434164094a4db843a"}, "id": "jp5bp9yjbz5c1"}}, "name": "t3_18h75g4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WEZ7Fu5-jfLcgi1aXUKwFChWDkwxa9avcFTl_9nfcHs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702437268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=1212&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4f46a2f6de66c1b0a001923434164094a4db843a\"&gt;https://preview.redd.it/jp5bp9yjbz5c1.jpg?width=1212&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4f46a2f6de66c1b0a001923434164094a4db843a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I use a grand total of 1.4MB out of my 2GB. The email is clearly implying that my Dropbox is full, without actually saying it. All they say is I have &amp;quot;limited space&amp;quot;, which is true. But the carefulness of the wording really reveals the underhanded intent. And older people who don&amp;#39;t even know where or how their files are stored might see an email like this and purchase immediately, thinking all their files everywhere might be jeopardized.&lt;/p&gt;\n\n&lt;p&gt;This is really fucking low.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18h75g4", "is_robot_indexable": true, "report_reasons": null, "author": "No_Matter_7246", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18h75g4/got_this_scummy_email_from_dropbox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18h75g4/got_this_scummy_email_from_dropbox/", "subreddit_subscribers": 717967, "created_utc": 1702437268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_cftf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Heads up for those of us still in Google Workspace's \"read-only\" mode - account suspensions (with loss of data) on the way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18hn61r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/P7VQzGIgX_sDJtXitx679SrsTnxkxyyEjmQquJMinD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702491219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techdirt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techdirt.com/2023/12/12/google-promises-unlimited-cloud-storage-then-cancels-plan-then-tells-journalist-his-lifes-work-will-be-deleted-without-enough-time-to-transfer-the-data/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?auto=webp&amp;s=03a5fde066455710ac1676ca5e55fdf4cff5f177", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd544711a911105c2c84e8778e42492e6627f7ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=581443ef43c416964d464ddd6b5b28eec7b2b77c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=98eca8135ccdbe0daad2aad9f2f21c250d1564ab", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60f612dc25b6693520e6d4a342645939506f3afd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30ce1c1c245620a8b70a4d0a298b39657711ef0e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b24b6b5a4ae88248a5d6b951b789da794832c41", "width": 1080, "height": 567}], "variants": {}, "id": "86PGtE2qmX3coS9Htmb8TUfXMSg2HaYO4Rk8A0YbGow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hn61r", "is_robot_indexable": true, "report_reasons": null, "author": "JCBird1012", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hn61r/heads_up_for_those_of_us_still_in_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techdirt.com/2023/12/12/google-promises-unlimited-cloud-storage-then-cancels-plan-then-tells-journalist-his-lifes-work-will-be-deleted-without-enough-time-to-transfer-the-data/", "subreddit_subscribers": 717967, "created_utc": 1702491219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My PC doesn't have a firewire port or an available expansion slot, so I have to convert from firewire 400 to USB somehow. Some guides online told me I have to do firewire 400 &gt; 800 &gt; thunderbolt 2 &gt; thunderbolt 3. Problem is, I can't find any firewire 800 to thunderbolt 2 adapters anywhere. Where can I find these, or is there another way to do this?", "author_fullname": "t2_7fs8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have almost all the equipment I need to digitize my hold Hi-8 tapes, but I'm missing some cables and can't find them online. Where can I find them? Do I even need them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hpaaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702496652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My PC doesn&amp;#39;t have a firewire port or an available expansion slot, so I have to convert from firewire 400 to USB somehow. Some guides online told me I have to do firewire 400 &amp;gt; 800 &amp;gt; thunderbolt 2 &amp;gt; thunderbolt 3. Problem is, I can&amp;#39;t find any firewire 800 to thunderbolt 2 adapters anywhere. Where can I find these, or is there another way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hpaaq", "is_robot_indexable": true, "report_reasons": null, "author": "cteno4", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hpaaq/i_have_almost_all_the_equipment_i_need_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hpaaq/i_have_almost_all_the_equipment_i_need_to/", "subreddit_subscribers": 717967, "created_utc": 1702496652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I found this hard drive on eBay: https://www.ebay.co.uk/itm/304814040552\n\nIt states that is brand new and sealed and I\u2019m not singling out this seller or stating they are lying about the condition, however I am aware that people can simply wipe the SMART values on a hard drive to make it look like it is brand new. What other ways are there to tell if it\u2019s new or not?", "author_fullname": "t2_3pkgbn5v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you know if a hard drive is actually brand new?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hovgu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702495592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found this hard drive on eBay: &lt;a href=\"https://www.ebay.co.uk/itm/304814040552\"&gt;https://www.ebay.co.uk/itm/304814040552&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It states that is brand new and sealed and I\u2019m not singling out this seller or stating they are lying about the condition, however I am aware that people can simply wipe the SMART values on a hard drive to make it look like it is brand new. What other ways are there to tell if it\u2019s new or not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MSMd7L12FDFZoE5pofN1boizVcRGM9L7XAEzMYtM7dg.jpg?auto=webp&amp;s=45cd0958fdb66847b5669166f086744329d570e8", "width": 271, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/MSMd7L12FDFZoE5pofN1boizVcRGM9L7XAEzMYtM7dg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=80602ab52fdf20e706e66c55d627ceea7c532616", "width": 108, "height": 159}, {"url": "https://external-preview.redd.it/MSMd7L12FDFZoE5pofN1boizVcRGM9L7XAEzMYtM7dg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=46798c4941b227022eb82cf90be0f9c90fe81bec", "width": 216, "height": 318}], "variants": {}, "id": "5fkpfMjvOXTbsVAsrFoJ6PeVQfwSGcvYb68bIItRHw0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hovgu", "is_robot_indexable": true, "report_reasons": null, "author": "nathan12581", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hovgu/how_would_you_know_if_a_hard_drive_is_actually/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hovgu/how_would_you_know_if_a_hard_drive_is_actually/", "subreddit_subscribers": 717967, "created_utc": 1702495592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I picked up the 11 XL and it's fantastic, but I'm frustrated that my scans don't capture the entire document on the bed. It's only a millimetre or two, but knowing my scans aren't complete has been enough for me to pause this project. Is there any kind of software level fix for this, or do I just need to create a 90 degree wedge to offset every scan?\n\nI know I could place smaller documents away from the edge and level the image later, but I'd rather do a one time fix for this issue, than do tens of thousands of fixes.   \n\n\nThanks!", "author_fullname": "t2_ihs3k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Epson Expression 11000XL scans cropped in from edges, how to scan full document?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hfz9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702471410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I picked up the 11 XL and it&amp;#39;s fantastic, but I&amp;#39;m frustrated that my scans don&amp;#39;t capture the entire document on the bed. It&amp;#39;s only a millimetre or two, but knowing my scans aren&amp;#39;t complete has been enough for me to pause this project. Is there any kind of software level fix for this, or do I just need to create a 90 degree wedge to offset every scan?&lt;/p&gt;\n\n&lt;p&gt;I know I could place smaller documents away from the edge and level the image later, but I&amp;#39;d rather do a one time fix for this issue, than do tens of thousands of fixes.   &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hfz9w", "is_robot_indexable": true, "report_reasons": null, "author": "Plebsolute", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hfz9w/epson_expression_11000xl_scans_cropped_in_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hfz9w/epson_expression_11000xl_scans_cropped_in_from/", "subreddit_subscribers": 717967, "created_utc": 1702471410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, I am an IT admin of a local videography company here in my area and I suggested that we should implement a NAS Server for their centralized storage so that they can increase their productivity. We settled on 4 IRONWOLF PRO 12TB drives that will run RAID 10 utilizing TrueNAS. However during the initial setup, I noticed that the connectors of the drives are different and they would not lock with a standard SATA connector for the motherboard and power supply.\n\nI read about NAS Drives being used in desktops but I suspect this requires a specific connector but I am not sure. Any suggestions?", "author_fullname": "t2_6mnjye6u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate IRONWOLF PRO 12TB for Homemade NAS Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h6bfs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702434785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am an IT admin of a local videography company here in my area and I suggested that we should implement a NAS Server for their centralized storage so that they can increase their productivity. We settled on 4 IRONWOLF PRO 12TB drives that will run RAID 10 utilizing TrueNAS. However during the initial setup, I noticed that the connectors of the drives are different and they would not lock with a standard SATA connector for the motherboard and power supply.&lt;/p&gt;\n\n&lt;p&gt;I read about NAS Drives being used in desktops but I suspect this requires a specific connector but I am not sure. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18h6bfs", "is_robot_indexable": true, "report_reasons": null, "author": "Crafty-Total-6978", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18h6bfs/seagate_ironwolf_pro_12tb_for_homemade_nas_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18h6bfs/seagate_ironwolf_pro_12tb_for_homemade_nas_server/", "subreddit_subscribers": 717967, "created_utc": 1702434785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_bqi43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Having massive wait times for tiny file transfers between imac to my WD external HD. Image shows how much space I have available!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_18hxfzf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AuxWvsoIzp02HBkc-QRLeYhfiUvKKPusQm-ILWBSiTY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702518686.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/cm2gdyi3266c1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/cm2gdyi3266c1.png?auto=webp&amp;s=60485a57ac5e068bb32f620bc33c070c14dbe0a6", "width": 916, "height": 920}, "resolutions": [{"url": "https://preview.redd.it/cm2gdyi3266c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8efc58dd851daddfbb618358f6fe0777247d9786", "width": 108, "height": 108}, {"url": "https://preview.redd.it/cm2gdyi3266c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ae91bec032295cc228d01ee937e4ad7378c63d1", "width": 216, "height": 216}, {"url": "https://preview.redd.it/cm2gdyi3266c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9d0d1cedeb92ba5e690ff6fff765fd7ba8a3ab7", "width": 320, "height": 321}, {"url": "https://preview.redd.it/cm2gdyi3266c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a80bd0f3b46c279cbbdaf7571d2b3f7f05871ef7", "width": 640, "height": 642}], "variants": {}, "id": "OXcwNvAFwMvImSRyDuhn6YE6DvX0LV-y3YgA-erOxbM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hxfzf", "is_robot_indexable": true, "report_reasons": null, "author": "Deathscua", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hxfzf/having_massive_wait_times_for_tiny_file_transfers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/cm2gdyi3266c1.png", "subreddit_subscribers": 717967, "created_utc": 1702518686.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm hoping you guys are the right ones to ask since you probably have the most experience with transfering data, but I'm trying to copy about 5ish TB from an older HDD to a new one.  The older one is a Seagate SRD0NF2 and the new one is a WD Easystore. They are both USB 3.0, and I'm using the cable that came with them. \n\nMy PC is showing an avg speed of about 33.4 MB/s on the transfer and it isn't even giving me an estimated time. I'm guessing days. Is there a better way to handle this that will speed things up?", "author_fullname": "t2_n0io2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are data transfers taking so long?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18hw7jk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702515016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m hoping you guys are the right ones to ask since you probably have the most experience with transfering data, but I&amp;#39;m trying to copy about 5ish TB from an older HDD to a new one.  The older one is a Seagate SRD0NF2 and the new one is a WD Easystore. They are both USB 3.0, and I&amp;#39;m using the cable that came with them. &lt;/p&gt;\n\n&lt;p&gt;My PC is showing an avg speed of about 33.4 MB/s on the transfer and it isn&amp;#39;t even giving me an estimated time. I&amp;#39;m guessing days. Is there a better way to handle this that will speed things up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hw7jk", "is_robot_indexable": true, "report_reasons": null, "author": "UKFan643", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hw7jk/why_are_data_transfers_taking_so_long/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hw7jk/why_are_data_transfers_taking_so_long/", "subreddit_subscribers": 717967, "created_utc": 1702515016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1k7i9k3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate's 24 TB SkyHawk AI HDDs Support 64 HD Cameras and 32 AI Streams with 0 Frame Loss", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18hovti", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3pS37yKRWVIbe3upOwgo0G5g5EnoPwx6tIOcKuuHFts.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702495613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hardwaretimes.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.hardwaretimes.com/seagates-24-tb-skyhawk-ai-hdds-support-64-hd-cameras-and-32-ai-streams-with-0-frame-loss/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yvImTR0yycv7yTTiackeWfPnFX8Ctc5FBd8UAKEnE0U.jpg?auto=webp&amp;s=47cad51025dd127f369c3245cd010770e29eb334", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/yvImTR0yycv7yTTiackeWfPnFX8Ctc5FBd8UAKEnE0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=896aeda55df50791c412081fa9f65ad76be25e8d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yvImTR0yycv7yTTiackeWfPnFX8Ctc5FBd8UAKEnE0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f303666615ae5ee83cebeb358d6095a30c52b98d", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/yvImTR0yycv7yTTiackeWfPnFX8Ctc5FBd8UAKEnE0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21bb563a66f5c445aeea47bb4b9288461322d911", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/yvImTR0yycv7yTTiackeWfPnFX8Ctc5FBd8UAKEnE0U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a48d6a7c3e49f033d2342ffe843912a83faf7114", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/yvImTR0yycv7yTTiackeWfPnFX8Ctc5FBd8UAKEnE0U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5dff4bec306ee7fa529dee93058206b7deeca19", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/yvImTR0yycv7yTTiackeWfPnFX8Ctc5FBd8UAKEnE0U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=396efc1f9fa4717c92455c5fa8292025b8d7b3e3", "width": 1080, "height": 607}], "variants": {}, "id": "e6KF9WnXgtIfnffS27HEx9ljLD0fa2vPb_S5tVPrAvU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hovti", "is_robot_indexable": true, "report_reasons": null, "author": "black_fang_XIII", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hovti/seagates_24_tb_skyhawk_ai_hdds_support_64_hd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.hardwaretimes.com/seagates-24-tb-skyhawk-ai-hdds-support-64-hd-cameras-and-32-ai-streams-with-0-frame-loss/", "subreddit_subscribers": 717967, "created_utc": 1702495613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.u-nas.com/xcart/cart.php?target=product&amp;product\\_id=17640&amp;category\\_id=277](https://www.u-nas.com/xcart/cart.php?target=product&amp;product_id=17640&amp;category_id=277)\n\n24 available as of 12/23 10am pst\n\nWas waiting a long time for these, ended up going with a fullsize for my backup server", "author_fullname": "t2_14bwrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "U-Nas NSC-810A in Stock", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hn5ww", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702491208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.u-nas.com/xcart/cart.php?target=product&amp;amp;product_id=17640&amp;amp;category_id=277\"&gt;https://www.u-nas.com/xcart/cart.php?target=product&amp;amp;product_id=17640&amp;amp;category_id=277&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;24 available as of 12/23 10am pst&lt;/p&gt;\n\n&lt;p&gt;Was waiting a long time for these, ended up going with a fullsize for my backup server&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Le-vEchp86-87a7D7FRR2f_4zbhdO07aanXdegGgITo.jpg?auto=webp&amp;s=3ece7db52e2fc0e36c8b962cda25911f638c37fa", "width": 4320, "height": 2880}, "resolutions": [{"url": "https://external-preview.redd.it/Le-vEchp86-87a7D7FRR2f_4zbhdO07aanXdegGgITo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ee4092206d119d4e0484e0526ea98b5b403e020", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/Le-vEchp86-87a7D7FRR2f_4zbhdO07aanXdegGgITo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa390742121b0e3a347340d79b39fc94597781f9", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/Le-vEchp86-87a7D7FRR2f_4zbhdO07aanXdegGgITo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ce8c0a9a9c857eb756a231c75bb56a9555070b5", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/Le-vEchp86-87a7D7FRR2f_4zbhdO07aanXdegGgITo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2dc6392a4b10dd90646ccee19cf863cfe4444a2c", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/Le-vEchp86-87a7D7FRR2f_4zbhdO07aanXdegGgITo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3cb1bbfa608aecdb83bb1e8019a52a8c7cb4397a", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/Le-vEchp86-87a7D7FRR2f_4zbhdO07aanXdegGgITo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6b01592d91427d2f7daa0ff42685c31ff5bd9b7", "width": 1080, "height": 720}], "variants": {}, "id": "WREKLSGo2ljb42apvUgj4KOg8ttBm_y4pWZDopiHNLI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hn5ww", "is_robot_indexable": true, "report_reasons": null, "author": "danimal1986", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hn5ww/unas_nsc810a_in_stock/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hn5ww/unas_nsc810a_in_stock/", "subreddit_subscribers": 717967, "created_utc": 1702491208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am making a fairly modest archive that is just a directory with a ton of pdfs and maybe some other files in it. I don't have the tech skills to implement any fancy solutions, so I want to keep it simple - throw the document in the pile, and to find it later search by filename (I use a specific format including key information like author) and keywords. The thing is, I don't want to add a ton of keywords to the end of a filename. Is there any way I can drop them into the file and have a software to quickly pull them without searching through the whole file? Again, I suck at technology so I can't do anything really fancy like all you professionals here.", "author_fullname": "t2_kda89q2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to add keywords to files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hldm7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702486770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am making a fairly modest archive that is just a directory with a ton of pdfs and maybe some other files in it. I don&amp;#39;t have the tech skills to implement any fancy solutions, so I want to keep it simple - throw the document in the pile, and to find it later search by filename (I use a specific format including key information like author) and keywords. The thing is, I don&amp;#39;t want to add a ton of keywords to the end of a filename. Is there any way I can drop them into the file and have a software to quickly pull them without searching through the whole file? Again, I suck at technology so I can&amp;#39;t do anything really fancy like all you professionals here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hldm7", "is_robot_indexable": true, "report_reasons": null, "author": "dakkablakka", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hldm7/how_to_add_keywords_to_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hldm7/how_to_add_keywords_to_files/", "subreddit_subscribers": 717967, "created_utc": 1702486770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was referred here by the folks over at /privacy, and I'm glad I found this community. I wanted to run my question by you guys as well.\n\nRight now, I'm using Windows + OneDrive + Backblaze, with a  smattering of Google services for Photos and email primarily. Obviously,  I'm wanting to move away from this, especially with the recent disaster  at Google Drive and 23andMe.   \n\nI messed around with Arch, decided it wasn't for me, and am now  using Debian, Mint, and Pop, and practicing with them. I've discovered several options  beyond that for replacing the other services I'm used to. Protonmail for  email, which has worked, for the most part, and Proton Drive to replace  OneDrive. Unfortunately, Backblaze doesn't work with Linux, or at  least, not the version that is affordable.   \n\nI was already going to set up a NAS with Synology, but am now  rethinking that as well and have been attempting to set up TrueNAS for the past week, which hasn't been working well. My goal was to throw  Nextcloud on that and try using that instead of Proton Drive to start so  that I could save money.   \n\nAs you can tell, I'm all over the place here. What combination of  tools should I look into? If I have a NAS, do I need Proton Drive at  all? What service would function as a good offsite backup since  Backblaze doesn't work on Linux? What can I use to backup my NAS to the  cloud if I end up using that?   ", "author_fullname": "t2_glj7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you recommend for a combination of cloud storage, local storage, and backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hkasx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702483954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was referred here by the folks over at /privacy, and I&amp;#39;m glad I found this community. I wanted to run my question by you guys as well.&lt;/p&gt;\n\n&lt;p&gt;Right now, I&amp;#39;m using Windows + OneDrive + Backblaze, with a  smattering of Google services for Photos and email primarily. Obviously,  I&amp;#39;m wanting to move away from this, especially with the recent disaster  at Google Drive and 23andMe.   &lt;/p&gt;\n\n&lt;p&gt;I messed around with Arch, decided it wasn&amp;#39;t for me, and am now  using Debian, Mint, and Pop, and practicing with them. I&amp;#39;ve discovered several options  beyond that for replacing the other services I&amp;#39;m used to. Protonmail for  email, which has worked, for the most part, and Proton Drive to replace  OneDrive. Unfortunately, Backblaze doesn&amp;#39;t work with Linux, or at  least, not the version that is affordable.   &lt;/p&gt;\n\n&lt;p&gt;I was already going to set up a NAS with Synology, but am now  rethinking that as well and have been attempting to set up TrueNAS for the past week, which hasn&amp;#39;t been working well. My goal was to throw  Nextcloud on that and try using that instead of Proton Drive to start so  that I could save money.   &lt;/p&gt;\n\n&lt;p&gt;As you can tell, I&amp;#39;m all over the place here. What combination of  tools should I look into? If I have a NAS, do I need Proton Drive at  all? What service would function as a good offsite backup since  Backblaze doesn&amp;#39;t work on Linux? What can I use to backup my NAS to the  cloud if I end up using that?   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hkasx", "is_robot_indexable": true, "report_reasons": null, "author": "Elarionus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hkasx/what_would_you_recommend_for_a_combination_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hkasx/what_would_you_recommend_for_a_combination_of/", "subreddit_subscribers": 717967, "created_utc": 1702483954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So here's the thing I need to batch download a bunch of images from [Zerochan.net](https://Zerochan.net) , I used wfdownloader to download them but every time I do it does not download all images because Zerochan disables the downloads until the downloader client is finished. Does anyone have anything that can help me with this problem?", "author_fullname": "t2_6bybo0w0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mass Download for Zerochan?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h97v9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702443981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So here&amp;#39;s the thing I need to batch download a bunch of images from &lt;a href=\"https://Zerochan.net\"&gt;Zerochan.net&lt;/a&gt; , I used wfdownloader to download them but every time I do it does not download all images because Zerochan disables the downloads until the downloader client is finished. Does anyone have anything that can help me with this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18h97v9", "is_robot_indexable": true, "report_reasons": null, "author": "Swiftmaker", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18h97v9/mass_download_for_zerochan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18h97v9/mass_download_for_zerochan/", "subreddit_subscribers": 717967, "created_utc": 1702443981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can you put HDDs/SSDs of different maximum capacity in a Synology, and have each one usable in the Synology?\n\n&amp;#x200B;\n\n*EX:*\n\n*Slot 1 = 4 TB,*\n\n*Slot 2 = 8 TB,*\n\n*Slot 3 = 16 TB*\n\n*Slot 4 = 4 TB*", "author_fullname": "t2_w9e90nsr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology HDD question.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h72t0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702437046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can you put HDDs/SSDs of different maximum capacity in a Synology, and have each one usable in the Synology?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;EX:&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Slot 1 = 4 TB,&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Slot 2 = 8 TB,&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Slot 3 = 16 TB&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Slot 4 = 4 TB&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18h72t0", "is_robot_indexable": true, "report_reasons": null, "author": "CuriousDivide2425", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18h72t0/synology_hdd_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18h72t0/synology_hdd_question/", "subreddit_subscribers": 717967, "created_utc": 1702437046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been collecting iOS games for a while, is there any way to archive them so I don't have to worry about developer unpublishing them or even App Store closing?", "author_fullname": "t2_x3s0wiz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any way to archive iOS games?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h6pel", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702435924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been collecting iOS games for a while, is there any way to archive them so I don&amp;#39;t have to worry about developer unpublishing them or even App Store closing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18h6pel", "is_robot_indexable": true, "report_reasons": null, "author": "slmjkdbtl", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18h6pel/is_there_any_way_to_archive_ios_games/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18h6pel/is_there_any_way_to_archive_ios_games/", "subreddit_subscribers": 717967, "created_utc": 1702435924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a bit of an interesting use case and a bit of an interesting problem going on and I am not sure where else to post. I have a few different servers running ZFS for storage as my main bulk, but I also have a decent amount of storage in my desktop for mass storage I need high speed access to. This includes things like VMs my desktop runs, games, code for development, so performance is important.\n\nTo that end I have 2x 1TB SSDs and 2x 8TB HDDs to pool together into the pool. For the longest time I have used Stablebit Drivepool. The SSD caching plugin and folder replication worked fantastically, but recently I have been having a ton of issues with my storage pool: mainly random segfaults when trying to compile my code. It doesn't happen on my laptop, and it doesn't happen when I move the repo to my OS drives off the pool. Support wasn't helpful and it's still happening so I am fed up. Either way I digress. I wanna replace Stablebit Drivepool.\n\nI need something performant and I would like to pool all of the drives together so I don't have to worry about it. Some folder will be pinned to the SSDs, while most will just be cached.\n\nMy first thought was Storage Space since I know I can pin files as needed to specific tiers. I created a tiered storage drive with mirrored  SSDs and mirrored HDDs and formatted with NTFS using powershell but performance was awful. Writes to the array seemed to go straight to the HDDs and skip the SSDs no matter if they were sequential or random. I tried to do some reading and from my understanding if I want decent performance from storage tiers, I need to use ReFS for its real time optimization. I used this set of scripts as a guide setting it up: https://github.com/freemansoft/win10-storage-spaces\n\nAm I needing to use ReFS to get decent performance from Storage Spaces on W11 (I'd have to get workstation, which is fine), or is there other alternatives or solutions I am unaware of. Obviously my goto would be ZFS but thay doesn't exactly work on Windows.", "author_fullname": "t2_n5n3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows 11 Pro Stablebit Drivepool Alternatives (Or making Storage Spaces Performant)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hv474", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702511894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bit of an interesting use case and a bit of an interesting problem going on and I am not sure where else to post. I have a few different servers running ZFS for storage as my main bulk, but I also have a decent amount of storage in my desktop for mass storage I need high speed access to. This includes things like VMs my desktop runs, games, code for development, so performance is important.&lt;/p&gt;\n\n&lt;p&gt;To that end I have 2x 1TB SSDs and 2x 8TB HDDs to pool together into the pool. For the longest time I have used Stablebit Drivepool. The SSD caching plugin and folder replication worked fantastically, but recently I have been having a ton of issues with my storage pool: mainly random segfaults when trying to compile my code. It doesn&amp;#39;t happen on my laptop, and it doesn&amp;#39;t happen when I move the repo to my OS drives off the pool. Support wasn&amp;#39;t helpful and it&amp;#39;s still happening so I am fed up. Either way I digress. I wanna replace Stablebit Drivepool.&lt;/p&gt;\n\n&lt;p&gt;I need something performant and I would like to pool all of the drives together so I don&amp;#39;t have to worry about it. Some folder will be pinned to the SSDs, while most will just be cached.&lt;/p&gt;\n\n&lt;p&gt;My first thought was Storage Space since I know I can pin files as needed to specific tiers. I created a tiered storage drive with mirrored  SSDs and mirrored HDDs and formatted with NTFS using powershell but performance was awful. Writes to the array seemed to go straight to the HDDs and skip the SSDs no matter if they were sequential or random. I tried to do some reading and from my understanding if I want decent performance from storage tiers, I need to use ReFS for its real time optimization. I used this set of scripts as a guide setting it up: &lt;a href=\"https://github.com/freemansoft/win10-storage-spaces\"&gt;https://github.com/freemansoft/win10-storage-spaces&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Am I needing to use ReFS to get decent performance from Storage Spaces on W11 (I&amp;#39;d have to get workstation, which is fine), or is there other alternatives or solutions I am unaware of. Obviously my goto would be ZFS but thay doesn&amp;#39;t exactly work on Windows.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jUd-WY2gqdXQciK4s1HrZFYSyN7lNhKX5ZQ5E4MwTHo.jpg?auto=webp&amp;s=7b0ce79204db2dba9be5e7f9ad4652d803099590", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/jUd-WY2gqdXQciK4s1HrZFYSyN7lNhKX5ZQ5E4MwTHo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e36395d6b949de1fa966bc1ad85bfb6eb1bdbddc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/jUd-WY2gqdXQciK4s1HrZFYSyN7lNhKX5ZQ5E4MwTHo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86976fc0a9ac65e9f3258e457ce481c160845dcf", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/jUd-WY2gqdXQciK4s1HrZFYSyN7lNhKX5ZQ5E4MwTHo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e9896e552cd191f069b53d206550849594ce3ad", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/jUd-WY2gqdXQciK4s1HrZFYSyN7lNhKX5ZQ5E4MwTHo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e12cb1829c5470789dcdd4321f9cff47d7c1d15", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/jUd-WY2gqdXQciK4s1HrZFYSyN7lNhKX5ZQ5E4MwTHo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2404f8e27d8cf05127426cf05ee94c545e05a08b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/jUd-WY2gqdXQciK4s1HrZFYSyN7lNhKX5ZQ5E4MwTHo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=016c00a45d0c0a614929a677fc591c560db344d8", "width": 1080, "height": 540}], "variants": {}, "id": "LpS_Ca1X6ZB1cSKWG9icWmWWcigYUWIlZbH_lOMPgKE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hv474", "is_robot_indexable": true, "report_reasons": null, "author": "Sir7empest", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hv474/windows_11_pro_stablebit_drivepool_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hv474/windows_11_pro_stablebit_drivepool_alternatives/", "subreddit_subscribers": 717967, "created_utc": 1702511894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Datahoarders! Lets finish 2023 on a strong note with a final giveaway for the year. After discussing with your mods about issues around unlimited data storage offerings from cloud providers, we've decided to make this one a bit more about capacity. We are giving one lucky r/DataHoarder subscriber a pair of 10TB IronWolf Pro hard drives.\n\nThe prize is: two units of 10TB IronWolf Pro hard drives for one winner.\n\n**How to enter: Just reply to this post once with a top-level comment detailing your current local setup with a verified picture (containing a written note with your username and date), what you hoard primarily (excluding pirated and/or illegal media), and why you're looking to expand your storage. Include the phrases RunWithIronwolf and Seagate.**\n\n\nSelection process/rules\n\nOne entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until December 27, 2023 at 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.\n\nGeographic restrictions:\n\nOur policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions.\n\nUS\n\nCanada (will require a basic skills-based question if winner is chosen by law)\n\nBrazil\n\nSouth America\n\nUnited Kingdom\n\nGermany\n\nFrance\n\nIberia\n\nAustralia\n\nNew Zealand\n\nKorea\n\nIndia\n\nMalaysia\n\nSingapore\n\nChina\n\n\n---\nSeagate Technology | Official Forums Team\n\n---", "author_fullname": "t2_16nn7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Official Giveaway: December 2023 Seagate 2 x 10TB IronWolf Pro Hard Drives Giveaway!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hqjo1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": "", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702499911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Datahoarders! Lets finish 2023 on a strong note with a final giveaway for the year. After discussing with your mods about issues around unlimited data storage offerings from cloud providers, we&amp;#39;ve decided to make this one a bit more about capacity. We are giving one lucky &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt; subscriber a pair of 10TB IronWolf Pro hard drives.&lt;/p&gt;\n\n&lt;p&gt;The prize is: two units of 10TB IronWolf Pro hard drives for one winner.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How to enter: Just reply to this post once with a top-level comment detailing your current local setup with a verified picture (containing a written note with your username and date), what you hoard primarily (excluding pirated and/or illegal media), and why you&amp;#39;re looking to expand your storage. Include the phrases RunWithIronwolf and Seagate.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Selection process/rules&lt;/p&gt;\n\n&lt;p&gt;One entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until December 27, 2023 at 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.&lt;/p&gt;\n\n&lt;p&gt;Geographic restrictions:&lt;/p&gt;\n\n&lt;p&gt;Our policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions.&lt;/p&gt;\n\n&lt;p&gt;US&lt;/p&gt;\n\n&lt;p&gt;Canada (will require a basic skills-based question if winner is chosen by law)&lt;/p&gt;\n\n&lt;p&gt;Brazil&lt;/p&gt;\n\n&lt;p&gt;South America&lt;/p&gt;\n\n&lt;p&gt;United Kingdom&lt;/p&gt;\n\n&lt;p&gt;Germany&lt;/p&gt;\n\n&lt;p&gt;France&lt;/p&gt;\n\n&lt;p&gt;Iberia&lt;/p&gt;\n\n&lt;p&gt;Australia&lt;/p&gt;\n\n&lt;p&gt;New Zealand&lt;/p&gt;\n\n&lt;p&gt;Korea&lt;/p&gt;\n\n&lt;p&gt;India&lt;/p&gt;\n\n&lt;p&gt;Malaysia&lt;/p&gt;\n\n&lt;p&gt;Singapore&lt;/p&gt;\n\n&lt;p&gt;China&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Seagate Technology | Official Forums Team&lt;/p&gt;\n\n&lt;hr/&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OFFICIAL SEAGATE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "18hqjo1", "is_robot_indexable": true, "report_reasons": null, "author": "Seagate_Surfer", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18hqjo1/official_giveaway_december_2023_seagate_2_x_10tb/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/18hqjo1/official_giveaway_december_2023_seagate_2_x_10tb/", "subreddit_subscribers": 717967, "created_utc": 1702499911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nI am a bit stuck, could you please give me some ideas?\n\n2 years ago I got 2 x 18 TB HDDs on wich to store data ... As space is at a premium in my flat and I don't need the drives online at all times, I have them in my main gaming rig - a Meshify 2 case, under the shroud near the PSU - I connect them whenever I need them, copy data from / to them then disconnect them. Originally I had them in RAID 1 but found out that disconnecting them from Windows and connecting them back they were always rebuilding the raid, so I have gave up o that and I make sure to copy the data to both of them - a bit tedious but it's a 4 hours effort every 4-5 months.\n\nNow 2 years later, these drives are on the verge of getting full ... and have no idea how to proceed.\n\nI guess I would need to commit one way or another - either build up some kind of storage server or get at least a 4 bay NAS. In that case, how easy would it be to extend that in 2-3 years when the 4 drives will get full?\n\nWhat would you guys recommend?\n\nThanks", "author_fullname": "t2_155ml1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me - How to continue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hjz0b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702483094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I am a bit stuck, could you please give me some ideas?&lt;/p&gt;\n\n&lt;p&gt;2 years ago I got 2 x 18 TB HDDs on wich to store data ... As space is at a premium in my flat and I don&amp;#39;t need the drives online at all times, I have them in my main gaming rig - a Meshify 2 case, under the shroud near the PSU - I connect them whenever I need them, copy data from / to them then disconnect them. Originally I had them in RAID 1 but found out that disconnecting them from Windows and connecting them back they were always rebuilding the raid, so I have gave up o that and I make sure to copy the data to both of them - a bit tedious but it&amp;#39;s a 4 hours effort every 4-5 months.&lt;/p&gt;\n\n&lt;p&gt;Now 2 years later, these drives are on the verge of getting full ... and have no idea how to proceed.&lt;/p&gt;\n\n&lt;p&gt;I guess I would need to commit one way or another - either build up some kind of storage server or get at least a 4 bay NAS. In that case, how easy would it be to extend that in 2-3 years when the 4 drives will get full?&lt;/p&gt;\n\n&lt;p&gt;What would you guys recommend?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hjz0b", "is_robot_indexable": true, "report_reasons": null, "author": "mariusmoga_2005", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hjz0b/help_me_how_to_continue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hjz0b/help_me_how_to_continue/", "subreddit_subscribers": 717967, "created_utc": 1702483094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Like, if you love seeing the files transfer in time, live. Love seeing the speed that it takes to transfer them and how cool it feels to have them in your HDD. Does the whole process of it feel good to you?  \n\n\nI am someone that finally switched from cloud storage and I can say that locally, it just feels much better.", "author_fullname": "t2_pzcs0b20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do y\u2019all get any type of satisfaction while transferring files to y\u2019all\u2019s HDDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hjdb4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.48, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702481484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like, if you love seeing the files transfer in time, live. Love seeing the speed that it takes to transfer them and how cool it feels to have them in your HDD. Does the whole process of it feel good to you?  &lt;/p&gt;\n\n&lt;p&gt;I am someone that finally switched from cloud storage and I can say that locally, it just feels much better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18hjdb4", "is_robot_indexable": true, "report_reasons": null, "author": "ImHidingtheRealMe", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hjdb4/do_yall_get_any_type_of_satisfaction_while/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hjdb4/do_yall_get_any_type_of_satisfaction_while/", "subreddit_subscribers": 717967, "created_utc": 1702481484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought two second-hand MG08ACA16TE with firmware version 4304. One produces continuous  clicking sounds when idle, even when powered on without a SATA cable. Clicking from a few times to dozens of times, with about 3 seconds pause in between. There are no clicking when the drive is in use during continuous read/write activity.\n\nI'm curious if there's a way to make the noisy drive about equally silent during idle as the other one. Any insights or suggestions would be appreciated.\n\nhttps://preview.redd.it/oe6obwxgf06c1.jpg?width=2159&amp;format=pjpg&amp;auto=webp&amp;s=1a6ed351f6a862dbdb188e970e0538a63e62f573", "author_fullname": "t2_810gprr7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One MG08 Clicks, the Other Stays Silent - Any Insights?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "media_metadata": {"oe6obwxgf06c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/oe6obwxgf06c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=410ba80e2db514409c9e529d527aba6c91c9e386"}, {"y": 97, "x": 216, "u": "https://preview.redd.it/oe6obwxgf06c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ceba9d6657e3090f30e1886c2c9d7a2b51e36437"}, {"y": 144, "x": 320, "u": "https://preview.redd.it/oe6obwxgf06c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ea4baab21f1cc45da4d4f7caccfeb0574765300"}, {"y": 289, "x": 640, "u": "https://preview.redd.it/oe6obwxgf06c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ba34c99da379c801c6517cf01378fa2a394b74a"}, {"y": 434, "x": 960, "u": "https://preview.redd.it/oe6obwxgf06c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3db9ab84411ca82ebe8556df1cd9775400e76cc"}, {"y": 488, "x": 1080, "u": "https://preview.redd.it/oe6obwxgf06c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f208dff77716a93611b6f9417311eb2e1a53b76b"}], "s": {"y": 977, "x": 2159, "u": "https://preview.redd.it/oe6obwxgf06c1.jpg?width=2159&amp;format=pjpg&amp;auto=webp&amp;s=1a6ed351f6a862dbdb188e970e0538a63e62f573"}, "id": "oe6obwxgf06c1"}}, "name": "t3_18hayrw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/T4DRcfwuoRcsl8N_brtaSF1k1aCxhYcAFIfL98tzOV4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702450581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought two second-hand MG08ACA16TE with firmware version 4304. One produces continuous  clicking sounds when idle, even when powered on without a SATA cable. Clicking from a few times to dozens of times, with about 3 seconds pause in between. There are no clicking when the drive is in use during continuous read/write activity.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if there&amp;#39;s a way to make the noisy drive about equally silent during idle as the other one. Any insights or suggestions would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/oe6obwxgf06c1.jpg?width=2159&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a6ed351f6a862dbdb188e970e0538a63e62f573\"&gt;https://preview.redd.it/oe6obwxgf06c1.jpg?width=2159&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a6ed351f6a862dbdb188e970e0538a63e62f573&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hayrw", "is_robot_indexable": true, "report_reasons": null, "author": "AM-Dig7006", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hayrw/one_mg08_clicks_the_other_stays_silent_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hayrw/one_mg08_clicks_the_other_stays_silent_any/", "subreddit_subscribers": 717967, "created_utc": 1702450581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using ACASIS 7port USB Hub with power supplied for a few months now.  \nrecently having issue of it getting disconnect and reconnecting quite frequently which is terribly affecting my daily use.  \nanyone else encountered this problem with solutions?  \ncant seem to find any link to a driver update or something...", "author_fullname": "t2_fva6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using ACASIS USB hub?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h740b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702437143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using ACASIS 7port USB Hub with power supplied for a few months now.&lt;br/&gt;\nrecently having issue of it getting disconnect and reconnecting quite frequently which is terribly affecting my daily use.&lt;br/&gt;\nanyone else encountered this problem with solutions?&lt;br/&gt;\ncant seem to find any link to a driver update or something...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18h740b", "is_robot_indexable": true, "report_reasons": null, "author": "roastedhead", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18h740b/anyone_using_acasis_usb_hub/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18h740b/anyone_using_acasis_usb_hub/", "subreddit_subscribers": 717967, "created_utc": 1702437143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi I had previously built a APFS volume on a concatenated Jbod I had 5 drives of different sizes between 2tb to 6 and am quickly approaching my 12tb total of space. I purchased a 2nd 2 bay enclosure which according to the company can daisy chain to the 1st enclosure i have. I did so and added under disk utilities the 18 tb hard drive to the JBOD. It shows up in there as online and a part of the volume.  I cant access any of that spaces, when i look at total space it has 30tb but then under free it still only shows 84gb from my original 12tb. I decided maybe I would remove the 18tb disk but after hours of looking online I couldnt figure out how to remove it. I havent added any files to the 18tb disk yet and it sits in 3rd place on my list of drives. How can move that disk down to the last on the list so i can hit the - on it to remove it or how do I make the jbod recognize the drive space? disconnecting that 18tb now causes the entire thing to disconnect but it wont show free space. what am i missing?", "author_fullname": "t2_5xx873r0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orico Drive Expansion on Mac", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18hw3v6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702514716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I had previously built a APFS volume on a concatenated Jbod I had 5 drives of different sizes between 2tb to 6 and am quickly approaching my 12tb total of space. I purchased a 2nd 2 bay enclosure which according to the company can daisy chain to the 1st enclosure i have. I did so and added under disk utilities the 18 tb hard drive to the JBOD. It shows up in there as online and a part of the volume.  I cant access any of that spaces, when i look at total space it has 30tb but then under free it still only shows 84gb from my original 12tb. I decided maybe I would remove the 18tb disk but after hours of looking online I couldnt figure out how to remove it. I havent added any files to the 18tb disk yet and it sits in 3rd place on my list of drives. How can move that disk down to the last on the list so i can hit the - on it to remove it or how do I make the jbod recognize the drive space? disconnecting that 18tb now causes the entire thing to disconnect but it wont show free space. what am i missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18hw3v6", "is_robot_indexable": true, "report_reasons": null, "author": "TCKParadox", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18hw3v6/orico_drive_expansion_on_mac/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18hw3v6/orico_drive_expansion_on_mac/", "subreddit_subscribers": 717967, "created_utc": 1702514716.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}