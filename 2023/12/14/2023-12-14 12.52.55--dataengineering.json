{"kind": "Listing", "data": {"after": "t3_18i5ovj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my previous job, I was a data analyst, but I've come across a lot of people who tell me that I should go into data engineering. Problem is, every time I ask someone who is in that field, they honestly cannot tell me what it is that they do. Like, I have never met a single data engineer in any company I've worked for who has given me a simple and reasonable explanation for what they do.\n\n\n\nAt my previous job, I designed ETL queries using SQL and Python, wrote APIs for interfacing with different database softwares for example I created an API in Python to automatically connect to Google BigQuery, retrieve data for the last 30 days, and then move it into other data sources \n\n\nI also performed audits on data, so where there were gaps in data or areas where they said that data was incorrect, I would go hunting and find gaps in the data that didn't make sense, for example, why is there missing data between these two linked tables? Is there a specific date that there's missing data?\n\n\nFinally, I created new links between data across different sources, for example from snowflake to BigQuery, even a little bit of access", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone explain the job of data engineer like I'm a baboon?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hlsqb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 128, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 128, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702488512.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702487800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my previous job, I was a data analyst, but I&amp;#39;ve come across a lot of people who tell me that I should go into data engineering. Problem is, every time I ask someone who is in that field, they honestly cannot tell me what it is that they do. Like, I have never met a single data engineer in any company I&amp;#39;ve worked for who has given me a simple and reasonable explanation for what they do.&lt;/p&gt;\n\n&lt;p&gt;At my previous job, I designed ETL queries using SQL and Python, wrote APIs for interfacing with different database softwares for example I created an API in Python to automatically connect to Google BigQuery, retrieve data for the last 30 days, and then move it into other data sources &lt;/p&gt;\n\n&lt;p&gt;I also performed audits on data, so where there were gaps in data or areas where they said that data was incorrect, I would go hunting and find gaps in the data that didn&amp;#39;t make sense, for example, why is there missing data between these two linked tables? Is there a specific date that there&amp;#39;s missing data?&lt;/p&gt;\n\n&lt;p&gt;Finally, I created new links between data across different sources, for example from snowflake to BigQuery, even a little bit of access&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hlsqb", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hlsqb/can_someone_explain_the_job_of_data_engineer_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hlsqb/can_someone_explain_the_job_of_data_engineer_like/", "subreddit_subscribers": 145842, "created_utc": 1702487800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Other than \"creating data pipelines over, and over again\" or \"practicing\", what are some things that have radically improved your data engineering skills?\n\ne.g.\n\n* Making each discrete phase of a given pipeline dumber (i.e. simpler, easier to deliver, easier to replace)\n* Starting with a dumb Python script almost every time, and making it more robust over time\n* Just getting started, and making things pretty later\n* Using a particular data pipeline orchestrator because &lt;x&gt;\n* Analyzing data a particular way before getting started\n* Creating documentation &lt;y&gt; before getting started\n* Sketching data composition and lineage out with pen and paper\n* Sketching out your data pipeline with pen and paper\n* Rendering your data pipeline using GraphViz and DOT markup, etc. so you can better visualize the structure of your data pipelines\n\nEDIT: if this is too vague, or a bad fit, I'll delete it, just let me know - AFAIK how you build things and the process by which you decide to build things is just as important as what tools you use to build those things - i.e., software architecture matters when developing software - but, I don't know - just, let me know.", "author_fullname": "t2_lwr1wvz8q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some things that have radically improved your data engineering skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hgf31", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 82, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 82, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702484360.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702472862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Other than &amp;quot;creating data pipelines over, and over again&amp;quot; or &amp;quot;practicing&amp;quot;, what are some things that have radically improved your data engineering skills?&lt;/p&gt;\n\n&lt;p&gt;e.g.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Making each discrete phase of a given pipeline dumber (i.e. simpler, easier to deliver, easier to replace)&lt;/li&gt;\n&lt;li&gt;Starting with a dumb Python script almost every time, and making it more robust over time&lt;/li&gt;\n&lt;li&gt;Just getting started, and making things pretty later&lt;/li&gt;\n&lt;li&gt;Using a particular data pipeline orchestrator because &amp;lt;x&amp;gt;&lt;/li&gt;\n&lt;li&gt;Analyzing data a particular way before getting started&lt;/li&gt;\n&lt;li&gt;Creating documentation &amp;lt;y&amp;gt; before getting started&lt;/li&gt;\n&lt;li&gt;Sketching data composition and lineage out with pen and paper&lt;/li&gt;\n&lt;li&gt;Sketching out your data pipeline with pen and paper&lt;/li&gt;\n&lt;li&gt;Rendering your data pipeline using GraphViz and DOT markup, etc. so you can better visualize the structure of your data pipelines&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;EDIT: if this is too vague, or a bad fit, I&amp;#39;ll delete it, just let me know - AFAIK how you build things and the process by which you decide to build things is just as important as what tools you use to build those things - i.e., software architecture matters when developing software - but, I don&amp;#39;t know - just, let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hgf31", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Importance-1605", "discussion_type": null, "num_comments": 68, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18hgf31/what_are_some_things_that_have_radically_improved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hgf31/what_are_some_things_that_have_radically_improved/", "subreddit_subscribers": 145842, "created_utc": 1702472862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious to hear what your current role is now and how you made the decision that you were ready for a change. \n\nCurrently looking to make a change but not sure if I should go into people management or an architect type role.\n\nCheers!", "author_fullname": "t2_5par13m8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What role did you go into after Sr. Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hs6ue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702504217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to hear what your current role is now and how you made the decision that you were ready for a change. &lt;/p&gt;\n\n&lt;p&gt;Currently looking to make a change but not sure if I should go into people management or an architect type role.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hs6ue", "is_robot_indexable": true, "report_reasons": null, "author": "rysnotnice", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hs6ue/what_role_did_you_go_into_after_sr_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hs6ue/what_role_did_you_go_into_after_sr_data_engineer/", "subreddit_subscribers": 145842, "created_utc": 1702504217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started at my current company one year ago. Raises were in June, and no one on the data team received one. There's the possibility of merit increases at the end of this year, but no word on it yet and some of us are skeptical we'll get anything at all. Just wondering how the things are looking out there.", "author_fullname": "t2_hzkq25fx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are pay raises looking this year?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hsdkk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702504696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started at my current company one year ago. Raises were in June, and no one on the data team received one. There&amp;#39;s the possibility of merit increases at the end of this year, but no word on it yet and some of us are skeptical we&amp;#39;ll get anything at all. Just wondering how the things are looking out there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18hsdkk", "is_robot_indexable": true, "report_reasons": null, "author": "peevedpedestrian", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hsdkk/how_are_pay_raises_looking_this_year/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hsdkk/how_are_pay_raises_looking_this_year/", "subreddit_subscribers": 145842, "created_utc": 1702504696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Fantastic post from Georg and Aleksandar", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster, dbt, duckdb as new local MDS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18hipz6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "#46d160", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/C8BThcaoCneGtqR55zryhvIDi2M_OoTkLPOgIn9tXAM.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702479709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "georgheiler.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fantastic post from Georg and Aleksandar&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://georgheiler.com/2023/12/11/dagster-dbt-duckdb-as-new-local-mds/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?auto=webp&amp;s=861e4f94dc91e2f5ba063c56da2f709e01437573", "width": 2048, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1cdb58b5a195232e633f8b91f78f9f24e22bcc1e", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3605171ded8bd689c2b60aa4bf84ed0536c09c7f", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4cbba5cea1ebe44625c4f4069a6c78dcfa85061", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df803cd4ca1683749c35eada75315eb588ef67e1", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=99501c4a7318bc43ffec2b0e60c50c810127a1a0", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e815925fa8a0e77f571fed51d348e38b1b9ddae", "width": 1080, "height": 1080}], "variants": {}, "id": "fi_LHP4YaKAqYxd02lcILIegMmkY3lQeK3OCDUu36vQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Lead Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hipz6", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/18hipz6/dagster_dbt_duckdb_as_new_local_mds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://georgheiler.com/2023/12/11/dagster-dbt-duckdb-as-new-local-mds/", "subreddit_subscribers": 145842, "created_utc": 1702479709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I started with a small company this year. The Executive that hired me stated they wanted to start a data team and I was hired on to start the group and get it off the ground. Great, hired, added in DataBricks for DE/orchestration and settled on QuickSight to keep things simple (AWS Shop). I have integrated all internal applications and financial data into usable reporting at this point.\n\nNow, I am a team of one doing multiple roles here: \n\nStrategy (Leadership), DE, DS, DA, PM, and building business logic with each part of the business (time consuming!!!).\n\n I am well paid (205k), but nobody understands what I do. I am on an island. The company has basically zero data literacy since the person that hired me has just left. So I am alone in more ways than one. I miss having a team and someone to at least talk to who understands what I do.\n\nAdditional context: There are no actual internal processes for the business leading to the data never matching and being a hot mess. This takes up a majority of my time fixing the years of mistakes and working with each business unit.\n\nAdditional Context 2: New Boss says absolutely not growing the team.\n\n&amp;#x200B;\n\nAny strategies to better communicate trying to untangle years of chaos data from spreadsheets? How long would you guys stick it out? \n\nTo me, standing up a Data stack and writing custom integration code to make usable data / reporting within a year is a success. Is it? Not sure how to convey how doing it all within a year myself is a success. Any thoughts appreciated! \n\nTLDR: Started with a company, executive that hired me left leaving zero data literacy. I stood up all critical data and stack within a year. What would you do as your next steps?", "author_fullname": "t2_mzk56ae", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Company - Minimal Data Literacy - Jack of all trades", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hntt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702492903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I started with a small company this year. The Executive that hired me stated they wanted to start a data team and I was hired on to start the group and get it off the ground. Great, hired, added in DataBricks for DE/orchestration and settled on QuickSight to keep things simple (AWS Shop). I have integrated all internal applications and financial data into usable reporting at this point.&lt;/p&gt;\n\n&lt;p&gt;Now, I am a team of one doing multiple roles here: &lt;/p&gt;\n\n&lt;p&gt;Strategy (Leadership), DE, DS, DA, PM, and building business logic with each part of the business (time consuming!!!).&lt;/p&gt;\n\n&lt;p&gt;I am well paid (205k), but nobody understands what I do. I am on an island. The company has basically zero data literacy since the person that hired me has just left. So I am alone in more ways than one. I miss having a team and someone to at least talk to who understands what I do.&lt;/p&gt;\n\n&lt;p&gt;Additional context: There are no actual internal processes for the business leading to the data never matching and being a hot mess. This takes up a majority of my time fixing the years of mistakes and working with each business unit.&lt;/p&gt;\n\n&lt;p&gt;Additional Context 2: New Boss says absolutely not growing the team.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any strategies to better communicate trying to untangle years of chaos data from spreadsheets? How long would you guys stick it out? &lt;/p&gt;\n\n&lt;p&gt;To me, standing up a Data stack and writing custom integration code to make usable data / reporting within a year is a success. Is it? Not sure how to convey how doing it all within a year myself is a success. Any thoughts appreciated! &lt;/p&gt;\n\n&lt;p&gt;TLDR: Started with a company, executive that hired me left leaving zero data literacy. I stood up all critical data and stack within a year. What would you do as your next steps?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hntt1", "is_robot_indexable": true, "report_reasons": null, "author": "what_is_ovaltine", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hntt1/new_company_minimal_data_literacy_jack_of_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hntt1/new_company_minimal_data_literacy_jack_of_all/", "subreddit_subscribers": 145842, "created_utc": 1702492903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My organization has an existing data engineering software stack but I have a large new project in the pipeline that will be supported by a dedicated team outside of our existing DE group.  I, as as solutions architect, am free to choose a new software stack for this project, although I do see value in using our existing toolsets and not creating duplicate solutions.  If funding wasn't a big issue, what solutions would you deploy today?  My only constraint is that it will be deployed in Azure.  Here's our current landscape:\n\nScheduling &amp; pipelines: Tidal\n\nETL for integrations: Talend\n\nData lake/warehouse: Snowflake\n\nAI/ML for DE: Databricks\n\nSQL-based transformations: dbt\n\nThanks!", "author_fullname": "t2_9lsmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What software stack would you choose if you were starting from scratch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hjcy6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702481458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My organization has an existing data engineering software stack but I have a large new project in the pipeline that will be supported by a dedicated team outside of our existing DE group.  I, as as solutions architect, am free to choose a new software stack for this project, although I do see value in using our existing toolsets and not creating duplicate solutions.  If funding wasn&amp;#39;t a big issue, what solutions would you deploy today?  My only constraint is that it will be deployed in Azure.  Here&amp;#39;s our current landscape:&lt;/p&gt;\n\n&lt;p&gt;Scheduling &amp;amp; pipelines: Tidal&lt;/p&gt;\n\n&lt;p&gt;ETL for integrations: Talend&lt;/p&gt;\n\n&lt;p&gt;Data lake/warehouse: Snowflake&lt;/p&gt;\n\n&lt;p&gt;AI/ML for DE: Databricks&lt;/p&gt;\n\n&lt;p&gt;SQL-based transformations: dbt&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hjcy6", "is_robot_indexable": true, "report_reasons": null, "author": "dantasticdotorg", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hjcy6/what_software_stack_would_you_choose_if_you_were/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hjcy6/what_software_stack_would_you_choose_if_you_were/", "subreddit_subscribers": 145842, "created_utc": 1702481458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "EDIT: will implement s3 -&gt; COPY -&gt; Redshift, but in the short term if anyone has advice on any performance boost over the default python redshift_connector for bulk inserts I'd appreciate it. I know that the clickhouse python connector does bulk inserts significantly faster and ch is also a columnar db. So just looking for any short term speed tips. \n\nSeems that the recommended way is to load into s3 and then COPY into redshift.\n\nBut wondering if there's ability in the python redshift_connector or in some other (SQLAlchemy, psycopg2, etc) package that can make this faster that I've missed. \n\nRight now rows are yielded from a generator in batches of 5k and the redshift_connector's execute_many() method is called. Looking at the source code it looks like this is just a loop that calls execute(). \n\nThe insert_data_bulk() method seems to be implemented with the first approach in mind - takes in a file name and reads rows into a list then calls execute.\n\nThinking that execute_many() is making a commit each loop...\n\nThanks for any pointers.", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inserting 10ks records into redshift with python redshift_connector is slow. Alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hwij9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702519207.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702515901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT: will implement s3 -&amp;gt; COPY -&amp;gt; Redshift, but in the short term if anyone has advice on any performance boost over the default python redshift_connector for bulk inserts I&amp;#39;d appreciate it. I know that the clickhouse python connector does bulk inserts significantly faster and ch is also a columnar db. So just looking for any short term speed tips. &lt;/p&gt;\n\n&lt;p&gt;Seems that the recommended way is to load into s3 and then COPY into redshift.&lt;/p&gt;\n\n&lt;p&gt;But wondering if there&amp;#39;s ability in the python redshift_connector or in some other (SQLAlchemy, psycopg2, etc) package that can make this faster that I&amp;#39;ve missed. &lt;/p&gt;\n\n&lt;p&gt;Right now rows are yielded from a generator in batches of 5k and the redshift_connector&amp;#39;s execute_many() method is called. Looking at the source code it looks like this is just a loop that calls execute(). &lt;/p&gt;\n\n&lt;p&gt;The insert_data_bulk() method seems to be implemented with the first approach in mind - takes in a file name and reads rows into a list then calls execute.&lt;/p&gt;\n\n&lt;p&gt;Thinking that execute_many() is making a commit each loop...&lt;/p&gt;\n\n&lt;p&gt;Thanks for any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18hwij9", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hwij9/inserting_10ks_records_into_redshift_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hwij9/inserting_10ks_records_into_redshift_with_python/", "subreddit_subscribers": 145842, "created_utc": 1702515901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Doing the fundamentals of dbt course right now and came across the following:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/snw7lecr326c1.png?width=1838&amp;format=png&amp;auto=webp&amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400\n\n&amp;#x200B;\n\nThey specify a test in some documentation file (yaml).\n\nI wonder why is this done on this level and not on the level of sql constraints?\n\nI skimmed a thread a found a comment where [someone argued](https://www.reddit.com/r/dataengineering/comments/119s7yv/comment/j9ozeus/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) that dbt is good for when one can live (has) dirty data and hence has not enforced the level of data completeness as done with sql constraints.\n\n&amp;#x200B;\n\nWhat is the philosophy there?\n\nWhy would I even consider living with dirty data? Is my goal as an engineer not to keep data records clean and sort the completeness out on the level where transactions (i.e. updates) are handled?", "author_fullname": "t2_1b2msvdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "why testing in dbt? why live with dirty data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 47, "top_awarded_type": null, "hide_score": false, "media_metadata": {"snw7lecr326c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/snw7lecr326c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dcdfdcc4acf994e7dea6193fb88688aeaa8a0129"}, {"y": 74, "x": 216, "u": "https://preview.redd.it/snw7lecr326c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac27c1ed016e48de40ac4fb89df5d0f51400e6a5"}, {"y": 109, "x": 320, "u": "https://preview.redd.it/snw7lecr326c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=463eeaecda7aea123587ca48ce6aa9b67568bd79"}, {"y": 219, "x": 640, "u": "https://preview.redd.it/snw7lecr326c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ebcf00dce7fc5f281758d0ba1f223237238a4fb"}, {"y": 329, "x": 960, "u": "https://preview.redd.it/snw7lecr326c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ad4f1c57916a9c19c40e918897dd1f82e39449d"}, {"y": 370, "x": 1080, "u": "https://preview.redd.it/snw7lecr326c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=769012686b0bfed9e39949fe00466e3971b66dad"}], "s": {"y": 630, "x": 1838, "u": "https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;format=png&amp;auto=webp&amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400"}, "id": "snw7lecr326c1"}}, "name": "t3_18hfvkl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TEifI7eBZWLbs2l77WkdatkskdQfmD-Qq6dGuIAzlio.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702471054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doing the fundamentals of dbt course right now and came across the following:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400\"&gt;https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;They specify a test in some documentation file (yaml).&lt;/p&gt;\n\n&lt;p&gt;I wonder why is this done on this level and not on the level of sql constraints?&lt;/p&gt;\n\n&lt;p&gt;I skimmed a thread a found a comment where &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/119s7yv/comment/j9ozeus/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;someone argued&lt;/a&gt; that dbt is good for when one can live (has) dirty data and hence has not enforced the level of data completeness as done with sql constraints.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is the philosophy there?&lt;/p&gt;\n\n&lt;p&gt;Why would I even consider living with dirty data? Is my goal as an engineer not to keep data records clean and sort the completeness out on the level where transactions (i.e. updates) are handled?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hfvkl", "is_robot_indexable": true, "report_reasons": null, "author": "HillTheBilly", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hfvkl/why_testing_in_dbt_why_live_with_dirty_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hfvkl/why_testing_in_dbt_why_live_with_dirty_data/", "subreddit_subscribers": 145842, "created_utc": 1702471054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey community \ud83d\udc4b\n\nI just implemented data quality tests with [Soda Core](https://github.com/sodadata/soda-core) and the [prefect-soda-core](https://github.com/sodadata/prefect-soda-core) extension within my data infrastructure for a [project centered around the English Premier League](https://github.com/digitalghost-dev/premier-league) that I have been working on lately that runs on a schedule using [Prefect](https://github.com/PrefectHQ/prefect).\n\n[Screenshot of the Prefect dashboard for the flow run.](https://preview.redd.it/zdlwdec8p36c1.png?width=2818&amp;format=png&amp;auto=webp&amp;s=66a926223af521160fda2c20176fecfc40bda2af)\n\nSome of the checks I have created are pretty simple but I aim to add more:\n\n    checks for news:\n      - row_count &gt; 1\n      - invalid_count(url) = 0:\n          valid regex: ^https://\n    \n    checks for stadiums:\n      - row_count = 20\n    \n    checks for standings:\n      - row_count = 20\n      - duplicate_count(team) = 0\n      - max(points) &lt; 114\n      - min(points) &gt; 0\n    \n    checks for teams:\n      - row_count = 20\n      - duplicate_count(team) = 0\n    \n    checks for top_scorers:\n      - row_count = 5\n\nThe `soda-core-bigquery` library connects directly to my BigQuery tables via default `gcloud` credentials on a virtual machine hosted on [Compute Engine](https://cloud.google.com/compute?hl=en) on Google Cloud. Has anyone else implement data quality checks with their data infrastructure?", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Data Quality Checks into the Data Infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zdlwdec8p36c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 43, "x": 108, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e96e63d94e2a7d05d8b3f336f379b03a48665f46"}, {"y": 86, "x": 216, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=197cd7f826df2ed4b0d9716e752768fa7810628e"}, {"y": 128, "x": 320, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e9bd64dd17aab81ce06f2db05a880f512b8727e"}, {"y": 257, "x": 640, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fcc9c2315bd5e58ad552b8cfed8ba7783e6d9423"}, {"y": 385, "x": 960, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc35d91d5f36b61d018a03b22f085281c1f24636"}, {"y": 433, "x": 1080, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7ce84fc6f990d99ca4db000bfa063b1c126fb91e"}], "s": {"y": 1132, "x": 2818, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=2818&amp;format=png&amp;auto=webp&amp;s=66a926223af521160fda2c20176fecfc40bda2af"}, "id": "zdlwdec8p36c1"}}, "name": "t3_18hmz09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CDs2oF8FNPg4ykWokPg_iyxFR2sys-rfFbxBCz0E4cg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1702490718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey community \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;I just implemented data quality tests with &lt;a href=\"https://github.com/sodadata/soda-core\"&gt;Soda Core&lt;/a&gt; and the &lt;a href=\"https://github.com/sodadata/prefect-soda-core\"&gt;prefect-soda-core&lt;/a&gt; extension within my data infrastructure for a &lt;a href=\"https://github.com/digitalghost-dev/premier-league\"&gt;project centered around the English Premier League&lt;/a&gt; that I have been working on lately that runs on a schedule using &lt;a href=\"https://github.com/PrefectHQ/prefect\"&gt;Prefect&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zdlwdec8p36c1.png?width=2818&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=66a926223af521160fda2c20176fecfc40bda2af\"&gt;Screenshot of the Prefect dashboard for the flow run.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Some of the checks I have created are pretty simple but I aim to add more:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;checks for news:\n  - row_count &amp;gt; 1\n  - invalid_count(url) = 0:\n      valid regex: ^https://\n\nchecks for stadiums:\n  - row_count = 20\n\nchecks for standings:\n  - row_count = 20\n  - duplicate_count(team) = 0\n  - max(points) &amp;lt; 114\n  - min(points) &amp;gt; 0\n\nchecks for teams:\n  - row_count = 20\n  - duplicate_count(team) = 0\n\nchecks for top_scorers:\n  - row_count = 5\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The &lt;code&gt;soda-core-bigquery&lt;/code&gt; library connects directly to my BigQuery tables via default &lt;code&gt;gcloud&lt;/code&gt; credentials on a virtual machine hosted on &lt;a href=\"https://cloud.google.com/compute?hl=en\"&gt;Compute Engine&lt;/a&gt; on Google Cloud. Has anyone else implement data quality checks with their data infrastructure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?auto=webp&amp;s=c2974d68ab4436ab42d31ac37ceb1edfab081328", "width": 1280, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9fa76990f149cb460631915757118e32ee3a17b5", "width": 108, "height": 42}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a415f27dbddcb0665a2adb50bd29357beca0ec5", "width": 216, "height": 84}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8faf8367e79f6fd7239e32abc7cff4bd4a16d09", "width": 320, "height": 125}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b55fb276b2670ced04c194eb844500ee4d4e48e1", "width": 640, "height": 250}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d0fd4433db81727e6230fcd321ac5cdcb29ca095", "width": 960, "height": 375}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f2184b1063031a46b41984fee785a0f66f9bb4aa", "width": 1080, "height": 421}], "variants": {}, "id": "HL_qWEYj1OF-fF8JgsOfD5dVK1TuuOcvdAW--omanMs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18hmz09", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hmz09/introducing_data_quality_checks_into_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hmz09/introducing_data_quality_checks_into_the_data/", "subreddit_subscribers": 145842, "created_utc": 1702490718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Background**\n\nI was recently let go in a mass layoff (CEO wanted to look good for the board before year end--par for the course) , and now I'm on the job hunt.\n\nI've worked as a Data Engineer for the last \\~5 years doing things like data model design and building ETL pipelines. At my last job I was encouraged to get some GCP certs, so I now hold a GCP ACE cert and a GCP Cloud Architect cert. Going forward I would really like to work with GCP (or cloud ops in general). The problem is I have little direct work experience with GCP--I was leading training for other engineers and was a SME for a couple of senior managers who weren't as technical\n\n**I have a few questions:**\n\n1. Do you think getting the GCP Data Engineer certification is overkill if I already have the other two?\n2. Any opinions on the Terraform certificate?  Is it worth the money or better to just learn it and skip the cert?\n3. Any other advice or comments are welcome!", "author_fullname": "t2_es03w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP Data Engineer certification advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hu4gl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702509158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I was recently let go in a mass layoff (CEO wanted to look good for the board before year end--par for the course) , and now I&amp;#39;m on the job hunt.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve worked as a Data Engineer for the last ~5 years doing things like data model design and building ETL pipelines. At my last job I was encouraged to get some GCP certs, so I now hold a GCP ACE cert and a GCP Cloud Architect cert. Going forward I would really like to work with GCP (or cloud ops in general). The problem is I have little direct work experience with GCP--I was leading training for other engineers and was a SME for a couple of senior managers who weren&amp;#39;t as technical&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have a few questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do you think getting the GCP Data Engineer certification is overkill if I already have the other two?&lt;/li&gt;\n&lt;li&gt;Any opinions on the Terraform certificate?  Is it worth the money or better to just learn it and skip the cert?&lt;/li&gt;\n&lt;li&gt;Any other advice or comments are welcome!&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18hu4gl", "is_robot_indexable": true, "report_reasons": null, "author": "skrillavilla", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hu4gl/gcp_data_engineer_certification_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hu4gl/gcp_data_engineer_certification_advice/", "subreddit_subscribers": 145842, "created_utc": 1702509158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9mc6bsvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Announcing Daft 0.2: 10x faster IO from S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18hnig9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6kAlrUkY0d1Am5zE5tJmFIqfa7ugiSpNHOtVd-KWuY4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702492084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.getdaft.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.getdaft.io/p/announcing-daft-02-10x-faster-io", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?auto=webp&amp;s=033ee28ec46c1da6de6b5e8b169b56adff892de9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da773c307287ff0175512954baacc60e62eb25e4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dbc0dca765c196e0d8b85bc54a1c05987489ea3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=66f4994c01506b7e9ee8f957375b238afc0d3183", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3e30aa26a2db2e44c5c022bc5bac0323d1d4cba", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=45ced61130e03ef0f68cee7b5b1c2dc50e911559", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bdfe16b73190ee87b682de77da1f313ab846e7c3", "width": 1080, "height": 540}], "variants": {}, "id": "3J-x3pcNAsUf9E9q4Zlw4hot4HRtb5WEEWmB5K_QhQk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18hnig9", "is_robot_indexable": true, "report_reasons": null, "author": "xylene25", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hnig9/announcing_daft_02_10x_faster_io_from_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.getdaft.io/p/announcing-daft-02-10x-faster-io", "subreddit_subscribers": 145842, "created_utc": 1702492084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a Junior data engineer with 1.5 yrs experience . I do have one doubt like which is better for us being a Data engineer ? Like, \nIs to provide Data to other team such as analysts so they can build an insight from the data and build a Dashboard which is actually possible for us too .\nOr \nTo build an end to end product (such as a dashboard) where we bring the transformed data and build the dashboard and give to the end users such as analyst.", "author_fullname": "t2_suskxfga", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do we do ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hhk3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702476434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a Junior data engineer with 1.5 yrs experience . I do have one doubt like which is better for us being a Data engineer ? Like, \nIs to provide Data to other team such as analysts so they can build an insight from the data and build a Dashboard which is actually possible for us too .\nOr \nTo build an end to end product (such as a dashboard) where we bring the transformed data and build the dashboard and give to the end users such as analyst.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hhk3e", "is_robot_indexable": true, "report_reasons": null, "author": "PressureCandid1989", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hhk3e/what_do_we_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hhk3e/what_do_we_do/", "subreddit_subscribers": 145842, "created_utc": 1702476434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, hope you are okay!\n\nSo, I work for a big company where we develop demands coming from other sectors.\n\nEach project has it's own particularities but in a nutshell it follows the \"Data ingestion, data trasnformation and data serving\" path\n\nIn many projects I face a situation where the project gets blocked because something is missing and this blocking could be avoided with a little bit more planning before we started it.\n\nI would like to know if you guys have a kind of \"check list\" of what you need to have before start a project?\n\n&amp;#x200B;", "author_fullname": "t2_84jw5rk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering project kickof checklist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hglcf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702473448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, hope you are okay!&lt;/p&gt;\n\n&lt;p&gt;So, I work for a big company where we develop demands coming from other sectors.&lt;/p&gt;\n\n&lt;p&gt;Each project has it&amp;#39;s own particularities but in a nutshell it follows the &amp;quot;Data ingestion, data trasnformation and data serving&amp;quot; path&lt;/p&gt;\n\n&lt;p&gt;In many projects I face a situation where the project gets blocked because something is missing and this blocking could be avoided with a little bit more planning before we started it.&lt;/p&gt;\n\n&lt;p&gt;I would like to know if you guys have a kind of &amp;quot;check list&amp;quot; of what you need to have before start a project?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hglcf", "is_robot_indexable": true, "report_reasons": null, "author": "El_Balde_K", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hglcf/data_engineering_project_kickof_checklist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hglcf/data_engineering_project_kickof_checklist/", "subreddit_subscribers": 145842, "created_utc": 1702473448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started working with a small business. Until now, they have been using Microsoft SharePoint to keep their system-generated Excel files every day and then access them with other programs like Power BI. Still, now they are thinking about some more reliable solution to have a database to keep the data  they were thinking about Microsoft access. Still, I am thinking about what is the best thing these days for a small business to keep their data just a few different tables with a few relationships or even none of them.", "author_fullname": "t2_q9hhtqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help with a database choice for small business", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i43ue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702542541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started working with a small business. Until now, they have been using Microsoft SharePoint to keep their system-generated Excel files every day and then access them with other programs like Power BI. Still, now they are thinking about some more reliable solution to have a database to keep the data  they were thinking about Microsoft access. Still, I am thinking about what is the best thing these days for a small business to keep their data just a few different tables with a few relationships or even none of them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18i43ue", "is_robot_indexable": true, "report_reasons": null, "author": "Mr-Nyan", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i43ue/help_with_a_database_choice_for_small_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i43ue/help_with_a_database_choice_for_small_business/", "subreddit_subscribers": 145842, "created_utc": 1702542541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I manage a lot of data pipelines that ingest from many different places. In addition I had tests/monitors on this data in many different places in these pipelines. It ultimately became very complicated to know that status and health of these data pipelines at any given moment.\n\nAs a result, I created [Panda Patrol](https://www.pandapatrol.io/) which is an open-source tool that is best described as [Sentry](https://sentry.io/welcome/) for your data. With just one function call, you can monitor and profile all the data in your pipelines. I tried to keep the package as simple and easy to use as possible. Hope some of you guys find it useful!", "author_fullname": "t2_dcdt0mh5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Panda Patrol: Data pipeline monitoring and profiling with 1 function call", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hu0gg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702508870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I manage a lot of data pipelines that ingest from many different places. In addition I had tests/monitors on this data in many different places in these pipelines. It ultimately became very complicated to know that status and health of these data pipelines at any given moment.&lt;/p&gt;\n\n&lt;p&gt;As a result, I created &lt;a href=\"https://www.pandapatrol.io/\"&gt;Panda Patrol&lt;/a&gt; which is an open-source tool that is best described as &lt;a href=\"https://sentry.io/welcome/\"&gt;Sentry&lt;/a&gt; for your data. With just one function call, you can monitor and profile all the data in your pipelines. I tried to keep the package as simple and easy to use as possible. Hope some of you guys find it useful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18hu0gg", "is_robot_indexable": true, "report_reasons": null, "author": "gnahznavia", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hu0gg/panda_patrol_data_pipeline_monitoring_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hu0gg/panda_patrol_data_pipeline_monitoring_and/", "subreddit_subscribers": 145842, "created_utc": 1702508870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Five Tips on Postgres Logical Decoding - [https://blog.peerdb.io/five-tips-on-postgres-logical-decoding](https://blog.peerdb.io/five-tips-on-postgres-logical-decoding)\n\n\ud83d\udcc8 What causes slot growth and how to monitor it?  \n\u26a0\ufe0f Large transactions can lead to Slot growth and this can be avoided.  \n\ud83c\udfaf PUBLICATION for advance filtering of logical decoding changes  \n\ud83d\ude80 logical\\_decoding\\_work\\_mem to improve performance  \n\ud83d\udd25 Logical decoding via standbys in Postgres 16", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Five Tips on Postgres Logical Decoding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hrz8m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702503649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Five Tips on Postgres Logical Decoding - &lt;a href=\"https://blog.peerdb.io/five-tips-on-postgres-logical-decoding\"&gt;https://blog.peerdb.io/five-tips-on-postgres-logical-decoding&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcc8 What causes slot growth and how to monitor it?&lt;br/&gt;\n\u26a0\ufe0f Large transactions can lead to Slot growth and this can be avoided.&lt;br/&gt;\n\ud83c\udfaf PUBLICATION for advance filtering of logical decoding changes&lt;br/&gt;\n\ud83d\ude80 logical_decoding_work_mem to improve performance&lt;br/&gt;\n\ud83d\udd25 Logical decoding via standbys in Postgres 16&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bjf4kfTSko_cnNKmm3gz4mrpn7AUYlPT5OAfh3nOzDk.jpg?auto=webp&amp;s=b3d3dba724de34ba8492121d2ef4205443f9f27e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/bjf4kfTSko_cnNKmm3gz4mrpn7AUYlPT5OAfh3nOzDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ce334d3b6851a3e9a3ed3455d6ea8343d3a9a4d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bjf4kfTSko_cnNKmm3gz4mrpn7AUYlPT5OAfh3nOzDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de66462522a0dc6e78f2493800c300efde97f731", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/bjf4kfTSko_cnNKmm3gz4mrpn7AUYlPT5OAfh3nOzDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=03981b9fbfb318f97ffacfa959e3729e7930920e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/bjf4kfTSko_cnNKmm3gz4mrpn7AUYlPT5OAfh3nOzDk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=213130677c094211950f9e6e6a209c8ea82b5669", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/bjf4kfTSko_cnNKmm3gz4mrpn7AUYlPT5OAfh3nOzDk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f309e935c56dd49e7b383781d3dd663a83618094", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/bjf4kfTSko_cnNKmm3gz4mrpn7AUYlPT5OAfh3nOzDk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37a761fbbfb69767c0e59a5e721e2e3533b52bd2", "width": 1080, "height": 540}], "variants": {}, "id": "uq7LB5yFnqPYyawsCNAIQifzedOSNA5wcCN75LJRQH4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hrz8m", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hrz8m/five_tips_on_postgres_logical_decoding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hrz8m/five_tips_on_postgres_logical_decoding/", "subreddit_subscribers": 145842, "created_utc": 1702503649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Regarding the aggregation of heterogeneous data imported by different pipelines, what is your opinion on performing the aggregation as much upstream or further down as possible?\n\nI've recently seen people in conferences suggesting to transform data as much upstream as possible to generate a single core model used to populate different target tables. I understand the idea that all the subsequent join operations will be more efficient due to having all the transformations done upstream, but I'm not really convinced that this might be a good approach since it implies a sacrifice in modularity. For example, we would lose the ability to rerun a single target without aggregating all the data from different sources first.  \na) Example of aggregation upstream:  \n\n\nhttps://preview.redd.it/88vk8dy9o26c1.png?width=798&amp;format=png&amp;auto=webp&amp;s=f69201070358bd6451b433054f2ae318e0d6dbfb\n\n  \nb) Example of aggregation downstream:  \n\n\nhttps://preview.redd.it/u0d2dvwdo26c1.png?width=554&amp;format=png&amp;auto=webp&amp;s=1993397031a042752778561f7d5fde0a3e3fdc8c\n\nThe main disadvantage I see in option a) is that an issue in a single pipeline gets propagated to all other models as well, while with option b) we still preserve the ability to rerun each single pipeline independently.", "author_fullname": "t2_vo4giaww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices on heterogeneous pipeline imports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 42, "top_awarded_type": null, "hide_score": false, "media_metadata": {"u0d2dvwdo26c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/u0d2dvwdo26c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f225d065722648dd3f0cd8b9ef6fe51c41f99818"}, {"y": 96, "x": 216, "u": "https://preview.redd.it/u0d2dvwdo26c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c25ed9d364b7dfca74d0914a0eef5073e4f7fd3"}, {"y": 143, "x": 320, "u": "https://preview.redd.it/u0d2dvwdo26c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f08949c5a1a2486b1c2cdcb129937268f0a8419c"}], "s": {"y": 248, "x": 554, "u": "https://preview.redd.it/u0d2dvwdo26c1.png?width=554&amp;format=png&amp;auto=webp&amp;s=1993397031a042752778561f7d5fde0a3e3fdc8c"}, "id": "u0d2dvwdo26c1"}, "88vk8dy9o26c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c3e9dce7679c3aed3dbc2a61e54c88c6c605496"}, {"y": 65, "x": 216, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e03b10b4636d893849298d4edb52f6186a0c822"}, {"y": 96, "x": 320, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=83cddbf66bf59cb424ecf0b12b25986c10ac1006"}, {"y": 193, "x": 640, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b4f18fa0c2152be9aadea07e6dc95014b837dcb"}], "s": {"y": 241, "x": 798, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=798&amp;format=png&amp;auto=webp&amp;s=f69201070358bd6451b433054f2ae318e0d6dbfb"}, "id": "88vk8dy9o26c1"}}, "name": "t3_18hi6zy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rGWvMoGtBzM-VbFy4OTj3OrFCDswU48h5proIGCxwkQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702478250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Regarding the aggregation of heterogeneous data imported by different pipelines, what is your opinion on performing the aggregation as much upstream or further down as possible?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently seen people in conferences suggesting to transform data as much upstream as possible to generate a single core model used to populate different target tables. I understand the idea that all the subsequent join operations will be more efficient due to having all the transformations done upstream, but I&amp;#39;m not really convinced that this might be a good approach since it implies a sacrifice in modularity. For example, we would lose the ability to rerun a single target without aggregating all the data from different sources first.&lt;br/&gt;\na) Example of aggregation upstream:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/88vk8dy9o26c1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f69201070358bd6451b433054f2ae318e0d6dbfb\"&gt;https://preview.redd.it/88vk8dy9o26c1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f69201070358bd6451b433054f2ae318e0d6dbfb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;b) Example of aggregation downstream:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u0d2dvwdo26c1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1993397031a042752778561f7d5fde0a3e3fdc8c\"&gt;https://preview.redd.it/u0d2dvwdo26c1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1993397031a042752778561f7d5fde0a3e3fdc8c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The main disadvantage I see in option a) is that an issue in a single pipeline gets propagated to all other models as well, while with option b) we still preserve the ability to rerun each single pipeline independently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hi6zy", "is_robot_indexable": true, "report_reasons": null, "author": "LnYmte", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hi6zy/best_practices_on_heterogeneous_pipeline_imports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hi6zy/best_practices_on_heterogeneous_pipeline_imports/", "subreddit_subscribers": 145842, "created_utc": 1702478250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.bytebase.com/changelog/bytebase-2-12-0/](https://www.bytebase.com/changelog/bytebase-2-12-0/)", "author_fullname": "t2_gxesw7ji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bytebase v2.12.0 released, Database DevOps &amp; CI/CD tool for engineering teams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i5hdu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702548661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.bytebase.com/changelog/bytebase-2-12-0/\"&gt;https://www.bytebase.com/changelog/bytebase-2-12-0/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?auto=webp&amp;s=5485407d9406aef94dbc618fc9bd92277d79553c", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c9538a4c1085c8724b674264582b7ee71129e2b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa17aeed2d215a80b66b54b5df5e640d95141375", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0127f128a25d80af9f1433bdab92cae115c3c8c1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa7d61d00774d86ca0256598f6b54ccad18e04e1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0abdac1d7890e4a0f3902a6cb512c29d80241378", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9bbc8c5b2db4e4f1b58c0654325ef6f40993c3c7", "width": 1080, "height": 607}], "variants": {}, "id": "p7SDgKrdHb2CMLnOC8NrxjbTLNsmuMWWU9jl0_vAhaI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18i5hdu", "is_robot_indexable": true, "report_reasons": null, "author": "Adela_freedom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i5hdu/bytebase_v2120_released_database_devops_cicd_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i5hdu/bytebase_v2120_released_database_devops_cicd_tool/", "subreddit_subscribers": 145842, "created_utc": 1702548661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a book/website/tool of some sort that helps guide you to the various options for data architecture? For example, if I only have unstructured data I might want to use something like MongoDB or Cosmos DB or Dynamo DB, depending on cloud familiarity, consistency/performance requirements, etc.\n\nThis is kind of what I'm looking for but something that actually has an understanding of all the various tools and their pros and cons: [https://mixpanel.com/blog/guide-to-choosing-your-data-architecture/](https://mixpanel.com/blog/guide-to-choosing-your-data-architecture/)", "author_fullname": "t2_wrave", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data architecture questionnaire/flowchart?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hrycn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702503586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a book/website/tool of some sort that helps guide you to the various options for data architecture? For example, if I only have unstructured data I might want to use something like MongoDB or Cosmos DB or Dynamo DB, depending on cloud familiarity, consistency/performance requirements, etc.&lt;/p&gt;\n\n&lt;p&gt;This is kind of what I&amp;#39;m looking for but something that actually has an understanding of all the various tools and their pros and cons: &lt;a href=\"https://mixpanel.com/blog/guide-to-choosing-your-data-architecture/\"&gt;https://mixpanel.com/blog/guide-to-choosing-your-data-architecture/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Nx4EQlIaVJXsEXZ_KlQFNjKAgJaIhIF1oJRR03RP7pg.jpg?auto=webp&amp;s=5753e13ab32e6b113340132395c805c184616efa", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/Nx4EQlIaVJXsEXZ_KlQFNjKAgJaIhIF1oJRR03RP7pg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=330854e19ecd0cfa3392fb61cebc32a3a6fd8d21", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Nx4EQlIaVJXsEXZ_KlQFNjKAgJaIhIF1oJRR03RP7pg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8317bfddc01c1b519e74bf9e8a5416621daf0e05", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Nx4EQlIaVJXsEXZ_KlQFNjKAgJaIhIF1oJRR03RP7pg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=161092330e5ddd46f8167c05258678e9ef9c05c6", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Nx4EQlIaVJXsEXZ_KlQFNjKAgJaIhIF1oJRR03RP7pg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6b0d26b27107a0e3a831f50c640b007631bf6fb", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Nx4EQlIaVJXsEXZ_KlQFNjKAgJaIhIF1oJRR03RP7pg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eac8afc8926fa941f9a9d33c0bf0c9f5a093d5de", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/Nx4EQlIaVJXsEXZ_KlQFNjKAgJaIhIF1oJRR03RP7pg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=332ec4e1a7d5c46ed134d852d39b7b34d3510cbf", "width": 1080, "height": 607}], "variants": {}, "id": "yLdaya-4WQSvsst3ZhrcjvNUwojgc137ssXYYii9MFg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hrycn", "is_robot_indexable": true, "report_reasons": null, "author": "FlukeStarbucker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hrycn/data_architecture_questionnaireflowchart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hrycn/data_architecture_questionnaireflowchart/", "subreddit_subscribers": 145842, "created_utc": 1702503586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a DE staffing question. \n\nI work for a small business that is early stage in its data maturity. I\u2019m in charge of digital transformation and am NOT an engineer. \n\nWe are looking at using Mulesoft as an iPaaS with a new CRM (Salesforce) and existing ERP (Unanet). We have some other API\u2019s we\u2019ll also integrate from public data sources.\n\nWe\u2019ll likely using Power BI for internal reporting since we already have access to it. \n\nMulesoft is setting up the initial integration, but I don\u2019t know how much staff time I need to set aside to maintain and optimize the data stack. \n\nIs this an FTE or would someone that manages our IT systems also cross-train in light weight data engineering?\n\nThanks in advance!", "author_fullname": "t2_41soa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small Business stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hn48b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702491084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a DE staffing question. &lt;/p&gt;\n\n&lt;p&gt;I work for a small business that is early stage in its data maturity. I\u2019m in charge of digital transformation and am NOT an engineer. &lt;/p&gt;\n\n&lt;p&gt;We are looking at using Mulesoft as an iPaaS with a new CRM (Salesforce) and existing ERP (Unanet). We have some other API\u2019s we\u2019ll also integrate from public data sources.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ll likely using Power BI for internal reporting since we already have access to it. &lt;/p&gt;\n\n&lt;p&gt;Mulesoft is setting up the initial integration, but I don\u2019t know how much staff time I need to set aside to maintain and optimize the data stack. &lt;/p&gt;\n\n&lt;p&gt;Is this an FTE or would someone that manages our IT systems also cross-train in light weight data engineering?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hn48b", "is_robot_indexable": true, "report_reasons": null, "author": "miqcie", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hn48b/small_business_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hn48b/small_business_stack/", "subreddit_subscribers": 145842, "created_utc": 1702491084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just noticed that AWS Datazone is now in GA and am considering using it as the Data Governance tool for my organisation. Any initial thoughts on whether it's a good product to consider? How does it compare to other tools like Privacera and Alation?", "author_fullname": "t2_15lzfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts about AWS Datazone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hh8yd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702475532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just noticed that AWS Datazone is now in GA and am considering using it as the Data Governance tool for my organisation. Any initial thoughts on whether it&amp;#39;s a good product to consider? How does it compare to other tools like Privacera and Alation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hh8yd", "is_robot_indexable": true, "report_reasons": null, "author": "kaiusang", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hh8yd/thoughts_about_aws_datazone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hh8yd/thoughts_about_aws_datazone/", "subreddit_subscribers": 145842, "created_utc": 1702475532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is special about IoV data analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hfg1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1702469492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "doris.apache.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://doris.apache.org/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hfg1g", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hfg1g/what_is_special_about_iov_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://doris.apache.org/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents", "subreddit_subscribers": 145842, "created_utc": 1702469492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone here used Nessie's branching to trigger a data quality check? That is, whenever a merge is performed on a nessie branch, a GX/Soda/MC job is triggered on the branch for a Write-Audit-Publish workflow?\n\nIs it something to do with Nessie hooks? ([https://projectnessie.org/nessie\\_provider/](https://projectnessie.org/nessie_provider/)). I am also curious to know how people run their data quality checks on Iceberg tables.  \n", "author_fullname": "t2_abpkolp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks on Nessie branch merges", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18i6617", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702551507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here used Nessie&amp;#39;s branching to trigger a data quality check? That is, whenever a merge is performed on a nessie branch, a GX/Soda/MC job is triggered on the branch for a Write-Audit-Publish workflow?&lt;/p&gt;\n\n&lt;p&gt;Is it something to do with Nessie hooks? (&lt;a href=\"https://projectnessie.org/nessie_provider/\"&gt;https://projectnessie.org/nessie_provider/&lt;/a&gt;). I am also curious to know how people run their data quality checks on Iceberg tables.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18i6617", "is_robot_indexable": true, "report_reasons": null, "author": "bytesapart", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i6617/data_quality_checks_on_nessie_branch_merges/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i6617/data_quality_checks_on_nessie_branch_merges/", "subreddit_subscribers": 145842, "created_utc": 1702551507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nI am currently working for a rather small consultancy company (5 FTEs + some working students). \n\nI am seeking some guidance/ feedback on some questions that are going through my mind and our current setup as a company. First, here's some information that should give you an overview about the situation.\n\n**Company background**\n\n* We're a consultancy that has developed a platform for our clients\n* Within this platform various tableau dashboard can be viewed, which we're embedding there.\n* Anything data engineering related we're doing involves some sort of ETL process and displaying the extracted data in a dashboard to our clients.\n\n**Team-Setup**\n\n* There are mostly non-IT people working here\n* There is one DevOps Guy that is mainly in charge for setting up infrastructure or, together with myself, fixing things if things go nuts. He's not working full time and is rather working on demand.\n* I am the only data engineer working here full time. When I started working here I mainly focused on building dashboards but as things are growing, I am focusing more on data engineering topics.\n* There is one other working student that has a data-background. He's mostly building dashboards or implementing some smaller ETL pipelines.\n* My Boss is thankfully supporting the topics I am working on, in case that matters for your feedback.\n\n**Tech-Stack**\n\n* On-Prem Postgres Cluster running on K8s as transactional database for our platform.\n* On-Prem Tableau Server for dashboards\n* Tableau Prep (UI-based ETL Tool)/Python for ETL processes\n* Prefect for Orchestration\n* Gitlab CI/CD\n\n**Current Challenges:**\n\n* Database / Data Warehouse: Currently everything is stored in the Postgres Database. So all the transactional data goes into the postgres (which is fine) but also the dashboard related data, which I know should not be stored there. As the number of projects we're doing, so is the database and everything starts to get more and more messy. I feel like I can still oversee everything but on the other hand this also feels like a ticking time bomb. Ideally I am thinking about setting up a data warehouse which we'll then use for all analytic related topics such as our dashboards. At the same time I feel like this is such a huge task that I am also underestimating. \n* We're not following any \"fixed\" guideline when it comes to how we're modeling/storing our dashboard related data in the postgres. I am currently making myself familiar with dimensional modeling (kimball) as I feel like we need a way to approach this, sometimes messy, situation on how we're storing our data.\n* Another thing on my list is DBT. We're currently using materialized views for some of our dashboards, mainly to shift aggregation away from tableau into the database. While this works fine, I feel like using DBT could improve our data quality and eg improve re usability among other things.\n\n**My questions to you:** \n\n* Has someone worked in a similar situation? How did you decide on \"what to do next\"? Quitting is not an option as I my pay is quite good for the YOE I have currently.\n* What topic would you start with? Is there any specific \"order\" that makes most sense and why?\n* Any other thoughts/feedback are of course appreciated.\n\n  \nTLDR; We're a small company and I am not sure what topic to tackle first to improve our data architecture.   \n\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_u88hoerb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance/ Feedback on data architecture and next steps planned", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i5ovj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702549525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I am currently working for a rather small consultancy company (5 FTEs + some working students). &lt;/p&gt;\n\n&lt;p&gt;I am seeking some guidance/ feedback on some questions that are going through my mind and our current setup as a company. First, here&amp;#39;s some information that should give you an overview about the situation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We&amp;#39;re a consultancy that has developed a platform for our clients&lt;/li&gt;\n&lt;li&gt;Within this platform various tableau dashboard can be viewed, which we&amp;#39;re embedding there.&lt;/li&gt;\n&lt;li&gt;Anything data engineering related we&amp;#39;re doing involves some sort of ETL process and displaying the extracted data in a dashboard to our clients.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Team-Setup&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;There are mostly non-IT people working here&lt;/li&gt;\n&lt;li&gt;There is one DevOps Guy that is mainly in charge for setting up infrastructure or, together with myself, fixing things if things go nuts. He&amp;#39;s not working full time and is rather working on demand.&lt;/li&gt;\n&lt;li&gt;I am the only data engineer working here full time. When I started working here I mainly focused on building dashboards but as things are growing, I am focusing more on data engineering topics.&lt;/li&gt;\n&lt;li&gt;There is one other working student that has a data-background. He&amp;#39;s mostly building dashboards or implementing some smaller ETL pipelines.&lt;/li&gt;\n&lt;li&gt;My Boss is thankfully supporting the topics I am working on, in case that matters for your feedback.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Tech-Stack&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On-Prem Postgres Cluster running on K8s as transactional database for our platform.&lt;/li&gt;\n&lt;li&gt;On-Prem Tableau Server for dashboards&lt;/li&gt;\n&lt;li&gt;Tableau Prep (UI-based ETL Tool)/Python for ETL processes&lt;/li&gt;\n&lt;li&gt;Prefect for Orchestration&lt;/li&gt;\n&lt;li&gt;Gitlab CI/CD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Challenges:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Database / Data Warehouse: Currently everything is stored in the Postgres Database. So all the transactional data goes into the postgres (which is fine) but also the dashboard related data, which I know should not be stored there. As the number of projects we&amp;#39;re doing, so is the database and everything starts to get more and more messy. I feel like I can still oversee everything but on the other hand this also feels like a ticking time bomb. Ideally I am thinking about setting up a data warehouse which we&amp;#39;ll then use for all analytic related topics such as our dashboards. At the same time I feel like this is such a huge task that I am also underestimating. &lt;/li&gt;\n&lt;li&gt;We&amp;#39;re not following any &amp;quot;fixed&amp;quot; guideline when it comes to how we&amp;#39;re modeling/storing our dashboard related data in the postgres. I am currently making myself familiar with dimensional modeling (kimball) as I feel like we need a way to approach this, sometimes messy, situation on how we&amp;#39;re storing our data.&lt;/li&gt;\n&lt;li&gt;Another thing on my list is DBT. We&amp;#39;re currently using materialized views for some of our dashboards, mainly to shift aggregation away from tableau into the database. While this works fine, I feel like using DBT could improve our data quality and eg improve re usability among other things.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;My questions to you:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Has someone worked in a similar situation? How did you decide on &amp;quot;what to do next&amp;quot;? Quitting is not an option as I my pay is quite good for the YOE I have currently.&lt;/li&gt;\n&lt;li&gt;What topic would you start with? Is there any specific &amp;quot;order&amp;quot; that makes most sense and why?&lt;/li&gt;\n&lt;li&gt;Any other thoughts/feedback are of course appreciated.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;TLDR; We&amp;#39;re a small company and I am not sure what topic to tackle first to improve our data architecture.   &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18i5ovj", "is_robot_indexable": true, "report_reasons": null, "author": "heggbert", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i5ovj/guidance_feedback_on_data_architecture_and_next/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i5ovj/guidance_feedback_on_data_architecture_and_next/", "subreddit_subscribers": 145842, "created_utc": 1702549525.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}