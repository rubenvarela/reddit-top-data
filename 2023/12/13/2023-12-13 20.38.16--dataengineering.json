{"kind": "Listing", "data": {"after": "t3_18hmz09", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Other than \"creating data pipelines over, and over again\" or \"practicing\", what are some things that have radically improved your data engineering skills?\n\ne.g.\n\n* Making each discrete phase of a given pipeline dumber (i.e. simpler, easier to deliver, easier to replace)\n* Starting with a dumb Python script almost every time, and making it more robust over time\n* Just getting started, and making things pretty later\n* Using a particular data pipeline orchestrator because &lt;x&gt;\n* Analyzing data a particular way before getting started\n* Creating documentation &lt;y&gt; before getting started\n* Sketching data composition and lineage out with pen and paper\n* Sketching out your data pipeline with pen and paper\n* Rendering your data pipeline using GraphViz and DOT markup, etc. so you can better visualize the structure of your data pipelines\n\nEDIT: if this is too vague, or a bad fit, I'll delete it, just let me know - AFAIK how you build things and the process by which you decide to build things is just as important as what tools you use to build those things - i.e., software architecture matters when developing software - but, I don't know - just, let me know.", "author_fullname": "t2_lwr1wvz8q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some things that have radically improved your data engineering skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hgf31", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702484360.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702472862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Other than &amp;quot;creating data pipelines over, and over again&amp;quot; or &amp;quot;practicing&amp;quot;, what are some things that have radically improved your data engineering skills?&lt;/p&gt;\n\n&lt;p&gt;e.g.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Making each discrete phase of a given pipeline dumber (i.e. simpler, easier to deliver, easier to replace)&lt;/li&gt;\n&lt;li&gt;Starting with a dumb Python script almost every time, and making it more robust over time&lt;/li&gt;\n&lt;li&gt;Just getting started, and making things pretty later&lt;/li&gt;\n&lt;li&gt;Using a particular data pipeline orchestrator because &amp;lt;x&amp;gt;&lt;/li&gt;\n&lt;li&gt;Analyzing data a particular way before getting started&lt;/li&gt;\n&lt;li&gt;Creating documentation &amp;lt;y&amp;gt; before getting started&lt;/li&gt;\n&lt;li&gt;Sketching data composition and lineage out with pen and paper&lt;/li&gt;\n&lt;li&gt;Sketching out your data pipeline with pen and paper&lt;/li&gt;\n&lt;li&gt;Rendering your data pipeline using GraphViz and DOT markup, etc. so you can better visualize the structure of your data pipelines&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;EDIT: if this is too vague, or a bad fit, I&amp;#39;ll delete it, just let me know - AFAIK how you build things and the process by which you decide to build things is just as important as what tools you use to build those things - i.e., software architecture matters when developing software - but, I don&amp;#39;t know - just, let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hgf31", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Importance-1605", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18hgf31/what_are_some_things_that_have_radically_improved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hgf31/what_are_some_things_that_have_radically_improved/", "subreddit_subscribers": 145694, "created_utc": 1702472862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been with my current company for about six months, and in that time, I've not only developed data pipelines but also found myself building the team-specific dashboards they feed into. Back at my old job, things were a bit different. I worked alongside an analyst who took care of the dashboards. They would give me the business needs, and I'd tailor the data pipelines to suit those requirements and the analysis being done. Personally, I don't think dashboard building is really part of a data engineer's job, but I'm curious to hear what others in this group think. Have you had similar experiences?", "author_fullname": "t2_103ndz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much dashboarding / viz do you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h7vek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702439534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been with my current company for about six months, and in that time, I&amp;#39;ve not only developed data pipelines but also found myself building the team-specific dashboards they feed into. Back at my old job, things were a bit different. I worked alongside an analyst who took care of the dashboards. They would give me the business needs, and I&amp;#39;d tailor the data pipelines to suit those requirements and the analysis being done. Personally, I don&amp;#39;t think dashboard building is really part of a data engineer&amp;#39;s job, but I&amp;#39;m curious to hear what others in this group think. Have you had similar experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18h7vek", "is_robot_indexable": true, "report_reasons": null, "author": "natelifts", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h7vek/how_much_dashboarding_viz_do_you_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h7vek/how_much_dashboarding_viz_do_you_do/", "subreddit_subscribers": 145694, "created_utc": 1702439534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my previous job, I was a data analyst, but I've come across a lot of people who tell me that I should go into data engineering. Problem is, every time I ask someone who is in that field, they honestly cannot tell me what it is that they do. Like, I have never met a single data engineer in any company I've worked for who has given me a simple and reasonable explanation for what they do.\n\n\n\nAt my previous job, I designed ETL queries using SQL and Python, wrote APIs for interfacing with different database softwares for example I created an API in Python to automatically connect to Google BigQuery, retrieve data for the last 30 days, and then move it into other data sources \n\n\nI also performed audits on data, so where there were gaps in data or areas where they said that data was incorrect, I would go hunting and find gaps in the data that didn't make sense, for example, why is there missing data between these two linked tables? Is there a specific date that there's missing data?\n\n\nFinally, I created new links between data across different sources, for example from snowflake to BigQuery, even a little bit of access", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone explain the job of data engineer like I'm a baboon?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hlsqb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702488512.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702487800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my previous job, I was a data analyst, but I&amp;#39;ve come across a lot of people who tell me that I should go into data engineering. Problem is, every time I ask someone who is in that field, they honestly cannot tell me what it is that they do. Like, I have never met a single data engineer in any company I&amp;#39;ve worked for who has given me a simple and reasonable explanation for what they do.&lt;/p&gt;\n\n&lt;p&gt;At my previous job, I designed ETL queries using SQL and Python, wrote APIs for interfacing with different database softwares for example I created an API in Python to automatically connect to Google BigQuery, retrieve data for the last 30 days, and then move it into other data sources &lt;/p&gt;\n\n&lt;p&gt;I also performed audits on data, so where there were gaps in data or areas where they said that data was incorrect, I would go hunting and find gaps in the data that didn&amp;#39;t make sense, for example, why is there missing data between these two linked tables? Is there a specific date that there&amp;#39;s missing data?&lt;/p&gt;\n\n&lt;p&gt;Finally, I created new links between data across different sources, for example from snowflake to BigQuery, even a little bit of access&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hlsqb", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hlsqb/can_someone_explain_the_job_of_data_engineer_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hlsqb/can_someone_explain_the_job_of_data_engineer_like/", "subreddit_subscribers": 145694, "created_utc": 1702487800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, Dear Data Engineers!\n\nI wrote a [blog post](https://www.mitzu.io/post/identifying-users-in-the-data-warehouse?source=der) about solving the **User ID Stitching** problem with Recursive SQL. You can use this in DBT with data warehouses supporting recursive CTEs.\n\n&gt;The **User ID Stitching** problem concerns primarily **B2C companies** that want to measure marketing, product, and sales metrics correctly. It is about creating a **user\\_aliases** table that maps all possible user identifiers for every user across all datasets  to a single **merged\\_id**\n\n**Possible user identifiers:**\n\n* device\\_id\n* user\\_id\n* anonymous\\_user\\_id\n* email\n* contact\\_id (Hubspot)\n* customer\\_id (Stripe)\n* \u2026 etc\u2026\n\nYou can then use the **merge\\_id** to join across all datasets, count unique users correctly, create funnel queries across all datasets, etc.I hope it helps whoever it may help :)\n\nLink to the [blog post](https://www.mitzu.io/post/identifying-users-in-the-data-warehouse?source=der)", "author_fullname": "t2_gnytqihqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "User ID Stitching with Recursive SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hc4jz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702482416.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702455492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, Dear Data Engineers!&lt;/p&gt;\n\n&lt;p&gt;I wrote a &lt;a href=\"https://www.mitzu.io/post/identifying-users-in-the-data-warehouse?source=der\"&gt;blog post&lt;/a&gt; about solving the &lt;strong&gt;User ID Stitching&lt;/strong&gt; problem with Recursive SQL. You can use this in DBT with data warehouses supporting recursive CTEs.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The &lt;strong&gt;User ID Stitching&lt;/strong&gt; problem concerns primarily &lt;strong&gt;B2C companies&lt;/strong&gt; that want to measure marketing, product, and sales metrics correctly. It is about creating a &lt;strong&gt;user_aliases&lt;/strong&gt; table that maps all possible user identifiers for every user across all datasets  to a single &lt;strong&gt;merged_id&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Possible user identifiers:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;device_id&lt;/li&gt;\n&lt;li&gt;user_id&lt;/li&gt;\n&lt;li&gt;anonymous_user_id&lt;/li&gt;\n&lt;li&gt;email&lt;/li&gt;\n&lt;li&gt;contact_id (Hubspot)&lt;/li&gt;\n&lt;li&gt;customer_id (Stripe)&lt;/li&gt;\n&lt;li&gt;\u2026 etc\u2026&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can then use the &lt;strong&gt;merge_id&lt;/strong&gt; to join across all datasets, count unique users correctly, create funnel queries across all datasets, etc.I hope it helps whoever it may help :)&lt;/p&gt;\n\n&lt;p&gt;Link to the &lt;a href=\"https://www.mitzu.io/post/identifying-users-in-the-data-warehouse?source=der\"&gt;blog post&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?auto=webp&amp;s=0d093ce954e3041f48a37532a5dcaf93dc69e403", "width": 1776, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc79b8bbb10e9f68c1d1d6c86af1f3841dc4f10e", "width": 108, "height": 48}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c07e77a7075812c321c03396caabe8b0df20257", "width": 216, "height": 97}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3ef5435010e00eaf07797ddfcec9de3febd41ce", "width": 320, "height": 144}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b745bec61cacf775819bc9c0dee5dc03fbf29420", "width": 640, "height": 288}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49e6f8c7db99048714c1629660d5ca625c2c9ba6", "width": 960, "height": 432}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32c1b03ffdc645de4619500f6d9673e4f4fcb49a", "width": 1080, "height": 486}], "variants": {}, "id": "PNhHEjsZUew5p0U0EcK53-ZbVngG-Nb-Q0Qqg7c5UcI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hc4jz", "is_robot_indexable": true, "report_reasons": null, "author": "MitzuIstvan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hc4jz/user_id_stitching_with_recursive_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hc4jz/user_id_stitching_with_recursive_sql/", "subreddit_subscribers": 145694, "created_utc": 1702455492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Fantastic post from Georg and Aleksandar", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster, dbt, duckdb as new local MDS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18hipz6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": "#46d160", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/C8BThcaoCneGtqR55zryhvIDi2M_OoTkLPOgIn9tXAM.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702479709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "georgheiler.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fantastic post from Georg and Aleksandar&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://georgheiler.com/2023/12/11/dagster-dbt-duckdb-as-new-local-mds/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?auto=webp&amp;s=861e4f94dc91e2f5ba063c56da2f709e01437573", "width": 2048, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1cdb58b5a195232e633f8b91f78f9f24e22bcc1e", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3605171ded8bd689c2b60aa4bf84ed0536c09c7f", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4cbba5cea1ebe44625c4f4069a6c78dcfa85061", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df803cd4ca1683749c35eada75315eb588ef67e1", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=99501c4a7318bc43ffec2b0e60c50c810127a1a0", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/HfWaiitkxeMzkIOXSRWTtTrhkiuK3kT-le8bS_Cjk-U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e815925fa8a0e77f571fed51d348e38b1b9ddae", "width": 1080, "height": 1080}], "variants": {}, "id": "fi_LHP4YaKAqYxd02lcILIegMmkY3lQeK3OCDUu36vQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Lead Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hipz6", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/18hipz6/dagster_dbt_duckdb_as_new_local_mds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://georgheiler.com/2023/12/11/dagster-dbt-duckdb-as-new-local-mds/", "subreddit_subscribers": 145694, "created_utc": 1702479709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got into a discussion with a visualization engineer who insists that never in her career had they created materialized views or regular views specific to a dashboard element. \nI\u2019m not in visualization, so honestly don\u2019t even have context to decide if this person\u2019s perspective is reasonable. From my perspective, as a dashboard is designed for a larger audience, if dealing with data from multiple tables, of course it makes sense to do this work upstream. Am I crazy here?", "author_fullname": "t2_5s7getlh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pattern: Table or View per Dashboard Element", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gv87y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702412142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got into a discussion with a visualization engineer who insists that never in her career had they created materialized views or regular views specific to a dashboard element. \nI\u2019m not in visualization, so honestly don\u2019t even have context to decide if this person\u2019s perspective is reasonable. From my perspective, as a dashboard is designed for a larger audience, if dealing with data from multiple tables, of course it makes sense to do this work upstream. Am I crazy here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18gv87y", "is_robot_indexable": true, "report_reasons": null, "author": "pro__acct__", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gv87y/pattern_table_or_view_per_dashboard_element/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gv87y/pattern_table_or_view_per_dashboard_element/", "subreddit_subscribers": 145694, "created_utc": 1702412142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Doing the fundamentals of dbt course right now and came across the following:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/snw7lecr326c1.png?width=1838&amp;format=png&amp;auto=webp&amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400\n\n&amp;#x200B;\n\nThey specify a test in some documentation file (yaml).\n\nI wonder why is this done on this level and not on the level of sql constraints?\n\nI skimmed a thread a found a comment where [someone argued](https://www.reddit.com/r/dataengineering/comments/119s7yv/comment/j9ozeus/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) that dbt is good for when one can live (has) dirty data and hence has not enforced the level of data completeness as done with sql constraints.\n\n&amp;#x200B;\n\nWhat is the philosophy there?\n\nWhy would I even consider living with dirty data? Is my goal as an engineer not to keep data records clean and sort the completeness out on the level where transactions (i.e. updates) are handled?", "author_fullname": "t2_1b2msvdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "why testing in dbt? why live with dirty data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 47, "top_awarded_type": null, "hide_score": false, "media_metadata": {"snw7lecr326c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/snw7lecr326c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dcdfdcc4acf994e7dea6193fb88688aeaa8a0129"}, {"y": 74, "x": 216, "u": "https://preview.redd.it/snw7lecr326c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac27c1ed016e48de40ac4fb89df5d0f51400e6a5"}, {"y": 109, "x": 320, "u": "https://preview.redd.it/snw7lecr326c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=463eeaecda7aea123587ca48ce6aa9b67568bd79"}, {"y": 219, "x": 640, "u": "https://preview.redd.it/snw7lecr326c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ebcf00dce7fc5f281758d0ba1f223237238a4fb"}, {"y": 329, "x": 960, "u": "https://preview.redd.it/snw7lecr326c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ad4f1c57916a9c19c40e918897dd1f82e39449d"}, {"y": 370, "x": 1080, "u": "https://preview.redd.it/snw7lecr326c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=769012686b0bfed9e39949fe00466e3971b66dad"}], "s": {"y": 630, "x": 1838, "u": "https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;format=png&amp;auto=webp&amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400"}, "id": "snw7lecr326c1"}}, "name": "t3_18hfvkl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TEifI7eBZWLbs2l77WkdatkskdQfmD-Qq6dGuIAzlio.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702471054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doing the fundamentals of dbt course right now and came across the following:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400\"&gt;https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;They specify a test in some documentation file (yaml).&lt;/p&gt;\n\n&lt;p&gt;I wonder why is this done on this level and not on the level of sql constraints?&lt;/p&gt;\n\n&lt;p&gt;I skimmed a thread a found a comment where &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/119s7yv/comment/j9ozeus/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;someone argued&lt;/a&gt; that dbt is good for when one can live (has) dirty data and hence has not enforced the level of data completeness as done with sql constraints.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is the philosophy there?&lt;/p&gt;\n\n&lt;p&gt;Why would I even consider living with dirty data? Is my goal as an engineer not to keep data records clean and sort the completeness out on the level where transactions (i.e. updates) are handled?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hfvkl", "is_robot_indexable": true, "report_reasons": null, "author": "HillTheBilly", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hfvkl/why_testing_in_dbt_why_live_with_dirty_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hfvkl/why_testing_in_dbt_why_live_with_dirty_data/", "subreddit_subscribers": 145694, "created_utc": 1702471054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My organization has an existing data engineering software stack but I have a large new project in the pipeline that will be supported by a dedicated team outside of our existing DE group.  I, as as solutions architect, am free to choose a new software stack for this project, although I do see value in using our existing toolsets and not creating duplicate solutions.  If funding wasn't a big issue, what solutions would you deploy today?  My only constraint is that it will be deployed in Azure.  Here's our current landscape:\n\nScheduling &amp; pipelines: Tidal\n\nETL for integrations: Talend\n\nData lake/warehouse: Snowflake\n\nAI/ML for DE: Databricks\n\nSQL-based transformations: dbt\n\nThanks!", "author_fullname": "t2_9lsmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What software stack would you choose if you were starting from scratch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hjcy6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702481458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My organization has an existing data engineering software stack but I have a large new project in the pipeline that will be supported by a dedicated team outside of our existing DE group.  I, as as solutions architect, am free to choose a new software stack for this project, although I do see value in using our existing toolsets and not creating duplicate solutions.  If funding wasn&amp;#39;t a big issue, what solutions would you deploy today?  My only constraint is that it will be deployed in Azure.  Here&amp;#39;s our current landscape:&lt;/p&gt;\n\n&lt;p&gt;Scheduling &amp;amp; pipelines: Tidal&lt;/p&gt;\n\n&lt;p&gt;ETL for integrations: Talend&lt;/p&gt;\n\n&lt;p&gt;Data lake/warehouse: Snowflake&lt;/p&gt;\n\n&lt;p&gt;AI/ML for DE: Databricks&lt;/p&gt;\n\n&lt;p&gt;SQL-based transformations: dbt&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hjcy6", "is_robot_indexable": true, "report_reasons": null, "author": "dantasticdotorg", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hjcy6/what_software_stack_would_you_choose_if_you_were/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hjcy6/what_software_stack_would_you_choose_if_you_were/", "subreddit_subscribers": 145694, "created_utc": 1702481458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_artaa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What it's like watching performance tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_18h8vaw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p-ScxshpaIH0LpOXfbiGYg_scjII6-q45KRcSku5VHE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702442790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/abq9civ6sz5c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/abq9civ6sz5c1.png?auto=webp&amp;s=8b185af5c90f3b84ccb0159c6b0d7a19acfcb67d", "width": 1792, "height": 1024}, "resolutions": [{"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=19733133c2e33da130bb2cc44e3903397044da4c", "width": 108, "height": 61}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d298fc84454e747f4142361fe3c2395ed7f728d3", "width": 216, "height": 123}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf8a5a0fe428bd9b9d43ec2081f14ca243adc9ac", "width": 320, "height": 182}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=674624077c2d64c5762ac4a3c5325f44a0c530ab", "width": 640, "height": 365}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d272489210b302a0b7f32c8b5623768d4cd14da5", "width": 960, "height": 548}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b087a167c1eed7ae6ccd1098f3f95280496ee499", "width": 1080, "height": 617}], "variants": {}, "id": "A-qG__OJXA_ovT6p9YyJ08z42GO-MzxjPebbjk5kS6U"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18h8vaw", "is_robot_indexable": true, "report_reasons": null, "author": "Toasty_toaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h8vaw/what_its_like_watching_performance_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/abq9civ6sz5c1.png", "subreddit_subscribers": 145694, "created_utc": 1702442790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The article shows you all the important differences between DuckDB/MotherDuck and other databases, including results from the first iteration of performance tests.  \n\n\n[https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5](https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5)", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is MotherDuck ProDUCKtion-Ready?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hdjry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702461903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The article shows you all the important differences between DuckDB/MotherDuck and other databases, including results from the first iteration of performance tests.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5\"&gt;https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?auto=webp&amp;s=464e982e8535936416c81b255d265e379a74453f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f22bd0b7a1eba0df9818d95987df2e90ff804a47", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a8808f867b2bc9b7afc8ee262e583033c0002a3f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=770ea05dd5058890d39eea27d143849e95b9e0aa", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b59f97e63fc83024129cf7a2e06b0c10401108aa", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9212e28c7f173a8df84f72500c4e1c5753b40e00", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d1558c54756703b248d71fde44619e7999fd4bd", "width": 1080, "height": 565}], "variants": {}, "id": "2XijoLGCZ5b01Blb6hT-fjIRZbscSJzrmSkZT1Q49_g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hdjry", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hdjry/is_motherduck_producktionready/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hdjry/is_motherduck_producktionready/", "subreddit_subscribers": 145694, "created_utc": 1702461903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After a cross-disciplinary Big Data (Engineering, Science) Master's degree, i have done an internship as a assistant DE then the company hired me as a junior DE. But i didn't really stuck to DE, of course i made a lot of pipelines from ingestion to exports...etc and all usual Data Engineer tasks. But i also did a lot (like really a lot) of Data Viz and some analysis (either descriptive or predictive). With great results for our company, and that's something i like doing.   \nNow the company is growing, we recently hired a huge data analysts team, that took over all the data analysis and data viz perimeter, and i kinda feel limited in my scope as i do really well in this hybrid role and i wanted to learn some ML (i have all the major certifications in the technologies we're already using like GCP professional DE)   \nAs i'm still \"new\" and young, i want to ask to the more experienced ones here what is a realistic career path for a \"DE\" that likes the business side of Data. Is it some like of utopia if i'm thinking to have some hybrid role that goes through all the Data lifecyle ? Especially in big companies", "author_fullname": "t2_7twd1xfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "After 2 years as a DE, i don't know where my career path is going", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h1u2e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702425940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After a cross-disciplinary Big Data (Engineering, Science) Master&amp;#39;s degree, i have done an internship as a assistant DE then the company hired me as a junior DE. But i didn&amp;#39;t really stuck to DE, of course i made a lot of pipelines from ingestion to exports...etc and all usual Data Engineer tasks. But i also did a lot (like really a lot) of Data Viz and some analysis (either descriptive or predictive). With great results for our company, and that&amp;#39;s something i like doing.&lt;br/&gt;\nNow the company is growing, we recently hired a huge data analysts team, that took over all the data analysis and data viz perimeter, and i kinda feel limited in my scope as i do really well in this hybrid role and i wanted to learn some ML (i have all the major certifications in the technologies we&amp;#39;re already using like GCP professional DE)&lt;br/&gt;\nAs i&amp;#39;m still &amp;quot;new&amp;quot; and young, i want to ask to the more experienced ones here what is a realistic career path for a &amp;quot;DE&amp;quot; that likes the business side of Data. Is it some like of utopia if i&amp;#39;m thinking to have some hybrid role that goes through all the Data lifecyle ? Especially in big companies&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18h1u2e", "is_robot_indexable": true, "report_reasons": null, "author": "BennyLauren", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h1u2e/after_2_years_as_a_de_i_dont_know_where_my_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h1u2e/after_2_years_as_a_de_i_dont_know_where_my_career/", "subreddit_subscribers": 145694, "created_utc": 1702425940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, hope you are okay!\n\nSo, I work for a big company where we develop demands coming from other sectors.\n\nEach project has it's own particularities but in a nutshell it follows the \"Data ingestion, data trasnformation and data serving\" path\n\nIn many projects I face a situation where the project gets blocked because something is missing and this blocking could be avoided with a little bit more planning before we started it.\n\nI would like to know if you guys have a kind of \"check list\" of what you need to have before start a project?\n\n&amp;#x200B;", "author_fullname": "t2_84jw5rk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering project kickof checklist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hglcf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702473448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, hope you are okay!&lt;/p&gt;\n\n&lt;p&gt;So, I work for a big company where we develop demands coming from other sectors.&lt;/p&gt;\n\n&lt;p&gt;Each project has it&amp;#39;s own particularities but in a nutshell it follows the &amp;quot;Data ingestion, data trasnformation and data serving&amp;quot; path&lt;/p&gt;\n\n&lt;p&gt;In many projects I face a situation where the project gets blocked because something is missing and this blocking could be avoided with a little bit more planning before we started it.&lt;/p&gt;\n\n&lt;p&gt;I would like to know if you guys have a kind of &amp;quot;check list&amp;quot; of what you need to have before start a project?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hglcf", "is_robot_indexable": true, "report_reasons": null, "author": "El_Balde_K", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hglcf/data_engineering_project_kickof_checklist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hglcf/data_engineering_project_kickof_checklist/", "subreddit_subscribers": 145694, "created_utc": 1702473448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn the the discussions around bronze layers / staging areas it is always said that the raw data should be stored (and never changed). However one aspect that is barely discussed in my opinion is how to ensure data protection e.g. GDPR in that data. \n\nIn my case we are not allowed to have any non-pseudonymized personal data stored. Therefore we already do transformations to do that before storing in \"bronze\". \nAlso data deletion on specific rules needs to take place in bronze. Since we work with json data where important identifiers are nested in the data this is quite complex.\n\nWhat are your experiences and opinions on this?", "author_fullname": "t2_rydqu8m3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GDPR in Bronze / Staging", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18haxma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702450447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In the the discussions around bronze layers / staging areas it is always said that the raw data should be stored (and never changed). However one aspect that is barely discussed in my opinion is how to ensure data protection e.g. GDPR in that data. &lt;/p&gt;\n\n&lt;p&gt;In my case we are not allowed to have any non-pseudonymized personal data stored. Therefore we already do transformations to do that before storing in &amp;quot;bronze&amp;quot;. \nAlso data deletion on specific rules needs to take place in bronze. Since we work with json data where important identifiers are nested in the data this is quite complex.&lt;/p&gt;\n\n&lt;p&gt;What are your experiences and opinions on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18haxma", "is_robot_indexable": true, "report_reasons": null, "author": "DecisionAgile7326", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18haxma/gdpr_in_bronze_staging/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18haxma/gdpr_in_bronze_staging/", "subreddit_subscribers": 145694, "created_utc": 1702450447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title basically. using azure devOps on prem for build.", "author_fullname": "t2_slq927f8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how are ms sql server DBs deployed on-prem using ci/cd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h30tl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702428163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title basically. using azure devOps on prem for build.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18h30tl", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Release930", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h30tl/how_are_ms_sql_server_dbs_deployed_onprem_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h30tl/how_are_ms_sql_server_dbs_deployed_onprem_using/", "subreddit_subscribers": 145694, "created_utc": 1702428163.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Regarding the aggregation of heterogeneous data imported by different pipelines, what is your opinion on performing the aggregation as much upstream or further down as possible?\n\nI've recently seen people in conferences suggesting to transform data as much upstream as possible to generate a single core model used to populate different target tables. I understand the idea that all the subsequent join operations will be more efficient due to having all the transformations done upstream, but I'm not really convinced that this might be a good approach since it implies a sacrifice in modularity. For example, we would lose the ability to rerun a single target without aggregating all the data from different sources first.  \na) Example of aggregation upstream:  \n\n\nhttps://preview.redd.it/88vk8dy9o26c1.png?width=798&amp;format=png&amp;auto=webp&amp;s=f69201070358bd6451b433054f2ae318e0d6dbfb\n\n  \nb) Example of aggregation downstream:  \n\n\nhttps://preview.redd.it/u0d2dvwdo26c1.png?width=554&amp;format=png&amp;auto=webp&amp;s=1993397031a042752778561f7d5fde0a3e3fdc8c\n\nThe main disadvantage I see in option a) is that an issue in a single pipeline gets propagated to all other models as well, while with option b) we still preserve the ability to rerun each single pipeline independently.", "author_fullname": "t2_vo4giaww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices on heterogeneous pipeline imports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 42, "top_awarded_type": null, "hide_score": false, "media_metadata": {"u0d2dvwdo26c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/u0d2dvwdo26c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f225d065722648dd3f0cd8b9ef6fe51c41f99818"}, {"y": 96, "x": 216, "u": "https://preview.redd.it/u0d2dvwdo26c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c25ed9d364b7dfca74d0914a0eef5073e4f7fd3"}, {"y": 143, "x": 320, "u": "https://preview.redd.it/u0d2dvwdo26c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f08949c5a1a2486b1c2cdcb129937268f0a8419c"}], "s": {"y": 248, "x": 554, "u": "https://preview.redd.it/u0d2dvwdo26c1.png?width=554&amp;format=png&amp;auto=webp&amp;s=1993397031a042752778561f7d5fde0a3e3fdc8c"}, "id": "u0d2dvwdo26c1"}, "88vk8dy9o26c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c3e9dce7679c3aed3dbc2a61e54c88c6c605496"}, {"y": 65, "x": 216, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e03b10b4636d893849298d4edb52f6186a0c822"}, {"y": 96, "x": 320, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=83cddbf66bf59cb424ecf0b12b25986c10ac1006"}, {"y": 193, "x": 640, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b4f18fa0c2152be9aadea07e6dc95014b837dcb"}], "s": {"y": 241, "x": 798, "u": "https://preview.redd.it/88vk8dy9o26c1.png?width=798&amp;format=png&amp;auto=webp&amp;s=f69201070358bd6451b433054f2ae318e0d6dbfb"}, "id": "88vk8dy9o26c1"}}, "name": "t3_18hi6zy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rGWvMoGtBzM-VbFy4OTj3OrFCDswU48h5proIGCxwkQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702478250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Regarding the aggregation of heterogeneous data imported by different pipelines, what is your opinion on performing the aggregation as much upstream or further down as possible?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently seen people in conferences suggesting to transform data as much upstream as possible to generate a single core model used to populate different target tables. I understand the idea that all the subsequent join operations will be more efficient due to having all the transformations done upstream, but I&amp;#39;m not really convinced that this might be a good approach since it implies a sacrifice in modularity. For example, we would lose the ability to rerun a single target without aggregating all the data from different sources first.&lt;br/&gt;\na) Example of aggregation upstream:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/88vk8dy9o26c1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f69201070358bd6451b433054f2ae318e0d6dbfb\"&gt;https://preview.redd.it/88vk8dy9o26c1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f69201070358bd6451b433054f2ae318e0d6dbfb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;b) Example of aggregation downstream:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u0d2dvwdo26c1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1993397031a042752778561f7d5fde0a3e3fdc8c\"&gt;https://preview.redd.it/u0d2dvwdo26c1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1993397031a042752778561f7d5fde0a3e3fdc8c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The main disadvantage I see in option a) is that an issue in a single pipeline gets propagated to all other models as well, while with option b) we still preserve the ability to rerun each single pipeline independently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hi6zy", "is_robot_indexable": true, "report_reasons": null, "author": "LnYmte", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hi6zy/best_practices_on_heterogeneous_pipeline_imports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hi6zy/best_practices_on_heterogeneous_pipeline_imports/", "subreddit_subscribers": 145694, "created_utc": 1702478250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a Junior data engineer with 1.5 yrs experience . I do have one doubt like which is better for us being a Data engineer ? Like, \nIs to provide Data to other team such as analysts so they can build an insight from the data and build a Dashboard which is actually possible for us too .\nOr \nTo build an end to end product (such as a dashboard) where we bring the transformed data and build the dashboard and give to the end users such as analyst.", "author_fullname": "t2_suskxfga", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do we do ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hhk3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702476434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a Junior data engineer with 1.5 yrs experience . I do have one doubt like which is better for us being a Data engineer ? Like, \nIs to provide Data to other team such as analysts so they can build an insight from the data and build a Dashboard which is actually possible for us too .\nOr \nTo build an end to end product (such as a dashboard) where we bring the transformed data and build the dashboard and give to the end users such as analyst.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hhk3e", "is_robot_indexable": true, "report_reasons": null, "author": "PressureCandid1989", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hhk3e/what_do_we_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hhk3e/what_do_we_do/", "subreddit_subscribers": 145694, "created_utc": 1702476434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n**Context:**\n\nI have a bit of a situation in my company due to the lack of skills of our data engineering team and BI team. I am in the DS team and we are much mature than them in terms of DevOps practices, software design patterns, cloud architectural knowledge etc....\n\nI am trying to setup a common standard based on polars+delta lake+sql server (serving layer for power BI) to develop fast and cheap data pipelines.\n\nWe are trying to avoid using spark in our data processing workflows as we do not have the neither the data size not the in-house competence (I am the only one who knows spark in a company of around 2k employees). The DS team is willing to learn pyspark (as usual) but I think polars will have a very low learning curve for them instead of spark. Also, polars integrates nicely with duckDB so it seems a middle ground that can make everyone happy.\n\n**Current setup:**\n\nThe data engineers here are very old-fashioned and they use MSSQL as a data warehouse instead of using it only as OLTP. Some on-premise servers are very well modelled with Kimball and the loads are kind of okay in terms of performance but the central DW is a bit of a mess. The DE and BI team is mostly skillful with SQL and have very very very basic python knowledge (I think over the 10 members only one is able to write a class and open a SQLAlchemy connection), so I thought of introducing polars + delta lake to them instead of going the spark way.\n\nAs a matter of comparison I have done a rewrite of one of their processes using polars and delta lake and I am able to make a full load in approx 4mins vs their 3h (they are not aware of this and I am not planning to share it as I dont want to take the refactoring on my shoulders).\n\nI am aware that due to their SQL knowledge probably DBT will be a better option but no one has the knowledge of DBT and they are very stuck with .DACPAC files deployment (a side note here, we had to build their CI CD pipelines for both .dacpac files and ADF workflows as they were deploying .dacpac files directly from the editor to production, without any integration test for ingesting new data or fake data with ADF). Also, DBT integration with SQL Server seems very poor in comparison with PostgreSQL systems. Finally we use azure and in ADF the easier way that I found to run dbt continously (outside the CI CD pipelines) is via an invocation of container instances (no one in these teams know how to use docker).\n\n**Problem I am trying to solve:**\n\nHowever I am afraid that I will face a lot of friction as they have a lot of data quality checks written in SQL. I am trying to find an equivalent to GE, Pandera or Dequee in Polars to make them their journey a bit easier.\n\nDoes someone knows about such tools? So far I can only think on using polars and switch to pandas for quality checks but seems to kill quite a lot of the performance gains that Im getting with polars.\n\nIs this a good idea at all? Honestly I think that putting dagster/airflow in AKS and then use the integration with DBT will be better for their setup but we are not skilfull with DBT in my team and if we are going to help them we would like to use technologies that we are a bit familiar with as currently they do not have the capabilities to go outside SQL Server.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4aht7cg8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Equivalent of Great Expectations/Pandera/Dequee in Polars", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hdotc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702497305.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702462468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a bit of a situation in my company due to the lack of skills of our data engineering team and BI team. I am in the DS team and we are much mature than them in terms of DevOps practices, software design patterns, cloud architectural knowledge etc....&lt;/p&gt;\n\n&lt;p&gt;I am trying to setup a common standard based on polars+delta lake+sql server (serving layer for power BI) to develop fast and cheap data pipelines.&lt;/p&gt;\n\n&lt;p&gt;We are trying to avoid using spark in our data processing workflows as we do not have the neither the data size not the in-house competence (I am the only one who knows spark in a company of around 2k employees). The DS team is willing to learn pyspark (as usual) but I think polars will have a very low learning curve for them instead of spark. Also, polars integrates nicely with duckDB so it seems a middle ground that can make everyone happy.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The data engineers here are very old-fashioned and they use MSSQL as a data warehouse instead of using it only as OLTP. Some on-premise servers are very well modelled with Kimball and the loads are kind of okay in terms of performance but the central DW is a bit of a mess. The DE and BI team is mostly skillful with SQL and have very very very basic python knowledge (I think over the 10 members only one is able to write a class and open a SQLAlchemy connection), so I thought of introducing polars + delta lake to them instead of going the spark way.&lt;/p&gt;\n\n&lt;p&gt;As a matter of comparison I have done a rewrite of one of their processes using polars and delta lake and I am able to make a full load in approx 4mins vs their 3h (they are not aware of this and I am not planning to share it as I dont want to take the refactoring on my shoulders).&lt;/p&gt;\n\n&lt;p&gt;I am aware that due to their SQL knowledge probably DBT will be a better option but no one has the knowledge of DBT and they are very stuck with .DACPAC files deployment (a side note here, we had to build their CI CD pipelines for both .dacpac files and ADF workflows as they were deploying .dacpac files directly from the editor to production, without any integration test for ingesting new data or fake data with ADF). Also, DBT integration with SQL Server seems very poor in comparison with PostgreSQL systems. Finally we use azure and in ADF the easier way that I found to run dbt continously (outside the CI CD pipelines) is via an invocation of container instances (no one in these teams know how to use docker).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem I am trying to solve:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;However I am afraid that I will face a lot of friction as they have a lot of data quality checks written in SQL. I am trying to find an equivalent to GE, Pandera or Dequee in Polars to make them their journey a bit easier.&lt;/p&gt;\n\n&lt;p&gt;Does someone knows about such tools? So far I can only think on using polars and switch to pandas for quality checks but seems to kill quite a lot of the performance gains that Im getting with polars.&lt;/p&gt;\n\n&lt;p&gt;Is this a good idea at all? Honestly I think that putting dagster/airflow in AKS and then use the integration with DBT will be better for their setup but we are not skilfull with DBT in my team and if we are going to help them we would like to use technologies that we are a bit familiar with as currently they do not have the capabilities to go outside SQL Server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18hdotc", "is_robot_indexable": true, "report_reasons": null, "author": "Lix021", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hdotc/equivalent_of_great_expectationspanderadequee_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hdotc/equivalent_of_great_expectationspanderadequee_in/", "subreddit_subscribers": 145694, "created_utc": 1702462468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to kafka. Want to understand its functionality and different use cases.", "author_fullname": "t2_q32966uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Eli5 Kafka learning resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h5f7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702432828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to kafka. Want to understand its functionality and different use cases.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18h5f7b", "is_robot_indexable": true, "report_reasons": null, "author": "PrestigiousCup7026", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h5f7b/eli5_kafka_learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h5f7b/eli5_kafka_learning_resources/", "subreddit_subscribers": 145694, "created_utc": 1702432828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I started with a small company this year. The Executive that hired me stated they wanted to start a data team and I was hired on to start the group and get it off the ground. Great, hired, added in DataBricks for DE/orchestration and settled on QuickSight to keep things simple (AWS Shop). I have integrated all internal applications and financial data into usable reporting at this point.\n\nNow, I am a team of one doing multiple roles here: \n\nStrategy (Leadership), DE, DS, DA, PM, and building business logic with each part of the business (time consuming!!!).\n\n I am well paid (205k), but nobody understands what I do. I am on an island. The company has basically zero data literacy since the person that hired me has just left. So I am alone in more ways than one. I miss having a team and someone to at least talk to who understands what I do.\n\nAdditional context: There are no actual internal processes for the business leading to the data never matching and being a hot mess. This takes up a majority of my time fixing the years of mistakes and working with each business unit.\n\nAdditional Context 2: New Boss says absolutely not growing the team.\n\n&amp;#x200B;\n\nAny strategies to better communicate trying to untangle years of chaos data from spreadsheets? How long would you guys stick it out? \n\nTo me, standing up a Data stack and writing custom integration code to make usable data / reporting within a year is a success. Is it? Not sure how to convey how doing it all within a year myself is a success. Any thoughts appreciated! \n\nTLDR: Started with a company, executive that hired me left leaving zero data literacy. I stood up all critical data and stack within a year. What would you do as your next steps?", "author_fullname": "t2_mzk56ae", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Company - Minimal Data Literacy - Jack of all trades", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18hntt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702492903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I started with a small company this year. The Executive that hired me stated they wanted to start a data team and I was hired on to start the group and get it off the ground. Great, hired, added in DataBricks for DE/orchestration and settled on QuickSight to keep things simple (AWS Shop). I have integrated all internal applications and financial data into usable reporting at this point.&lt;/p&gt;\n\n&lt;p&gt;Now, I am a team of one doing multiple roles here: &lt;/p&gt;\n\n&lt;p&gt;Strategy (Leadership), DE, DS, DA, PM, and building business logic with each part of the business (time consuming!!!).&lt;/p&gt;\n\n&lt;p&gt;I am well paid (205k), but nobody understands what I do. I am on an island. The company has basically zero data literacy since the person that hired me has just left. So I am alone in more ways than one. I miss having a team and someone to at least talk to who understands what I do.&lt;/p&gt;\n\n&lt;p&gt;Additional context: There are no actual internal processes for the business leading to the data never matching and being a hot mess. This takes up a majority of my time fixing the years of mistakes and working with each business unit.&lt;/p&gt;\n\n&lt;p&gt;Additional Context 2: New Boss says absolutely not growing the team.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any strategies to better communicate trying to untangle years of chaos data from spreadsheets? How long would you guys stick it out? &lt;/p&gt;\n\n&lt;p&gt;To me, standing up a Data stack and writing custom integration code to make usable data / reporting within a year is a success. Is it? Not sure how to convey how doing it all within a year myself is a success. Any thoughts appreciated! &lt;/p&gt;\n\n&lt;p&gt;TLDR: Started with a company, executive that hired me left leaving zero data literacy. I stood up all critical data and stack within a year. What would you do as your next steps?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hntt1", "is_robot_indexable": true, "report_reasons": null, "author": "what_is_ovaltine", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hntt1/new_company_minimal_data_literacy_jack_of_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hntt1/new_company_minimal_data_literacy_jack_of_all/", "subreddit_subscribers": 145694, "created_utc": 1702492903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a DE staffing question. \n\nI work for a small business that is early stage in its data maturity. I\u2019m in charge of digital transformation and am NOT an engineer. \n\nWe are looking at using Mulesoft as an iPaaS with a new CRM (Salesforce) and existing ERP (Unanet). We have some other API\u2019s we\u2019ll also integrate from public data sources.\n\nWe\u2019ll likely using Power BI for internal reporting since we already have access to it. \n\nMulesoft is setting up the initial integration, but I don\u2019t know how much staff time I need to set aside to maintain and optimize the data stack. \n\nIs this an FTE or would someone that manages our IT systems also cross-train in light weight data engineering?\n\nThanks in advance!", "author_fullname": "t2_41soa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small Business stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hn48b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702491084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a DE staffing question. &lt;/p&gt;\n\n&lt;p&gt;I work for a small business that is early stage in its data maturity. I\u2019m in charge of digital transformation and am NOT an engineer. &lt;/p&gt;\n\n&lt;p&gt;We are looking at using Mulesoft as an iPaaS with a new CRM (Salesforce) and existing ERP (Unanet). We have some other API\u2019s we\u2019ll also integrate from public data sources.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ll likely using Power BI for internal reporting since we already have access to it. &lt;/p&gt;\n\n&lt;p&gt;Mulesoft is setting up the initial integration, but I don\u2019t know how much staff time I need to set aside to maintain and optimize the data stack. &lt;/p&gt;\n\n&lt;p&gt;Is this an FTE or would someone that manages our IT systems also cross-train in light weight data engineering?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hn48b", "is_robot_indexable": true, "report_reasons": null, "author": "miqcie", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hn48b/small_business_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hn48b/small_business_stack/", "subreddit_subscribers": 145694, "created_utc": 1702491084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is special about IoV data analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hfg1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1702469492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "doris.apache.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://doris.apache.org/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hfg1g", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hfg1g/what_is_special_about_iov_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://doris.apache.org/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents", "subreddit_subscribers": 145694, "created_utc": 1702469492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I am finishing my Master's degree in Big Data and Data Science, so I have to develop a final project.\n\nThe project topic is \"Optimum IoT data storage\". I have to simulate an IoT data streaming (that I would do in Python, generating random data), process it, and store it.\n\nGiven the context, my questions are:\n\n\\- Are there any IoT data streams publicly available? I can't find anything useful.\n\n\\- Which tech stack would you use to develop the ELT/ETL and the data storage? Keeping in mind that I would like to avoid AWS and Azure, but I am ok with GCP or tech that runs on my gaming PC.\n\nThank you guys in advance!", "author_fullname": "t2_p4nzh8v1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IoT doubt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18hpjso", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702497327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I am finishing my Master&amp;#39;s degree in Big Data and Data Science, so I have to develop a final project.&lt;/p&gt;\n\n&lt;p&gt;The project topic is &amp;quot;Optimum IoT data storage&amp;quot;. I have to simulate an IoT data streaming (that I would do in Python, generating random data), process it, and store it.&lt;/p&gt;\n\n&lt;p&gt;Given the context, my questions are:&lt;/p&gt;\n\n&lt;p&gt;- Are there any IoT data streams publicly available? I can&amp;#39;t find anything useful.&lt;/p&gt;\n\n&lt;p&gt;- Which tech stack would you use to develop the ELT/ETL and the data storage? Keeping in mind that I would like to avoid AWS and Azure, but I am ok with GCP or tech that runs on my gaming PC.&lt;/p&gt;\n\n&lt;p&gt;Thank you guys in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18hpjso", "is_robot_indexable": true, "report_reasons": null, "author": "data_macrolide", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hpjso/iot_doubt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hpjso/iot_doubt/", "subreddit_subscribers": 145694, "created_utc": 1702497327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The team i am on have recently given us Azure Data Factory so we're pulling all of the available toys out of the box. We want to move a lot of our processes onto it with a fresh start but i want to ensure that things are done correctly from the get go.\n\nLooking at resources online, it seems that when people use managed airflow, they run ADF pipelines within it, rather than the opposite. Is this correct?", "author_fullname": "t2_ityxycxf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which is the standard practice when integrating ADF with Airflow? Running pipelines from airflow dags, or airflow dags from ADF pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18hp258", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702496068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The team i am on have recently given us Azure Data Factory so we&amp;#39;re pulling all of the available toys out of the box. We want to move a lot of our processes onto it with a fresh start but i want to ensure that things are done correctly from the get go.&lt;/p&gt;\n\n&lt;p&gt;Looking at resources online, it seems that when people use managed airflow, they run ADF pipelines within it, rather than the opposite. Is this correct?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18hp258", "is_robot_indexable": true, "report_reasons": null, "author": "icecoldfeedback", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hp258/which_is_the_standard_practice_when_integrating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hp258/which_is_the_standard_practice_when_integrating/", "subreddit_subscribers": 145694, "created_utc": 1702496068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9mc6bsvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Announcing Daft 0.2: 10x faster IO from S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18hnig9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6kAlrUkY0d1Am5zE5tJmFIqfa7ugiSpNHOtVd-KWuY4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702492084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.getdaft.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.getdaft.io/p/announcing-daft-02-10x-faster-io", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?auto=webp&amp;s=033ee28ec46c1da6de6b5e8b169b56adff892de9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da773c307287ff0175512954baacc60e62eb25e4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dbc0dca765c196e0d8b85bc54a1c05987489ea3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=66f4994c01506b7e9ee8f957375b238afc0d3183", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3e30aa26a2db2e44c5c022bc5bac0323d1d4cba", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=45ced61130e03ef0f68cee7b5b1c2dc50e911559", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8hdsgMHlzdTfLywKJ40pSWPBSPTO3D3038s82J0eTlI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bdfe16b73190ee87b682de77da1f313ab846e7c3", "width": 1080, "height": 540}], "variants": {}, "id": "3J-x3pcNAsUf9E9q4Zlw4hot4HRtb5WEEWmB5K_QhQk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18hnig9", "is_robot_indexable": true, "report_reasons": null, "author": "xylene25", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hnig9/announcing_daft_02_10x_faster_io_from_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.getdaft.io/p/announcing-daft-02-10x-faster-io", "subreddit_subscribers": 145694, "created_utc": 1702492084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey community \ud83d\udc4b\n\nI just implemented data quality tests with [Soda Core](https://github.com/sodadata/soda-core) and the [prefect-soda-core](https://github.com/sodadata/prefect-soda-core) extension within my data infrastructure for a [project centered around the English Premier League](https://github.com/digitalghost-dev/premier-league) that I have been working on lately that runs on a schedule using [Prefect](https://github.com/PrefectHQ/prefect).\n\n[Screenshot of the Prefect dashboard for the flow run.](https://preview.redd.it/zdlwdec8p36c1.png?width=2818&amp;format=png&amp;auto=webp&amp;s=66a926223af521160fda2c20176fecfc40bda2af)\n\nSome of the checks I have created are pretty simple but I aim to add more:\n\n    checks for news:\n      - row_count &gt; 1\n      - invalid_count(url) = 0:\n          valid regex: ^https://\n    \n    checks for stadiums:\n      - row_count = 20\n    \n    checks for standings:\n      - row_count = 20\n      - duplicate_count(team) = 0\n      - max(points) &lt; 114\n      - min(points) &gt; 0\n    \n    checks for teams:\n      - row_count = 20\n      - duplicate_count(team) = 0\n    \n    checks for top_scorers:\n      - row_count = 5\n\nThe `soda-core-bigquery` library connects directly to my BigQuery tables via default `gcloud` credentials on a virtual machine hosted on [Compute Engine](https://cloud.google.com/compute?hl=en) on Google Cloud. Has anyone else implement data quality checks with their data infrastructure?", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Data Quality Checks into the Data Infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zdlwdec8p36c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 43, "x": 108, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e96e63d94e2a7d05d8b3f336f379b03a48665f46"}, {"y": 86, "x": 216, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=197cd7f826df2ed4b0d9716e752768fa7810628e"}, {"y": 128, "x": 320, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e9bd64dd17aab81ce06f2db05a880f512b8727e"}, {"y": 257, "x": 640, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fcc9c2315bd5e58ad552b8cfed8ba7783e6d9423"}, {"y": 385, "x": 960, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc35d91d5f36b61d018a03b22f085281c1f24636"}, {"y": 433, "x": 1080, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7ce84fc6f990d99ca4db000bfa063b1c126fb91e"}], "s": {"y": 1132, "x": 2818, "u": "https://preview.redd.it/zdlwdec8p36c1.png?width=2818&amp;format=png&amp;auto=webp&amp;s=66a926223af521160fda2c20176fecfc40bda2af"}, "id": "zdlwdec8p36c1"}}, "name": "t3_18hmz09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CDs2oF8FNPg4ykWokPg_iyxFR2sys-rfFbxBCz0E4cg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1702490718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey community \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;I just implemented data quality tests with &lt;a href=\"https://github.com/sodadata/soda-core\"&gt;Soda Core&lt;/a&gt; and the &lt;a href=\"https://github.com/sodadata/prefect-soda-core\"&gt;prefect-soda-core&lt;/a&gt; extension within my data infrastructure for a &lt;a href=\"https://github.com/digitalghost-dev/premier-league\"&gt;project centered around the English Premier League&lt;/a&gt; that I have been working on lately that runs on a schedule using &lt;a href=\"https://github.com/PrefectHQ/prefect\"&gt;Prefect&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zdlwdec8p36c1.png?width=2818&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=66a926223af521160fda2c20176fecfc40bda2af\"&gt;Screenshot of the Prefect dashboard for the flow run.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Some of the checks I have created are pretty simple but I aim to add more:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;checks for news:\n  - row_count &amp;gt; 1\n  - invalid_count(url) = 0:\n      valid regex: ^https://\n\nchecks for stadiums:\n  - row_count = 20\n\nchecks for standings:\n  - row_count = 20\n  - duplicate_count(team) = 0\n  - max(points) &amp;lt; 114\n  - min(points) &amp;gt; 0\n\nchecks for teams:\n  - row_count = 20\n  - duplicate_count(team) = 0\n\nchecks for top_scorers:\n  - row_count = 5\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The &lt;code&gt;soda-core-bigquery&lt;/code&gt; library connects directly to my BigQuery tables via default &lt;code&gt;gcloud&lt;/code&gt; credentials on a virtual machine hosted on &lt;a href=\"https://cloud.google.com/compute?hl=en\"&gt;Compute Engine&lt;/a&gt; on Google Cloud. Has anyone else implement data quality checks with their data infrastructure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?auto=webp&amp;s=c2974d68ab4436ab42d31ac37ceb1edfab081328", "width": 1280, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9fa76990f149cb460631915757118e32ee3a17b5", "width": 108, "height": 42}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a415f27dbddcb0665a2adb50bd29357beca0ec5", "width": 216, "height": 84}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8faf8367e79f6fd7239e32abc7cff4bd4a16d09", "width": 320, "height": 125}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b55fb276b2670ced04c194eb844500ee4d4e48e1", "width": 640, "height": 250}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d0fd4433db81727e6230fcd321ac5cdcb29ca095", "width": 960, "height": 375}, {"url": "https://external-preview.redd.it/gUCam2LmiMaBBw8u_K3ZB8SFhlDuobipvHofnAVqH_c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f2184b1063031a46b41984fee785a0f66f9bb4aa", "width": 1080, "height": 421}], "variants": {}, "id": "HL_qWEYj1OF-fF8JgsOfD5dVK1TuuOcvdAW--omanMs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18hmz09", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hmz09/introducing_data_quality_checks_into_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hmz09/introducing_data_quality_checks_into_the_data/", "subreddit_subscribers": 145694, "created_utc": 1702490718.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}