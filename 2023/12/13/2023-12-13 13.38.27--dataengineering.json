{"kind": "Listing", "data": {"after": "t3_18gupwv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Although I work as a Machine Learning Engineer, sometimes I'm requested to build some queries, for instance for dashboarding purposes.\n\nHowever, I find it really tedious when working with SQL. My main reasons being:\n\n1. Most of the times we work with tables in the DataLake we don't own, hence it takes an awful lot of time to get an understanding of how that data is structured (and what kind of data problems it might have)\n2. I feel really unproductive having to wait for query results. It really slows down any kind of exploration one might want do with the data like one would with tools like Pandas or Polars (and loading data locally is not an option as we're talking of billions of records here, which we're handling with Spark)\n3. Sometimes queries grow to be extremely complex, which makes it harder for team mates to review\n4. Along the previous point, I really feel SQL is extremely unreadable as compared to a programming language", "author_fullname": "t2_21rg1aff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you make working with SQL enjoyable (or less tedious)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gn43u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 77, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 77, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702391339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Although I work as a Machine Learning Engineer, sometimes I&amp;#39;m requested to build some queries, for instance for dashboarding purposes.&lt;/p&gt;\n\n&lt;p&gt;However, I find it really tedious when working with SQL. My main reasons being:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Most of the times we work with tables in the DataLake we don&amp;#39;t own, hence it takes an awful lot of time to get an understanding of how that data is structured (and what kind of data problems it might have)&lt;/li&gt;\n&lt;li&gt;I feel really unproductive having to wait for query results. It really slows down any kind of exploration one might want do with the data like one would with tools like Pandas or Polars (and loading data locally is not an option as we&amp;#39;re talking of billions of records here, which we&amp;#39;re handling with Spark)&lt;/li&gt;\n&lt;li&gt;Sometimes queries grow to be extremely complex, which makes it harder for team mates to review&lt;/li&gt;\n&lt;li&gt;Along the previous point, I really feel SQL is extremely unreadable as compared to a programming language&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18gn43u", "is_robot_indexable": true, "report_reasons": null, "author": "barberogaston", "discussion_type": null, "num_comments": 95, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gn43u/how_do_you_make_working_with_sql_enjoyable_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gn43u/how_do_you_make_working_with_sql_enjoyable_or/", "subreddit_subscribers": 145623, "created_utc": 1702391339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for a new job and I wanted to make sure that I know the basic things moving forward.", "author_fullname": "t2_e8k9c3l7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some basic requirements or skills that are least expected from a Data Engineer with one year of experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hc0wr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702455054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a new job and I wanted to make sure that I know the basic things moving forward.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18hc0wr", "is_robot_indexable": true, "report_reasons": null, "author": "SignalCrew739", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hc0wr/what_are_some_basic_requirements_or_skills_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hc0wr/what_are_some_basic_requirements_or_skills_that/", "subreddit_subscribers": 145623, "created_utc": 1702455054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been modeling NBA data for a couple months, and this is one of my favorite insights so far!\n\n\\- \ud835\udc08\ud835\udc27\ud835\udc20\ud835\udc1e\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27: public NBA API + Python  \n\\- \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1a\ud835\udc20\ud835\udc1e: DuckDB (development) &amp; Snowflake (Production)  \n\\- \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c: [paradime.io](https://www.linkedin.com/company/paradimelabs/) (dbt)  \n\\- \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc27\ud835\udc20 (\ud835\udc01\ud835\udc08) - [Lightdash](https://www.linkedin.com/company/lightdash/)\n\nSo, why do the Jazz have the lowest avg. cost per win?  \n\ud83e\ude84 2nd most regular-season wins since 1990. This is due to many factors, including: Stockton -&gt; Malone, Great home-court advantage, stable coaching.  \n\ud83e\ude84 7th lowest luxury tax bill since 1990 (out of 30 teams)  \n\ud83e\ude84 Salt Lake City doesn't attract top (expensive) NBA talent \ud83e\udd23  \n\ud83e\ude84 Consistent &amp; competent leadership  \nSeparate note - I'm still shocked by how terrible the Knicks have been historically. They're the biggest market, they're willing to spend (obviously) yet they can't pull it together... Ever\n\nYou can find, critique, and contribute to my NBA project here: [https://github.com/jpooksy/NBA\\_Data\\_Modeling](https://github.com/jpooksy/NBA_Data_Modeling)  \n\n\n&amp;#x200B;\n\nhttps://preview.redd.it/lrw4oybekw5c1.png?width=564&amp;format=png&amp;auto=webp&amp;s=31483f90ad3e5ac997b8c9956b59af9f029dc960", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NBA data modeling wth dbt + Paradime", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"lrw4oybekw5c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 130, "x": 108, "u": "https://preview.redd.it/lrw4oybekw5c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49598bbed60ac1dfad30e730226a940d1617ac82"}, {"y": 261, "x": 216, "u": "https://preview.redd.it/lrw4oybekw5c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2633eed8b2e34880131642200e2dacb2083d1b71"}, {"y": 386, "x": 320, "u": "https://preview.redd.it/lrw4oybekw5c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1ad4371601d2889998d2164deba121e4f822f96"}], "s": {"y": 682, "x": 564, "u": "https://preview.redd.it/lrw4oybekw5c1.png?width=564&amp;format=png&amp;auto=webp&amp;s=31483f90ad3e5ac997b8c9956b59af9f029dc960"}, "id": "lrw4oybekw5c1"}}, "name": "t3_18grxlj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_pUtcBckyyefRo0H27ezy2qeaxhbYnsrdXQXQjFIhAw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702403763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been modeling NBA data for a couple months, and this is one of my favorite insights so far!&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc08\ud835\udc27\ud835\udc20\ud835\udc1e\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27: public NBA API + Python&lt;br/&gt;\n- \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1a\ud835\udc20\ud835\udc1e: DuckDB (development) &amp;amp; Snowflake (Production)&lt;br/&gt;\n- \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c: &lt;a href=\"https://www.linkedin.com/company/paradimelabs/\"&gt;paradime.io&lt;/a&gt; (dbt)&lt;br/&gt;\n- \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc27\ud835\udc20 (\ud835\udc01\ud835\udc08) - &lt;a href=\"https://www.linkedin.com/company/lightdash/\"&gt;Lightdash&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So, why do the Jazz have the lowest avg. cost per win?&lt;br/&gt;\n\ud83e\ude84 2nd most regular-season wins since 1990. This is due to many factors, including: Stockton -&amp;gt; Malone, Great home-court advantage, stable coaching.&lt;br/&gt;\n\ud83e\ude84 7th lowest luxury tax bill since 1990 (out of 30 teams)&lt;br/&gt;\n\ud83e\ude84 Salt Lake City doesn&amp;#39;t attract top (expensive) NBA talent \ud83e\udd23&lt;br/&gt;\n\ud83e\ude84 Consistent &amp;amp; competent leadership&lt;br/&gt;\nSeparate note - I&amp;#39;m still shocked by how terrible the Knicks have been historically. They&amp;#39;re the biggest market, they&amp;#39;re willing to spend (obviously) yet they can&amp;#39;t pull it together... Ever&lt;/p&gt;\n\n&lt;p&gt;You can find, critique, and contribute to my NBA project here: &lt;a href=\"https://github.com/jpooksy/NBA_Data_Modeling\"&gt;https://github.com/jpooksy/NBA_Data_Modeling&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lrw4oybekw5c1.png?width=564&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31483f90ad3e5ac997b8c9956b59af9f029dc960\"&gt;https://preview.redd.it/lrw4oybekw5c1.png?width=564&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31483f90ad3e5ac997b8c9956b59af9f029dc960&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18grxlj", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18grxlj/nba_data_modeling_wth_dbt_paradime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18grxlj/nba_data_modeling_wth_dbt_paradime/", "subreddit_subscribers": 145623, "created_utc": 1702403763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been with my current company for about six months, and in that time, I've not only developed data pipelines but also found myself building the team-specific dashboards they feed into. Back at my old job, things were a bit different. I worked alongside an analyst who took care of the dashboards. They would give me the business needs, and I'd tailor the data pipelines to suit those requirements and the analysis being done. Personally, I don't think dashboard building is really part of a data engineer's job, but I'm curious to hear what others in this group think. Have you had similar experiences?", "author_fullname": "t2_103ndz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much dashboarding / viz do you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h7vek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702439534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been with my current company for about six months, and in that time, I&amp;#39;ve not only developed data pipelines but also found myself building the team-specific dashboards they feed into. Back at my old job, things were a bit different. I worked alongside an analyst who took care of the dashboards. They would give me the business needs, and I&amp;#39;d tailor the data pipelines to suit those requirements and the analysis being done. Personally, I don&amp;#39;t think dashboard building is really part of a data engineer&amp;#39;s job, but I&amp;#39;m curious to hear what others in this group think. Have you had similar experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18h7vek", "is_robot_indexable": true, "report_reasons": null, "author": "natelifts", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h7vek/how_much_dashboarding_viz_do_you_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h7vek/how_much_dashboarding_viz_do_you_do/", "subreddit_subscribers": 145623, "created_utc": 1702439534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got into a discussion with a visualization engineer who insists that never in her career had they created materialized views or regular views specific to a dashboard element. \nI\u2019m not in visualization, so honestly don\u2019t even have context to decide if this person\u2019s perspective is reasonable. From my perspective, as a dashboard is designed for a larger audience, if dealing with data from multiple tables, of course it makes sense to do this work upstream. Am I crazy here?", "author_fullname": "t2_5s7getlh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pattern: Table or View per Dashboard Element", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gv87y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702412142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got into a discussion with a visualization engineer who insists that never in her career had they created materialized views or regular views specific to a dashboard element. \nI\u2019m not in visualization, so honestly don\u2019t even have context to decide if this person\u2019s perspective is reasonable. From my perspective, as a dashboard is designed for a larger audience, if dealing with data from multiple tables, of course it makes sense to do this work upstream. Am I crazy here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18gv87y", "is_robot_indexable": true, "report_reasons": null, "author": "pro__acct__", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gv87y/pattern_table_or_view_per_dashboard_element/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gv87y/pattern_table_or_view_per_dashboard_element/", "subreddit_subscribers": 145623, "created_utc": 1702412142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, Dear Data Engineers!\n\nI wrote a [blog post](https://www.mitzu.io/post/identifying-users-in-the-data-warehouse?source=der) about solving the **User ID Stitching** problem with Recursive SQL. You can use this in DBT with data warehouses supporting recursive CTEs.  \n\n\n&gt;The **User ID Stitching** problem concerns primarily **B2C companies** that want to measure marketing, product, and sales metrics correctly from the delta lake.  \nIt is about creating a **user\\_aliases** table that maps all possible user identifiers for every user across all datasets in a Delta Lake to a single **merged\\_id**\n\n**Possible user identifiers:**\n\n* device\\_id\n* user\\_id\n* anonymous\\_user\\_id\n* email\n* contact\\_id (Hubspot)\n* customer\\_id (Stripe)\n* \u2026 etc\u2026\n\nYou can then use the **merge\\_id** to join across all datasets, count unique users correctly, create funnel queries across all datasets, etc.  \nI hope it helps whoever it may help :)\n\nLink to the [blog post](https://www.mitzu.io/post/identifying-users-in-the-data-warehouse?source=der)", "author_fullname": "t2_gnytqihqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "User ID Stitching with Recursive SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hc4jz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702455492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, Dear Data Engineers!&lt;/p&gt;\n\n&lt;p&gt;I wrote a &lt;a href=\"https://www.mitzu.io/post/identifying-users-in-the-data-warehouse?source=der\"&gt;blog post&lt;/a&gt; about solving the &lt;strong&gt;User ID Stitching&lt;/strong&gt; problem with Recursive SQL. You can use this in DBT with data warehouses supporting recursive CTEs.  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The &lt;strong&gt;User ID Stitching&lt;/strong&gt; problem concerns primarily &lt;strong&gt;B2C companies&lt;/strong&gt; that want to measure marketing, product, and sales metrics correctly from the delta lake.&lt;br/&gt;\nIt is about creating a &lt;strong&gt;user_aliases&lt;/strong&gt; table that maps all possible user identifiers for every user across all datasets in a Delta Lake to a single &lt;strong&gt;merged_id&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Possible user identifiers:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;device_id&lt;/li&gt;\n&lt;li&gt;user_id&lt;/li&gt;\n&lt;li&gt;anonymous_user_id&lt;/li&gt;\n&lt;li&gt;email&lt;/li&gt;\n&lt;li&gt;contact_id (Hubspot)&lt;/li&gt;\n&lt;li&gt;customer_id (Stripe)&lt;/li&gt;\n&lt;li&gt;\u2026 etc\u2026&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can then use the &lt;strong&gt;merge_id&lt;/strong&gt; to join across all datasets, count unique users correctly, create funnel queries across all datasets, etc.&lt;br/&gt;\nI hope it helps whoever it may help :)&lt;/p&gt;\n\n&lt;p&gt;Link to the &lt;a href=\"https://www.mitzu.io/post/identifying-users-in-the-data-warehouse?source=der\"&gt;blog post&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?auto=webp&amp;s=0d093ce954e3041f48a37532a5dcaf93dc69e403", "width": 1776, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc79b8bbb10e9f68c1d1d6c86af1f3841dc4f10e", "width": 108, "height": 48}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c07e77a7075812c321c03396caabe8b0df20257", "width": 216, "height": 97}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3ef5435010e00eaf07797ddfcec9de3febd41ce", "width": 320, "height": 144}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b745bec61cacf775819bc9c0dee5dc03fbf29420", "width": 640, "height": 288}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49e6f8c7db99048714c1629660d5ca625c2c9ba6", "width": 960, "height": 432}, {"url": "https://external-preview.redd.it/6cDKS_NF0x-XxokIrxw9chJ_xZYQVp7HT_0BZyjKrNQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32c1b03ffdc645de4619500f6d9673e4f4fcb49a", "width": 1080, "height": 486}], "variants": {}, "id": "PNhHEjsZUew5p0U0EcK53-ZbVngG-Nb-Q0Qqg7c5UcI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hc4jz", "is_robot_indexable": true, "report_reasons": null, "author": "MitzuIstvan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hc4jz/user_id_stitching_with_recursive_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hc4jz/user_id_stitching_with_recursive_sql/", "subreddit_subscribers": 145623, "created_utc": 1702455492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_artaa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What it's like watching performance tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_18h8vaw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p-ScxshpaIH0LpOXfbiGYg_scjII6-q45KRcSku5VHE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702442790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/abq9civ6sz5c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/abq9civ6sz5c1.png?auto=webp&amp;s=8b185af5c90f3b84ccb0159c6b0d7a19acfcb67d", "width": 1792, "height": 1024}, "resolutions": [{"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=19733133c2e33da130bb2cc44e3903397044da4c", "width": 108, "height": 61}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d298fc84454e747f4142361fe3c2395ed7f728d3", "width": 216, "height": 123}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf8a5a0fe428bd9b9d43ec2081f14ca243adc9ac", "width": 320, "height": 182}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=674624077c2d64c5762ac4a3c5325f44a0c530ab", "width": 640, "height": 365}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d272489210b302a0b7f32c8b5623768d4cd14da5", "width": 960, "height": 548}, {"url": "https://preview.redd.it/abq9civ6sz5c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b087a167c1eed7ae6ccd1098f3f95280496ee499", "width": 1080, "height": 617}], "variants": {}, "id": "A-qG__OJXA_ovT6p9YyJ08z42GO-MzxjPebbjk5kS6U"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18h8vaw", "is_robot_indexable": true, "report_reasons": null, "author": "Toasty_toaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h8vaw/what_its_like_watching_performance_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/abq9civ6sz5c1.png", "subreddit_subscribers": 145623, "created_utc": 1702442790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u8kebhp6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upgrading Jobs Code between Databricks Runtime Versions Made Easier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_18gq1fy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vHriYbg9RY72KUXRAAYKX7WkPAf_TJCsD36ioLiCkEw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702399004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/databricks-labs/upgrading-jobs-code-between-databricks-runtime-versions-made-seamless-29c22e701c3a", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6CGEVIm1q1TobUixQuDUP9Ws43TA5G0TONrrR2zYDAY.jpg?auto=webp&amp;s=e6fff503363810a1a844243d4dc30e20374a7e61", "width": 1200, "height": 639}, "resolutions": [{"url": "https://external-preview.redd.it/6CGEVIm1q1TobUixQuDUP9Ws43TA5G0TONrrR2zYDAY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b1bd7c5baa426d324568692cf26b63002b7b9ab", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/6CGEVIm1q1TobUixQuDUP9Ws43TA5G0TONrrR2zYDAY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=632346f8e065aae9f74ea8fe594679ac441da0d7", "width": 216, "height": 115}, {"url": "https://external-preview.redd.it/6CGEVIm1q1TobUixQuDUP9Ws43TA5G0TONrrR2zYDAY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a7c5312e6d82b74496104ca1ba5fd37d8edfc97", "width": 320, "height": 170}, {"url": "https://external-preview.redd.it/6CGEVIm1q1TobUixQuDUP9Ws43TA5G0TONrrR2zYDAY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d87a636f09f8ab2bf8a00a4eee4eae61323608f8", "width": 640, "height": 340}, {"url": "https://external-preview.redd.it/6CGEVIm1q1TobUixQuDUP9Ws43TA5G0TONrrR2zYDAY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9002d9c593d7a4124e0ce8d3e0cba46f56a69233", "width": 960, "height": 511}, {"url": "https://external-preview.redd.it/6CGEVIm1q1TobUixQuDUP9Ws43TA5G0TONrrR2zYDAY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a322c150c48715bdd366d61e7a96a10d7e03776c", "width": 1080, "height": 575}], "variants": {}, "id": "uhonE0yWOOsFkvPBPnnGD6Eg9m5tkICAwlDw0bCdu6E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18gq1fy", "is_robot_indexable": true, "report_reasons": null, "author": "serge_databricks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gq1fy/upgrading_jobs_code_between_databricks_runtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/databricks-labs/upgrading-jobs-code-between-databricks-runtime-versions-made-seamless-29c22e701c3a", "subreddit_subscribers": 145623, "created_utc": 1702399004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After a cross-disciplinary Big Data (Engineering, Science) Master's degree, i have done an internship as a assistant DE then the company hired me as a junior DE. But i didn't really stuck to DE, of course i made a lot of pipelines from ingestion to exports...etc and all usual Data Engineer tasks. But i also did a lot (like really a lot) of Data Viz and some analysis (either descriptive or predictive). With great results for our company, and that's something i like doing.   \nNow the company is growing, we recently hired a huge data analysts team, that took over all the data analysis and data viz perimeter, and i kinda feel limited in my scope as i do really well in this hybrid role and i wanted to learn some ML (i have all the major certifications in the technologies we're already using like GCP professional DE)   \nAs i'm still \"new\" and young, i want to ask to the more experienced ones here what is a realistic career path for a \"DE\" that likes the business side of Data. Is it some like of utopia if i'm thinking to have some hybrid role that goes through all the Data lifecyle ? Especially in big companies", "author_fullname": "t2_7twd1xfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "After 2 years as a DE, i don't know where my career path is going", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h1u2e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702425940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After a cross-disciplinary Big Data (Engineering, Science) Master&amp;#39;s degree, i have done an internship as a assistant DE then the company hired me as a junior DE. But i didn&amp;#39;t really stuck to DE, of course i made a lot of pipelines from ingestion to exports...etc and all usual Data Engineer tasks. But i also did a lot (like really a lot) of Data Viz and some analysis (either descriptive or predictive). With great results for our company, and that&amp;#39;s something i like doing.&lt;br/&gt;\nNow the company is growing, we recently hired a huge data analysts team, that took over all the data analysis and data viz perimeter, and i kinda feel limited in my scope as i do really well in this hybrid role and i wanted to learn some ML (i have all the major certifications in the technologies we&amp;#39;re already using like GCP professional DE)&lt;br/&gt;\nAs i&amp;#39;m still &amp;quot;new&amp;quot; and young, i want to ask to the more experienced ones here what is a realistic career path for a &amp;quot;DE&amp;quot; that likes the business side of Data. Is it some like of utopia if i&amp;#39;m thinking to have some hybrid role that goes through all the Data lifecyle ? Especially in big companies&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18h1u2e", "is_robot_indexable": true, "report_reasons": null, "author": "BennyLauren", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h1u2e/after_2_years_as_a_de_i_dont_know_where_my_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h1u2e/after_2_years_as_a_de_i_dont_know_where_my_career/", "subreddit_subscribers": 145623, "created_utc": 1702425940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distributed Data Processing with Ray Data and MinIO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 41, "top_awarded_type": null, "hide_score": false, "name": "t3_18gskm2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uzAf_NxB0nLPzpd6yZHTuYbIip8pyxSiP67aPyuEhSU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702405394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=ray_data_and_minio", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?auto=webp&amp;s=60a9a5bb13ad52dc6f983d22a24a53b3b835a119", "width": 1200, "height": 359}, "resolutions": [{"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db2aa38b11de95fa42f9dbf716d73c3b2a64e4f3", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c18b4e8bc4062930b8490b5721bf20e5d91b96a", "width": 216, "height": 64}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e065467bf6807e1f7014eb05a7e8aef79c5b4e88", "width": 320, "height": 95}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74d1b5b114a455946e7c372fa0cc71907ebdd6dd", "width": 640, "height": 191}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d0ea6ae4d3f00a6c97c690b38471318183ee2c8", "width": 960, "height": 287}, {"url": "https://external-preview.redd.it/elFSPAdTCUARL5snaD1gQznx3b133pDeGEBCTuTl2Po.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=011a2c8ca8070e4782e8cf151170e64f2db7fd85", "width": 1080, "height": 323}], "variants": {}, "id": "ycJVbgf3bDDq06hpn_WuGtC2gZGgwo5DkVBTrJOe5j8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18gskm2", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gskm2/distributed_data_processing_with_ray_data_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.min.io/distributed-data-processing-with-ray-data-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=ray_data_and_minio", "subreddit_subscribers": 145623, "created_utc": 1702405394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn the the discussions around bronze layers / staging areas it is always said that the raw data should be stored (and never changed). However one aspect that is barely discussed in my opinion is how to ensure data protection e.g. GDPR in that data. \n\nIn my case we are not allowed to have any non-pseudonymized personal data stored. Therefore we already do transformations to do that before storing in \"bronze\". \nAlso data deletion on specific rules needs to take place in bronze. Since we work with json data where important identifiers are nested in the data this is quite complex.\n\nWhat are your experiences and opinions on this?", "author_fullname": "t2_rydqu8m3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GDPR in Bronze / Staging", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18haxma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702450447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In the the discussions around bronze layers / staging areas it is always said that the raw data should be stored (and never changed). However one aspect that is barely discussed in my opinion is how to ensure data protection e.g. GDPR in that data. &lt;/p&gt;\n\n&lt;p&gt;In my case we are not allowed to have any non-pseudonymized personal data stored. Therefore we already do transformations to do that before storing in &amp;quot;bronze&amp;quot;. \nAlso data deletion on specific rules needs to take place in bronze. Since we work with json data where important identifiers are nested in the data this is quite complex.&lt;/p&gt;\n\n&lt;p&gt;What are your experiences and opinions on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18haxma", "is_robot_indexable": true, "report_reasons": null, "author": "DecisionAgile7326", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18haxma/gdpr_in_bronze_staging/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18haxma/gdpr_in_bronze_staging/", "subreddit_subscribers": 145623, "created_utc": 1702450447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title basically. using azure devOps on prem for build.", "author_fullname": "t2_slq927f8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how are ms sql server DBs deployed on-prem using ci/cd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h30tl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702428163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title basically. using azure devOps on prem for build.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18h30tl", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Release930", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h30tl/how_are_ms_sql_server_dbs_deployed_onprem_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h30tl/how_are_ms_sql_server_dbs_deployed_onprem_using/", "subreddit_subscribers": 145623, "created_utc": 1702428163.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, what are you interested to know about orchestrators? \n\nI am looking to do a deep dive on the more popular ones and write about it. \n\nWhat I am interested to understand is what use cases are each of them best at, and the kinds of criteria **you** use to make that choice.  \n\n\nSo my questions to you are:\n\n  \n**- what do you want to know about orchestrators?**\n\n**- what would you compare them on?**\n\n\\- **what killer features or orchestrators do you think I should check out and investigate?**  \n\n\nCurrently on my list are mainstream orchestrators such as Airflow, but also smaller ones, such as git actions. So tell me what you want to know, and we'll dig in :)  \n\n\nThanks in advance!\n\n&amp;#x200B;", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you want to know about orchestrators?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gqg6q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702400055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, what are you interested to know about orchestrators? &lt;/p&gt;\n\n&lt;p&gt;I am looking to do a deep dive on the more popular ones and write about it. &lt;/p&gt;\n\n&lt;p&gt;What I am interested to understand is what use cases are each of them best at, and the kinds of criteria &lt;strong&gt;you&lt;/strong&gt; use to make that choice.  &lt;/p&gt;\n\n&lt;p&gt;So my questions to you are:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;- what do you want to know about orchestrators?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;- what would you compare them on?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;what killer features or orchestrators do you think I should check out and investigate?&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Currently on my list are mainstream orchestrators such as Airflow, but also smaller ones, such as git actions. So tell me what you want to know, and we&amp;#39;ll dig in :)  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18gqg6q", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gqg6q/what_do_you_want_to_know_about_orchestrators/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gqg6q/what_do_you_want_to_know_about_orchestrators/", "subreddit_subscribers": 145623, "created_utc": 1702400055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to kafka. Want to understand its functionality and different use cases.", "author_fullname": "t2_q32966uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Eli5 Kafka learning resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h5f7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702432828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to kafka. Want to understand its functionality and different use cases.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18h5f7b", "is_robot_indexable": true, "report_reasons": null, "author": "PrestigiousCup7026", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h5f7b/eli5_kafka_learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h5f7b/eli5_kafka_learning_resources/", "subreddit_subscribers": 145623, "created_utc": 1702432828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is not entirely dedicated to data engineering, but it has touch points.\n\n **Some context:**\n\nIn our small to medium-sized enterprise operating in a production environment with a myriad of applications, the majority of which have point-to-point interfaces with our ERP (a local SQL database), and a considerable amount of legacy systems, the IT team, consisting of just two members, manages all applications and integrations. It's noteworthy that we lack in-house developers. Many of our integrations rely on XML files for data exchange, employing SQL and mapping tools like Altova and sometimes even BizTalk.\n\nLooking ahead, by the end of 2024, we plan to implement a new ERP, likely Dynamics 365 F&amp;O. This transition necessitates the rebuilding of numerous integrations to align with the new ERP. Ideally, we aim to centralize these integrations for improved oversight, addressing issues proactively rather than reactively as is often the case currently.\n\nOur challenges are further compounded when dealing with modern web applications utilizing RESTAPI, especially with our legacy ERP. Typically, we resort to scripting PowerShell and requesting vendors to set up an SFTP server for XML file exchange, incurring additional costs for their extra efforts. To address these issues, we have explored integration platforms such as Mulesoft, Workato, and Boomi.\n\nBoomi was discarded after practical testing due to perceived shortcomings. \n\nMulesoft proved comprehensible, enabling successful use cases, yet its deployment and maintenance seemed burdensome. \n\nWorkato, while lacking robust debugging and offering basic logging, demonstrated ease of use in creating integrations, aligning well with the simplicity of our integration needs. Its variety of connectors reduces the need for custom wrappers or connectors, and it presents automation possibilities.\n\nSome of the use cases that I finalized in the 2 last tools are:  \n\\- I was able to build an extensive integration between ERP and Salesforce, \n\n\\- create an API based on openapi specs and send data between ERP and the web app\n\n\\- do data transformation into an XML file with iterations \n\n**The question at hand** is whether an Integration Platform as a Service (iPaaS) is a suitable solution for our scenario, given that we do not anticipate hiring an in-house developer. Additionally, are there other platforms or tools we might be overlooking? We are seeking insights from those with experience in the listed tools, with Workato emerging as our preferred choice due to its balance of simplicity and functionality, even though Mulesoft offers more robust features. The overarching question is whether the additional functionalities of Mulesoft are necessary for our specific requirements.\n\nTL;DR: We're a small-to-medium enterprise handling various applications with point-to-point interfaces and legacy systems. Two-person IT team, no in-house developers. Looking to rebuild integrations for a new ERP (probably Dynamics 365 F&amp;O) by 2024. Currently considering Workato, Mulesoft, and Boomi. Workato seems suitable due to simplicity and variety of connectors. Wondering if Integration Platform as a Service (iPaaS) is the right solution for us and seeking insights on other tools or experiences with the mentioned platforms. Open to suggestions given our context and the absence of in-house developers.", "author_fullname": "t2_8mnqz0ek", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about IPAAS platforms (Mulesoft, Workato,...)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gm0jn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702388156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is not entirely dedicated to data engineering, but it has touch points.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Some context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In our small to medium-sized enterprise operating in a production environment with a myriad of applications, the majority of which have point-to-point interfaces with our ERP (a local SQL database), and a considerable amount of legacy systems, the IT team, consisting of just two members, manages all applications and integrations. It&amp;#39;s noteworthy that we lack in-house developers. Many of our integrations rely on XML files for data exchange, employing SQL and mapping tools like Altova and sometimes even BizTalk.&lt;/p&gt;\n\n&lt;p&gt;Looking ahead, by the end of 2024, we plan to implement a new ERP, likely Dynamics 365 F&amp;amp;O. This transition necessitates the rebuilding of numerous integrations to align with the new ERP. Ideally, we aim to centralize these integrations for improved oversight, addressing issues proactively rather than reactively as is often the case currently.&lt;/p&gt;\n\n&lt;p&gt;Our challenges are further compounded when dealing with modern web applications utilizing RESTAPI, especially with our legacy ERP. Typically, we resort to scripting PowerShell and requesting vendors to set up an SFTP server for XML file exchange, incurring additional costs for their extra efforts. To address these issues, we have explored integration platforms such as Mulesoft, Workato, and Boomi.&lt;/p&gt;\n\n&lt;p&gt;Boomi was discarded after practical testing due to perceived shortcomings. &lt;/p&gt;\n\n&lt;p&gt;Mulesoft proved comprehensible, enabling successful use cases, yet its deployment and maintenance seemed burdensome. &lt;/p&gt;\n\n&lt;p&gt;Workato, while lacking robust debugging and offering basic logging, demonstrated ease of use in creating integrations, aligning well with the simplicity of our integration needs. Its variety of connectors reduces the need for custom wrappers or connectors, and it presents automation possibilities.&lt;/p&gt;\n\n&lt;p&gt;Some of the use cases that I finalized in the 2 last tools are:&lt;br/&gt;\n- I was able to build an extensive integration between ERP and Salesforce, &lt;/p&gt;\n\n&lt;p&gt;- create an API based on openapi specs and send data between ERP and the web app&lt;/p&gt;\n\n&lt;p&gt;- do data transformation into an XML file with iterations &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The question at hand&lt;/strong&gt; is whether an Integration Platform as a Service (iPaaS) is a suitable solution for our scenario, given that we do not anticipate hiring an in-house developer. Additionally, are there other platforms or tools we might be overlooking? We are seeking insights from those with experience in the listed tools, with Workato emerging as our preferred choice due to its balance of simplicity and functionality, even though Mulesoft offers more robust features. The overarching question is whether the additional functionalities of Mulesoft are necessary for our specific requirements.&lt;/p&gt;\n\n&lt;p&gt;TL;DR: We&amp;#39;re a small-to-medium enterprise handling various applications with point-to-point interfaces and legacy systems. Two-person IT team, no in-house developers. Looking to rebuild integrations for a new ERP (probably Dynamics 365 F&amp;amp;O) by 2024. Currently considering Workato, Mulesoft, and Boomi. Workato seems suitable due to simplicity and variety of connectors. Wondering if Integration Platform as a Service (iPaaS) is the right solution for us and seeking insights on other tools or experiences with the mentioned platforms. Open to suggestions given our context and the absence of in-house developers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18gm0jn", "is_robot_indexable": true, "report_reasons": null, "author": "LangeHamburger", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gm0jn/question_about_ipaas_platforms_mulesoft_workato/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gm0jn/question_about_ipaas_platforms_mulesoft_workato/", "subreddit_subscribers": 145623, "created_utc": 1702388156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Doing the fundamentals of dbt course right now and came across the following:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/snw7lecr326c1.png?width=1838&amp;format=png&amp;auto=webp&amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400\n\n&amp;#x200B;\n\nThey specify a test in some documentation file (yaml).\n\nI wonder why is this done on this level and not on the level of sql constraints?\n\nI skimmed a thread a found a comment where [someone argued](https://www.reddit.com/r/dataengineering/comments/119s7yv/comment/j9ozeus/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) that dbt is good for when one can live (has) dirty data and hence has not enforced the level of data completeness as done with sql constraints.\n\n&amp;#x200B;\n\nWhat is the philosophy there?\n\nWhy would I even consider living with dirty data? Is my goal as an engineer not to keep data records clean and sort the completeness out on the level where transactions (i.e. updates) are handled?", "author_fullname": "t2_1b2msvdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "why testing in dbt? why live with dirty data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 47, "top_awarded_type": null, "hide_score": true, "media_metadata": {"snw7lecr326c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/snw7lecr326c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dcdfdcc4acf994e7dea6193fb88688aeaa8a0129"}, {"y": 74, "x": 216, "u": "https://preview.redd.it/snw7lecr326c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac27c1ed016e48de40ac4fb89df5d0f51400e6a5"}, {"y": 109, "x": 320, "u": "https://preview.redd.it/snw7lecr326c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=463eeaecda7aea123587ca48ce6aa9b67568bd79"}, {"y": 219, "x": 640, "u": "https://preview.redd.it/snw7lecr326c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ebcf00dce7fc5f281758d0ba1f223237238a4fb"}, {"y": 329, "x": 960, "u": "https://preview.redd.it/snw7lecr326c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ad4f1c57916a9c19c40e918897dd1f82e39449d"}, {"y": 370, "x": 1080, "u": "https://preview.redd.it/snw7lecr326c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=769012686b0bfed9e39949fe00466e3971b66dad"}], "s": {"y": 630, "x": 1838, "u": "https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;format=png&amp;auto=webp&amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400"}, "id": "snw7lecr326c1"}}, "name": "t3_18hfvkl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TEifI7eBZWLbs2l77WkdatkskdQfmD-Qq6dGuIAzlio.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702471054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doing the fundamentals of dbt course right now and came across the following:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400\"&gt;https://preview.redd.it/snw7lecr326c1.png?width=1838&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29cf9cafb2922f6bbe5c177381f90f9cea7d7400&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;They specify a test in some documentation file (yaml).&lt;/p&gt;\n\n&lt;p&gt;I wonder why is this done on this level and not on the level of sql constraints?&lt;/p&gt;\n\n&lt;p&gt;I skimmed a thread a found a comment where &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/119s7yv/comment/j9ozeus/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;someone argued&lt;/a&gt; that dbt is good for when one can live (has) dirty data and hence has not enforced the level of data completeness as done with sql constraints.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is the philosophy there?&lt;/p&gt;\n\n&lt;p&gt;Why would I even consider living with dirty data? Is my goal as an engineer not to keep data records clean and sort the completeness out on the level where transactions (i.e. updates) are handled?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18hfvkl", "is_robot_indexable": true, "report_reasons": null, "author": "HillTheBilly", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hfvkl/why_testing_in_dbt_why_live_with_dirty_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hfvkl/why_testing_in_dbt_why_live_with_dirty_data/", "subreddit_subscribers": 145623, "created_utc": 1702471054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,  \n\n\n**Context:**\n\nI have a bit of a situation in my company due to the lack of skills of our data engineering team and BI team. I am in the DS team and honestly we are incredibly ahead from them in terms of DevOps practices, software design patterns, cloud architectural knowledge etc....\n\nI am trying to setup a common standard based on polars+delta lake+sql server (serving layer for power BI) to develop fast and cheap data pipelines.\n\nWe are trying to avoid using spark in our data processing workflows as we do not have the neither the data size not the in-house competence (I am the only one who knows spark in a company of around 2k employees). The DS team is willing to learn pyspark (as usual) but I think polars will have a very low learning curve for them instead of spark. Also, polars integrates nicely with duckDB so it seems a middle ground that can make everyone happy.\n\n**Current setup:**\n\nThe data engineers here are very old-fashioned and they use MSSQL as a data warehouse instead of using it only as OLTP. Some on-premise servers are very well modelled with Kimball and the loads are kind of okay in terms of performance but the central DW is a bit of a mess. The DE and BI team is mostly skillful with SQL and have very very very basic python knowledge (I think over the 10 members only one is able to write a class), so I thought of introducing polars + delta lake to them instead of going the spark way.\n\nAs a matter of comparison I have done a rewrite of one of their processes using polars and delta lake and I am able to make a full load in approx 4mins vs their 3h (they are not aware of this and I am not planning to share it as I dont want to take the refactoring on my shoulders). \n\nI am aware that due to their SQL knowledge probably DBT will be a better option but no one has the knowledge of DBT and they are very stuck with .DACPAC files deployment (a side note here, we had to build their CI CD pipelines for both .dacpac files and ADF workflows as they were deploying .dacpac files directly from the editor to production, without any integration test for ingesting new data or fake data with ADF). Also, DBT integration with SQL Server seems very poor in comparison with PostgreSQL systems. Finally we use azure and in ADF the easier way that I found to run dbt continously (outside the CI CD pipelines) is via an invocation of container instances (no one in these teams know how to use docker).\n\n**Problem I am trying to solve:**\n\nHowever I am afraid that I will face a lot of friction as they have a lot of data quality checks written in SQL. I am trying to find an equivalent to GE, Pandera or Dequee in Polars to make them their journey a bit easier.\n\nDoes someone knows about such tools? So far I can only think on using polars and switch to pandas for quality checks but seems to kill quite a lot of the performance gains that Im getting with polars.\n\nIs this a good idea at all? Honestly I think that putting dagster/airflow in AKS and then use the integration with DBT will be better for their setup but we are not skilfull with DBT in my team and if we are going to help them we would like to use technologies that we are a bit familiar with as currently they do not have the capabilities to go outside SQL Server.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4aht7cg8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Equivalent of Great Expectations/Pandera/Dequee in Polars", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hdotc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702462468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a bit of a situation in my company due to the lack of skills of our data engineering team and BI team. I am in the DS team and honestly we are incredibly ahead from them in terms of DevOps practices, software design patterns, cloud architectural knowledge etc....&lt;/p&gt;\n\n&lt;p&gt;I am trying to setup a common standard based on polars+delta lake+sql server (serving layer for power BI) to develop fast and cheap data pipelines.&lt;/p&gt;\n\n&lt;p&gt;We are trying to avoid using spark in our data processing workflows as we do not have the neither the data size not the in-house competence (I am the only one who knows spark in a company of around 2k employees). The DS team is willing to learn pyspark (as usual) but I think polars will have a very low learning curve for them instead of spark. Also, polars integrates nicely with duckDB so it seems a middle ground that can make everyone happy.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The data engineers here are very old-fashioned and they use MSSQL as a data warehouse instead of using it only as OLTP. Some on-premise servers are very well modelled with Kimball and the loads are kind of okay in terms of performance but the central DW is a bit of a mess. The DE and BI team is mostly skillful with SQL and have very very very basic python knowledge (I think over the 10 members only one is able to write a class), so I thought of introducing polars + delta lake to them instead of going the spark way.&lt;/p&gt;\n\n&lt;p&gt;As a matter of comparison I have done a rewrite of one of their processes using polars and delta lake and I am able to make a full load in approx 4mins vs their 3h (they are not aware of this and I am not planning to share it as I dont want to take the refactoring on my shoulders). &lt;/p&gt;\n\n&lt;p&gt;I am aware that due to their SQL knowledge probably DBT will be a better option but no one has the knowledge of DBT and they are very stuck with .DACPAC files deployment (a side note here, we had to build their CI CD pipelines for both .dacpac files and ADF workflows as they were deploying .dacpac files directly from the editor to production, without any integration test for ingesting new data or fake data with ADF). Also, DBT integration with SQL Server seems very poor in comparison with PostgreSQL systems. Finally we use azure and in ADF the easier way that I found to run dbt continously (outside the CI CD pipelines) is via an invocation of container instances (no one in these teams know how to use docker).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem I am trying to solve:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;However I am afraid that I will face a lot of friction as they have a lot of data quality checks written in SQL. I am trying to find an equivalent to GE, Pandera or Dequee in Polars to make them their journey a bit easier.&lt;/p&gt;\n\n&lt;p&gt;Does someone knows about such tools? So far I can only think on using polars and switch to pandas for quality checks but seems to kill quite a lot of the performance gains that Im getting with polars.&lt;/p&gt;\n\n&lt;p&gt;Is this a good idea at all? Honestly I think that putting dagster/airflow in AKS and then use the integration with DBT will be better for their setup but we are not skilfull with DBT in my team and if we are going to help them we would like to use technologies that we are a bit familiar with as currently they do not have the capabilities to go outside SQL Server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18hdotc", "is_robot_indexable": true, "report_reasons": null, "author": "Lix021", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hdotc/equivalent_of_great_expectationspanderadequee_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hdotc/equivalent_of_great_expectationspanderadequee_in/", "subreddit_subscribers": 145623, "created_utc": 1702462468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The article shows you all the important differences between DuckDB/MotherDuck and other databases, including results from the first iteration of performance tests.  \n\n\n[https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5](https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5)", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is MotherDuck ProDUCKtion-Ready?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hdjry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702461903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The article shows you all the important differences between DuckDB/MotherDuck and other databases, including results from the first iteration of performance tests.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5\"&gt;https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?auto=webp&amp;s=464e982e8535936416c81b255d265e379a74453f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f22bd0b7a1eba0df9818d95987df2e90ff804a47", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a8808f867b2bc9b7afc8ee262e583033c0002a3f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=770ea05dd5058890d39eea27d143849e95b9e0aa", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b59f97e63fc83024129cf7a2e06b0c10401108aa", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9212e28c7f173a8df84f72500c4e1c5753b40e00", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/oYI3oqL44mavsShI5_lKc4btUA_xK8RPhIL_vdYdB1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d1558c54756703b248d71fde44619e7999fd4bd", "width": 1080, "height": 565}], "variants": {}, "id": "2XijoLGCZ5b01Blb6hT-fjIRZbscSJzrmSkZT1Q49_g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hdjry", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hdjry/is_motherduck_producktionready/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hdjry/is_motherduck_producktionready/", "subreddit_subscribers": 145623, "created_utc": 1702461903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is special about IoV data analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18hfg1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1702469492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "doris.apache.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://doris.apache.org/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18hfg1g", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hfg1g/what_is_special_about_iov_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://doris.apache.org/blog/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents", "subreddit_subscribers": 145623, "created_utc": 1702469492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a project to normalise existing production tables and I would like a tool or method to verify the data matches in the old denormalised tables vs the new normalised tables. \n\nThe plan is to create the new normalised tables. Modify the web app to populate new normalised tables alongside the existing denormalised tables. Then we can compare the results in each table with the long term goal of moving the web app over to the new tables and decommissioning the legacy denormalised tables.\n\nLittle example to explain:\n\nThis would be a denormalised table\n\n    CREATE TABLE Sales (\n        customer_id INT PRIMARY KEY,\n        customer_name VARCHAR(50),\n        customer_email VARCHAR(50),\n        order_id INT,\n        order_date DATE,\n        product_id INT,\n        product_name VARCHAR(50),\n        product_price DECIMAL(10, 2),\n        quantity INT\n    );\n\nNext we normalise the table\n\n\n    CREATE TABLE Customers (\n        customer_id INT PRIMARY KEY,\n        customer_name VARCHAR(50),\n        customer_email VARCHAR(50)\n    );\n\n    CREATE TABLE Products (\n        product_id INT PRIMARY KEY,\n        product_name VARCHAR(50),\n        product_price DECIMAL(10, 2)\n    );\n\n    CREATE TABLE Orders (\n        order_id INT PRIMARY KEY,\n        customer_id INT,\n        order_date DATE,\n        FOREIGN KEY (customer_id) REFERENCES Customers(customer_id)\n    );\n\n    CREATE TABLE OrderDetails (\n        order_id INT,\n        product_id INT,\n        quantity INT,\n        PRIMARY KEY (order_id, product_id),\n        FOREIGN KEY (order_id) REFERENCES Orders(order_id),\n        FOREIGN KEY (product_id) REFERENCES Products(product_id)\n    );\n\nAll tables will exist in production and be populated, as part of validating the data I would need to check that Sales.customer_email is equal to Customers.customer_email that Sales.product_name is equal to Products.product_name and so on for every column.\n\nWriting a unique SQL for each check would explode in complexity as we have many tables to compare.\n\nTo be clear I'm not talking about migrating data from the denormalised tables to the normalised tables the web app will populated both sets I wish to verify the data to spot mistakes in population between old and new tables.\n\nAny help or guidance is appreciated.", "author_fullname": "t2_1hj7ckkr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reconciling data in denormalised vs normalised tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18hdec0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702461267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a project to normalise existing production tables and I would like a tool or method to verify the data matches in the old denormalised tables vs the new normalised tables. &lt;/p&gt;\n\n&lt;p&gt;The plan is to create the new normalised tables. Modify the web app to populate new normalised tables alongside the existing denormalised tables. Then we can compare the results in each table with the long term goal of moving the web app over to the new tables and decommissioning the legacy denormalised tables.&lt;/p&gt;\n\n&lt;p&gt;Little example to explain:&lt;/p&gt;\n\n&lt;p&gt;This would be a denormalised table&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE Sales (\n    customer_id INT PRIMARY KEY,\n    customer_name VARCHAR(50),\n    customer_email VARCHAR(50),\n    order_id INT,\n    order_date DATE,\n    product_id INT,\n    product_name VARCHAR(50),\n    product_price DECIMAL(10, 2),\n    quantity INT\n);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Next we normalise the table&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE Customers (\n    customer_id INT PRIMARY KEY,\n    customer_name VARCHAR(50),\n    customer_email VARCHAR(50)\n);\n\nCREATE TABLE Products (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(50),\n    product_price DECIMAL(10, 2)\n);\n\nCREATE TABLE Orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date DATE,\n    FOREIGN KEY (customer_id) REFERENCES Customers(customer_id)\n);\n\nCREATE TABLE OrderDetails (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    PRIMARY KEY (order_id, product_id),\n    FOREIGN KEY (order_id) REFERENCES Orders(order_id),\n    FOREIGN KEY (product_id) REFERENCES Products(product_id)\n);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;All tables will exist in production and be populated, as part of validating the data I would need to check that Sales.customer_email is equal to Customers.customer_email that Sales.product_name is equal to Products.product_name and so on for every column.&lt;/p&gt;\n\n&lt;p&gt;Writing a unique SQL for each check would explode in complexity as we have many tables to compare.&lt;/p&gt;\n\n&lt;p&gt;To be clear I&amp;#39;m not talking about migrating data from the denormalised tables to the normalised tables the web app will populated both sets I wish to verify the data to spot mistakes in population between old and new tables.&lt;/p&gt;\n\n&lt;p&gt;Any help or guidance is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18hdec0", "is_robot_indexable": true, "report_reasons": null, "author": "OisinWard", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18hdec0/reconciling_data_in_denormalised_vs_normalised/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18hdec0/reconciling_data_in_denormalised_vs_normalised/", "subreddit_subscribers": 145623, "created_utc": 1702461267.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,  \n\n\nI'm in the first step of building a end-to-end data analytics solution, including design &amp; build data warehouse,  then push data to BI tool &amp; Visualize. This project is for job hunting (There's not many DE jobs for junior level in my area.) \n\nBut my concern is that...I might want to expand the project into a bigger one at later stages (such as scraping raw data, building Datalake solution or ETL, or migrating to cloud, etc.), so I want to start off correctly.   \n\n\nCould you give me the advice such as - the topic? the industry? any experience you think that might be useful?  \nThanks for reading. ", "author_fullname": "t2_80m5fstj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal project: To build a datawarehouse &amp; push data to BI platform (Tableau) &amp; Visualize. Ideas!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18h2zta", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702428108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the first step of building a end-to-end data analytics solution, including design &amp;amp; build data warehouse,  then push data to BI tool &amp;amp; Visualize. This project is for job hunting (There&amp;#39;s not many DE jobs for junior level in my area.) &lt;/p&gt;\n\n&lt;p&gt;But my concern is that...I might want to expand the project into a bigger one at later stages (such as scraping raw data, building Datalake solution or ETL, or migrating to cloud, etc.), so I want to start off correctly.   &lt;/p&gt;\n\n&lt;p&gt;Could you give me the advice such as - the topic? the industry? any experience you think that might be useful?&lt;br/&gt;\nThanks for reading. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18h2zta", "is_robot_indexable": true, "report_reasons": null, "author": "Solid-Exchange-8447", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18h2zta/personal_project_to_build_a_datawarehouse_push/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18h2zta/personal_project_to_build_a_datawarehouse_push/", "subreddit_subscribers": 145623, "created_utc": 1702428108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I posted this in IndieDev at first until I realised in my last sentence that this is fundamentally, I think, a data engineering issue.\n\nI'm working alongside a dev team who have a game published in the appstore with a decent user base and organic traffic. Their need for analytics has outgrown what's available in the Google Play Console and I've volunteered to help out.\n\nThe problem is that I've no idea how to extract, or even access, more data than what's in the dashboard. How do I query all of the data google must keep about the game and its users? And if Google doesn't keep all that much data, how can I set up a data pipeline to start this?\n\nMy experience is in analytics rather than data engineering so any help is much appreciated.", "author_fullname": "t2_m27f9f79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I query data / setup a data pipeline for my Google Playstore App", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gzzxc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702422552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted this in IndieDev at first until I realised in my last sentence that this is fundamentally, I think, a data engineering issue.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working alongside a dev team who have a game published in the appstore with a decent user base and organic traffic. Their need for analytics has outgrown what&amp;#39;s available in the Google Play Console and I&amp;#39;ve volunteered to help out.&lt;/p&gt;\n\n&lt;p&gt;The problem is that I&amp;#39;ve no idea how to extract, or even access, more data than what&amp;#39;s in the dashboard. How do I query all of the data google must keep about the game and its users? And if Google doesn&amp;#39;t keep all that much data, how can I set up a data pipeline to start this?&lt;/p&gt;\n\n&lt;p&gt;My experience is in analytics rather than data engineering so any help is much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18gzzxc", "is_robot_indexable": true, "report_reasons": null, "author": "TommyGunQuartet", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gzzxc/how_can_i_query_data_setup_a_data_pipeline_for_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gzzxc/how_can_i_query_data_setup_a_data_pipeline_for_my/", "subreddit_subscribers": 145623, "created_utc": 1702422552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Yall,   \n\n\nI want to move off of azure synapse, and im trying to build a proof of concept for upper leadership to show them. As im getting into Databricks i feel like i really dove into the deep end here!  \n\n\nCouple of questions I have:  \nRegarding compute, what are some best practices here. How do you guys manage your compute clusters? E.g. right now as I'm building a PoC, i just have a personal compute. Is that going to be sufficient for having automated jobs etc setup in databricks? On that note, I have some python code i was running in azure batch i would like to port over. Essentially the code just queries some APIs and writes the JSON to the datalake. Is this something i can use databricks for instead?   \n\n\nSpeaking of compute, I obviously dont want a compute cluster running all the time, right? But it seems if i create a view from a dataframe in spark, that dataframe is temporary. What do you guys use to integrate everything youre doing in databricks into your BI tools (this case im looking at powerbi)? I'm looking at SQL warehouses, but thats even more DBUs and im afraid im going to incur quite a bit of costs. I want to optimize costs as much as possible I'm im worried that databricks is going to rob me in compute costs lol if i start spinning up all these fancy tools. Would love to hear some best practices here. \n\n  \nI hear a lot of marketing buzzwordyness on delta live tables, bronze silver gold yada yada. Do i need to buy into the delta live table stuff in databricks? or is this something i can manage with code on my own? I guess im just confused how this all fits together.   \n", "author_fullname": "t2_jrmn04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting started with databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gxsdq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702418500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Yall,   &lt;/p&gt;\n\n&lt;p&gt;I want to move off of azure synapse, and im trying to build a proof of concept for upper leadership to show them. As im getting into Databricks i feel like i really dove into the deep end here!  &lt;/p&gt;\n\n&lt;p&gt;Couple of questions I have:&lt;br/&gt;\nRegarding compute, what are some best practices here. How do you guys manage your compute clusters? E.g. right now as I&amp;#39;m building a PoC, i just have a personal compute. Is that going to be sufficient for having automated jobs etc setup in databricks? On that note, I have some python code i was running in azure batch i would like to port over. Essentially the code just queries some APIs and writes the JSON to the datalake. Is this something i can use databricks for instead?   &lt;/p&gt;\n\n&lt;p&gt;Speaking of compute, I obviously dont want a compute cluster running all the time, right? But it seems if i create a view from a dataframe in spark, that dataframe is temporary. What do you guys use to integrate everything youre doing in databricks into your BI tools (this case im looking at powerbi)? I&amp;#39;m looking at SQL warehouses, but thats even more DBUs and im afraid im going to incur quite a bit of costs. I want to optimize costs as much as possible I&amp;#39;m im worried that databricks is going to rob me in compute costs lol if i start spinning up all these fancy tools. Would love to hear some best practices here. &lt;/p&gt;\n\n&lt;p&gt;I hear a lot of marketing buzzwordyness on delta live tables, bronze silver gold yada yada. Do i need to buy into the delta live table stuff in databricks? or is this something i can manage with code on my own? I guess im just confused how this all fits together.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18gxsdq", "is_robot_indexable": true, "report_reasons": null, "author": "soricellia", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gxsdq/getting_started_with_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gxsdq/getting_started_with_databricks/", "subreddit_subscribers": 145623, "created_utc": 1702418500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. So I was just recently accepted into a sponsored program to learn data engineering. This programs is going to run for the entire of next year with the last 3 months being an internship with the company that will be sponsoring me. \n\nThis is something that I find exciting. However, even though the sponsoring company does give us an allowance - this won\u2019t be enough to live off of. So I was wondering if I am unrealistic for thinking that I could maybe try to find a remote internship or part time job in data as I\u2019m learning?\n\nIt is said that the program will require 40-50 hours a week. That is why I would like to find something not entirely full time. I\u2019ve heard of some people being able to do this while working full time though. \n\nIf it is indeed possible and not unrealistic - how would I go about finding something like that (especially in South Africa)? \n\nAny advice would be greatly appreciated.", "author_fullname": "t2_d54pk8w0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How common are data engineering internships/part time jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gx9yb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702417410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. So I was just recently accepted into a sponsored program to learn data engineering. This programs is going to run for the entire of next year with the last 3 months being an internship with the company that will be sponsoring me. &lt;/p&gt;\n\n&lt;p&gt;This is something that I find exciting. However, even though the sponsoring company does give us an allowance - this won\u2019t be enough to live off of. So I was wondering if I am unrealistic for thinking that I could maybe try to find a remote internship or part time job in data as I\u2019m learning?&lt;/p&gt;\n\n&lt;p&gt;It is said that the program will require 40-50 hours a week. That is why I would like to find something not entirely full time. I\u2019ve heard of some people being able to do this while working full time though. &lt;/p&gt;\n\n&lt;p&gt;If it is indeed possible and not unrealistic - how would I go about finding something like that (especially in South Africa)? &lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18gx9yb", "is_robot_indexable": true, "report_reasons": null, "author": "Fearless_Jicama2909", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gx9yb/how_common_are_data_engineering_internshipspart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gx9yb/how_common_are_data_engineering_internshipspart/", "subreddit_subscribers": 145623, "created_utc": 1702417410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nDoes anyone have any recommendations for data dictionary software? I've used excel before, but really want something more interactive!\n\nThanks", "author_fullname": "t2_674ibeps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data dictionaries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gupwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702410846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any recommendations for data dictionary software? I&amp;#39;ve used excel before, but really want something more interactive!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18gupwv", "is_robot_indexable": true, "report_reasons": null, "author": "atrifleamused", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gupwv/data_dictionaries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gupwv/data_dictionaries/", "subreddit_subscribers": 145623, "created_utc": 1702410846.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}