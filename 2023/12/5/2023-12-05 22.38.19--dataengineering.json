{"kind": "Listing", "data": {"after": "t3_18bgn7k", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w6z0w1b6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg tables are now available in public preview on snowflake \ud83e\uddca", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_18ax35n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8QBnsA-udToCB0eUxpLWu_nlmjnY3DOdPUmQkHt1hPY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701731120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "snowflake.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.snowflake.com/blog/build-open-data-lakehouse-iceberg-tables/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WSCbrQ9Y5xiwCJFn0EzLW4rGqiohXF2V0W4MOUN541U.jpg?auto=webp&amp;s=20e99a7370b8b8d6920e58fcea529e63ddb0530c", "width": 1200, "height": 686}, "resolutions": [{"url": "https://external-preview.redd.it/WSCbrQ9Y5xiwCJFn0EzLW4rGqiohXF2V0W4MOUN541U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ba6f073abce05b583d40832800ee8633cc8b923", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/WSCbrQ9Y5xiwCJFn0EzLW4rGqiohXF2V0W4MOUN541U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=788c5f7fde7d0efab8bb98c5833e6604080240f3", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/WSCbrQ9Y5xiwCJFn0EzLW4rGqiohXF2V0W4MOUN541U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd83a37ecce085187c095f8e276633570c605b4c", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/WSCbrQ9Y5xiwCJFn0EzLW4rGqiohXF2V0W4MOUN541U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b485bb8f85f945da7ea3f04936788b22fc673cf9", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/WSCbrQ9Y5xiwCJFn0EzLW4rGqiohXF2V0W4MOUN541U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdc7f824cb1d7f8fb4dfac7b43fb9adddb398c7d", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/WSCbrQ9Y5xiwCJFn0EzLW4rGqiohXF2V0W4MOUN541U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ae8c57ab613f24b8a4e6dd94faae1b9429c78a6", "width": 1080, "height": 617}], "variants": {}, "id": "wZU6TOAsdmNumIaPYZLsxAjPhKz2fqFiv4S_j_QcAw8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ax35n", "is_robot_indexable": true, "report_reasons": null, "author": "sdc-msimon", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ax35n/iceberg_tables_are_now_available_in_public/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.snowflake.com/blog/build-open-data-lakehouse-iceberg-tables/", "subreddit_subscribers": 144008, "created_utc": 1701731120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the [Metabase Community Data Stack Report](https://www.metabase.com/data-stack-report-2023#data-ingestion-in-house) 31% of responders said they're using in-house ingestion. \n\nWhy do companies still build data ingestion tooling instead of using third-party tools? Wouldn\u2019t it be more expensive, in terms of cost and time, to engineer and maintain your own ingestion pipeline? ", "author_fullname": "t2_e5fjdth0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do companies still build data ingestion tooling instead of using a third-party tool like Airbyte?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bi2xj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701799473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the &lt;a href=\"https://www.metabase.com/data-stack-report-2023#data-ingestion-in-house\"&gt;Metabase Community Data Stack Report&lt;/a&gt; 31% of responders said they&amp;#39;re using in-house ingestion. &lt;/p&gt;\n\n&lt;p&gt;Why do companies still build data ingestion tooling instead of using third-party tools? Wouldn\u2019t it be more expensive, in terms of cost and time, to engineer and maintain your own ingestion pipeline? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?auto=webp&amp;s=a918001adbcbdf0507b477cb314c79c6fa6890b2", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=beab2fb16fa06d31be725b3c39e0cebf62af8b6a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c0012b27a40b3722d7239204936cbc7d8a0f0ac", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1430fd4f043f9832a6f3740c48f801c8d56c6fb7", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e76e9eb0b2f08f73ef01612cdc9b63d7b480f92", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d75bc2370d5dde94891301fb285d00362f9374fe", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=07791994b1d6632f6a5c11d1fed9a8e6481756a5", "width": 1080, "height": 564}], "variants": {}, "id": "ftoBvI6rzzCZb3wJpMIOqE5SrDrO0vg2LvvVJ6z3_GE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bi2xj", "is_robot_indexable": true, "report_reasons": null, "author": "Miserable_Fold4086", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bi2xj/why_do_companies_still_build_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bi2xj/why_do_companies_still_build_data_ingestion/", "subreddit_subscribers": 144008, "created_utc": 1701799473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_n0ywlxbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-Time ML: A Collection of 300+ Case Studies, each tagged properly and accompanied by engineering blogs for a deep dive. Personally, I am exploring these to understand not just the ML models but also the data infra supporting these systems, highlighting the growing significance of streaming.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18b9q56", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hqOeBTd9REnubNF9BjnEsrwznp8PPKnkQABYRNP8YYg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701774737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "evidentlyai.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.evidentlyai.com/ml-system-design", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?auto=webp&amp;s=2a960a10bcb94146d03fd9340e48df7eeee3578d", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=300ba58fa86e509ec4080507dc1a7185c3a2fefb", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=592303c0d3d63b69a1c3b66a7964b3804c7846f3", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a770c418fa19a60c0fde7b56a23aa502937db490", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a60fa146ad2523066d0edf58146b142f1c8ee5c", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=71b854a40b256c24dbd38099c7b7b1742373afc9", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96190d3785e9efa4720c498c90523d2c4836e9db", "width": 1080, "height": 564}], "variants": {}, "id": "jqoclQmWoAxFYjwbGE7GQt9lG-yEiYAl5EtPWQXQK84"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18b9q56", "is_robot_indexable": true, "report_reasons": null, "author": "muditjps", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b9q56/realtime_ml_a_collection_of_300_case_studies_each/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.evidentlyai.com/ml-system-design", "subreddit_subscribers": 144008, "created_utc": 1701774737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to get a sense of how data coming out of a graph database might be used. I've read the common use cases of Netflix's list of recommendations based on a user's history, and also Amazon's \"people also bought\" list, but what are some more business or internal use cases for data coming out of a graph database? Would it make sense to output data to some basic text file? Excel spreadsheet? I realize this is a broad question, but I'm intentionally leaving it at that to hear some practical examples.", "author_fullname": "t2_2pyy4c8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is data used downstream from graph databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b007u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701739118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to get a sense of how data coming out of a graph database might be used. I&amp;#39;ve read the common use cases of Netflix&amp;#39;s list of recommendations based on a user&amp;#39;s history, and also Amazon&amp;#39;s &amp;quot;people also bought&amp;quot; list, but what are some more business or internal use cases for data coming out of a graph database? Would it make sense to output data to some basic text file? Excel spreadsheet? I realize this is a broad question, but I&amp;#39;m intentionally leaving it at that to hear some practical examples.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18b007u", "is_robot_indexable": true, "report_reasons": null, "author": "opabm", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b007u/how_is_data_used_downstream_from_graph_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b007u/how_is_data_used_downstream_from_graph_databases/", "subreddit_subscribers": 144008, "created_utc": 1701739118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a scenario where we are receiving information on a Kafka Topic and are decomposing the complex message structure and storing the records to a relational DB. We originally tried using jq to decompose the json messages but we are not seeing good speeds. \n\nIn your opinion is it possible to have a performant pipeline in Python or will Java be be required for such a task?", "author_fullname": "t2_5ts1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Near Real Time Ingestion to DB using Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18be34k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701789322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a scenario where we are receiving information on a Kafka Topic and are decomposing the complex message structure and storing the records to a relational DB. We originally tried using jq to decompose the json messages but we are not seeing good speeds. &lt;/p&gt;\n\n&lt;p&gt;In your opinion is it possible to have a performant pipeline in Python or will Java be be required for such a task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18be34k", "is_robot_indexable": true, "report_reasons": null, "author": "lspdv18", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18be34k/near_real_time_ingestion_to_db_using_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18be34k/near_real_time_ingestion_to_db_using_python/", "subreddit_subscribers": 144008, "created_utc": 1701789322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As title...\n\nI've been building [phidata](https://github.com/phidatahq/phidata)'s Junior DE for automating day-to-day DE tasks using GPT4. Its not perfect but helps me bootstrap python code for data analysis, which saves me an hour or so here and there. \n\nI'm reasonable pleased with it. Will give it a 6/10 in current form.\n\nWhat parts do you think will be automated?\n\nhttps://reddit.com/link/18aw6bb/video/and4vbsssc4c1/player", "author_fullname": "t2_insol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What parts of Data Engineering do you think will be automated by AI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"and4vbsssc4c1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/18aw6bb/asset/and4vbsssc4c1/DASHPlaylist.mpd?a=1704407899%2CNWQ1ZGJkZTdmNjY1ODM3ODlkODg1NWJmMDAyMGJkZTZjMDM0OTViN2VjM2NhMTYxYTc1MTQ1NjJjMDM1ZGEwOQ%3D%3D&amp;v=1&amp;f=sd", "x": 920, "y": 720, "hlsUrl": "https://v.redd.it/link/18aw6bb/asset/and4vbsssc4c1/HLSPlaylist.m3u8?a=1704407899%2COWM2ZjhhMzE1ZmU3ZjRmYjE1M2RjNzhkODA2NTYxMGJiZWI2MTFmYzJiMjcyMjBjZDYwZjIwZjM0NDhhYTZlNw%3D%3D&amp;v=1&amp;f=sd", "id": "and4vbsssc4c1", "isGif": false}}, "name": "t3_18aw6bb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/P96diACr2_5ACTc6_SFr53NOQSljnqGYEDuUlUBmd8E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1701728739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As title...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been building &lt;a href=\"https://github.com/phidatahq/phidata\"&gt;phidata&lt;/a&gt;&amp;#39;s Junior DE for automating day-to-day DE tasks using GPT4. Its not perfect but helps me bootstrap python code for data analysis, which saves me an hour or so here and there. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m reasonable pleased with it. Will give it a 6/10 in current form.&lt;/p&gt;\n\n&lt;p&gt;What parts do you think will be automated?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/18aw6bb/video/and4vbsssc4c1/player\"&gt;https://reddit.com/link/18aw6bb/video/and4vbsssc4c1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/43ft09hfuQbzzTwWTE51wbLU60C-Wq25guYmFyG9rFI.jpg?auto=webp&amp;s=3b33a7bc1d4bb4394cfdcfa9e819c7af924106a2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/43ft09hfuQbzzTwWTE51wbLU60C-Wq25guYmFyG9rFI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=00f8510c0039797d3db0d4b49f82b4442142bd3e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/43ft09hfuQbzzTwWTE51wbLU60C-Wq25guYmFyG9rFI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f9f3db15f93db82ba553778db56146598bfba702", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/43ft09hfuQbzzTwWTE51wbLU60C-Wq25guYmFyG9rFI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=264e7a86482015aded8c8ef4e885ae3ae371c51e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/43ft09hfuQbzzTwWTE51wbLU60C-Wq25guYmFyG9rFI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccd99990ab68f3f05a211a3c8127c6946425f05e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/43ft09hfuQbzzTwWTE51wbLU60C-Wq25guYmFyG9rFI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4ba77d75811d189df8a369e7e414398a4cfadea4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/43ft09hfuQbzzTwWTE51wbLU60C-Wq25guYmFyG9rFI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9460a19b3956c7e8e8ba59caf29fe17075abec6", "width": 1080, "height": 540}], "variants": {}, "id": "CzyTjY7RABDsSOx8cm-L6WKDQv-aNl6WY22M1hVTW9E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18aw6bb", "is_robot_indexable": true, "report_reasons": null, "author": "ashpreetbedi", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18aw6bb/what_parts_of_data_engineering_do_you_think_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18aw6bb/what_parts_of_data_engineering_do_you_think_will/", "subreddit_subscribers": 144008, "created_utc": 1701728739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Every year, the data industry finds a new buzzword to latch onto. This year, I feel like everyone was focused on the idea of data \"activation.\"\n\nWhat are the new trends/buzzwords that the data industry will lean into this year? ", "author_fullname": "t2_nypbpupk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data trends for 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bhf64", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701797755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every year, the data industry finds a new buzzword to latch onto. This year, I feel like everyone was focused on the idea of data &amp;quot;activation.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;What are the new trends/buzzwords that the data industry will lean into this year? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bhf64", "is_robot_indexable": true, "report_reasons": null, "author": "Techxpeare", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bhf64/data_trends_for_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bhf64/data_trends_for_2024/", "subreddit_subscribers": 144008, "created_utc": 1701797755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked with traditional ETL tools for past 5 years (Talend/ SSIS) and recently Azure DataFactory/ LogicApps.\n\nWhats the best way to learn about DataBricks? There is a potential that my next project may be using DataBricks\n\n&amp;#x200B;\n\nI also aim to get  **Databricks Certified Data Engineer Associate/ Professional certification**  in the next few months.\n\n&amp;#x200B;\n\nWhats the best pathway/ resources? I have some free time at hand and would love to see recommendations for courses with practice/ projects \n\n&amp;#x200B;\n\nThanks in advance ", "author_fullname": "t2_yok0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learn DataBricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b2d8e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701746174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked with traditional ETL tools for past 5 years (Talend/ SSIS) and recently Azure DataFactory/ LogicApps.&lt;/p&gt;\n\n&lt;p&gt;Whats the best way to learn about DataBricks? There is a potential that my next project may be using DataBricks&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I also aim to get  &lt;strong&gt;Databricks Certified Data Engineer Associate/ Professional certification&lt;/strong&gt;  in the next few months.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Whats the best pathway/ resources? I have some free time at hand and would love to see recommendations for courses with practice/ projects &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18b2d8e", "is_robot_indexable": true, "report_reasons": null, "author": "nodonaldplease", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b2d8e/learn_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b2d8e/learn_databricks/", "subreddit_subscribers": 144008, "created_utc": 1701746174.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just started my data career as DataOps, 5 months ago. I just learned excel, SQL, and Python barely a year in and Im already being pushed to be the acting data engineer for a big backend dev project. The project was forced on me because the old backend which I was originally tasked to maintain is not flexible to the files provided. Altho it is an opportunity, it is stressful since the backend needs to deliver monthly data submissions. I am pressured to develop the new backend and  submit data every month. Ive been forced to work during the wkends and OT for the past 2-3wks because of this project. Worst of all the data engineers given me for assistance are new hires and I need to guide them in every process of the development (ALL AS A NEW DATA OPS). Please send advice and prayers. Thanks.", "author_fullname": "t2_ie84p724", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Off my chest: Is this normal?? Please send advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b7m3o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701765384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just started my data career as DataOps, 5 months ago. I just learned excel, SQL, and Python barely a year in and Im already being pushed to be the acting data engineer for a big backend dev project. The project was forced on me because the old backend which I was originally tasked to maintain is not flexible to the files provided. Altho it is an opportunity, it is stressful since the backend needs to deliver monthly data submissions. I am pressured to develop the new backend and  submit data every month. Ive been forced to work during the wkends and OT for the past 2-3wks because of this project. Worst of all the data engineers given me for assistance are new hires and I need to guide them in every process of the development (ALL AS A NEW DATA OPS). Please send advice and prayers. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18b7m3o", "is_robot_indexable": true, "report_reasons": null, "author": "awKnoodle", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b7m3o/off_my_chest_is_this_normal_please_send_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b7m3o/off_my_chest_is_this_normal_please_send_advice/", "subreddit_subscribers": 144008, "created_utc": 1701765384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm newish to data engineering/analytics and working for a public health non-profit that is getting height and weight measurements across the population, and want to apply survey weightings to provide more accurate obesity rates and mean BMIs. I have some dev experience but only picked up a couple of stats subjects in uni and we don't have much in-house expertise yet. Can you please let me know if the following approach sounds reasonable?\n\nI'm in AWS, so S3, Glue/PySpark, and Athena. Observation data streams into parquet buckets, and I've constructed a parquet view combining data from the last few census years (2021, 2016, 2011) which is the period measurements have been taken. This gives the population counts for each population 'cell', e.g. permutation of geography (postcode in this case), sex, and age bucket:\n\ncell | year | age_bucket | sex | postcode | population\n:--|--:|--:|:--|:--|--:\n1036 | 2021 | 25-30 | Male | 2000 |  4,390\n\nMy intended next steps are to take the height and weight observation data and create a user/period-keyed summary table (since a user may have multiple observations in a period, we'll take the last). I expect to create this as a materialised parquet table incrementally updated nightly.\n\nuser | year | height | weight | age_bucket | sex | postcode\n:---|--:|--:|--:|--:|:--|:--\na89bf96e | 2021| 180 | 80 | 25-30 | Male | 2000\n\nFrom the **user** and **cell** tables above, I can count the number of observations for a given cell/year, and take the ratio between that and the population to get a simple survey weight. I would then take the survey weight and multiply through to infer the actual population that is or is not obese, for example. I expected I should do this join and calculation at query time in Athena (which would be how the data would be pulled into BI tools or web views).\n\nHowever I also want to also attach a relative standard error (RSE) to each measure estimate, which may be beyond Athena and require regenerating a summary table using spark each night. My broad approach here was going to be the following, given the joined table of users with cell weights calculated and addressing the convenience sampling via a design effect factor:\n\n\n    # get the total population\n    total_population = data['population'].sum()\n    \n    # get the weighted estimate\n    weighted_bmi_estimate = (data['survey_weight'] * data['bmi']).sum() / total_population\n    \n    # work out the design effect by comparing simple (SRS) and complex variance measures\n    \n    mean_bmi = data['bmi'].mean()\n    simple_variance = ((data['bmi'] - mean_bmi)**2).sum() / (total_population - 1)\n    \n    data['weighted_deviance'] = data['survey_weight'] * (data['bmi'] - weighted_bmi_estimate)**2\n    complex_variance = data['weighted_deviance'].sum() / total_population\n    \n    deff = complex_variance / simple_variance\n    \n    # use the deff to get the rse for the bmi estimate\n    rse = (deff * simple_variance)**0.5\n    \n    # NOTE: when doing an obesity _rate_ instead of BMI we'd use a different calc for the simple_variance\n\n\nTbh I could probably do the RSE for a single measure in Athena like this, but spark seems like it'd be better if I want to generalise to different measures and cell dimensions and then not worry if downstream consumers hit the table too frequently.\n\nFurther thoughts:\n\n- I will need to interpolate population counts for non-census years or otherwise just take the closest\n- Though I haven't yet used delta/hudi/iceberg table formats I think the user table may be a good use-case for it\n- I'll need to handle summation across cells when weighting by only a subset of the cell dimensions \n- To handle distortion from small counts, weight trimming seemed like the simplest solution but I'm unsure if any adjustments are necessary when determining RSEs\n- I'm not sure if there are any native statistical functions I should use instead when converting the above pandas to pyspark, or any efficiency issues to watch out for (e.g. using `approxQuantile` to set the weight caps, broadcasting my means and total_populations)\n\nThank you!", "author_fullname": "t2_i0a3z0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please sense-check my approach for applying survey weights and RSEs in spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b11wb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701744779.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701742215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m newish to data engineering/analytics and working for a public health non-profit that is getting height and weight measurements across the population, and want to apply survey weightings to provide more accurate obesity rates and mean BMIs. I have some dev experience but only picked up a couple of stats subjects in uni and we don&amp;#39;t have much in-house expertise yet. Can you please let me know if the following approach sounds reasonable?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in AWS, so S3, Glue/PySpark, and Athena. Observation data streams into parquet buckets, and I&amp;#39;ve constructed a parquet view combining data from the last few census years (2021, 2016, 2011) which is the period measurements have been taken. This gives the population counts for each population &amp;#39;cell&amp;#39;, e.g. permutation of geography (postcode in this case), sex, and age bucket:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;cell&lt;/th&gt;\n&lt;th align=\"right\"&gt;year&lt;/th&gt;\n&lt;th align=\"right\"&gt;age_bucket&lt;/th&gt;\n&lt;th align=\"left\"&gt;sex&lt;/th&gt;\n&lt;th align=\"left\"&gt;postcode&lt;/th&gt;\n&lt;th align=\"right\"&gt;population&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1036&lt;/td&gt;\n&lt;td align=\"right\"&gt;2021&lt;/td&gt;\n&lt;td align=\"right\"&gt;25-30&lt;/td&gt;\n&lt;td align=\"left\"&gt;Male&lt;/td&gt;\n&lt;td align=\"left\"&gt;2000&lt;/td&gt;\n&lt;td align=\"right\"&gt;4,390&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;My intended next steps are to take the height and weight observation data and create a user/period-keyed summary table (since a user may have multiple observations in a period, we&amp;#39;ll take the last). I expect to create this as a materialised parquet table incrementally updated nightly.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;user&lt;/th&gt;\n&lt;th align=\"right\"&gt;year&lt;/th&gt;\n&lt;th align=\"right\"&gt;height&lt;/th&gt;\n&lt;th align=\"right\"&gt;weight&lt;/th&gt;\n&lt;th align=\"right\"&gt;age_bucket&lt;/th&gt;\n&lt;th align=\"left\"&gt;sex&lt;/th&gt;\n&lt;th align=\"left\"&gt;postcode&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;a89bf96e&lt;/td&gt;\n&lt;td align=\"right\"&gt;2021&lt;/td&gt;\n&lt;td align=\"right\"&gt;180&lt;/td&gt;\n&lt;td align=\"right\"&gt;80&lt;/td&gt;\n&lt;td align=\"right\"&gt;25-30&lt;/td&gt;\n&lt;td align=\"left\"&gt;Male&lt;/td&gt;\n&lt;td align=\"left\"&gt;2000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;From the &lt;strong&gt;user&lt;/strong&gt; and &lt;strong&gt;cell&lt;/strong&gt; tables above, I can count the number of observations for a given cell/year, and take the ratio between that and the population to get a simple survey weight. I would then take the survey weight and multiply through to infer the actual population that is or is not obese, for example. I expected I should do this join and calculation at query time in Athena (which would be how the data would be pulled into BI tools or web views).&lt;/p&gt;\n\n&lt;p&gt;However I also want to also attach a relative standard error (RSE) to each measure estimate, which may be beyond Athena and require regenerating a summary table using spark each night. My broad approach here was going to be the following, given the joined table of users with cell weights calculated and addressing the convenience sampling via a design effect factor:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# get the total population\ntotal_population = data[&amp;#39;population&amp;#39;].sum()\n\n# get the weighted estimate\nweighted_bmi_estimate = (data[&amp;#39;survey_weight&amp;#39;] * data[&amp;#39;bmi&amp;#39;]).sum() / total_population\n\n# work out the design effect by comparing simple (SRS) and complex variance measures\n\nmean_bmi = data[&amp;#39;bmi&amp;#39;].mean()\nsimple_variance = ((data[&amp;#39;bmi&amp;#39;] - mean_bmi)**2).sum() / (total_population - 1)\n\ndata[&amp;#39;weighted_deviance&amp;#39;] = data[&amp;#39;survey_weight&amp;#39;] * (data[&amp;#39;bmi&amp;#39;] - weighted_bmi_estimate)**2\ncomplex_variance = data[&amp;#39;weighted_deviance&amp;#39;].sum() / total_population\n\ndeff = complex_variance / simple_variance\n\n# use the deff to get the rse for the bmi estimate\nrse = (deff * simple_variance)**0.5\n\n# NOTE: when doing an obesity _rate_ instead of BMI we&amp;#39;d use a different calc for the simple_variance\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Tbh I could probably do the RSE for a single measure in Athena like this, but spark seems like it&amp;#39;d be better if I want to generalise to different measures and cell dimensions and then not worry if downstream consumers hit the table too frequently.&lt;/p&gt;\n\n&lt;p&gt;Further thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I will need to interpolate population counts for non-census years or otherwise just take the closest&lt;/li&gt;\n&lt;li&gt;Though I haven&amp;#39;t yet used delta/hudi/iceberg table formats I think the user table may be a good use-case for it&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ll need to handle summation across cells when weighting by only a subset of the cell dimensions &lt;/li&gt;\n&lt;li&gt;To handle distortion from small counts, weight trimming seemed like the simplest solution but I&amp;#39;m unsure if any adjustments are necessary when determining RSEs&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m not sure if there are any native statistical functions I should use instead when converting the above pandas to pyspark, or any efficiency issues to watch out for (e.g. using &lt;code&gt;approxQuantile&lt;/code&gt; to set the weight caps, broadcasting my means and total_populations)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18b11wb", "is_robot_indexable": true, "report_reasons": null, "author": "sansampersamp", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b11wb/please_sensecheck_my_approach_for_applying_survey/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b11wb/please_sensecheck_my_approach_for_applying_survey/", "subreddit_subscribers": 144008, "created_utc": 1701742215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just finished putting together a project trying to demonstrate some knowledge of Terraform, Dataproc, and Pyspark.\n\nHave a look here, and let me know your thoughts:  things to improve, things you think I did well here, random insults, whatever :).\n\nGithub repo:  [https://github.com/Kaizen91/spark-housing-market-canada](https://github.com/Kaizen91/spark-housing-market-canada)\n\n[Dataflow](https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;format=png&amp;auto=webp&amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a)\n\nThe main.tf terraform file will create all the infrastructure needed for this pipeline: a Google Cloud Storage bucket, a Dataproc cluster, a Dataproc job, and a BigQuery dataset. It will also upload the source csv file and the transform.py script to the Google Cloud Storage bucket, so that they can be accessed by the Dataproc Pyspark Job running on the Dataproc cluster.", "author_fullname": "t2_es03w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline using Dataproc and Pyspark to Build a BigQuery dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nt0l632b0j4c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f71b3510ccf7e37efd8e896e9cc6bafafdb672df"}, {"y": 142, "x": 216, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a08da1ce9b88c07ac0c18ff6c742311f7ff4abf3"}, {"y": 211, "x": 320, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=559d0cfe801beca97c01548ced39eead77ee7e92"}, {"y": 423, "x": 640, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eda773773344d5f542ddf98debc57227d964032"}, {"y": 635, "x": 960, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16fd81d560dfa3c80c582dac0643d02a902131c9"}, {"y": 714, "x": 1080, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5144c0d9914948a9bafabf8e6026849538311d76"}], "s": {"y": 891, "x": 1346, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;format=png&amp;auto=webp&amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a"}, "id": "nt0l632b0j4c1"}}, "name": "t3_18bjrw6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KzeSekC5wRmyiK6QYlsfiFanFErH26SsToOv35Ig4vg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1701803857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just finished putting together a project trying to demonstrate some knowledge of Terraform, Dataproc, and Pyspark.&lt;/p&gt;\n\n&lt;p&gt;Have a look here, and let me know your thoughts:  things to improve, things you think I did well here, random insults, whatever :).&lt;/p&gt;\n\n&lt;p&gt;Github repo:  &lt;a href=\"https://github.com/Kaizen91/spark-housing-market-canada\"&gt;https://github.com/Kaizen91/spark-housing-market-canada&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a\"&gt;Dataflow&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The main.tf terraform file will create all the infrastructure needed for this pipeline: a Google Cloud Storage bucket, a Dataproc cluster, a Dataproc job, and a BigQuery dataset. It will also upload the source csv file and the transform.py script to the Google Cloud Storage bucket, so that they can be accessed by the Dataproc Pyspark Job running on the Dataproc cluster.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?auto=webp&amp;s=6d7a27fb64915a5965dae69c1e99d9244f9e59b7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9860095d4a53941571b8cc81fc35924502222ea4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=001de624dab65e9800d86afa7a4ceb81ddbb5315", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d413903a6dc8a57f70ae18bf31d541e1a32cea54", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=22c2910f2dda528aa033c6586204b79aa3b67e2b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd3fd7060d5aacc57186140e857cc85d8074f764", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e7f868569092adbf738725c62b3f4d9e6a61bef", "width": 1080, "height": 540}], "variants": {}, "id": "Ln8mP_mwXDuC3OX-8V42DgslTrqg3_oCYk-rjdF319Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18bjrw6", "is_robot_indexable": true, "report_reasons": null, "author": "skrillavilla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bjrw6/pipeline_using_dataproc_and_pyspark_to_build_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bjrw6/pipeline_using_dataproc_and_pyspark_to_build_a/", "subreddit_subscribers": 144008, "created_utc": 1701803857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found this list the other night. It has a ton of good resources for working with dbt\n\n&amp;#x200B;\n\n[https://github.com/Hiflylabs/awesome-dbt](https://github.com/Hiflylabs/awesome-dbt)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good list of dbt tools/resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bhcp4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701797581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found this list the other night. It has a ton of good resources for working with dbt&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Hiflylabs/awesome-dbt\"&gt;https://github.com/Hiflylabs/awesome-dbt&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?auto=webp&amp;s=176e13ff72d3539d5767f43a8afe1a74ff147c22", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2dd3283325149ef5d275d3a81fa389126e63f6dc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a4f30011f8a49d95b307b2a23ddfe55cd2d0ac6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f50ff1fd15562d44a72d1c9e9ef3501d106e438", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c4aa808f0987e4fb5ae998545416dddb38f7f93", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57cb9b22361687461d619af9ed1204a212844246", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f5895bf2b2b84890a02c2022006d133e6cc9723", "width": 1080, "height": 540}], "variants": {}, "id": "-hQXbWYxMy1LKsqRjHBv0nu0FBgk3UtXPqm_FMKcUbE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18bhcp4", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bhcp4/good_list_of_dbt_toolsresources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bhcp4/good_list_of_dbt_toolsresources/", "subreddit_subscribers": 144008, "created_utc": 1701797581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI hope you\u2019re fine. \n\nRight now, I\u2019m working on a Data Quality Proof Of Concept for my company. \nI managed to find some good libraries like \u00ab\u00a0ydata-profiling\u00a0\u00bb. \nNow the hierarchy wants me to add another layer to the solution which consists of cleaning the \u00ab\u00a0bad\u00a0\u00bb data that doesn\u2019t comply to our requirements. \n\nI\u2019ve implemented one using Pandas, as I receive only csv files.\n\nI was wondering if someone know or has already implemented something like this, cleaning Data and subtracting rows that don\u2019t comply to some requirements from the original dataset, using an open-source library in Python ?\n\nGuidance appreciated,\nThanks in advance folks.", "author_fullname": "t2_mrrjzrpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any Python libraries for Data Cleansing ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b8t2x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701770804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I hope you\u2019re fine. &lt;/p&gt;\n\n&lt;p&gt;Right now, I\u2019m working on a Data Quality Proof Of Concept for my company. \nI managed to find some good libraries like \u00ab\u00a0ydata-profiling\u00a0\u00bb. \nNow the hierarchy wants me to add another layer to the solution which consists of cleaning the \u00ab\u00a0bad\u00a0\u00bb data that doesn\u2019t comply to our requirements. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve implemented one using Pandas, as I receive only csv files.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if someone know or has already implemented something like this, cleaning Data and subtracting rows that don\u2019t comply to some requirements from the original dataset, using an open-source library in Python ?&lt;/p&gt;\n\n&lt;p&gt;Guidance appreciated,\nThanks in advance folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18b8t2x", "is_robot_indexable": true, "report_reasons": null, "author": "TheLazyDataEng", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b8t2x/are_there_any_python_libraries_for_data_cleansing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b8t2x/are_there_any_python_libraries_for_data_cleansing/", "subreddit_subscribers": 144008, "created_utc": 1701770804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry for probably niche question, but I'm practicing my data modeling skills for interviews, so I'm trying to apply Kimball modeling to something I'm sort of familiar with (Teamfight Tactics). Basically how the game works is that a match has 8 players, each with their own board with several champions and the API returns information on who is in each game and some statistics/information about the state of their board at the end of the game (remaining gold, units played, units' items, level, traits activated, etc. [Link to API docs for reference](https://developer.riotgames.com/apis#tft-match-v1/GET_getMatch)) \n\nIf I'm trying to take a Kimball approach to modeling, these are the dimensions that I think I would include:\n\n* dim\\_user\n* dim\\_set\n* dim\\_trait\n* dim\\_champion\n* dim\\_item\n\nThen, I would have a couple of fact tables defined as follows:\n\n* fact\\_game\n   * game\\_id\n   * set\\_id (FK to dim\\_set)\n   * start\\_timestamp\n   * end\\_timestamp\n   * duration\n   * user\\_ids (List of ids of users who are in the game, FK to dim\\_user)\n* fact\\_final\\_board\n   * game\\_id\n   * user\\_id\n   * death\\_timestamp\n   * champions (List of ids of champions on the board, FK to dim\\_champion)\n   * active\\_traits (List of ids of active traits, FK to dim\\_trait)\n   * remaining\\_gold\n   * last\\_round\n   * placement\n   * player\\_level\n\nThere are a couple of issues that I've found with how I'm solving this. First, AFAIK I generally don't see tutorials online using a List type for ids connecting Facts to Dimensions and I'm wondering if there would be performance issues querying if I were to design the Fact tables that way. Second, with the way that I have designed the schema right now I'm not including anything that uses dim\\_item and I am unable to query to perform analysis on which units are holding what items. Would the solution to this be for me to create another fact table like so?\n\n* fact\\_final\\_board\\_champion\n   * game\\_id\n   * user\\_id\n   * champion\\_id\n   * items (List of ids of up to 3 items that champion is holding, FK to dim\\_item)\n\nDoing this I think I'm repeating bad design where I'm using a List to hold foreign keys. So from this point I'm kind of confused as to how I would improve my design. Thanks for any help possible", "author_fullname": "t2_co1kho03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I take a Kimball approach to creating a data model for TFT matches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b76rr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701763480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for probably niche question, but I&amp;#39;m practicing my data modeling skills for interviews, so I&amp;#39;m trying to apply Kimball modeling to something I&amp;#39;m sort of familiar with (Teamfight Tactics). Basically how the game works is that a match has 8 players, each with their own board with several champions and the API returns information on who is in each game and some statistics/information about the state of their board at the end of the game (remaining gold, units played, units&amp;#39; items, level, traits activated, etc. &lt;a href=\"https://developer.riotgames.com/apis#tft-match-v1/GET_getMatch\"&gt;Link to API docs for reference&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m trying to take a Kimball approach to modeling, these are the dimensions that I think I would include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;dim_user&lt;/li&gt;\n&lt;li&gt;dim_set&lt;/li&gt;\n&lt;li&gt;dim_trait&lt;/li&gt;\n&lt;li&gt;dim_champion&lt;/li&gt;\n&lt;li&gt;dim_item&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then, I would have a couple of fact tables defined as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;fact_game\n\n&lt;ul&gt;\n&lt;li&gt;game_id&lt;/li&gt;\n&lt;li&gt;set_id (FK to dim_set)&lt;/li&gt;\n&lt;li&gt;start_timestamp&lt;/li&gt;\n&lt;li&gt;end_timestamp&lt;/li&gt;\n&lt;li&gt;duration&lt;/li&gt;\n&lt;li&gt;user_ids (List of ids of users who are in the game, FK to dim_user)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;fact_final_board\n\n&lt;ul&gt;\n&lt;li&gt;game_id&lt;/li&gt;\n&lt;li&gt;user_id&lt;/li&gt;\n&lt;li&gt;death_timestamp&lt;/li&gt;\n&lt;li&gt;champions (List of ids of champions on the board, FK to dim_champion)&lt;/li&gt;\n&lt;li&gt;active_traits (List of ids of active traits, FK to dim_trait)&lt;/li&gt;\n&lt;li&gt;remaining_gold&lt;/li&gt;\n&lt;li&gt;last_round&lt;/li&gt;\n&lt;li&gt;placement&lt;/li&gt;\n&lt;li&gt;player_level&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;There are a couple of issues that I&amp;#39;ve found with how I&amp;#39;m solving this. First, AFAIK I generally don&amp;#39;t see tutorials online using a List type for ids connecting Facts to Dimensions and I&amp;#39;m wondering if there would be performance issues querying if I were to design the Fact tables that way. Second, with the way that I have designed the schema right now I&amp;#39;m not including anything that uses dim_item and I am unable to query to perform analysis on which units are holding what items. Would the solution to this be for me to create another fact table like so?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;fact_final_board_champion\n\n&lt;ul&gt;\n&lt;li&gt;game_id&lt;/li&gt;\n&lt;li&gt;user_id&lt;/li&gt;\n&lt;li&gt;champion_id&lt;/li&gt;\n&lt;li&gt;items (List of ids of up to 3 items that champion is holding, FK to dim_item)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Doing this I think I&amp;#39;m repeating bad design where I&amp;#39;m using a List to hold foreign keys. So from this point I&amp;#39;m kind of confused as to how I would improve my design. Thanks for any help possible&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18b76rr", "is_robot_indexable": true, "report_reasons": null, "author": "69hehehe69", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b76rr/how_would_i_take_a_kimball_approach_to_creating_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b76rr/how_would_i_take_a_kimball_approach_to_creating_a/", "subreddit_subscribers": 144008, "created_utc": 1701763480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a table that I want to validate if the records have been inserted or updated. I thought of doing a row count of the records from the day before and comparing them to the current days row count to see if more records have been inserted. If yes then the record has been inserted. If not then it wasn\u2019t. I thought that was simple enough but I don\u2019t know if it\u2019s possible to compare the row count if todays record to yesterday\u2019s. \n\nAnd if the record has been updated, how would I even know?? There are various date columns like \u201cupdated_date\u201d which if the record has been updated then it would show today\u2019s date. There is another column \u201ceff_date\u201d that gets updated if the record has been updated to inserted. There is no concrete way for me to see if the record has been updated or inserted based off the date columns. And there is no date column for me to know if this date is today then it\u2019s been updated. If \u201cupdated_date\u201d is today\u2019s date and \u201ceff_date\u201d is an old date then that record could have been updated or inserted. If \u201cupdated_date\u201d is old but \u201ceff_date\u201d is newer then that could have been updated or inserted as well. \n\nI don\u2019t know what to do how would I even validate this? :/", "author_fullname": "t2_dx4lay6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How the heck do I validate records with this kind of data??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b1pdb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701744158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a table that I want to validate if the records have been inserted or updated. I thought of doing a row count of the records from the day before and comparing them to the current days row count to see if more records have been inserted. If yes then the record has been inserted. If not then it wasn\u2019t. I thought that was simple enough but I don\u2019t know if it\u2019s possible to compare the row count if todays record to yesterday\u2019s. &lt;/p&gt;\n\n&lt;p&gt;And if the record has been updated, how would I even know?? There are various date columns like \u201cupdated_date\u201d which if the record has been updated then it would show today\u2019s date. There is another column \u201ceff_date\u201d that gets updated if the record has been updated to inserted. There is no concrete way for me to see if the record has been updated or inserted based off the date columns. And there is no date column for me to know if this date is today then it\u2019s been updated. If \u201cupdated_date\u201d is today\u2019s date and \u201ceff_date\u201d is an old date then that record could have been updated or inserted. If \u201cupdated_date\u201d is old but \u201ceff_date\u201d is newer then that could have been updated or inserted as well. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t know what to do how would I even validate this? :/&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18b1pdb", "is_robot_indexable": true, "report_reasons": null, "author": "fluffofknowledge", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b1pdb/how_the_heck_do_i_validate_records_with_this_kind/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b1pdb/how_the_heck_do_i_validate_records_with_this_kind/", "subreddit_subscribers": 144008, "created_utc": 1701744158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What data controls do you implement day to day?\n\nMy project has come under scrutiny by CDO audit team and have asked what controls are in place across a number of circumstances. We had a very tactical build that was under a \"get it done by any means\" direction due to a strict deadline, which although doesn't excuse the limited controls, gives reason to why I requested a 6 to 8 week clean up period post delivery. As usual, this has now be superseded by next phase changes, along with planning to move it to a more permanent solution.\n\nI currently have the below being loaded to a Teradata database:\n* Excel - converted to CSV for easier loading loaded through TPT scripts\n* Tables from a different Teradata server using TPT Easy Loader\n* Data pulled from a Hadoop cluster via QueryGrid\n* Data pulled from tables on the same server we are hosting on\n\nThe audit team have requested we apply the following controls:\n* Validate all data loaded from Excel by providing row counts and doing random manual sampling to ensure that data hasn't changed\n* Apply row count and data format checks when loading entire tables from the other Teradata instance\n* Provide checks on expected row counts and data integrity when querying Hadoop and our own server\n\nIs this overkill or just a standard? I have raised my concerns over why we would question if data can change in flight with external forces being applied.", "author_fullname": "t2_79a43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Controls", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18bmawo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701810230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What data controls do you implement day to day?&lt;/p&gt;\n\n&lt;p&gt;My project has come under scrutiny by CDO audit team and have asked what controls are in place across a number of circumstances. We had a very tactical build that was under a &amp;quot;get it done by any means&amp;quot; direction due to a strict deadline, which although doesn&amp;#39;t excuse the limited controls, gives reason to why I requested a 6 to 8 week clean up period post delivery. As usual, this has now be superseded by next phase changes, along with planning to move it to a more permanent solution.&lt;/p&gt;\n\n&lt;p&gt;I currently have the below being loaded to a Teradata database:\n* Excel - converted to CSV for easier loading loaded through TPT scripts\n* Tables from a different Teradata server using TPT Easy Loader\n* Data pulled from a Hadoop cluster via QueryGrid\n* Data pulled from tables on the same server we are hosting on&lt;/p&gt;\n\n&lt;p&gt;The audit team have requested we apply the following controls:\n* Validate all data loaded from Excel by providing row counts and doing random manual sampling to ensure that data hasn&amp;#39;t changed\n* Apply row count and data format checks when loading entire tables from the other Teradata instance\n* Provide checks on expected row counts and data integrity when querying Hadoop and our own server&lt;/p&gt;\n\n&lt;p&gt;Is this overkill or just a standard? I have raised my concerns over why we would question if data can change in flight with external forces being applied.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bmawo", "is_robot_indexable": true, "report_reasons": null, "author": "Stychey", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmawo/data_controls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmawo/data_controls/", "subreddit_subscribers": 144008, "created_utc": 1701810230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working on existing project that involves Data warehousing  in IoT system, specifically dealing with real time data collection from GPS trackers on cars. I have successfully implemented the initial data collection using PHP, but I am now at juncture where I would greatly appreciate your expertise in proposing optional architecture for subsequent stage of data processing analysis and visualization.\n\nto provide a brief overview, this project  Have the following components : \n\n1. Real time that the processing and analysis :\n\nI have  data streaming in from GPS trackers on cars in real time and I am keen on understanding. The best practices and open source tools that  can be employed to process and analyze this data efficiently. The goal is to extract valuable insights in real time.\n\n2.data warehousing: \n\nthe next closest step involved, setting up a robust data Warehouse  solution .\n \nThank you  you in advance for your time and assistance", "author_fullname": "t2_e1i3duqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dear data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18blijw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701808243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on existing project that involves Data warehousing  in IoT system, specifically dealing with real time data collection from GPS trackers on cars. I have successfully implemented the initial data collection using PHP, but I am now at juncture where I would greatly appreciate your expertise in proposing optional architecture for subsequent stage of data processing analysis and visualization.&lt;/p&gt;\n\n&lt;p&gt;to provide a brief overview, this project  Have the following components : &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Real time that the processing and analysis :&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have  data streaming in from GPS trackers on cars in real time and I am keen on understanding. The best practices and open source tools that  can be employed to process and analyze this data efficiently. The goal is to extract valuable insights in real time.&lt;/p&gt;\n\n&lt;p&gt;2.data warehousing: &lt;/p&gt;\n\n&lt;p&gt;the next closest step involved, setting up a robust data Warehouse  solution .&lt;/p&gt;\n\n&lt;p&gt;Thank you  you in advance for your time and assistance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18blijw", "is_robot_indexable": true, "report_reasons": null, "author": "ryan7ait", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18blijw/dear_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18blijw/dear_data_engineers/", "subreddit_subscribers": 144008, "created_utc": 1701808243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt basics refresher", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18bl5sg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tlFUmRGlDFrCPnNGzTHo0xfNtMwHC8XkpUSb1bK-ZAw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701807346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/dbt-basics-refresher", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?auto=webp&amp;s=3279967ceb601646cb41028ac8d4ca73b7f49f38", "width": 960, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=711b55ead92676cfed46175950e598fb11169eef", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3351e12cdb4729acc293d3189901cd6fb23a05b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=726542216ba15170d4a2b857bf8954b8193335f6", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=efa3574a1dbd7076fa3461704248c28e5f1a4e71", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=64bd860b29935c769a96e58efa422e8278460f66", "width": 960, "height": 540}], "variants": {}, "id": "c10qH1P3epOVeKPFeR8yyKKpf08s_LIw8MS0wY3eKUs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18bl5sg", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bl5sg/dbt_basics_refresher/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/dbt-basics-refresher", "subreddit_subscribers": 144008, "created_utc": 1701807346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit: putting TLDR at top.\n\nTLDR: our database takes too long to update because transformation takes place in SQL and the procedures are highly complex and intertwined. What tools or processes can we use to improve this?\n\nContext: I\u2019m a Data Analyst working in a large international corporation. I don\u2019t (currently) have any admin privileges in our Azure Portal and the only team that does is the \u201cNational\u201d team. We have an Azure database with SSMS that we use to connect to it. This database is updated and managed by the National team. I have been in this role about a year and a half. We have five divisions of the same job function that use this database that have the same tables (mostly) created/updated for their division with their data. This database was created organically by one of the divisions to suit their needs and each division just mostly copied their process then made them their own on their own servers before they ultimately got migrated to a centralized azure database.\n\nProblem: Updating our SQL database takes a full day in the week so we have static data for one full week. My understanding of the process of these updates (from the limited amount I have been told by National) are that there are jobs set up to pull reports from SAP into batch files to import into SQL and then we have a number of (probably over 40+) stored procedures that are run to parse and transform the data into even more tables than procedures (we have like 200 tables\u2026). All of the procedures reference multiple source tables and can, sometimes, even be updating multiple tables within them. A large portion of them are dynamic SQL scripts with table, schema, dates, and temp table names (used for the transformation process) all set to variables so the procedures can be copied and built for the other divisions and the variables just have to change (although even those are still not all alike as changes have been made to them per requests from divisions). The procedures can sometimes be highly complex in their transformations and it can be very hard to change or see what it\u2019s doing.\n\nOver the last half-year I\u2019ve done my fair share of research into data engineering tools and practices and I know there have to be significantly better methods to handle this data, but no one seems to be looking at this as a serious problem, but I want to push for a better data structure and to possibly integrate our data process with the process/tools of the larger company (like, everyone else that\u2019s not us) that uses Azure Data Warehouses and shares data with teams per requests.\n\nSolution?: What tools/processes should I be looking into to learn so that I can effectively present a case to change our structure for the better?", "author_fullname": "t2_9oy62qlm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better ETL options than current set up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bied6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701800489.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701800278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit: putting TLDR at top.&lt;/p&gt;\n\n&lt;p&gt;TLDR: our database takes too long to update because transformation takes place in SQL and the procedures are highly complex and intertwined. What tools or processes can we use to improve this?&lt;/p&gt;\n\n&lt;p&gt;Context: I\u2019m a Data Analyst working in a large international corporation. I don\u2019t (currently) have any admin privileges in our Azure Portal and the only team that does is the \u201cNational\u201d team. We have an Azure database with SSMS that we use to connect to it. This database is updated and managed by the National team. I have been in this role about a year and a half. We have five divisions of the same job function that use this database that have the same tables (mostly) created/updated for their division with their data. This database was created organically by one of the divisions to suit their needs and each division just mostly copied their process then made them their own on their own servers before they ultimately got migrated to a centralized azure database.&lt;/p&gt;\n\n&lt;p&gt;Problem: Updating our SQL database takes a full day in the week so we have static data for one full week. My understanding of the process of these updates (from the limited amount I have been told by National) are that there are jobs set up to pull reports from SAP into batch files to import into SQL and then we have a number of (probably over 40+) stored procedures that are run to parse and transform the data into even more tables than procedures (we have like 200 tables\u2026). All of the procedures reference multiple source tables and can, sometimes, even be updating multiple tables within them. A large portion of them are dynamic SQL scripts with table, schema, dates, and temp table names (used for the transformation process) all set to variables so the procedures can be copied and built for the other divisions and the variables just have to change (although even those are still not all alike as changes have been made to them per requests from divisions). The procedures can sometimes be highly complex in their transformations and it can be very hard to change or see what it\u2019s doing.&lt;/p&gt;\n\n&lt;p&gt;Over the last half-year I\u2019ve done my fair share of research into data engineering tools and practices and I know there have to be significantly better methods to handle this data, but no one seems to be looking at this as a serious problem, but I want to push for a better data structure and to possibly integrate our data process with the process/tools of the larger company (like, everyone else that\u2019s not us) that uses Azure Data Warehouses and shares data with teams per requests.&lt;/p&gt;\n\n&lt;p&gt;Solution?: What tools/processes should I be looking into to learn so that I can effectively present a case to change our structure for the better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18bied6", "is_robot_indexable": true, "report_reasons": null, "author": "Talk-Much", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bied6/better_etl_options_than_current_set_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bied6/better_etl_options_than_current_set_up/", "subreddit_subscribers": 144008, "created_utc": 1701800278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am quite new to data modeling other than a few projects at school where we learned about kimball etc. I have a bit of a strange task right where I need to make a data model for a decision system (rules engine). I am dealing with a lot of complex medical data, and relationships between entities. The model needs to be reasoned over, and be logically coherent for the use case. I am however at a bit of a loss as to how to go about this. My first thought was to use an ontology to create a knowledge base, and then build rules on top of this. So for example make the ontology using OWL, then use SWRL to create the conditions and rules. I would then potentially use RDFlib or owlready2 to create Python functions to test the usability of the model by creating instances. The use case is, given a diagnosis what is the preferred medication of choice. If anyone has any other ideas, I would love to hear them. ", "author_fullname": "t2_2u9k11la", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling Ontologies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b85x2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701767919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am quite new to data modeling other than a few projects at school where we learned about kimball etc. I have a bit of a strange task right where I need to make a data model for a decision system (rules engine). I am dealing with a lot of complex medical data, and relationships between entities. The model needs to be reasoned over, and be logically coherent for the use case. I am however at a bit of a loss as to how to go about this. My first thought was to use an ontology to create a knowledge base, and then build rules on top of this. So for example make the ontology using OWL, then use SWRL to create the conditions and rules. I would then potentially use RDFlib or owlready2 to create Python functions to test the usability of the model by creating instances. The use case is, given a diagnosis what is the preferred medication of choice. If anyone has any other ideas, I would love to hear them. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18b85x2", "is_robot_indexable": true, "report_reasons": null, "author": "koobakak-kid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b85x2/data_modeling_ontologies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b85x2/data_modeling_ontologies/", "subreddit_subscribers": 144008, "created_utc": 1701767919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I face data spread over multiple columns.\n\n&amp;#x200B;\n\nSome rows hold duplicate data for a key. \n\nSome semantically similar data\n\nOthers new fresh data (sometimes only for different keys - however sometimes also for the same key)\n\n&amp;#x200B;\n\nWhat could be a great strategy to make this data accessible? I want to have one row for the key.\n\n&amp;#x200B;\n\nA full example can be obtained here: [https://stackoverflow.com/questions/77604377/harmonizing-data-spread-over-multiple-rows-with-duplicate-values-for-some-keys](https://stackoverflow.com/questions/77604377/harmonizing-data-spread-over-multiple-rows-with-duplicate-values-for-some-keys)", "author_fullname": "t2_8dvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Harmonizing data spread over multiple rows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b6eyn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701760175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I face data spread over multiple columns.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Some rows hold duplicate data for a key. &lt;/p&gt;\n\n&lt;p&gt;Some semantically similar data&lt;/p&gt;\n\n&lt;p&gt;Others new fresh data (sometimes only for different keys - however sometimes also for the same key)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What could be a great strategy to make this data accessible? I want to have one row for the key.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A full example can be obtained here: &lt;a href=\"https://stackoverflow.com/questions/77604377/harmonizing-data-spread-over-multiple-rows-with-duplicate-values-for-some-keys\"&gt;https://stackoverflow.com/questions/77604377/harmonizing-data-spread-over-multiple-rows-with-duplicate-values-for-some-keys&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "18b6eyn", "is_robot_indexable": true, "report_reasons": null, "author": "geoheil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/18b6eyn/harmonizing_data_spread_over_multiple_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b6eyn/harmonizing_data_spread_over_multiple_rows/", "subreddit_subscribers": 144008, "created_utc": 1701760175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, asking for some help. Have a couple of interviews next week and would like any help that can be offered in preparing for them. \n\nOne is with Booz Allen Hamilton for a Data Engineer role and the other is the technical for a Bloomberg Senior Data Engineer role.\n\nFor the Bloomberg one I've been practising the tagged questions on Leetcode. Any other help/hints someone could give is appreciated. \n\nWhile for the Booz Allen Hamilton role it's only one round. From what I've heard it's conversational and theoretical in nature and I won't be expected to write code during the interview. If anyone has any experience interviewing with them I'd appreciate any insight you could offer", "author_fullname": "t2_p5wlf0g4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18bmnwy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701811132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, asking for some help. Have a couple of interviews next week and would like any help that can be offered in preparing for them. &lt;/p&gt;\n\n&lt;p&gt;One is with Booz Allen Hamilton for a Data Engineer role and the other is the technical for a Bloomberg Senior Data Engineer role.&lt;/p&gt;\n\n&lt;p&gt;For the Bloomberg one I&amp;#39;ve been practising the tagged questions on Leetcode. Any other help/hints someone could give is appreciated. &lt;/p&gt;\n\n&lt;p&gt;While for the Booz Allen Hamilton role it&amp;#39;s only one round. From what I&amp;#39;ve heard it&amp;#39;s conversational and theoretical in nature and I won&amp;#39;t be expected to write code during the interview. If anyone has any experience interviewing with them I&amp;#39;d appreciate any insight you could offer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18bmnwy", "is_robot_indexable": true, "report_reasons": null, "author": "El_Cato_Crande", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmnwy/interview_prep_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmnwy/interview_prep_help/", "subreddit_subscribers": 144008, "created_utc": 1701811132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I come from a software development leadership background as part of product development teams in tech companies. \n\nI just came across an opportunity where I may get a good jump to become an executive at a company and lead their data team. What I have seen from my experience and talking to other friends in the industry, data leadership jobs are shitty jobs where you always get hammered by all stakeholders (CFO, CIO, Product Executives etc,) and no matter how good you do there are always some issues (data quality, platform incidents, pipeline incidents etc) as data itself is a complex field. Also, you never got to work on the real meat (analytics/AI) as business or product teams who have domain knowledge are doing that. \n\nWhat do you guys think about Data Leadership vs Software Development Leadership?", "author_fullname": "t2_i0rx07jv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Leadership VS. Product Software Development Leadership", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18bmkgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701810895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from a software development leadership background as part of product development teams in tech companies. &lt;/p&gt;\n\n&lt;p&gt;I just came across an opportunity where I may get a good jump to become an executive at a company and lead their data team. What I have seen from my experience and talking to other friends in the industry, data leadership jobs are shitty jobs where you always get hammered by all stakeholders (CFO, CIO, Product Executives etc,) and no matter how good you do there are always some issues (data quality, platform incidents, pipeline incidents etc) as data itself is a complex field. Also, you never got to work on the real meat (analytics/AI) as business or product teams who have domain knowledge are doing that. &lt;/p&gt;\n\n&lt;p&gt;What do you guys think about Data Leadership vs Software Development Leadership?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18bmkgf", "is_robot_indexable": true, "report_reasons": null, "author": "Adorable-Diver-1919", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmkgf/data_engineering_leadership_vs_product_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmkgf/data_engineering_leadership_vs_product_software/", "subreddit_subscribers": 144008, "created_utc": 1701810895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have a requirement to collect data from non technical users. They need a userfriendly tool to be able to login and upload their data. Team is recommending to use Excel, but I am open to other ideas as long as the file format is easy to use for non technical users.\n\nOnce the user have uploaded this file, it needs to be validated. This way we can return the file and our non-technical users fix the errors and reupload the file.\n\nAny recommendations / suggestion are welcome on how such a scenario can be handled or if there are any prebuilt tools that can be used. ", "author_fullname": "t2_3m94wwfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Collecting data from non-technical users.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bkj5d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701805780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have a requirement to collect data from non technical users. They need a userfriendly tool to be able to login and upload their data. Team is recommending to use Excel, but I am open to other ideas as long as the file format is easy to use for non technical users.&lt;/p&gt;\n\n&lt;p&gt;Once the user have uploaded this file, it needs to be validated. This way we can return the file and our non-technical users fix the errors and reupload the file.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations / suggestion are welcome on how such a scenario can be handled or if there are any prebuilt tools that can be used. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bkj5d", "is_robot_indexable": true, "report_reasons": null, "author": "Gujjubhai2019", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bkj5d/collecting_data_from_nontechnical_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bkj5d/collecting_data_from_nontechnical_users/", "subreddit_subscribers": 144008, "created_utc": 1701805780.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I'm a student looking to get into data engineering and I came to this thread because I was wondering if the 512G SSD option is necessary in your experience. I read some of the previous posts and saw that it's optional other than 16G ram, so let me know what you guys think.\n\nThanks.", "author_fullname": "t2_djmduqi1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "512G SSD laptop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bgn7k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701795718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m a student looking to get into data engineering and I came to this thread because I was wondering if the 512G SSD option is necessary in your experience. I read some of the previous posts and saw that it&amp;#39;s optional other than 16G ram, so let me know what you guys think.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18bgn7k", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Chef-277", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bgn7k/512g_ssd_laptop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bgn7k/512g_ssd_laptop/", "subreddit_subscribers": 144008, "created_utc": 1701795718.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}