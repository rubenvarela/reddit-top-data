{"kind": "Listing", "data": {"after": "t3_18aif0q", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ZFS, snapshots, ECC Ram, 3 backups and a single fuckup is all it takes. I had a major pool of twelve 18TB of zRaid 3. I had 2 smaller pools of about four 14TB drives and four 16TB drives. I decided to merge them to make a single larger backup pool. Before I did that though, I tried to do a replication task to my main pool of something I didn't want to lose.\n\nThe 16 x4 drives were remote. I brought them back to location as moving 40TB of data over the internet is not ideal.\n\nGuess I screwed up the location or something and didn't notice anything wrong. Wiped my backups to be merged instead of just adding another vdev to one of them. I wanted the extra write speed performance that comes with a fresh dual vdev pool when writing as it had multiple purposes.\n\nLow and behold I noticed my personal files were just gone. The Datasets they were in just vanished. The fear sets in. That's okay, I have an encrypted 4th backup of my personal files. The encryption password wasn't working? Oh fuck, oh fuck! My most important files were there! After almost having a panic attack I keep trying different keys I have for encrypted pools but they don't work. After manually opening a json file to extract just the key for one of them does it work.\n\nWhew! I am in the clear. I back up that data. Lesson learned, have another drive unencrypted stored safely somewhere in case you also lose access to the key too.\n\nAt least my plex library looked like it wasn't touched. Try to play something but it errors out. Hmm, strange. I wonder if the permissions accidentally got changed? They did, lets fix that and get the new backup going, don't want any other heart attacks. Nope, still can't play it. Huh, strange. Go to try to play a file manually. They aren't there. Oh no. That's okay, I have snapshots I can revert to. No, all my snapshots from before today are also just gone. The data is still taking the same amount of space according to truenas. However, nothing is there. Is it corrupted now? I don't know. I can try to run a scrub but all my snapshots are just gone.\n\nMaybe when the back finishes it will allow me to view the files, but that is likely just wishful thinking. For some reason my movies are fine, but all else seems gone.\n\nNo matter how prepared you are, a little bit of misfortune and bad timing can just take it all away. If you have any potential solution to files that appear to be taking space but don't show up, I would be thrilled to hear it. The thing I am most upset about now is that I had a massive lossless music library and all the hard work I put into curating and editing metadata is just gone.\n\nIt seemed reasonable at the time, sure I would have only one copy during that time for about 24 hours until it finishes replicating, but with 3 drives of redundancy, how could it ever fail?\n\nEdit: I appear to have also had a 4th copy of my music library, unfortunately before my major lossless addition, but at least I am not at ground zero.  \n\n\nEdit 2: Holy fuck, I might just have a chance of recovery. For whatever reason, making a replication of the bad Data appears to to produce potentially good versions. There may still be hope yet lads!", "author_fullname": "t2_3cxp0v7b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Well it happened, I think lost almost everything. 40 Terabytes gone.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18auavd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 251, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 251, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701734899.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701723872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ZFS, snapshots, ECC Ram, 3 backups and a single fuckup is all it takes. I had a major pool of twelve 18TB of zRaid 3. I had 2 smaller pools of about four 14TB drives and four 16TB drives. I decided to merge them to make a single larger backup pool. Before I did that though, I tried to do a replication task to my main pool of something I didn&amp;#39;t want to lose.&lt;/p&gt;\n\n&lt;p&gt;The 16 x4 drives were remote. I brought them back to location as moving 40TB of data over the internet is not ideal.&lt;/p&gt;\n\n&lt;p&gt;Guess I screwed up the location or something and didn&amp;#39;t notice anything wrong. Wiped my backups to be merged instead of just adding another vdev to one of them. I wanted the extra write speed performance that comes with a fresh dual vdev pool when writing as it had multiple purposes.&lt;/p&gt;\n\n&lt;p&gt;Low and behold I noticed my personal files were just gone. The Datasets they were in just vanished. The fear sets in. That&amp;#39;s okay, I have an encrypted 4th backup of my personal files. The encryption password wasn&amp;#39;t working? Oh fuck, oh fuck! My most important files were there! After almost having a panic attack I keep trying different keys I have for encrypted pools but they don&amp;#39;t work. After manually opening a json file to extract just the key for one of them does it work.&lt;/p&gt;\n\n&lt;p&gt;Whew! I am in the clear. I back up that data. Lesson learned, have another drive unencrypted stored safely somewhere in case you also lose access to the key too.&lt;/p&gt;\n\n&lt;p&gt;At least my plex library looked like it wasn&amp;#39;t touched. Try to play something but it errors out. Hmm, strange. I wonder if the permissions accidentally got changed? They did, lets fix that and get the new backup going, don&amp;#39;t want any other heart attacks. Nope, still can&amp;#39;t play it. Huh, strange. Go to try to play a file manually. They aren&amp;#39;t there. Oh no. That&amp;#39;s okay, I have snapshots I can revert to. No, all my snapshots from before today are also just gone. The data is still taking the same amount of space according to truenas. However, nothing is there. Is it corrupted now? I don&amp;#39;t know. I can try to run a scrub but all my snapshots are just gone.&lt;/p&gt;\n\n&lt;p&gt;Maybe when the back finishes it will allow me to view the files, but that is likely just wishful thinking. For some reason my movies are fine, but all else seems gone.&lt;/p&gt;\n\n&lt;p&gt;No matter how prepared you are, a little bit of misfortune and bad timing can just take it all away. If you have any potential solution to files that appear to be taking space but don&amp;#39;t show up, I would be thrilled to hear it. The thing I am most upset about now is that I had a massive lossless music library and all the hard work I put into curating and editing metadata is just gone.&lt;/p&gt;\n\n&lt;p&gt;It seemed reasonable at the time, sure I would have only one copy during that time for about 24 hours until it finishes replicating, but with 3 drives of redundancy, how could it ever fail?&lt;/p&gt;\n\n&lt;p&gt;Edit: I appear to have also had a 4th copy of my music library, unfortunately before my major lossless addition, but at least I am not at ground zero.  &lt;/p&gt;\n\n&lt;p&gt;Edit 2: Holy fuck, I might just have a chance of recovery. For whatever reason, making a replication of the bad Data appears to to produce potentially good versions. There may still be hope yet lads!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18auavd", "is_robot_indexable": true, "report_reasons": null, "author": "ALittleBurnerAccount", "discussion_type": null, "num_comments": 124, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18auavd/well_it_happened_i_think_lost_almost_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18auavd/well_it_happened_i_think_lost_almost_everything/", "subreddit_subscribers": 716411, "created_utc": 1701723872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nSo, this company, Atlas Obscura, is well-known for unusual places and destinations. I use it all the time to plan my own trips. I think they're not doing great as of recent, and I'm afraid they might discontinue the map - as they did with the community forums last year.\n\nIs there any reasonable way to essentially \"download\" it and store locally?\n\nHere's the link: [https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map](https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map)\n\nEdit: So, even if I can make a local copy - it will look like individual place pages like this: [https://www.atlasobscura.com/places/wall-street-bombing](https://www.atlasobscura.com/places/wall-street-bombing)  \\- where it technically has Google Maps link, GPS coordinates, etc. I can even navigate individual countries through menu - as the places are grouped there. But the map (the actual map available via the link above) is hosted at Google Maps and is owned by AO, so I'm not sure if I'll be  able to download it via WGET.\n\nThanks, and sorry if this is a stupid question to ask", "author_fullname": "t2_6l9t4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A company is probably going to disappear at some point - want to save their map, need help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18arn2a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 162, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 162, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701719790.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701717127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;So, this company, Atlas Obscura, is well-known for unusual places and destinations. I use it all the time to plan my own trips. I think they&amp;#39;re not doing great as of recent, and I&amp;#39;m afraid they might discontinue the map - as they did with the community forums last year.&lt;/p&gt;\n\n&lt;p&gt;Is there any reasonable way to essentially &amp;quot;download&amp;quot; it and store locally?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the link: &lt;a href=\"https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map\"&gt;https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit: So, even if I can make a local copy - it will look like individual place pages like this: &lt;a href=\"https://www.atlasobscura.com/places/wall-street-bombing\"&gt;https://www.atlasobscura.com/places/wall-street-bombing&lt;/a&gt;  - where it technically has Google Maps link, GPS coordinates, etc. I can even navigate individual countries through menu - as the places are grouped there. But the map (the actual map available via the link above) is hosted at Google Maps and is owned by AO, so I&amp;#39;m not sure if I&amp;#39;ll be  able to download it via WGET.&lt;/p&gt;\n\n&lt;p&gt;Thanks, and sorry if this is a stupid question to ask&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HuDDtkAwRY8NpI1livrMY3laApR0q9yJBRz_FYUK2Cc.jpg?auto=webp&amp;s=13c8c463939502ec1104fa517253b5e8fa212c15", "width": 600, "height": 296}, "resolutions": [{"url": "https://external-preview.redd.it/HuDDtkAwRY8NpI1livrMY3laApR0q9yJBRz_FYUK2Cc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfb72ade213bd2a37a296ab2d5e05d9732282e49", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/HuDDtkAwRY8NpI1livrMY3laApR0q9yJBRz_FYUK2Cc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55065974b8f54d6975c02fd2460d67a004df67b8", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/HuDDtkAwRY8NpI1livrMY3laApR0q9yJBRz_FYUK2Cc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f14cfdf7d104f4c056d39537caad70f0a5d71ecb", "width": 320, "height": 157}], "variants": {}, "id": "rP7xfMhC5JJC_8Z2vdB5yVOmZvELEYCGifZb-RRYLa8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18arn2a", "is_robot_indexable": true, "report_reasons": null, "author": "FlamebergU", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18arn2a/a_company_is_probably_going_to_disappear_at_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18arn2a/a_company_is_probably_going_to_disappear_at_some/", "subreddit_subscribers": 716411, "created_utc": 1701717127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "FYI... Technodealsusa has the 16TB Exos X16 (ST16000NM001G) on sale for **$180** with free shipping. Here's a [link](https://technodealsusa.com/products/seagate-exos-16tb-enterprise-hdd-x16-sata-6gb-s-512e-4kn-7200-rpm-256mb-cache-3-5-internal-hard-drive-st16000nm001g.html) to that page. Technodealsusa looks legit (1000's of customer reviews on their Newegg and Amazon store pages) so I pulled the trigger on 4 of these puppies.\n\nI can't make any guarantees but this looks like a steal if you're in the market for some new drives especially since these drives, refurbished, cost about $150 on ServerPartsDeals.", "author_fullname": "t2_61d9v4ak", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos 16TB on Sale for $180", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18al2e2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701699235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;FYI... Technodealsusa has the 16TB Exos X16 (ST16000NM001G) on sale for &lt;strong&gt;$180&lt;/strong&gt; with free shipping. Here&amp;#39;s a &lt;a href=\"https://technodealsusa.com/products/seagate-exos-16tb-enterprise-hdd-x16-sata-6gb-s-512e-4kn-7200-rpm-256mb-cache-3-5-internal-hard-drive-st16000nm001g.html\"&gt;link&lt;/a&gt; to that page. Technodealsusa looks legit (1000&amp;#39;s of customer reviews on their Newegg and Amazon store pages) so I pulled the trigger on 4 of these puppies.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t make any guarantees but this looks like a steal if you&amp;#39;re in the market for some new drives especially since these drives, refurbished, cost about $150 on ServerPartsDeals.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nDocmX_jRT3GQcuZt7bJMBEDDMDh1yEN7w4og8kd1dQ.jpg?auto=webp&amp;s=3e3d0a3df1d6c9bf20f527a6bb3e447ea6a7c256", "width": 500, "height": 375}, "resolutions": [{"url": "https://external-preview.redd.it/nDocmX_jRT3GQcuZt7bJMBEDDMDh1yEN7w4og8kd1dQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a14605e606fa63c7bb0357c0ced3c59ce231ae55", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/nDocmX_jRT3GQcuZt7bJMBEDDMDh1yEN7w4og8kd1dQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0dba131acac584d49b9aeb6dd9f92c3dd6e7a186", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/nDocmX_jRT3GQcuZt7bJMBEDDMDh1yEN7w4og8kd1dQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7366c0608eb4c97087527a661e3bdfb1de470bca", "width": 320, "height": 240}], "variants": {}, "id": "nCg50H6n5w0bAIU1VqyMHDOLsMp_6GtnU5mKIjCOyGA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "18al2e2", "is_robot_indexable": true, "report_reasons": null, "author": "Ben4425", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18al2e2/seagate_exos_16tb_on_sale_for_180/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18al2e2/seagate_exos_16tb_on_sale_for_180/", "subreddit_subscribers": 716411, "created_utc": 1701699235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got complaints that Plex wasn't up.  I discovered the media mount disappeared.  Checked the Areca controller logs.  Full of \"Under Voltage\" warnings then \"Recovered\" 2 seconds later.  Then today's log shows Drive 1 going offline, then Drive 2, then Drive 16.\n\nWhat the belly rubbin' pit scratchin' nut dunkin' hell is going on.  Drive failures?  PSU?  Cage backplane? Brownouts?\n\nDrive 1 and 2 had gone back to being free drives in the array, meaning \"duh, I dunno what I am but here I am anyway\", and Drive 16 was simply offline.  I put 16 back online, which brought the RAID6 volume back online but in a degraded state.  \n\nSo full marks for Areca RAID controllers - I haven't lost any data.  \"So far\".\n\nBut having two drives fail at the same time is sus as hell but possible since those were both from the same batch/order from 5 years ago. But I dunno.  Drives 1 and 2 in the 24-bay cage failing seems like a backplane problem, but that's going to be a nightmare to isolate.  And I'm not going to rush out to buy a new PSU without knowing it's the culprit.  But at least it's not a mains voltage problem (I think - because it's all connected to a hefty UPS that feeds it nice clean smooth power.  IT BETTER BE.)\n\nI've moved drives 1 and 2 to some free slots to see if the problem follows the drive or the slot, and started a 10 hour rebuild of a 140TB array.  With 2 drives failing though I have to fallback should this thrashing session cause another drive to go offline.\n\nThis is fun.\n\nI'm fine.", "author_fullname": "t2_ldfi7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Years of non-stop 16-drive hardware RAID6 service. Today, 2 drives failed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ay6pm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701734016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got complaints that Plex wasn&amp;#39;t up.  I discovered the media mount disappeared.  Checked the Areca controller logs.  Full of &amp;quot;Under Voltage&amp;quot; warnings then &amp;quot;Recovered&amp;quot; 2 seconds later.  Then today&amp;#39;s log shows Drive 1 going offline, then Drive 2, then Drive 16.&lt;/p&gt;\n\n&lt;p&gt;What the belly rubbin&amp;#39; pit scratchin&amp;#39; nut dunkin&amp;#39; hell is going on.  Drive failures?  PSU?  Cage backplane? Brownouts?&lt;/p&gt;\n\n&lt;p&gt;Drive 1 and 2 had gone back to being free drives in the array, meaning &amp;quot;duh, I dunno what I am but here I am anyway&amp;quot;, and Drive 16 was simply offline.  I put 16 back online, which brought the RAID6 volume back online but in a degraded state.  &lt;/p&gt;\n\n&lt;p&gt;So full marks for Areca RAID controllers - I haven&amp;#39;t lost any data.  &amp;quot;So far&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;But having two drives fail at the same time is sus as hell but possible since those were both from the same batch/order from 5 years ago. But I dunno.  Drives 1 and 2 in the 24-bay cage failing seems like a backplane problem, but that&amp;#39;s going to be a nightmare to isolate.  And I&amp;#39;m not going to rush out to buy a new PSU without knowing it&amp;#39;s the culprit.  But at least it&amp;#39;s not a mains voltage problem (I think - because it&amp;#39;s all connected to a hefty UPS that feeds it nice clean smooth power.  IT BETTER BE.)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve moved drives 1 and 2 to some free slots to see if the problem follows the drive or the slot, and started a 10 hour rebuild of a 140TB array.  With 2 drives failing though I have to fallback should this thrashing session cause another drive to go offline.&lt;/p&gt;\n\n&lt;p&gt;This is fun.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "150TB Areca RAID6, near, off &amp; online backup; 25 yrs 0bytes lost", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ay6pm", "is_robot_indexable": true, "report_reasons": null, "author": "SpinCharm", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18ay6pm/5_years_of_nonstop_16drive_hardware_raid6_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ay6pm/5_years_of_nonstop_16drive_hardware_raid6_service/", "subreddit_subscribers": 716411, "created_utc": 1701734016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_44aeawzh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Molex to sata... Are these safe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18awjek", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DVwjXsZBTtmUYhxd91LbdUqn4jvvZ_jgw-qI0QiKmYY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701729695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/c2aa6iuxvc4c1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/c2aa6iuxvc4c1.jpg?auto=webp&amp;s=436b5958f7ba2b498419496639af347d8c04e15a", "width": 1545, "height": 1600}, "resolutions": [{"url": "https://preview.redd.it/c2aa6iuxvc4c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2142b690369405f78b3bbc2519e620be5a7b47c", "width": 108, "height": 111}, {"url": "https://preview.redd.it/c2aa6iuxvc4c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed3149f848a8b1c9f7583631d5b5f3457f46c102", "width": 216, "height": 223}, {"url": "https://preview.redd.it/c2aa6iuxvc4c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb5dbcecaa44bf0d34d201c225e812a8dfe3575b", "width": 320, "height": 331}, {"url": "https://preview.redd.it/c2aa6iuxvc4c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=38704fd91e376b1aa11c25d906bb9832a4250063", "width": 640, "height": 662}, {"url": "https://preview.redd.it/c2aa6iuxvc4c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e96b818731d30387f20de9a67ec4d17e3cdcbb4", "width": 960, "height": 994}, {"url": "https://preview.redd.it/c2aa6iuxvc4c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb2c698d58c7bd5ae829cdb8e750be80a9f685f9", "width": 1080, "height": 1118}], "variants": {}, "id": "8xZ54LThVSrOxepXEYgrno_RFTcaNBVE15R2ivHgdO8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18awjek", "is_robot_indexable": true, "report_reasons": null, "author": "clickyk2019", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18awjek/molex_to_sata_are_these_safe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/c2aa6iuxvc4c1.jpg", "subreddit_subscribers": 716411, "created_utc": 1701729695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Background**\n\nA popular music genre exploration site [everynoise.com](https://everynoise.com/) may be suddenly shut down soon according to [this tweet](https://twitter.com/EveryNoise/status/1731690293391466897). Most likely due to Spotify wanting to charge more for API requests is what most people are thinking.\n\nOne of the best parts of this website is the ability to click on a genre or music artist and instantly hear a \"**snippet**\"(30 second audio file) of music relevant to said genre or artist. \n\n**The Problem**\n\nI've used wget to back up the main page and all individual genre links using this command:\n\n    wget --recursive --level=1 --page-requests https://everynoise.com/\n\nHowever the problem with this is that it doesn't save the \"**snippets**\" of music locally on my machine. And I can't figure out a combination of arguments to achieve this. \n\nLet me show you an example of what I'm trying to achieve. Here is a music genre page. [https://everynoise.com/engenremap-techtrance.html](https://everynoise.com/engenremap-techtrance.html)\n\nNotice that if you click on an artist's name, it will play a 30 second snippet of music. \n\n Now here's a screenshot of a portion of the page source of the previous page:  \n\n[https://i.imgur.com/qtlRCur.jpg](https://i.imgur.com/qtlRCur.jpg)\n\n\nI'm trying to download all the links highlighted in red. That way when I visit the site from my local backup on my machine, I can click on a music artist or music genre and the **snippet** of music I hear is coming from my local machine instead of the internet. \n\nAnd ultimately, my only goal is to back up the everynoise.com page, and just 1 directory down, which would be all the music genre links, artists in the music genre pages, and all the **snippets** of music. I'm not aiming to back up the ENTIRE site, as there is a lot there. Like there are links going all over the place. I just want the top directory, and 1 directory below that. \n\nMaybe this requires a script but I feel like this should be an easy thing to do. I've briefly read through the wget man page and did some googling, but I can't figure this out. And time isn't on my side, which is why I'm urgently asking for help! Thank you so much!", "author_fullname": "t2_slb8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The end may be near for everynoise.com! Need help with preserving audio files with Wget.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18armdx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701717081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A popular music genre exploration site &lt;a href=\"https://everynoise.com/\"&gt;everynoise.com&lt;/a&gt; may be suddenly shut down soon according to &lt;a href=\"https://twitter.com/EveryNoise/status/1731690293391466897\"&gt;this tweet&lt;/a&gt;. Most likely due to Spotify wanting to charge more for API requests is what most people are thinking.&lt;/p&gt;\n\n&lt;p&gt;One of the best parts of this website is the ability to click on a genre or music artist and instantly hear a &amp;quot;&lt;strong&gt;snippet&lt;/strong&gt;&amp;quot;(30 second audio file) of music relevant to said genre or artist. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used wget to back up the main page and all individual genre links using this command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;wget --recursive --level=1 --page-requests https://everynoise.com/\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;However the problem with this is that it doesn&amp;#39;t save the &amp;quot;&lt;strong&gt;snippets&lt;/strong&gt;&amp;quot; of music locally on my machine. And I can&amp;#39;t figure out a combination of arguments to achieve this. &lt;/p&gt;\n\n&lt;p&gt;Let me show you an example of what I&amp;#39;m trying to achieve. Here is a music genre page. &lt;a href=\"https://everynoise.com/engenremap-techtrance.html\"&gt;https://everynoise.com/engenremap-techtrance.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Notice that if you click on an artist&amp;#39;s name, it will play a 30 second snippet of music. &lt;/p&gt;\n\n&lt;p&gt;Now here&amp;#39;s a screenshot of a portion of the page source of the previous page:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/qtlRCur.jpg\"&gt;https://i.imgur.com/qtlRCur.jpg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to download all the links highlighted in red. That way when I visit the site from my local backup on my machine, I can click on a music artist or music genre and the &lt;strong&gt;snippet&lt;/strong&gt; of music I hear is coming from my local machine instead of the internet. &lt;/p&gt;\n\n&lt;p&gt;And ultimately, my only goal is to back up the everynoise.com page, and just 1 directory down, which would be all the music genre links, artists in the music genre pages, and all the &lt;strong&gt;snippets&lt;/strong&gt; of music. I&amp;#39;m not aiming to back up the ENTIRE site, as there is a lot there. Like there are links going all over the place. I just want the top directory, and 1 directory below that. &lt;/p&gt;\n\n&lt;p&gt;Maybe this requires a script but I feel like this should be an easy thing to do. I&amp;#39;ve briefly read through the wget man page and did some googling, but I can&amp;#39;t figure this out. And time isn&amp;#39;t on my side, which is why I&amp;#39;m urgently asking for help! Thank you so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?auto=webp&amp;s=42c4139479a07790615db7101f095a1e8bc358f0", "width": 1042, "height": 169}, "resolutions": [{"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1c8bfe02046269afc188827ea6299e822aa141c", "width": 108, "height": 17}, {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1804ea0f26be212bfbc1fd876df6eb564dcd1519", "width": 216, "height": 35}, {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=12c274020ee5b46c1adc1f2612a145ea46dffeeb", "width": 320, "height": 51}, {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7882954a7bdd4d7fd6c8dfd89be684294f7b743", "width": 640, "height": 103}, {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7b61c725fb5fdda32bc4b6160ae913575732231", "width": 960, "height": 155}], "variants": {}, "id": "Jk7T7SPIF1kUshftBdiaPfxxTCrARyVW1KOf3T3iYnM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18armdx", "is_robot_indexable": true, "report_reasons": null, "author": "suddenlycirclejerk", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18armdx/the_end_may_be_near_for_everynoisecom_need_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18armdx/the_end_may_be_near_for_everynoisecom_need_help/", "subreddit_subscribers": 716411, "created_utc": 1701717081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi.\n\nSo I was finally pretty comfortable with my data storage and recovery solution (BTRFS NAS and Kopia backups), but today I realised i may need something more.\n\nI came across an issue where I wanted to watch a video. I was sure i had downloaded it previously but i couldnt find the file anywhere. It wasnt misplaced into a wrong folder, it was just gone. I wouldnt have deleted it on purpose and to delete a file i would have had to press delete AND confirm deletion. I cant see that happening. My synology logs dont show any recent changes so I cant find out when or how it was removed.\n\nSo I got out one of my 14TB externals which has a Kopia repository on it and sure enough, a snapshot from March this year had the video thats missing. this shows the importance of having incremental backups and data duplication!\n\nI am now considering restoring all of my files from around that date and comparing to my current files just to see if anyhting else is missing.\n\nIs there anything that could have been done to prevent this? I am thinking about turning off automatic emptying of the recycling bin on the NAS. Is there anything else?\n\nIs it possible that there is filesystem corruption or would the Synology have notified me? I do SMART scans every month or 3 i believe and no issues detected.", "author_fullname": "t2_qpfcy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lost and recovered a file. How to prevent losing again (accidental deletion or corruption?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18aueew", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701724135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;So I was finally pretty comfortable with my data storage and recovery solution (BTRFS NAS and Kopia backups), but today I realised i may need something more.&lt;/p&gt;\n\n&lt;p&gt;I came across an issue where I wanted to watch a video. I was sure i had downloaded it previously but i couldnt find the file anywhere. It wasnt misplaced into a wrong folder, it was just gone. I wouldnt have deleted it on purpose and to delete a file i would have had to press delete AND confirm deletion. I cant see that happening. My synology logs dont show any recent changes so I cant find out when or how it was removed.&lt;/p&gt;\n\n&lt;p&gt;So I got out one of my 14TB externals which has a Kopia repository on it and sure enough, a snapshot from March this year had the video thats missing. this shows the importance of having incremental backups and data duplication!&lt;/p&gt;\n\n&lt;p&gt;I am now considering restoring all of my files from around that date and comparing to my current files just to see if anyhting else is missing.&lt;/p&gt;\n\n&lt;p&gt;Is there anything that could have been done to prevent this? I am thinking about turning off automatic emptying of the recycling bin on the NAS. Is there anything else?&lt;/p&gt;\n\n&lt;p&gt;Is it possible that there is filesystem corruption or would the Synology have notified me? I do SMART scans every month or 3 i believe and no issues detected.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18aueew", "is_robot_indexable": true, "report_reasons": null, "author": "BorisTheBladee", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18aueew/lost_and_recovered_a_file_how_to_prevent_losing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18aueew/lost_and_recovered_a_file_how_to_prevent_losing/", "subreddit_subscribers": 716411, "created_utc": 1701724135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My work replaced a few servers sometime last year and IT finally got given the task of disposing of the old equipment. \n\nThe drives were wiped but management also wants us to do additional physical destruction. \n\nWe are just a normal small business so not government related or anything very secure.\n\nWe have a drill press and tried to drill through them. It worked to drill 1 hole in the first one then the bit for metal immeidately got dull and no longer cut through a 2nd hole. \n\nIs there a more specialised drill bit for drilling through whatever material are in HDDs.\n\nIs there an easier way to this? I read some suggestions about magnets. What sort of magnets would be required for this task. ", "author_fullname": "t2_ugxs4jup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best method for physical destruction of HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b3422", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701748507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My work replaced a few servers sometime last year and IT finally got given the task of disposing of the old equipment. &lt;/p&gt;\n\n&lt;p&gt;The drives were wiped but management also wants us to do additional physical destruction. &lt;/p&gt;\n\n&lt;p&gt;We are just a normal small business so not government related or anything very secure.&lt;/p&gt;\n\n&lt;p&gt;We have a drill press and tried to drill through them. It worked to drill 1 hole in the first one then the bit for metal immeidately got dull and no longer cut through a 2nd hole. &lt;/p&gt;\n\n&lt;p&gt;Is there a more specialised drill bit for drilling through whatever material are in HDDs.&lt;/p&gt;\n\n&lt;p&gt;Is there an easier way to this? I read some suggestions about magnets. What sort of magnets would be required for this task. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b3422", "is_robot_indexable": true, "report_reasons": null, "author": "netryn10", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18b3422/best_method_for_physical_destruction_of_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b3422/best_method_for_physical_destruction_of_hdd/", "subreddit_subscribers": 716411, "created_utc": 1701748507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to scan larger format Japanese magazines. The sheets are \"A4 wide\", averaging around 9.2\" x 11.7\". Every sheet fed scanner I can find, mostly being the ScanSnap line, only go up to A4. I initially bought a ScanSnap iX1400 thinking it would potentially have the slight bit of extra clearance, but it didn't and the sheet were less than half an inch too wide. My fault. So then I order the ScanSnap iX1500 because the product page specifically states it does up to A3. Great I'm thinking! A3 is wider than I need but will at least allow me to scan my 20 goddamn magazines!\n\nNOPE. Because it doesn't support up to A3 size, it's the SAME GODDAMN SIZE. You have to use a special feeding sheet, and completely bend the oversized sheet in half to scan it. Not only would that defeat the whole point of speed using a sheet-fed scanner, but it would ruin the page which is also not an option. I'm pissed that I now have $900 in pending Amazon returns because both these stupid goddamn scanners weren't wide enough.\n\nSo I need advice from anyone else who's scanned oversized magazines. Do I:\n\nA) Buy this actual A3 scanner from a lesser known brand: [Scanner](https://www.amazon.com/Plustek-SmartOffice-Document-100-page-Legal-Size/dp/B0BJZBRNQR/ref=cm_cr_arp_d_product_top?ie=UTF8). While this one seems like it would work, it has minimal reviews and multiple of them mention that the scanner is bad for photos. I'm scanning magazine pages that feature full-color illustrations almost every page and have a slight gloss to them, and being that I want to archive them quality is important. I would hate to buy a 3rd goddamn scanner just to have to initiate yet another return because it doesn't work.\n\nB) Do I cut my losses completely and just buy a A3 size flatbed scanner. I wanted a sheet-fed scanner because I have many hundreds of pages to scan, and doing it page by page sounded like a nightmare. At this point I'm starting to feel it's the only option.\n\nPlease, if anyone has a product that they think will work for my unique problem, I'm very open to suggestions at this point.\n\nBefore anyone asks, it's not possible to cut the pages down. Many of the pages would loose sections of artwork or even text if I cut the spine all the way to be exactly A4 size. Defeats the whole point of preservation.", "author_fullname": "t2_1063dn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At my wits ends trying to find a way to scan magazines larger than A4 size, any recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b02tx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701739325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to scan larger format Japanese magazines. The sheets are &amp;quot;A4 wide&amp;quot;, averaging around 9.2&amp;quot; x 11.7&amp;quot;. Every sheet fed scanner I can find, mostly being the ScanSnap line, only go up to A4. I initially bought a ScanSnap iX1400 thinking it would potentially have the slight bit of extra clearance, but it didn&amp;#39;t and the sheet were less than half an inch too wide. My fault. So then I order the ScanSnap iX1500 because the product page specifically states it does up to A3. Great I&amp;#39;m thinking! A3 is wider than I need but will at least allow me to scan my 20 goddamn magazines!&lt;/p&gt;\n\n&lt;p&gt;NOPE. Because it doesn&amp;#39;t support up to A3 size, it&amp;#39;s the SAME GODDAMN SIZE. You have to use a special feeding sheet, and completely bend the oversized sheet in half to scan it. Not only would that defeat the whole point of speed using a sheet-fed scanner, but it would ruin the page which is also not an option. I&amp;#39;m pissed that I now have $900 in pending Amazon returns because both these stupid goddamn scanners weren&amp;#39;t wide enough.&lt;/p&gt;\n\n&lt;p&gt;So I need advice from anyone else who&amp;#39;s scanned oversized magazines. Do I:&lt;/p&gt;\n\n&lt;p&gt;A) Buy this actual A3 scanner from a lesser known brand: &lt;a href=\"https://www.amazon.com/Plustek-SmartOffice-Document-100-page-Legal-Size/dp/B0BJZBRNQR/ref=cm_cr_arp_d_product_top?ie=UTF8\"&gt;Scanner&lt;/a&gt;. While this one seems like it would work, it has minimal reviews and multiple of them mention that the scanner is bad for photos. I&amp;#39;m scanning magazine pages that feature full-color illustrations almost every page and have a slight gloss to them, and being that I want to archive them quality is important. I would hate to buy a 3rd goddamn scanner just to have to initiate yet another return because it doesn&amp;#39;t work.&lt;/p&gt;\n\n&lt;p&gt;B) Do I cut my losses completely and just buy a A3 size flatbed scanner. I wanted a sheet-fed scanner because I have many hundreds of pages to scan, and doing it page by page sounded like a nightmare. At this point I&amp;#39;m starting to feel it&amp;#39;s the only option.&lt;/p&gt;\n\n&lt;p&gt;Please, if anyone has a product that they think will work for my unique problem, I&amp;#39;m very open to suggestions at this point.&lt;/p&gt;\n\n&lt;p&gt;Before anyone asks, it&amp;#39;s not possible to cut the pages down. Many of the pages would loose sections of artwork or even text if I cut the spine all the way to be exactly A4 size. Defeats the whole point of preservation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b02tx", "is_robot_indexable": true, "report_reasons": null, "author": "Killerabbet", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18b02tx/at_my_wits_ends_trying_to_find_a_way_to_scan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b02tx/at_my_wits_ends_trying_to_find_a_way_to_scan/", "subreddit_subscribers": 716411, "created_utc": 1701739325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since this is about data, how would you go by in terms of digitizing hard copy documents. Do you just normally scan individual pages. What software do you use? I have a brother printer and scanner all in one. It has multifeed function. But wondering if this is the effiencent way.", "author_fullname": "t2_xq820", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you guys recommend about digitisation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ax3t5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701731164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since this is about data, how would you go by in terms of digitizing hard copy documents. Do you just normally scan individual pages. What software do you use? I have a brother printer and scanner all in one. It has multifeed function. But wondering if this is the effiencent way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ax3t5", "is_robot_indexable": true, "report_reasons": null, "author": "Inkopol", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ax3t5/what_do_you_guys_recommend_about_digitisation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ax3t5/what_do_you_guys_recommend_about_digitisation/", "subreddit_subscribers": 716411, "created_utc": 1701731164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wondering if anyone who uses Drivepool with Snapraid can help with a question I have. I've searched all over and can't find an answer.\n\nI have a Drivepool set up with some aging drives, mainly for Emby as well as photos etc. My essential files are duplicated as well as backed up on Backblaze but the media files are just freeballing.\n\nI would like to set up a parity drive using Snapraid for the media files since it would be a PIA to figure out whats missing and then replace it in the event of a drive failure. Snapraid would just be for these media files which are static and don't get modified. \n\nI understand automatic rebalancing should be turned off so files do not get moved around too much for Snapraid.\n\nMy question is I also have Stablebit Scanner and one of the options is automatic evacuation if it detects a drive failure.\n\nAm I right to assume I should disable this since it would try to remove files off the dying drive to the remaining drives as it would screw up the parity?\n\nTyping it out I think I know the answer but just want confirmation as I've never used a RAID config before.", "author_fullname": "t2_4sppm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stablebit Drivepool with Snapraid, evacuating a failing drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ai5js", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701688870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if anyone who uses Drivepool with Snapraid can help with a question I have. I&amp;#39;ve searched all over and can&amp;#39;t find an answer.&lt;/p&gt;\n\n&lt;p&gt;I have a Drivepool set up with some aging drives, mainly for Emby as well as photos etc. My essential files are duplicated as well as backed up on Backblaze but the media files are just freeballing.&lt;/p&gt;\n\n&lt;p&gt;I would like to set up a parity drive using Snapraid for the media files since it would be a PIA to figure out whats missing and then replace it in the event of a drive failure. Snapraid would just be for these media files which are static and don&amp;#39;t get modified. &lt;/p&gt;\n\n&lt;p&gt;I understand automatic rebalancing should be turned off so files do not get moved around too much for Snapraid.&lt;/p&gt;\n\n&lt;p&gt;My question is I also have Stablebit Scanner and one of the options is automatic evacuation if it detects a drive failure.&lt;/p&gt;\n\n&lt;p&gt;Am I right to assume I should disable this since it would try to remove files off the dying drive to the remaining drives as it would screw up the parity?&lt;/p&gt;\n\n&lt;p&gt;Typing it out I think I know the answer but just want confirmation as I&amp;#39;ve never used a RAID config before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ai5js", "is_robot_indexable": true, "report_reasons": null, "author": "glide_si", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ai5js/stablebit_drivepool_with_snapraid_evacuating_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ai5js/stablebit_drivepool_with_snapraid_evacuating_a/", "subreddit_subscribers": 716411, "created_utc": 1701688870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I could use a cron script, but I was wondering if there were any better ideas. I'm using Unraid currently.", "author_fullname": "t2_c76e4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to auto-mirror my NAS to a separate hard drive every week?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18b7ltl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701765350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I could use a cron script, but I was wondering if there were any better ideas. I&amp;#39;m using Unraid currently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b7ltl", "is_robot_indexable": true, "report_reasons": null, "author": "AstroCaptain", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18b7ltl/any_way_to_automirror_my_nas_to_a_separate_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b7ltl/any_way_to_automirror_my_nas_to_a_separate_hard/", "subreddit_subscribers": 716411, "created_utc": 1701765350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all! \nI've run out of internal HDD space on my home unraid server (it's a simple tower I had laying around.) I still have sata ports on the mobile but I'm also running out of sata power on my PSU, however I could use some converters splitters because my avg load is around 50-100W and my PSU is 500W with 1 unused 6pin and 1 6+2 pin pci-e. \nWhat do you guys think / recommend as the best solution? \nLooking for some external bays with power, without power, or simply getting some adapters for the unused pcie lanes if there is even such a thing? \n\nThank you in advance!", "author_fullname": "t2_1381kq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDD enclosure with power to disks? Or other solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18b74ga", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701763226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! \nI&amp;#39;ve run out of internal HDD space on my home unraid server (it&amp;#39;s a simple tower I had laying around.) I still have sata ports on the mobile but I&amp;#39;m also running out of sata power on my PSU, however I could use some converters splitters because my avg load is around 50-100W and my PSU is 500W with 1 unused 6pin and 1 6+2 pin pci-e. \nWhat do you guys think / recommend as the best solution? \nLooking for some external bays with power, without power, or simply getting some adapters for the unused pcie lanes if there is even such a thing? &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "18TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b74ga", "is_robot_indexable": true, "report_reasons": null, "author": "Shapperd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18b74ga/external_hdd_enclosure_with_power_to_disks_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b74ga/external_hdd_enclosure_with_power_to_disks_or/", "subreddit_subscribers": 716411, "created_utc": 1701763226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there!\n\nSo having purchased a Mini ITX, I have no room for the 3.5\" Ultrastars I have at 18TB.\n\nI am looking for a nice sleek dual bay desk enclosure (USB 3.1, no RAID) preferably silver or white, that will work with these drives :)\n\n&amp;#x200B;\n\nThe Yottamaster ones on Amazon and Ali Express say \"2X16TB MAX\" but I am not sure if it's because that was the max when it was made or whatever.... ", "author_fullname": "t2_9a5802tg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dual 3.5\" USB 3.1 Enclosure for 2x18TB Ultrastar?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b69f5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701759554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!&lt;/p&gt;\n\n&lt;p&gt;So having purchased a Mini ITX, I have no room for the 3.5&amp;quot; Ultrastars I have at 18TB.&lt;/p&gt;\n\n&lt;p&gt;I am looking for a nice sleek dual bay desk enclosure (USB 3.1, no RAID) preferably silver or white, that will work with these drives :)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The Yottamaster ones on Amazon and Ali Express say &amp;quot;2X16TB MAX&amp;quot; but I am not sure if it&amp;#39;s because that was the max when it was made or whatever.... &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b69f5", "is_robot_indexable": true, "report_reasons": null, "author": "testicularbat", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18b69f5/dual_35_usb_31_enclosure_for_2x18tb_ultrastar/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b69f5/dual_35_usb_31_enclosure_for_2x18tb_ultrastar/", "subreddit_subscribers": 716411, "created_utc": 1701759554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm pretty new to data hoarding. Right now I'm just connecting a single drive to a Raspberry Pi and using that as a media server so I'm not too worried about a disk corruption in the near future (at least until I get the time and money to buy or set up a proper NAS with RAID).\n\nAt least for regular wear and tear (ignoring catastrophic events like my house burning down or a power surge), is it possible to predict when a drive is reaching its end of life? I know SMART exists but is there any Linux software that I can use to check it routinely and notify me (through an email or something like that) when it's going bad? At least this way, I can replace it before it goes bad or be more proactive about setting up RAID.\n\nHow do you usually detect a bad drive (or do you only find out after the drive has failed)?", "author_fullname": "t2_14ubh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to data hoarding: How do I spot a drive that's getting old and is closer to failing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b61fe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701758662.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pretty new to data hoarding. Right now I&amp;#39;m just connecting a single drive to a Raspberry Pi and using that as a media server so I&amp;#39;m not too worried about a disk corruption in the near future (at least until I get the time and money to buy or set up a proper NAS with RAID).&lt;/p&gt;\n\n&lt;p&gt;At least for regular wear and tear (ignoring catastrophic events like my house burning down or a power surge), is it possible to predict when a drive is reaching its end of life? I know SMART exists but is there any Linux software that I can use to check it routinely and notify me (through an email or something like that) when it&amp;#39;s going bad? At least this way, I can replace it before it goes bad or be more proactive about setting up RAID.&lt;/p&gt;\n\n&lt;p&gt;How do you usually detect a bad drive (or do you only find out after the drive has failed)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b61fe", "is_robot_indexable": true, "report_reasons": null, "author": "preethamrn", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18b61fe/new_to_data_hoarding_how_do_i_spot_a_drive_thats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b61fe/new_to_data_hoarding_how_do_i_spot_a_drive_thats/", "subreddit_subscribers": 716411, "created_utc": 1701758662.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I current have the Sabrent EC-WPNE losses connection to the slightest bump. It's extremely frustrating when I'm trying to move files. \n\nSince I'm upgrading, I'm hoping I can find something that can work with both HDD and SSDs. I'll be using it with a Macbook. \n\nDo you guys also have any suggestions for cables? Maybe the connection issue is also due to a bad cable. Would it also be possible that it's just due to a worn out port? \n\nThank you!", "author_fullname": "t2_wo67s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended SSD/HDD External docks/adapters?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b2okm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701747149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I current have the Sabrent EC-WPNE losses connection to the slightest bump. It&amp;#39;s extremely frustrating when I&amp;#39;m trying to move files. &lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m upgrading, I&amp;#39;m hoping I can find something that can work with both HDD and SSDs. I&amp;#39;ll be using it with a Macbook. &lt;/p&gt;\n\n&lt;p&gt;Do you guys also have any suggestions for cables? Maybe the connection issue is also due to a bad cable. Would it also be possible that it&amp;#39;s just due to a worn out port? &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "26TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b2okm", "is_robot_indexable": true, "report_reasons": null, "author": "Jasonwj322a", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18b2okm/recommended_ssdhdd_external_docksadapters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b2okm/recommended_ssdhdd_external_docksadapters/", "subreddit_subscribers": 716411, "created_utc": 1701747149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, my name is Nathan and I am a data hoarder.   \n\n\nAlright now that I've gotten that out of the way I want to lay out my use cases. I have been a photographer for 15+ years and have saved just about every single image that I've taken. In 2020 I started a photography business to start to fund my addiction and this has of course, added tremendously to the amount of storage that I use. I am currently using \\~30TB in storage with just over a million photos. At the current growth that I'm experiencing I expect that I'll hit 1PB of storage within about 15 years... If not sooner given that my camera can shoot 8K RAW video at a rate of 3TB/hr. I also thoroughly enjoy astrophotography and timelapses and those also EAT storage. For instance I've added 8.3TB of data so far this year.   \n\n\nMy question is should I start researching a tape array at least for a backup and/or archival medium?   \n\n\nAnd yes I do have backups in multiple locations specifically at my parents in a fireproof enclosure. ", "author_fullname": "t2_atmah", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I start planning on buying tape storage or wait?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18apgyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701711574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, my name is Nathan and I am a data hoarder.   &lt;/p&gt;\n\n&lt;p&gt;Alright now that I&amp;#39;ve gotten that out of the way I want to lay out my use cases. I have been a photographer for 15+ years and have saved just about every single image that I&amp;#39;ve taken. In 2020 I started a photography business to start to fund my addiction and this has of course, added tremendously to the amount of storage that I use. I am currently using ~30TB in storage with just over a million photos. At the current growth that I&amp;#39;m experiencing I expect that I&amp;#39;ll hit 1PB of storage within about 15 years... If not sooner given that my camera can shoot 8K RAW video at a rate of 3TB/hr. I also thoroughly enjoy astrophotography and timelapses and those also EAT storage. For instance I&amp;#39;ve added 8.3TB of data so far this year.   &lt;/p&gt;\n\n&lt;p&gt;My question is should I start researching a tape array at least for a backup and/or archival medium?   &lt;/p&gt;\n\n&lt;p&gt;And yes I do have backups in multiple locations specifically at my parents in a fireproof enclosure. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB TrueNAS ZFS ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18apgyd", "is_robot_indexable": true, "report_reasons": null, "author": "nks12345", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18apgyd/should_i_start_planning_on_buying_tape_storage_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18apgyd/should_i_start_planning_on_buying_tape_storage_or/", "subreddit_subscribers": 716411, "created_utc": 1701711574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,I'm new to this sub and looked at your posts about HDDs.What I retain is that recertified are almost as good as any drives as they all fail in the end.\n\nI need more spaces as I'm finally implement redundancy.serverpartdeals seems to be the best choice with the \"manufacturer recertified\" option.I found this drives: [x22 20Tb](https://serverpartdeals.com/products/seagate-exos-x22-st20000nm004e-20tb-7-2k-rpm-sata-6gb-s-512e-3-5-recertified-hard-drive) and [x20 20Tb](https://serverpartdeals.com/products/seagate-exos-x20-st20000nm007d-20tb-7-2k-rpm-sata-6gb-s-3-5-hard-drive) drives for \\~ 200Euros\n\nMy question is: Do I go for the refurb at 200Euros + 2y Warrant or do I go for the New at 300Euros+ 5y Warrant?\n\nThanks :D  \n\n\nEDIT:  \nThanks for your answer.  \nI tried to buy one from mindfactory: [x20 20Tb](https://www.mindfactory.de/product_info.php/20TB-Seagate-Exos-X20-ST20000NM007D-256MB-3-5Zoll--8-9cm--SATA_1440042.html)  \nUsing [mailbox.de](https://mailbox.de) as a proxy.  \n\n\nI'll keep you informed.", "author_fullname": "t2_4f1rf3oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "serverpartdeals as EU citizen + Waranty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18an5gf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701720463.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701705339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,I&amp;#39;m new to this sub and looked at your posts about HDDs.What I retain is that recertified are almost as good as any drives as they all fail in the end.&lt;/p&gt;\n\n&lt;p&gt;I need more spaces as I&amp;#39;m finally implement redundancy.serverpartdeals seems to be the best choice with the &amp;quot;manufacturer recertified&amp;quot; option.I found this drives: &lt;a href=\"https://serverpartdeals.com/products/seagate-exos-x22-st20000nm004e-20tb-7-2k-rpm-sata-6gb-s-512e-3-5-recertified-hard-drive\"&gt;x22 20Tb&lt;/a&gt; and &lt;a href=\"https://serverpartdeals.com/products/seagate-exos-x20-st20000nm007d-20tb-7-2k-rpm-sata-6gb-s-3-5-hard-drive\"&gt;x20 20Tb&lt;/a&gt; drives for ~ 200Euros&lt;/p&gt;\n\n&lt;p&gt;My question is: Do I go for the refurb at 200Euros + 2y Warrant or do I go for the New at 300Euros+ 5y Warrant?&lt;/p&gt;\n\n&lt;p&gt;Thanks :D  &lt;/p&gt;\n\n&lt;p&gt;EDIT:&lt;br/&gt;\nThanks for your answer.&lt;br/&gt;\nI tried to buy one from mindfactory: &lt;a href=\"https://www.mindfactory.de/product_info.php/20TB-Seagate-Exos-X20-ST20000NM007D-256MB-3-5Zoll--8-9cm--SATA_1440042.html\"&gt;x20 20Tb&lt;/a&gt;&lt;br/&gt;\nUsing &lt;a href=\"https://mailbox.de\"&gt;mailbox.de&lt;/a&gt; as a proxy.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll keep you informed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?auto=webp&amp;s=d0aff868d87cd3d89bf3ebc1c42c204b8623b7d4", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c81d5071a1d46812ec8ffc7a9f1b72459e370c6", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9995e1f61e0a9241c793c47fd090610ddbd9e006", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2bcfeb59e856dad5dbda44af5b5b3690f3d7fb3a", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0a28485f9c0a14f2669cc025b34817a8dfad6895", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=36338fef61fa2a9a9ddb0586162fcae3cb8ed7dc", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d74e6fcbd2c88f77f21064372614a8b441629267", "width": 1080, "height": 1080}], "variants": {}, "id": "pYYK78LoI3VS0jVCwkkjFru_L6_TdWA-vLrtjpwowCk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18an5gf", "is_robot_indexable": true, "report_reasons": null, "author": "momokinou", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18an5gf/serverpartdeals_as_eu_citizen_waranty/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18an5gf/serverpartdeals_as_eu_citizen_waranty/", "subreddit_subscribers": 716411, "created_utc": 1701705339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,\n\nso I had a 2 bay QNAP NAS for 2 years with 2 12Tb HDDs with movies and tv shows that I Play with my Nvidia Shield.\nBut now the storage is nearly full so I plan to expand the NAS. I have an old PC and everything is ready, now my question is which is the best fitting OS for my purpose?\nI don't need any complicated things, I just want to put the drives in and share them in my local network to my office PC and the Shield\n\nI tried truenas but something didn't work...\nOn one drive a created a dataset and put in a folder with movies, circa 8Tb, on the other drive I created more datasets, 1 for movies, 1 for shows, 1 for music, 1 for documents..\nI put in a folder in each dataset and put the data inside the folder but when I wanted to move some movies in the movie folder suddenly it said it's full after around 3Tb even tho the HDD has still  a few Tb free space left.\nI didn't touch the quotas or permissions, it's all on default, I just created the dataset and shared them via Samba to my office PC and copied the data to the nas drives", "author_fullname": "t2_x0243", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First DIY NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18an05u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701704939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;so I had a 2 bay QNAP NAS for 2 years with 2 12Tb HDDs with movies and tv shows that I Play with my Nvidia Shield.\nBut now the storage is nearly full so I plan to expand the NAS. I have an old PC and everything is ready, now my question is which is the best fitting OS for my purpose?\nI don&amp;#39;t need any complicated things, I just want to put the drives in and share them in my local network to my office PC and the Shield&lt;/p&gt;\n\n&lt;p&gt;I tried truenas but something didn&amp;#39;t work...\nOn one drive a created a dataset and put in a folder with movies, circa 8Tb, on the other drive I created more datasets, 1 for movies, 1 for shows, 1 for music, 1 for documents..\nI put in a folder in each dataset and put the data inside the folder but when I wanted to move some movies in the movie folder suddenly it said it&amp;#39;s full after around 3Tb even tho the HDD has still  a few Tb free space left.\nI didn&amp;#39;t touch the quotas or permissions, it&amp;#39;s all on default, I just created the dataset and shared them via Samba to my office PC and copied the data to the nas drives&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18an05u", "is_robot_indexable": true, "report_reasons": null, "author": "RentonThursten", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18an05u/first_diy_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18an05u/first_diy_nas/", "subreddit_subscribers": 716411, "created_utc": 1701704939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for sata to usb enclosures that has USAP and TRIM support. And if more features like smart reporting work well then thats a plus. I will be using it with WD Blue SSD.   \n  \nSo far I looked at https://www.amazon.in/ORICO-External-Enclosure-Supported-25PW1-U3/dp/B0B936X2XH. It uses JMS578 and the firmware can be flashed to support trim.  \n  \nI found some other enclosures but I don't know how to find the chipset used in them.  \n  \nWhich are some good enclosures that work without any issues in linux/bsd ?", "author_fullname": "t2_o0ru1n1hc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended Sata to USB enclosure ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ah442", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701684349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for sata to usb enclosures that has USAP and TRIM support. And if more features like smart reporting work well then thats a plus. I will be using it with WD Blue SSD.   &lt;/p&gt;\n\n&lt;p&gt;So far I looked at &lt;a href=\"https://www.amazon.in/ORICO-External-Enclosure-Supported-25PW1-U3/dp/B0B936X2XH\"&gt;https://www.amazon.in/ORICO-External-Enclosure-Supported-25PW1-U3/dp/B0B936X2XH&lt;/a&gt;. It uses JMS578 and the firmware can be flashed to support trim.  &lt;/p&gt;\n\n&lt;p&gt;I found some other enclosures but I don&amp;#39;t know how to find the chipset used in them.  &lt;/p&gt;\n\n&lt;p&gt;Which are some good enclosures that work without any issues in linux/bsd ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ah442", "is_robot_indexable": true, "report_reasons": null, "author": "user655362024", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ah442/recommended_sata_to_usb_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ah442/recommended_sata_to_usb_enclosure/", "subreddit_subscribers": 716411, "created_utc": 1701684349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have tried everything I can think of to grab these episodes but nothing I have tried has succeeded. My last attempt was using \"getFLV\" it failed as well. \n\nThis is the site:\n\n[http://starcade.tv/starcade/games/shows.html](http://starcade.tv/starcade/games/shows.html)\n\nAny help would be appreciated.\n\nSolved!!!\n\nThanks to you all!\n\n", "author_fullname": "t2_7w9n5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help grabbing videos from a website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18azaho", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701739273.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701737057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried everything I can think of to grab these episodes but nothing I have tried has succeeded. My last attempt was using &amp;quot;getFLV&amp;quot; it failed as well. &lt;/p&gt;\n\n&lt;p&gt;This is the site:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://starcade.tv/starcade/games/shows.html\"&gt;http://starcade.tv/starcade/games/shows.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Solved!!!&lt;/p&gt;\n\n&lt;p&gt;Thanks to you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18azaho", "is_robot_indexable": true, "report_reasons": null, "author": "Cosmologyman", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18azaho/need_help_grabbing_videos_from_a_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18azaho/need_help_grabbing_videos_from_a_website/", "subreddit_subscribers": 716411, "created_utc": 1701737057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I realize initially the question might sound insane \n\n\"Yes you idiot\" you're screaming at me BACKUP EVERYTHING!\n\nbut as we all know backing up routinely and hw maintenance and upkeep is a process in it of itself so you have to do some cost- benefit assessment.\n\ndo you trust Google drive and Microsoft OneDrive enough to save low to middle priority stuff without doing full backup constantly\n\nI also realize many of you are sophisticated enough to automate this I am however not this savvy. I know with the desktop app you can mirror the same data locally but my issue is I don't have the hardware resources to spare running this from a memory and storage perspective (older hardware) as the computer I'm using is also my daily driver \n\nmaybe if they mirrored to a NAS or something that I didn't have to look at or didnt run persistently on my desktop it would be more feasible. \n\nI ask because I have used lesser known cloud storage services that have since gone belly up and I don't see the aforementioned services going away anytime soon.", "author_fullname": "t2_cr0oek3e9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you bother with backing up small quick data saved on Google or oneDrive into local backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18awtjf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701730424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I realize initially the question might sound insane &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Yes you idiot&amp;quot; you&amp;#39;re screaming at me BACKUP EVERYTHING!&lt;/p&gt;\n\n&lt;p&gt;but as we all know backing up routinely and hw maintenance and upkeep is a process in it of itself so you have to do some cost- benefit assessment.&lt;/p&gt;\n\n&lt;p&gt;do you trust Google drive and Microsoft OneDrive enough to save low to middle priority stuff without doing full backup constantly&lt;/p&gt;\n\n&lt;p&gt;I also realize many of you are sophisticated enough to automate this I am however not this savvy. I know with the desktop app you can mirror the same data locally but my issue is I don&amp;#39;t have the hardware resources to spare running this from a memory and storage perspective (older hardware) as the computer I&amp;#39;m using is also my daily driver &lt;/p&gt;\n\n&lt;p&gt;maybe if they mirrored to a NAS or something that I didn&amp;#39;t have to look at or didnt run persistently on my desktop it would be more feasible. &lt;/p&gt;\n\n&lt;p&gt;I ask because I have used lesser known cloud storage services that have since gone belly up and I don&amp;#39;t see the aforementioned services going away anytime soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18awtjf", "is_robot_indexable": true, "report_reasons": null, "author": "Saergreen", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18awtjf/do_you_bother_with_backing_up_small_quick_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18awtjf/do_you_bother_with_backing_up_small_quick_data/", "subreddit_subscribers": 716411, "created_utc": 1701730424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "RTVE, a Spanish radio station, has a collection of great old jazz podcasts available for free. \n\nThe program is about to be canceled and removed, so I am trying to download the podcasts before they take them down. \n\nThe only issue is the somewhat cumbersome process of manually downloading them by clicking around on their website. I attempted to use yt-dlp to simplify the process, but unfortunately, I encountered error messages.\n\nYou can find the jazz and swing classics on this website: [https://www.rtve.es/play/audios/clasicos-del-jazz-y-del-swing/](https://www.rtve.es/play/audios/clasicos-del-jazz-y-del-swing/)\n\nTo download a podcast manually, you have to click on the three dots next to the podcast, then select 'Download.' This opens a new tab, and you need to click 'Download' again.\n\nI'm not proficient in coding, so I'm looking for suggestions on how to make this process less click-heavy, possibly with a yt-dlp command. Any help in simplifying this would be greatly appreciated.", "author_fullname": "t2_obe8azxwu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm trying to grab a bunch of jazz/swing podcasts before they're taken down, but can't figure out how to do it all at once. Any quick ideas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b1dar", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701743148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;RTVE, a Spanish radio station, has a collection of great old jazz podcasts available for free. &lt;/p&gt;\n\n&lt;p&gt;The program is about to be canceled and removed, so I am trying to download the podcasts before they take them down. &lt;/p&gt;\n\n&lt;p&gt;The only issue is the somewhat cumbersome process of manually downloading them by clicking around on their website. I attempted to use yt-dlp to simplify the process, but unfortunately, I encountered error messages.&lt;/p&gt;\n\n&lt;p&gt;You can find the jazz and swing classics on this website: &lt;a href=\"https://www.rtve.es/play/audios/clasicos-del-jazz-y-del-swing/\"&gt;https://www.rtve.es/play/audios/clasicos-del-jazz-y-del-swing/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To download a podcast manually, you have to click on the three dots next to the podcast, then select &amp;#39;Download.&amp;#39; This opens a new tab, and you need to click &amp;#39;Download&amp;#39; again.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not proficient in coding, so I&amp;#39;m looking for suggestions on how to make this process less click-heavy, possibly with a yt-dlp command. Any help in simplifying this would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HcEFovrxBMHXvAOB-w1fCY6ODcFdh7KoIu_Ywv4-DCo.jpg?auto=webp&amp;s=09229bdd6aa7644bddea748742b0b764ceee199d", "width": 800, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/HcEFovrxBMHXvAOB-w1fCY6ODcFdh7KoIu_Ywv4-DCo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e3810dcfd8844c3ba1895a1181ebbd9c0747cb0", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/HcEFovrxBMHXvAOB-w1fCY6ODcFdh7KoIu_Ywv4-DCo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1c910e1e291cad5a77c7f8530d7d6ff72913747", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/HcEFovrxBMHXvAOB-w1fCY6ODcFdh7KoIu_Ywv4-DCo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3659836a51922d4c2bdd2e48d33184fd76b86b7e", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/HcEFovrxBMHXvAOB-w1fCY6ODcFdh7KoIu_Ywv4-DCo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec0fef9855dcda0f925966f904d6367d64b906f0", "width": 640, "height": 640}], "variants": {}, "id": "jVHqgwCGbZigvpp-x4472jhDjy4qAjF1LyHgYMzxyGc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b1dar", "is_robot_indexable": true, "report_reasons": null, "author": "PuzzleheadedTie6013", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18b1dar/im_trying_to_grab_a_bunch_of_jazzswing_podcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b1dar/im_trying_to_grab_a_bunch_of_jazzswing_podcasts/", "subreddit_subscribers": 716411, "created_utc": 1701743148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been backing up my data with google drive / google photos exclusively for some time now and have been weary of the security of it being my only copy, for obvious reasons. Now with the recent data loss that some google users have been experiencing, I'm all the more concerned. \n\nMy only computer right now is a laptop with a 500 GB SSD. \n\nI plan to buy a 2 or 4 TB external HDD or NAS and backup my google contents periodically using google takeout. However, even if I do this every month or even every week, there is no way for me to ensure no data has been lost or corrupted since the last backup, unless of course the latest backup is smaller than the previous. \n\nHow could one go about ensuring no files have been lost since the last backup? Is there software out there that will somehow compare two data sets and check for file deletion and corruption? I am a true data hoarder, in the sense that most of my drive is in an archival state, I ever really delete from it, just add. But it would be nice if I could retain some of the typical flexibility that make google drive nice to use, ie still being able to move files around at will, copy, rename, delete, etc. \n\nThanks!", "author_fullname": "t2_1193w6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Protect Against Google Accidentally Deleting your Data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18aygjj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701734742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been backing up my data with google drive / google photos exclusively for some time now and have been weary of the security of it being my only copy, for obvious reasons. Now with the recent data loss that some google users have been experiencing, I&amp;#39;m all the more concerned. &lt;/p&gt;\n\n&lt;p&gt;My only computer right now is a laptop with a 500 GB SSD. &lt;/p&gt;\n\n&lt;p&gt;I plan to buy a 2 or 4 TB external HDD or NAS and backup my google contents periodically using google takeout. However, even if I do this every month or even every week, there is no way for me to ensure no data has been lost or corrupted since the last backup, unless of course the latest backup is smaller than the previous. &lt;/p&gt;\n\n&lt;p&gt;How could one go about ensuring no files have been lost since the last backup? Is there software out there that will somehow compare two data sets and check for file deletion and corruption? I am a true data hoarder, in the sense that most of my drive is in an archival state, I ever really delete from it, just add. But it would be nice if I could retain some of the typical flexibility that make google drive nice to use, ie still being able to move files around at will, copy, rename, delete, etc. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18aygjj", "is_robot_indexable": true, "report_reasons": null, "author": "kidneyretailer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18aygjj/how_to_protect_against_google_accidentally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18aygjj/how_to_protect_against_google_accidentally/", "subreddit_subscribers": 716411, "created_utc": 1701734742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been getting a 502 Bad Gateway for a few weeks now. At first, I thought it was because of the Cloudflare DNS issue, so I added exceptions to /etc/hosts as per the instructions found on Reddit. That didn't help. Next I disabled Cloudflare DNS entirely in my modem settings (I had set it up manually there), but even with it disabled, I'm still getting the 502 error. So is archive.ph really just down?", "author_fullname": "t2_yi8x3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive.ph down?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18aif0q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701689958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been getting a 502 Bad Gateway for a few weeks now. At first, I thought it was because of the Cloudflare DNS issue, so I added exceptions to /etc/hosts as per the instructions found on Reddit. That didn&amp;#39;t help. Next I disabled Cloudflare DNS entirely in my modem settings (I had set it up manually there), but even with it disabled, I&amp;#39;m still getting the 502 error. So is archive.ph really just down?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18aif0q", "is_robot_indexable": true, "report_reasons": null, "author": "Vistaus", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18aif0q/archiveph_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18aif0q/archiveph_down/", "subreddit_subscribers": 716411, "created_utc": 1701689958.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}