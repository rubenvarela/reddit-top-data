{"kind": "Listing", "data": {"after": "t3_18ari8m", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nSo, this company, Atlas Obscura, is well-known for unusual places and destinations. I use it all the time to plan my own trips. I think they're not doing great as of recent, and I'm afraid they might discontinue the map - as they did with the community forums last year.\n\nIs there any reasonable way to essentially \"download\" it and store locally?\n\nHere's the link: [https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map](https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map)\n\nEdit: So, even if I can make a local copy - it will look like individual place pages like this: [https://www.atlasobscura.com/places/wall-street-bombing](https://www.atlasobscura.com/places/wall-street-bombing)  \\- where it technically has Google Maps link, GPS coordinates, etc. I can even navigate individual countries through menu - as the places are grouped there. But the map (the actual map available via the link above) is hosted at Google Maps and is owned by AO, so I'm not sure if I'll be  able to download it via WGET.\n\nThanks, and sorry if this is a stupid question to ask", "author_fullname": "t2_6l9t4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A company is probably going to disappear at some point - want to save their map, need help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18arn2a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 111, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 111, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701719790.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701717127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;So, this company, Atlas Obscura, is well-known for unusual places and destinations. I use it all the time to plan my own trips. I think they&amp;#39;re not doing great as of recent, and I&amp;#39;m afraid they might discontinue the map - as they did with the community forums last year.&lt;/p&gt;\n\n&lt;p&gt;Is there any reasonable way to essentially &amp;quot;download&amp;quot; it and store locally?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the link: &lt;a href=\"https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map\"&gt;https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit: So, even if I can make a local copy - it will look like individual place pages like this: &lt;a href=\"https://www.atlasobscura.com/places/wall-street-bombing\"&gt;https://www.atlasobscura.com/places/wall-street-bombing&lt;/a&gt;  - where it technically has Google Maps link, GPS coordinates, etc. I can even navigate individual countries through menu - as the places are grouped there. But the map (the actual map available via the link above) is hosted at Google Maps and is owned by AO, so I&amp;#39;m not sure if I&amp;#39;ll be  able to download it via WGET.&lt;/p&gt;\n\n&lt;p&gt;Thanks, and sorry if this is a stupid question to ask&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HuDDtkAwRY8NpI1livrMY3laApR0q9yJBRz_FYUK2Cc.jpg?auto=webp&amp;s=13c8c463939502ec1104fa517253b5e8fa212c15", "width": 600, "height": 296}, "resolutions": [{"url": "https://external-preview.redd.it/HuDDtkAwRY8NpI1livrMY3laApR0q9yJBRz_FYUK2Cc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfb72ade213bd2a37a296ab2d5e05d9732282e49", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/HuDDtkAwRY8NpI1livrMY3laApR0q9yJBRz_FYUK2Cc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55065974b8f54d6975c02fd2460d67a004df67b8", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/HuDDtkAwRY8NpI1livrMY3laApR0q9yJBRz_FYUK2Cc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f14cfdf7d104f4c056d39537caad70f0a5d71ecb", "width": 320, "height": 157}], "variants": {}, "id": "rP7xfMhC5JJC_8Z2vdB5yVOmZvELEYCGifZb-RRYLa8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18arn2a", "is_robot_indexable": true, "report_reasons": null, "author": "FlamebergU", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18arn2a/a_company_is_probably_going_to_disappear_at_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18arn2a/a_company_is_probably_going_to_disappear_at_some/", "subreddit_subscribers": 716362, "created_utc": 1701717127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ZFS, snapshots, ECC Ram, 3 backups and a single fuckup is all it takes. I had a major pool of twelve 18TB of zRaid 3. I had 2 smaller pools of about four 14TB drives and four 16TB drives. I decided to merge them to make a single larger backup pool. Before I did that though, I tried to do a replication task to my main pool of something I didn't want to lose.\n\nThe 16 x4 drives were remote. I brought them back to location as moving 40TB of data over the internet is not ideal.\n\nGuess I screwed up the location or something and didn't notice anything wrong. Wiped my backups to be merged instead of just adding another vdev to one of them. I wanted the extra write speed performance that comes with a fresh dual vdev pool when writing as it had multiple purposes.\n\nLow and behold I noticed my personal files were just gone. The Datasets they were in just vanished. The fear sets in. That's okay, I have an encrypted 4th backup of my personal files. The encryption password wasn't working? Oh fuck, oh fuck! My most important files were there! After almost having a panic attack I keep trying different keys I have for encrypted pools but they don't work. After manually opening a json file to extract just the key for one of them does it work.\n\nWhew! I am in the clear. I back up that data. Lesson learned, have another drive unencrypted stored safely somewhere in case you also lose access to the key too.\n\nAt least my plex library looked like it wasn't touched. Try to play something but it errors out. Hmm, strange. I wonder if the permissions accidentally got changed? They did, lets fix that and get the new backup going, don't want any other heart attacks. Nope, still can't play it. Huh, strange. Go to try to play a file manually. They aren't there. Oh no. That's okay, I have snapshots I can revert to. No, all my snapshots from before today are also just gone. The data is still taking the same amount of space according to truenas. However, nothing is there. Is it corrupted now? I don't know. I can try to run a scrub but all my snapshots are just gone.\n\nMaybe when the back finishes it will allow me to view the files, but that is likely just wishful thinking. For some reason my movies are fine, but all else seems gone.\n\nNo matter how prepared you are, a little bit of misfortune and bad timing can just take it all away. If you have any potential solution to files that appear to be taking space but don't show up, I would be thrilled to hear it. The thing I am most upset about now is that I had a massive lossless music library and all the hard work I put into curating and editing metadata is just gone.\n\nIt seemed reasonable at the time, sure I would have only one copy during that time for about 24 hours until it finishes replicating, but with 3 drives of redundancy, how could it ever fail?\n\nEdit: I appear to have also had a 4th copy of my music library, unfortunately before my major lossless addition, but at least I am not at ground zero.  \n\n\nEdit 2: Holy fuck, I might just have a chance of recovery. For whatever reason, making a replication of the bad Data appears to to produce potentially good versions. There may still be hope yet lads!", "author_fullname": "t2_3cxp0v7b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Well it happened, I think lost almost everything. 40 Terabytes gone.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18auavd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 95, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 95, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701734899.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701723872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ZFS, snapshots, ECC Ram, 3 backups and a single fuckup is all it takes. I had a major pool of twelve 18TB of zRaid 3. I had 2 smaller pools of about four 14TB drives and four 16TB drives. I decided to merge them to make a single larger backup pool. Before I did that though, I tried to do a replication task to my main pool of something I didn&amp;#39;t want to lose.&lt;/p&gt;\n\n&lt;p&gt;The 16 x4 drives were remote. I brought them back to location as moving 40TB of data over the internet is not ideal.&lt;/p&gt;\n\n&lt;p&gt;Guess I screwed up the location or something and didn&amp;#39;t notice anything wrong. Wiped my backups to be merged instead of just adding another vdev to one of them. I wanted the extra write speed performance that comes with a fresh dual vdev pool when writing as it had multiple purposes.&lt;/p&gt;\n\n&lt;p&gt;Low and behold I noticed my personal files were just gone. The Datasets they were in just vanished. The fear sets in. That&amp;#39;s okay, I have an encrypted 4th backup of my personal files. The encryption password wasn&amp;#39;t working? Oh fuck, oh fuck! My most important files were there! After almost having a panic attack I keep trying different keys I have for encrypted pools but they don&amp;#39;t work. After manually opening a json file to extract just the key for one of them does it work.&lt;/p&gt;\n\n&lt;p&gt;Whew! I am in the clear. I back up that data. Lesson learned, have another drive unencrypted stored safely somewhere in case you also lose access to the key too.&lt;/p&gt;\n\n&lt;p&gt;At least my plex library looked like it wasn&amp;#39;t touched. Try to play something but it errors out. Hmm, strange. I wonder if the permissions accidentally got changed? They did, lets fix that and get the new backup going, don&amp;#39;t want any other heart attacks. Nope, still can&amp;#39;t play it. Huh, strange. Go to try to play a file manually. They aren&amp;#39;t there. Oh no. That&amp;#39;s okay, I have snapshots I can revert to. No, all my snapshots from before today are also just gone. The data is still taking the same amount of space according to truenas. However, nothing is there. Is it corrupted now? I don&amp;#39;t know. I can try to run a scrub but all my snapshots are just gone.&lt;/p&gt;\n\n&lt;p&gt;Maybe when the back finishes it will allow me to view the files, but that is likely just wishful thinking. For some reason my movies are fine, but all else seems gone.&lt;/p&gt;\n\n&lt;p&gt;No matter how prepared you are, a little bit of misfortune and bad timing can just take it all away. If you have any potential solution to files that appear to be taking space but don&amp;#39;t show up, I would be thrilled to hear it. The thing I am most upset about now is that I had a massive lossless music library and all the hard work I put into curating and editing metadata is just gone.&lt;/p&gt;\n\n&lt;p&gt;It seemed reasonable at the time, sure I would have only one copy during that time for about 24 hours until it finishes replicating, but with 3 drives of redundancy, how could it ever fail?&lt;/p&gt;\n\n&lt;p&gt;Edit: I appear to have also had a 4th copy of my music library, unfortunately before my major lossless addition, but at least I am not at ground zero.  &lt;/p&gt;\n\n&lt;p&gt;Edit 2: Holy fuck, I might just have a chance of recovery. For whatever reason, making a replication of the bad Data appears to to produce potentially good versions. There may still be hope yet lads!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18auavd", "is_robot_indexable": true, "report_reasons": null, "author": "ALittleBurnerAccount", "discussion_type": null, "num_comments": 59, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18auavd/well_it_happened_i_think_lost_almost_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18auavd/well_it_happened_i_think_lost_almost_everything/", "subreddit_subscribers": 716362, "created_utc": 1701723872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "FYI... Technodealsusa has the 16TB Exos X16 (ST16000NM001G) on sale for **$180** with free shipping. Here's a [link](https://technodealsusa.com/products/seagate-exos-16tb-enterprise-hdd-x16-sata-6gb-s-512e-4kn-7200-rpm-256mb-cache-3-5-internal-hard-drive-st16000nm001g.html) to that page. Technodealsusa looks legit (1000's of customer reviews on their Newegg and Amazon store pages) so I pulled the trigger on 4 of these puppies.\n\nI can't make any guarantees but this looks like a steal if you're in the market for some new drives especially since these drives, refurbished, cost about $150 on ServerPartsDeals.", "author_fullname": "t2_61d9v4ak", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos 16TB on Sale for $180", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18al2e2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701699235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;FYI... Technodealsusa has the 16TB Exos X16 (ST16000NM001G) on sale for &lt;strong&gt;$180&lt;/strong&gt; with free shipping. Here&amp;#39;s a &lt;a href=\"https://technodealsusa.com/products/seagate-exos-16tb-enterprise-hdd-x16-sata-6gb-s-512e-4kn-7200-rpm-256mb-cache-3-5-internal-hard-drive-st16000nm001g.html\"&gt;link&lt;/a&gt; to that page. Technodealsusa looks legit (1000&amp;#39;s of customer reviews on their Newegg and Amazon store pages) so I pulled the trigger on 4 of these puppies.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t make any guarantees but this looks like a steal if you&amp;#39;re in the market for some new drives especially since these drives, refurbished, cost about $150 on ServerPartsDeals.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nDocmX_jRT3GQcuZt7bJMBEDDMDh1yEN7w4og8kd1dQ.jpg?auto=webp&amp;s=3e3d0a3df1d6c9bf20f527a6bb3e447ea6a7c256", "width": 500, "height": 375}, "resolutions": [{"url": "https://external-preview.redd.it/nDocmX_jRT3GQcuZt7bJMBEDDMDh1yEN7w4og8kd1dQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a14605e606fa63c7bb0357c0ced3c59ce231ae55", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/nDocmX_jRT3GQcuZt7bJMBEDDMDh1yEN7w4og8kd1dQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0dba131acac584d49b9aeb6dd9f92c3dd6e7a186", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/nDocmX_jRT3GQcuZt7bJMBEDDMDh1yEN7w4og8kd1dQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7366c0608eb4c97087527a661e3bdfb1de470bca", "width": 320, "height": 240}], "variants": {}, "id": "nCg50H6n5w0bAIU1VqyMHDOLsMp_6GtnU5mKIjCOyGA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "18al2e2", "is_robot_indexable": true, "report_reasons": null, "author": "Ben4425", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18al2e2/seagate_exos_16tb_on_sale_for_180/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18al2e2/seagate_exos_16tb_on_sale_for_180/", "subreddit_subscribers": 716362, "created_utc": 1701699235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got complaints that Plex wasn't up.  I discovered the media mount disappeared.  Checked the Areca controller logs.  Full of \"Under Voltage\" warnings then \"Recovered\" 2 seconds later.  Then today's log shows Drive 1 going offline, then Drive 2, then Drive 16.\n\nWhat the belly rubbin' pit scratchin' nut dunkin' hell is going on.  Drive failures?  PSU?  Cage backplane? Brownouts?\n\nDrive 1 and 2 had gone back to being free drives in the array, meaning \"duh, I dunno what I am but here I am anyway\", and Drive 16 was simply offline.  I put 16 back online, which brought the RAID6 volume back online but in a degraded state.  \n\nSo full marks for Areca RAID controllers - I haven't lost any data.  \"So far\".\n\nBut having two drives fail at the same time is sus as hell but possible since those were both from the same batch/order from 5 years ago. But I dunno.  Drives 1 and 2 in the 24-bay cage failing seems like a backplane problem, but that's going to be a nightmare to isolate.  And I'm not going to rush out to buy a new PSU without knowing it's the culprit.  But at least it's not a mains voltage problem (I think - because it's all connected to a hefty UPS that feeds it nice clean smooth power.  IT BETTER BE.)\n\nI've moved drives 1 and 2 to some free slots to see if the problem follows the drive or the slot, and started a 10 hour rebuild of a 140TB array.  With 2 drives failing though I have to fallback should this thrashing session cause another drive to go offline.\n\nThis is fun.\n\nI'm fine.", "author_fullname": "t2_ldfi7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Years of non-stop 16-drive hardware RAID6 service. Today, 2 drives failed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ay6pm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701734016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got complaints that Plex wasn&amp;#39;t up.  I discovered the media mount disappeared.  Checked the Areca controller logs.  Full of &amp;quot;Under Voltage&amp;quot; warnings then &amp;quot;Recovered&amp;quot; 2 seconds later.  Then today&amp;#39;s log shows Drive 1 going offline, then Drive 2, then Drive 16.&lt;/p&gt;\n\n&lt;p&gt;What the belly rubbin&amp;#39; pit scratchin&amp;#39; nut dunkin&amp;#39; hell is going on.  Drive failures?  PSU?  Cage backplane? Brownouts?&lt;/p&gt;\n\n&lt;p&gt;Drive 1 and 2 had gone back to being free drives in the array, meaning &amp;quot;duh, I dunno what I am but here I am anyway&amp;quot;, and Drive 16 was simply offline.  I put 16 back online, which brought the RAID6 volume back online but in a degraded state.  &lt;/p&gt;\n\n&lt;p&gt;So full marks for Areca RAID controllers - I haven&amp;#39;t lost any data.  &amp;quot;So far&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;But having two drives fail at the same time is sus as hell but possible since those were both from the same batch/order from 5 years ago. But I dunno.  Drives 1 and 2 in the 24-bay cage failing seems like a backplane problem, but that&amp;#39;s going to be a nightmare to isolate.  And I&amp;#39;m not going to rush out to buy a new PSU without knowing it&amp;#39;s the culprit.  But at least it&amp;#39;s not a mains voltage problem (I think - because it&amp;#39;s all connected to a hefty UPS that feeds it nice clean smooth power.  IT BETTER BE.)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve moved drives 1 and 2 to some free slots to see if the problem follows the drive or the slot, and started a 10 hour rebuild of a 140TB array.  With 2 drives failing though I have to fallback should this thrashing session cause another drive to go offline.&lt;/p&gt;\n\n&lt;p&gt;This is fun.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ay6pm", "is_robot_indexable": true, "report_reasons": null, "author": "SpinCharm", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ay6pm/5_years_of_nonstop_16drive_hardware_raid6_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ay6pm/5_years_of_nonstop_16drive_hardware_raid6_service/", "subreddit_subscribers": 716362, "created_utc": 1701734016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Background**\n\nA popular music genre exploration site [everynoise.com](https://everynoise.com/) may be suddenly shut down soon according to [this tweet](https://twitter.com/EveryNoise/status/1731690293391466897). Most likely due to Spotify wanting to charge more for API requests is what most people are thinking.\n\nOne of the best parts of this website is the ability to click on a genre or music artist and instantly hear a \"**snippet**\"(30 second audio file) of music relevant to said genre or artist. \n\n**The Problem**\n\nI've used wget to back up the main page and all individual genre links using this command:\n\n    wget --recursive --level=1 --page-requests https://everynoise.com/\n\nHowever the problem with this is that it doesn't save the \"**snippets**\" of music locally on my machine. And I can't figure out a combination of arguments to achieve this. \n\nLet me show you an example of what I'm trying to achieve. Here is a music genre page. [https://everynoise.com/engenremap-techtrance.html](https://everynoise.com/engenremap-techtrance.html)\n\nNotice that if you click on an artist's name, it will play a 30 second snippet of music. \n\n Now here's a screenshot of a portion of the page source of the previous page:  \n\n[https://i.imgur.com/qtlRCur.jpg](https://i.imgur.com/qtlRCur.jpg)\n\n\nI'm trying to download all the links highlighted in red. That way when I visit the site from my local backup on my machine, I can click on a music artist or music genre and the **snippet** of music I hear is coming from my local machine instead of the internet. \n\nAnd ultimately, my only goal is to back up the everynoise.com page, and just 1 directory down, which would be all the music genre links, artists in the music genre pages, and all the **snippets** of music. I'm not aiming to back up the ENTIRE site, as there is a lot there. Like there are links going all over the place. I just want the top directory, and 1 directory below that. \n\nMaybe this requires a script but I feel like this should be an easy thing to do. I've briefly read through the wget man page and did some googling, but I can't figure this out. And time isn't on my side, which is why I'm urgently asking for help! Thank you so much!", "author_fullname": "t2_slb8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The end may be near for everynoise.com! Need help with preserving audio files with Wget.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18armdx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701717081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A popular music genre exploration site &lt;a href=\"https://everynoise.com/\"&gt;everynoise.com&lt;/a&gt; may be suddenly shut down soon according to &lt;a href=\"https://twitter.com/EveryNoise/status/1731690293391466897\"&gt;this tweet&lt;/a&gt;. Most likely due to Spotify wanting to charge more for API requests is what most people are thinking.&lt;/p&gt;\n\n&lt;p&gt;One of the best parts of this website is the ability to click on a genre or music artist and instantly hear a &amp;quot;&lt;strong&gt;snippet&lt;/strong&gt;&amp;quot;(30 second audio file) of music relevant to said genre or artist. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used wget to back up the main page and all individual genre links using this command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;wget --recursive --level=1 --page-requests https://everynoise.com/\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;However the problem with this is that it doesn&amp;#39;t save the &amp;quot;&lt;strong&gt;snippets&lt;/strong&gt;&amp;quot; of music locally on my machine. And I can&amp;#39;t figure out a combination of arguments to achieve this. &lt;/p&gt;\n\n&lt;p&gt;Let me show you an example of what I&amp;#39;m trying to achieve. Here is a music genre page. &lt;a href=\"https://everynoise.com/engenremap-techtrance.html\"&gt;https://everynoise.com/engenremap-techtrance.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Notice that if you click on an artist&amp;#39;s name, it will play a 30 second snippet of music. &lt;/p&gt;\n\n&lt;p&gt;Now here&amp;#39;s a screenshot of a portion of the page source of the previous page:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/qtlRCur.jpg\"&gt;https://i.imgur.com/qtlRCur.jpg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to download all the links highlighted in red. That way when I visit the site from my local backup on my machine, I can click on a music artist or music genre and the &lt;strong&gt;snippet&lt;/strong&gt; of music I hear is coming from my local machine instead of the internet. &lt;/p&gt;\n\n&lt;p&gt;And ultimately, my only goal is to back up the everynoise.com page, and just 1 directory down, which would be all the music genre links, artists in the music genre pages, and all the &lt;strong&gt;snippets&lt;/strong&gt; of music. I&amp;#39;m not aiming to back up the ENTIRE site, as there is a lot there. Like there are links going all over the place. I just want the top directory, and 1 directory below that. &lt;/p&gt;\n\n&lt;p&gt;Maybe this requires a script but I feel like this should be an easy thing to do. I&amp;#39;ve briefly read through the wget man page and did some googling, but I can&amp;#39;t figure this out. And time isn&amp;#39;t on my side, which is why I&amp;#39;m urgently asking for help! Thank you so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?auto=webp&amp;s=42c4139479a07790615db7101f095a1e8bc358f0", "width": 1042, "height": 169}, "resolutions": [{"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1c8bfe02046269afc188827ea6299e822aa141c", "width": 108, "height": 17}, {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1804ea0f26be212bfbc1fd876df6eb564dcd1519", "width": 216, "height": 35}, {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=12c274020ee5b46c1adc1f2612a145ea46dffeeb", "width": 320, "height": 51}, {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7882954a7bdd4d7fd6c8dfd89be684294f7b743", "width": 640, "height": 103}, {"url": "https://external-preview.redd.it/q8meJ9x4AIbB-xwuoahJJqauUqcVgvvdZjiRkFYryAc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7b61c725fb5fdda32bc4b6160ae913575732231", "width": 960, "height": 155}], "variants": {}, "id": "Jk7T7SPIF1kUshftBdiaPfxxTCrARyVW1KOf3T3iYnM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18armdx", "is_robot_indexable": true, "report_reasons": null, "author": "suddenlycirclejerk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18armdx/the_end_may_be_near_for_everynoisecom_need_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18armdx/the_end_may_be_near_for_everynoisecom_need_help/", "subreddit_subscribers": 716362, "created_utc": 1701717081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi.\n\nSo I was finally pretty comfortable with my data storage and recovery solution (BTRFS NAS and Kopia backups), but today I realised i may need something more.\n\nI came across an issue where I wanted to watch a video. I was sure i had downloaded it previously but i couldnt find the file anywhere. It wasnt misplaced into a wrong folder, it was just gone. I wouldnt have deleted it on purpose and to delete a file i would have had to press delete AND confirm deletion. I cant see that happening. My synology logs dont show any recent changes so I cant find out when or how it was removed.\n\nSo I got out one of my 14TB externals which has a Kopia repository on it and sure enough, a snapshot from March this year had the video thats missing. this shows the importance of having incremental backups and data duplication!\n\nI am now considering restoring all of my files from around that date and comparing to my current files just to see if anyhting else is missing.\n\nIs there anything that could have been done to prevent this? I am thinking about turning off automatic emptying of the recycling bin on the NAS. Is there anything else?\n\nIs it possible that there is filesystem corruption or would the Synology have notified me? I do SMART scans every month or 3 i believe and no issues detected.", "author_fullname": "t2_qpfcy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lost and recovered a file. How to prevent losing again (accidental deletion or corruption?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18aueew", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701724135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;So I was finally pretty comfortable with my data storage and recovery solution (BTRFS NAS and Kopia backups), but today I realised i may need something more.&lt;/p&gt;\n\n&lt;p&gt;I came across an issue where I wanted to watch a video. I was sure i had downloaded it previously but i couldnt find the file anywhere. It wasnt misplaced into a wrong folder, it was just gone. I wouldnt have deleted it on purpose and to delete a file i would have had to press delete AND confirm deletion. I cant see that happening. My synology logs dont show any recent changes so I cant find out when or how it was removed.&lt;/p&gt;\n\n&lt;p&gt;So I got out one of my 14TB externals which has a Kopia repository on it and sure enough, a snapshot from March this year had the video thats missing. this shows the importance of having incremental backups and data duplication!&lt;/p&gt;\n\n&lt;p&gt;I am now considering restoring all of my files from around that date and comparing to my current files just to see if anyhting else is missing.&lt;/p&gt;\n\n&lt;p&gt;Is there anything that could have been done to prevent this? I am thinking about turning off automatic emptying of the recycling bin on the NAS. Is there anything else?&lt;/p&gt;\n\n&lt;p&gt;Is it possible that there is filesystem corruption or would the Synology have notified me? I do SMART scans every month or 3 i believe and no issues detected.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18aueew", "is_robot_indexable": true, "report_reasons": null, "author": "BorisTheBladee", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18aueew/lost_and_recovered_a_file_how_to_prevent_losing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18aueew/lost_and_recovered_a_file_how_to_prevent_losing/", "subreddit_subscribers": 716362, "created_utc": 1701724135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://forums.aida64.com/topic/667-share-your-sensorpanel/\n\nAIDA64 sensor panels are not archived anywhere as far as I know and trying to find them is horrible, it's a nearly 500 page thread. \n\nGotta be a solution out there already right? I foumd this https://creativeandcritical.net/fups but doesn't work with Invision forums.", "author_fullname": "t2_13xcos", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I scrape every .sensorpanel attachment from this thread?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18aaho6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701658156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://forums.aida64.com/topic/667-share-your-sensorpanel/\"&gt;https://forums.aida64.com/topic/667-share-your-sensorpanel/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;AIDA64 sensor panels are not archived anywhere as far as I know and trying to find them is horrible, it&amp;#39;s a nearly 500 page thread. &lt;/p&gt;\n\n&lt;p&gt;Gotta be a solution out there already right? I foumd this &lt;a href=\"https://creativeandcritical.net/fups\"&gt;https://creativeandcritical.net/fups&lt;/a&gt; but doesn&amp;#39;t work with Invision forums.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18aaho6", "is_robot_indexable": true, "report_reasons": null, "author": "ocp-paradox", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18aaho6/how_can_i_scrape_every_sensorpanel_attachment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18aaho6/how_can_i_scrape_every_sensorpanel_attachment/", "subreddit_subscribers": 716362, "created_utc": 1701658156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, my name is Nathan and I am a data hoarder.   \n\n\nAlright now that I've gotten that out of the way I want to lay out my use cases. I have been a photographer for 15+ years and have saved just about every single image that I've taken. In 2020 I started a photography business to start to fund my addiction and this has of course, added tremendously to the amount of storage that I use. I am currently using \\~30TB in storage with just over a million photos. At the current growth that I'm experiencing I expect that I'll hit 1PB of storage within about 15 years... If not sooner given that my camera can shoot 8K RAW video at a rate of 3TB/hr. I also thoroughly enjoy astrophotography and timelapses and those also EAT storage. For instance I've added 8.3TB of data so far this year.   \n\n\nMy question is should I start researching a tape array at least for a backup and/or archival medium?   \n\n\nAnd yes I do have backups in multiple locations specifically at my parents in a fireproof enclosure. ", "author_fullname": "t2_atmah", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I start planning on buying tape storage or wait?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18apgyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701711574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, my name is Nathan and I am a data hoarder.   &lt;/p&gt;\n\n&lt;p&gt;Alright now that I&amp;#39;ve gotten that out of the way I want to lay out my use cases. I have been a photographer for 15+ years and have saved just about every single image that I&amp;#39;ve taken. In 2020 I started a photography business to start to fund my addiction and this has of course, added tremendously to the amount of storage that I use. I am currently using ~30TB in storage with just over a million photos. At the current growth that I&amp;#39;m experiencing I expect that I&amp;#39;ll hit 1PB of storage within about 15 years... If not sooner given that my camera can shoot 8K RAW video at a rate of 3TB/hr. I also thoroughly enjoy astrophotography and timelapses and those also EAT storage. For instance I&amp;#39;ve added 8.3TB of data so far this year.   &lt;/p&gt;\n\n&lt;p&gt;My question is should I start researching a tape array at least for a backup and/or archival medium?   &lt;/p&gt;\n\n&lt;p&gt;And yes I do have backups in multiple locations specifically at my parents in a fireproof enclosure. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB TrueNAS ZFS ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18apgyd", "is_robot_indexable": true, "report_reasons": null, "author": "nks12345", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18apgyd/should_i_start_planning_on_buying_tape_storage_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18apgyd/should_i_start_planning_on_buying_tape_storage_or/", "subreddit_subscribers": 716362, "created_utc": 1701711574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wondering if anyone who uses Drivepool with Snapraid can help with a question I have. I've searched all over and can't find an answer.\n\nI have a Drivepool set up with some aging drives, mainly for Emby as well as photos etc. My essential files are duplicated as well as backed up on Backblaze but the media files are just freeballing.\n\nI would like to set up a parity drive using Snapraid for the media files since it would be a PIA to figure out whats missing and then replace it in the event of a drive failure. Snapraid would just be for these media files which are static and don't get modified. \n\nI understand automatic rebalancing should be turned off so files do not get moved around too much for Snapraid.\n\nMy question is I also have Stablebit Scanner and one of the options is automatic evacuation if it detects a drive failure.\n\nAm I right to assume I should disable this since it would try to remove files off the dying drive to the remaining drives as it would screw up the parity?\n\nTyping it out I think I know the answer but just want confirmation as I've never used a RAID config before.", "author_fullname": "t2_4sppm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stablebit Drivepool with Snapraid, evacuating a failing drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ai5js", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701688870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if anyone who uses Drivepool with Snapraid can help with a question I have. I&amp;#39;ve searched all over and can&amp;#39;t find an answer.&lt;/p&gt;\n\n&lt;p&gt;I have a Drivepool set up with some aging drives, mainly for Emby as well as photos etc. My essential files are duplicated as well as backed up on Backblaze but the media files are just freeballing.&lt;/p&gt;\n\n&lt;p&gt;I would like to set up a parity drive using Snapraid for the media files since it would be a PIA to figure out whats missing and then replace it in the event of a drive failure. Snapraid would just be for these media files which are static and don&amp;#39;t get modified. &lt;/p&gt;\n\n&lt;p&gt;I understand automatic rebalancing should be turned off so files do not get moved around too much for Snapraid.&lt;/p&gt;\n\n&lt;p&gt;My question is I also have Stablebit Scanner and one of the options is automatic evacuation if it detects a drive failure.&lt;/p&gt;\n\n&lt;p&gt;Am I right to assume I should disable this since it would try to remove files off the dying drive to the remaining drives as it would screw up the parity?&lt;/p&gt;\n\n&lt;p&gt;Typing it out I think I know the answer but just want confirmation as I&amp;#39;ve never used a RAID config before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ai5js", "is_robot_indexable": true, "report_reasons": null, "author": "glide_si", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ai5js/stablebit_drivepool_with_snapraid_evacuating_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ai5js/stablebit_drivepool_with_snapraid_evacuating_a/", "subreddit_subscribers": 716362, "created_utc": 1701688870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Will files be omitted for illegal windows characters when using RSYNC to copy files from a Mac to a Windows machine? Is there a flag to just tell rsync to rename the fie if it contains an illegal character?\n\nI have files that have pipes in the name, like \u201csome file | form somewhere\u201d that work fine on a Mac but that Windows and OneDrive do not like.\n\nI do clean my file regular of illegal characters using Name Mangler, so I understand the need to do that.", "author_fullname": "t2_6ognx3yr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RSYNC flag to omit illegal characters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18acyl9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701666438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Will files be omitted for illegal windows characters when using RSYNC to copy files from a Mac to a Windows machine? Is there a flag to just tell rsync to rename the fie if it contains an illegal character?&lt;/p&gt;\n\n&lt;p&gt;I have files that have pipes in the name, like \u201csome file | form somewhere\u201d that work fine on a Mac but that Windows and OneDrive do not like.&lt;/p&gt;\n\n&lt;p&gt;I do clean my file regular of illegal characters using Name Mangler, so I understand the need to do that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18acyl9", "is_robot_indexable": true, "report_reasons": null, "author": "tekfranz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18acyl9/rsync_flag_to_omit_illegal_characters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18acyl9/rsync_flag_to_omit_illegal_characters/", "subreddit_subscribers": 716362, "created_utc": 1701666438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to scan larger format Japanese magazines. The sheets are \"A4 wide\", averaging around 9.2\" x 11.7\". Every sheet fed scanner I can find, mostly being the ScanSnap line, only go up to A4. I initially bought a ScanSnap iX1400 thinking it would potentially have the slight bit of extra clearance, but it didn't and the sheet were less than half an inch too wide. My fault. So then I order the ScanSnap iX1500 because the product page specifically states it does up to A3. Great I'm thinking! A3 is wider than I need but will at least allow me to scan my 20 goddamn magazines!\n\nNOPE. Because it doesn't support up to A3 size, it's the SAME GODDAMN SIZE. You have to use a special feeding sheet, and completely bend the oversized sheet in half to scan it. Not only would that defeat the whole point of speed using a sheet-fed scanner, but it would ruin the page which is also not an option. I'm pissed that I now have $900 in pending Amazon returns because both these stupid goddamn scanners weren't wide enough.\n\nSo I need advice from anyone else who's scanned oversized magazines. Do I:\n\nA) Buy this actual A3 scanner from a lesser known brand: [Scanner](https://www.amazon.com/Plustek-SmartOffice-Document-100-page-Legal-Size/dp/B0BJZBRNQR/ref=cm_cr_arp_d_product_top?ie=UTF8). While this one seems like it would work, it has minimal reviews and multiple of them mention that the scanner is bad for photos. I'm scanning magazine pages that feature full-color illustrations almost every page and have a slight gloss to them, and being that I want to archive them quality is important. I would hate to buy a 3rd goddamn scanner just to have to initiate yet another return because it doesn't work.\n\nB) Do I cut my losses completely and just buy a A3 size flatbed scanner. I wanted a sheet-fed scanner because I have many hundreds of pages to scan, and doing it page by page sounded like a nightmare. At this point I'm starting to feel it's the only option.\n\nPlease, if anyone has a product that they think will work for my unique problem, I'm very open to suggestions at this point.\n\nBefore anyone asks, it's not possible to cut the pages down. Many of the pages would loose sections of artwork or even text if I cut the spine all the way to be exactly A4 size. Defeats the whole point of preservation.", "author_fullname": "t2_1063dn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At my wits ends trying to find a way to scan magazines larger than A4 size, any recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18b02tx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701739325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to scan larger format Japanese magazines. The sheets are &amp;quot;A4 wide&amp;quot;, averaging around 9.2&amp;quot; x 11.7&amp;quot;. Every sheet fed scanner I can find, mostly being the ScanSnap line, only go up to A4. I initially bought a ScanSnap iX1400 thinking it would potentially have the slight bit of extra clearance, but it didn&amp;#39;t and the sheet were less than half an inch too wide. My fault. So then I order the ScanSnap iX1500 because the product page specifically states it does up to A3. Great I&amp;#39;m thinking! A3 is wider than I need but will at least allow me to scan my 20 goddamn magazines!&lt;/p&gt;\n\n&lt;p&gt;NOPE. Because it doesn&amp;#39;t support up to A3 size, it&amp;#39;s the SAME GODDAMN SIZE. You have to use a special feeding sheet, and completely bend the oversized sheet in half to scan it. Not only would that defeat the whole point of speed using a sheet-fed scanner, but it would ruin the page which is also not an option. I&amp;#39;m pissed that I now have $900 in pending Amazon returns because both these stupid goddamn scanners weren&amp;#39;t wide enough.&lt;/p&gt;\n\n&lt;p&gt;So I need advice from anyone else who&amp;#39;s scanned oversized magazines. Do I:&lt;/p&gt;\n\n&lt;p&gt;A) Buy this actual A3 scanner from a lesser known brand: &lt;a href=\"https://www.amazon.com/Plustek-SmartOffice-Document-100-page-Legal-Size/dp/B0BJZBRNQR/ref=cm_cr_arp_d_product_top?ie=UTF8\"&gt;Scanner&lt;/a&gt;. While this one seems like it would work, it has minimal reviews and multiple of them mention that the scanner is bad for photos. I&amp;#39;m scanning magazine pages that feature full-color illustrations almost every page and have a slight gloss to them, and being that I want to archive them quality is important. I would hate to buy a 3rd goddamn scanner just to have to initiate yet another return because it doesn&amp;#39;t work.&lt;/p&gt;\n\n&lt;p&gt;B) Do I cut my losses completely and just buy a A3 size flatbed scanner. I wanted a sheet-fed scanner because I have many hundreds of pages to scan, and doing it page by page sounded like a nightmare. At this point I&amp;#39;m starting to feel it&amp;#39;s the only option.&lt;/p&gt;\n\n&lt;p&gt;Please, if anyone has a product that they think will work for my unique problem, I&amp;#39;m very open to suggestions at this point.&lt;/p&gt;\n\n&lt;p&gt;Before anyone asks, it&amp;#39;s not possible to cut the pages down. Many of the pages would loose sections of artwork or even text if I cut the spine all the way to be exactly A4 size. Defeats the whole point of preservation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18b02tx", "is_robot_indexable": true, "report_reasons": null, "author": "Killerabbet", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18b02tx/at_my_wits_ends_trying_to_find_a_way_to_scan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18b02tx/at_my_wits_ends_trying_to_find_a_way_to_scan/", "subreddit_subscribers": 716362, "created_utc": 1701739325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "URL: https://www.youtube.com/watch?v=4uiLoq4QSlU\n\nIt got saved a few times on the Wayback Machine, but won't even load the webpage: https://web.archive.org/web/20230000000000*/https://www.youtube.com/watch?v=4uiLoq4QSlU\n\nI also have this embed of the video from telegram: https://i.imgur.com/LrN7tyc.png\n\nThumbnail: https://i.imgur.com/kqilEy5.png\n\nWas sent to me by my friend on October 21 2023, when it was probably blowing up on the algorithm.\n\nIt was actually half decent and pretty interesting to watch, I wanted to inspire some techniques from it. I really enjoy exploring videos like these.\n\nIt basically was a lot of action movie scenes put together, looked pretty GTA-ey but was probably modded GTA 5, had real car brands in it, etc. Here's one still from the video I got: https://i.imgur.com/kibEj7A.png\n\nGiven that the video blew up, I'm guessing it was downloaded and reposted to twitter or tiktok somewhere.\n\nEven if its lower quality, or we could find the original uploader, it would be great.\n\nSearched many sources with no avail.\n\nThanks in advance.", "author_fullname": "t2_5tc3oklu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a specific GTA 6 Fanmade trailer from October that's now been removed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18azy2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701740286.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701738933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;URL: &lt;a href=\"https://www.youtube.com/watch?v=4uiLoq4QSlU\"&gt;https://www.youtube.com/watch?v=4uiLoq4QSlU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It got saved a few times on the Wayback Machine, but won&amp;#39;t even load the webpage: &lt;a href=\"https://web.archive.org/web/20230000000000*/https://www.youtube.com/watch?v=4uiLoq4QSlU\"&gt;https://web.archive.org/web/20230000000000*/https://www.youtube.com/watch?v=4uiLoq4QSlU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I also have this embed of the video from telegram: &lt;a href=\"https://i.imgur.com/LrN7tyc.png\"&gt;https://i.imgur.com/LrN7tyc.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thumbnail: &lt;a href=\"https://i.imgur.com/kqilEy5.png\"&gt;https://i.imgur.com/kqilEy5.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Was sent to me by my friend on October 21 2023, when it was probably blowing up on the algorithm.&lt;/p&gt;\n\n&lt;p&gt;It was actually half decent and pretty interesting to watch, I wanted to inspire some techniques from it. I really enjoy exploring videos like these.&lt;/p&gt;\n\n&lt;p&gt;It basically was a lot of action movie scenes put together, looked pretty GTA-ey but was probably modded GTA 5, had real car brands in it, etc. Here&amp;#39;s one still from the video I got: &lt;a href=\"https://i.imgur.com/kibEj7A.png\"&gt;https://i.imgur.com/kibEj7A.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Given that the video blew up, I&amp;#39;m guessing it was downloaded and reposted to twitter or tiktok somewhere.&lt;/p&gt;\n\n&lt;p&gt;Even if its lower quality, or we could find the original uploader, it would be great.&lt;/p&gt;\n\n&lt;p&gt;Searched many sources with no avail.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Zfb3zaTZbQLcD_Fc3I9vKuI7OVM6DDVU2U4f_acgFSQ.png?auto=webp&amp;s=d19f17bbc90c26118c6889c21e90e6fe36f4baec", "width": 633, "height": 557}, "resolutions": [{"url": "https://external-preview.redd.it/Zfb3zaTZbQLcD_Fc3I9vKuI7OVM6DDVU2U4f_acgFSQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2afac94e5e4288eb4d77d5455f4c80d301f0d61", "width": 108, "height": 95}, {"url": "https://external-preview.redd.it/Zfb3zaTZbQLcD_Fc3I9vKuI7OVM6DDVU2U4f_acgFSQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=557d6ccedb033bacc9d3b50583391388a818f248", "width": 216, "height": 190}, {"url": "https://external-preview.redd.it/Zfb3zaTZbQLcD_Fc3I9vKuI7OVM6DDVU2U4f_acgFSQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5811129ecce0a915e0cbe52b03c1eb61a623737", "width": 320, "height": 281}], "variants": {}, "id": "cAhOa1hQoSrw-2H8ouAcxZNNpgcD5Uo-4_GWduwW1r4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18azy2k", "is_robot_indexable": true, "report_reasons": null, "author": "Gold-Advisor", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18azy2k/looking_for_a_specific_gta_6_fanmade_trailer_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18azy2k/looking_for_a_specific_gta_6_fanmade_trailer_from/", "subreddit_subscribers": 716362, "created_utc": 1701738933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,I'm new to this sub and looked at your posts about HDDs.What I retain is that recertified are almost as good as any drives as they all fail in the end.\n\nI need more spaces as I'm finally implement redundancy.serverpartdeals seems to be the best choice with the \"manufacturer recertified\" option.I found this drives: [x22 20Tb](https://serverpartdeals.com/products/seagate-exos-x22-st20000nm004e-20tb-7-2k-rpm-sata-6gb-s-512e-3-5-recertified-hard-drive) and [x20 20Tb](https://serverpartdeals.com/products/seagate-exos-x20-st20000nm007d-20tb-7-2k-rpm-sata-6gb-s-3-5-hard-drive) drives for \\~ 200Euros\n\nMy question is: Do I go for the refurb at 200Euros + 2y Warrant or do I go for the New at 300Euros+ 5y Warrant?\n\nThanks :D  \n\n\nEDIT:  \nThanks for your answer.  \nI tried to buy one from mindfactory: [x20 20Tb](https://www.mindfactory.de/product_info.php/20TB-Seagate-Exos-X20-ST20000NM007D-256MB-3-5Zoll--8-9cm--SATA_1440042.html)  \nUsing [mailbox.de](https://mailbox.de) as a proxy.  \n\n\nI'll keep you informed.", "author_fullname": "t2_4f1rf3oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "serverpartdeals as EU citizen + Waranty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18an5gf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701720463.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701705339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,I&amp;#39;m new to this sub and looked at your posts about HDDs.What I retain is that recertified are almost as good as any drives as they all fail in the end.&lt;/p&gt;\n\n&lt;p&gt;I need more spaces as I&amp;#39;m finally implement redundancy.serverpartdeals seems to be the best choice with the &amp;quot;manufacturer recertified&amp;quot; option.I found this drives: &lt;a href=\"https://serverpartdeals.com/products/seagate-exos-x22-st20000nm004e-20tb-7-2k-rpm-sata-6gb-s-512e-3-5-recertified-hard-drive\"&gt;x22 20Tb&lt;/a&gt; and &lt;a href=\"https://serverpartdeals.com/products/seagate-exos-x20-st20000nm007d-20tb-7-2k-rpm-sata-6gb-s-3-5-hard-drive\"&gt;x20 20Tb&lt;/a&gt; drives for ~ 200Euros&lt;/p&gt;\n\n&lt;p&gt;My question is: Do I go for the refurb at 200Euros + 2y Warrant or do I go for the New at 300Euros+ 5y Warrant?&lt;/p&gt;\n\n&lt;p&gt;Thanks :D  &lt;/p&gt;\n\n&lt;p&gt;EDIT:&lt;br/&gt;\nThanks for your answer.&lt;br/&gt;\nI tried to buy one from mindfactory: &lt;a href=\"https://www.mindfactory.de/product_info.php/20TB-Seagate-Exos-X20-ST20000NM007D-256MB-3-5Zoll--8-9cm--SATA_1440042.html\"&gt;x20 20Tb&lt;/a&gt;&lt;br/&gt;\nUsing &lt;a href=\"https://mailbox.de\"&gt;mailbox.de&lt;/a&gt; as a proxy.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll keep you informed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?auto=webp&amp;s=d0aff868d87cd3d89bf3ebc1c42c204b8623b7d4", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c81d5071a1d46812ec8ffc7a9f1b72459e370c6", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9995e1f61e0a9241c793c47fd090610ddbd9e006", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2bcfeb59e856dad5dbda44af5b5b3690f3d7fb3a", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0a28485f9c0a14f2669cc025b34817a8dfad6895", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=36338fef61fa2a9a9ddb0586162fcae3cb8ed7dc", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/8JhyvFiYnLBP1pRRZHJd9AvfP6EZgBErm5eTajF3RsI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d74e6fcbd2c88f77f21064372614a8b441629267", "width": 1080, "height": 1080}], "variants": {}, "id": "pYYK78LoI3VS0jVCwkkjFru_L6_TdWA-vLrtjpwowCk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18an5gf", "is_robot_indexable": true, "report_reasons": null, "author": "momokinou", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18an5gf/serverpartdeals_as_eu_citizen_waranty/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18an5gf/serverpartdeals_as_eu_citizen_waranty/", "subreddit_subscribers": 716362, "created_utc": 1701705339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,\n\nso I had a 2 bay QNAP NAS for 2 years with 2 12Tb HDDs with movies and tv shows that I Play with my Nvidia Shield.\nBut now the storage is nearly full so I plan to expand the NAS. I have an old PC and everything is ready, now my question is which is the best fitting OS for my purpose?\nI don't need any complicated things, I just want to put the drives in and share them in my local network to my office PC and the Shield\n\nI tried truenas but something didn't work...\nOn one drive a created a dataset and put in a folder with movies, circa 8Tb, on the other drive I created more datasets, 1 for movies, 1 for shows, 1 for music, 1 for documents..\nI put in a folder in each dataset and put the data inside the folder but when I wanted to move some movies in the movie folder suddenly it said it's full after around 3Tb even tho the HDD has still  a few Tb free space left.\nI didn't touch the quotas or permissions, it's all on default, I just created the dataset and shared them via Samba to my office PC and copied the data to the nas drives", "author_fullname": "t2_x0243", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First DIY NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18an05u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701704939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;so I had a 2 bay QNAP NAS for 2 years with 2 12Tb HDDs with movies and tv shows that I Play with my Nvidia Shield.\nBut now the storage is nearly full so I plan to expand the NAS. I have an old PC and everything is ready, now my question is which is the best fitting OS for my purpose?\nI don&amp;#39;t need any complicated things, I just want to put the drives in and share them in my local network to my office PC and the Shield&lt;/p&gt;\n\n&lt;p&gt;I tried truenas but something didn&amp;#39;t work...\nOn one drive a created a dataset and put in a folder with movies, circa 8Tb, on the other drive I created more datasets, 1 for movies, 1 for shows, 1 for music, 1 for documents..\nI put in a folder in each dataset and put the data inside the folder but when I wanted to move some movies in the movie folder suddenly it said it&amp;#39;s full after around 3Tb even tho the HDD has still  a few Tb free space left.\nI didn&amp;#39;t touch the quotas or permissions, it&amp;#39;s all on default, I just created the dataset and shared them via Samba to my office PC and copied the data to the nas drives&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18an05u", "is_robot_indexable": true, "report_reasons": null, "author": "RentonThursten", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18an05u/first_diy_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18an05u/first_diy_nas/", "subreddit_subscribers": 716362, "created_utc": 1701704939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for sata to usb enclosures that has USAP and TRIM support. And if more features like smart reporting work well then thats a plus. I will be using it with WD Blue SSD.   \n  \nSo far I looked at https://www.amazon.in/ORICO-External-Enclosure-Supported-25PW1-U3/dp/B0B936X2XH. It uses JMS578 and the firmware can be flashed to support trim.  \n  \nI found some other enclosures but I don't know how to find the chipset used in them.  \n  \nWhich are some good enclosures that work without any issues in linux/bsd ?", "author_fullname": "t2_o0ru1n1hc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended Sata to USB enclosure ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ah442", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701684349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for sata to usb enclosures that has USAP and TRIM support. And if more features like smart reporting work well then thats a plus. I will be using it with WD Blue SSD.   &lt;/p&gt;\n\n&lt;p&gt;So far I looked at &lt;a href=\"https://www.amazon.in/ORICO-External-Enclosure-Supported-25PW1-U3/dp/B0B936X2XH\"&gt;https://www.amazon.in/ORICO-External-Enclosure-Supported-25PW1-U3/dp/B0B936X2XH&lt;/a&gt;. It uses JMS578 and the firmware can be flashed to support trim.  &lt;/p&gt;\n\n&lt;p&gt;I found some other enclosures but I don&amp;#39;t know how to find the chipset used in them.  &lt;/p&gt;\n\n&lt;p&gt;Which are some good enclosures that work without any issues in linux/bsd ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ah442", "is_robot_indexable": true, "report_reasons": null, "author": "user655362024", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ah442/recommended_sata_to_usb_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ah442/recommended_sata_to_usb_enclosure/", "subreddit_subscribers": 716362, "created_utc": 1701684349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a data hoarder for sure. Have many SCSI Hard Drives in my closet to prove it... I sure miss the olden days when it comes to downloading (and not necessarily pirating). Anyone remember Hotline and/or Carracho? People would set up little servers and take pride in building little communities around specific content. English Dubbed Anime, Web UI resources and files, porn of course, Freeware, foriegn movies, even FONT servers were always fun... It was not just straight forward (you could even chat or email an admin and get a response fairly fast) but it was fun and you could meet like minded people.\n\nI guess I'm just venting because my mom got sick and I haven't logged into any of my trackers in a year and now I'm back to square one with invites. Uhhhg.", "author_fullname": "t2_nv9mx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "So tired of private trackers and usenet.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18aewut", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.51, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701674065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data hoarder for sure. Have many SCSI Hard Drives in my closet to prove it... I sure miss the olden days when it comes to downloading (and not necessarily pirating). Anyone remember Hotline and/or Carracho? People would set up little servers and take pride in building little communities around specific content. English Dubbed Anime, Web UI resources and files, porn of course, Freeware, foriegn movies, even FONT servers were always fun... It was not just straight forward (you could even chat or email an admin and get a response fairly fast) but it was fun and you could meet like minded people.&lt;/p&gt;\n\n&lt;p&gt;I guess I&amp;#39;m just venting because my mom got sick and I haven&amp;#39;t logged into any of my trackers in a year and now I&amp;#39;m back to square one with invites. Uhhhg.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18aewut", "is_robot_indexable": true, "report_reasons": null, "author": "malachi347", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18aewut/so_tired_of_private_trackers_and_usenet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18aewut/so_tired_of_private_trackers_and_usenet/", "subreddit_subscribers": 716362, "created_utc": 1701674065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A recent post by a person headed to Antarctica has me wondering...\n\nWhat's the most remote / disconnected data hoard you're aware of? I'm picturing a cabin in the mountains with terabytes of data on a solar powered server, that kind of thing. Starlink has given us connectivity pretty much planet-wide, but there's no substitute for a massive server with your own data on a local network. ", "author_fullname": "t2_d9g94zfn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Remote Data Hoards", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18aazqm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701659762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A recent post by a person headed to Antarctica has me wondering...&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the most remote / disconnected data hoard you&amp;#39;re aware of? I&amp;#39;m picturing a cabin in the mountains with terabytes of data on a solar powered server, that kind of thing. Starlink has given us connectivity pretty much planet-wide, but there&amp;#39;s no substitute for a massive server with your own data on a local network. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18aazqm", "is_robot_indexable": true, "report_reasons": null, "author": "FizzicalLayer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18aazqm/remote_data_hoards/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18aazqm/remote_data_hoards/", "subreddit_subscribers": 716362, "created_utc": 1701659762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have tried everything I can think of to grab these episodes but nothing I have tried has succeeded. My last attempt was using \"getFLV\" it failed as well. \n\nThis is the site:\n\n[http://starcade.tv/starcade/games/shows.html](http://starcade.tv/starcade/games/shows.html)\n\nAny help would be appreciated.\n\nSolved!!!\n\nThanks to you all!\n\n", "author_fullname": "t2_7w9n5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help grabbing videos from a website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18azaho", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701739273.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701737057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried everything I can think of to grab these episodes but nothing I have tried has succeeded. My last attempt was using &amp;quot;getFLV&amp;quot; it failed as well. &lt;/p&gt;\n\n&lt;p&gt;This is the site:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://starcade.tv/starcade/games/shows.html\"&gt;http://starcade.tv/starcade/games/shows.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Solved!!!&lt;/p&gt;\n\n&lt;p&gt;Thanks to you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18azaho", "is_robot_indexable": true, "report_reasons": null, "author": "Cosmologyman", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18azaho/need_help_grabbing_videos_from_a_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18azaho/need_help_grabbing_videos_from_a_website/", "subreddit_subscribers": 716362, "created_utc": 1701737057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since this is about data, how would you go by in terms of digitizing hard copy documents. Do you just normally scan individual pages. What software do you use? I have a brother printer and scanner all in one. It has multifeed function. But wondering if this is the effiencent way.", "author_fullname": "t2_xq820", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you guys recommend about digitisation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ax3t5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701731164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since this is about data, how would you go by in terms of digitizing hard copy documents. Do you just normally scan individual pages. What software do you use? I have a brother printer and scanner all in one. It has multifeed function. But wondering if this is the effiencent way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ax3t5", "is_robot_indexable": true, "report_reasons": null, "author": "Inkopol", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ax3t5/what_do_you_guys_recommend_about_digitisation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ax3t5/what_do_you_guys_recommend_about_digitisation/", "subreddit_subscribers": 716362, "created_utc": 1701731164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all been lurking around trying to get some info on here. Figured Id try my luck and ask my Q. Ive never run an NAS , just have external hard drives (even a few zip disks) laying around for the last 2 decades or so. Im very interested in the idea of a central place to secure everything. I probably need around 15tb of storage (without redundancy).  was looking at the **Synology DiskStation DS923+.** is the higher price of SSD worth it for NAS? What would you guys recommend for me for a diskstation and hard drives. I dont have an unlimited budget would love to stay below 1500$ if possible. Would alos love if this could double as a mediaserver. thanks in advance", "author_fullname": "t2_l7ru9g05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "reasonable NAS for a beginner", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18abqkl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701662247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all been lurking around trying to get some info on here. Figured Id try my luck and ask my Q. Ive never run an NAS , just have external hard drives (even a few zip disks) laying around for the last 2 decades or so. Im very interested in the idea of a central place to secure everything. I probably need around 15tb of storage (without redundancy).  was looking at the &lt;strong&gt;Synology DiskStation DS923+.&lt;/strong&gt; is the higher price of SSD worth it for NAS? What would you guys recommend for me for a diskstation and hard drives. I dont have an unlimited budget would love to stay below 1500$ if possible. Would alos love if this could double as a mediaserver. thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18abqkl", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial_Two_224", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18abqkl/reasonable_nas_for_a_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18abqkl/reasonable_nas_for_a_beginner/", "subreddit_subscribers": 716362, "created_utc": 1701662247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got an email saying that Amazon photos as a service is getting the axe. \n\nI'm assuming it's because that sector of their business was unprofitable because they were late to the party and most people have adopted other platforms already into their ecosystem.\n\nI still have an Amazon s3 glacier account but I need some non-cold storage for cloud storage and web hosting,\n\nIs Amazon still recognized in the data hoarder community as being the best deal in terms of Gigabytes per dollar?\n\nWhat are some of its competitors that beat it?", "author_fullname": "t2_dbk1lkbhd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Amazon AWS still the king in giving the best GB per dollar for cloud storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18axbs0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701731726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got an email saying that Amazon photos as a service is getting the axe. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m assuming it&amp;#39;s because that sector of their business was unprofitable because they were late to the party and most people have adopted other platforms already into their ecosystem.&lt;/p&gt;\n\n&lt;p&gt;I still have an Amazon s3 glacier account but I need some non-cold storage for cloud storage and web hosting,&lt;/p&gt;\n\n&lt;p&gt;Is Amazon still recognized in the data hoarder community as being the best deal in terms of Gigabytes per dollar?&lt;/p&gt;\n\n&lt;p&gt;What are some of its competitors that beat it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18axbs0", "is_robot_indexable": true, "report_reasons": null, "author": "Nicenap", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18axbs0/is_amazon_aws_still_the_king_in_giving_the_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18axbs0/is_amazon_aws_still_the_king_in_giving_the_best/", "subreddit_subscribers": 716362, "created_utc": 1701731726.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been getting a 502 Bad Gateway for a few weeks now. At first, I thought it was because of the Cloudflare DNS issue, so I added exceptions to /etc/hosts as per the instructions found on Reddit. That didn't help. Next I disabled Cloudflare DNS entirely in my modem settings (I had set it up manually there), but even with it disabled, I'm still getting the 502 error. So is archive.ph really just down?", "author_fullname": "t2_yi8x3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive.ph down?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18aif0q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701689958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been getting a 502 Bad Gateway for a few weeks now. At first, I thought it was because of the Cloudflare DNS issue, so I added exceptions to /etc/hosts as per the instructions found on Reddit. That didn&amp;#39;t help. Next I disabled Cloudflare DNS entirely in my modem settings (I had set it up manually there), but even with it disabled, I&amp;#39;m still getting the 502 error. So is archive.ph really just down?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18aif0q", "is_robot_indexable": true, "report_reasons": null, "author": "Vistaus", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18aif0q/archiveph_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18aif0q/archiveph_down/", "subreddit_subscribers": 716362, "created_utc": 1701689958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone know which tool or extension to download full album image facebook in a user ? I found extension ESUIT Photo Downloader for Facebook worked but it limit 500 image and some function, force to pay premium version.", "author_fullname": "t2_l6idjqag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool download album Facebook in a user", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ag89m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701680184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know which tool or extension to download full album image facebook in a user ? I found extension ESUIT Photo Downloader for Facebook worked but it limit 500 image and some function, force to pay premium version.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ag89m", "is_robot_indexable": true, "report_reasons": null, "author": "siparet389", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ag89m/tool_download_album_facebook_in_a_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ag89m/tool_download_album_facebook_in_a_user/", "subreddit_subscribers": 716362, "created_utc": 1701680184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking to build a new server. I want to go all ssd. I don't care to mount the ssds properly. I will be installing 14 2.5 ssds. What case would have enough space to cram 14 ssds.", "author_fullname": "t2_13bqq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "14 2.5 ssds case", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ack85", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701665097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to build a new server. I want to go all ssd. I don&amp;#39;t care to mount the ssds properly. I will be installing 14 2.5 ssds. What case would have enough space to cram 14 ssds.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ack85", "is_robot_indexable": true, "report_reasons": null, "author": "Derkainer", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ack85/14_25_ssds_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ack85/14_25_ssds_case/", "subreddit_subscribers": 716362, "created_utc": 1701665097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've a 1tb and 7gb drive. I was wondering what I can do with them. I've considered using them for a linux boot but I don't think I'll use it.\n\nI've a USB set up to store my music and as a Win10 install drive so I'm covered there. Aside from giving them to charity, does anyone else have any other ideas what I can use them for?", "author_fullname": "t2_bm3qfqzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use for old USB drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ari8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701716793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve a 1tb and 7gb drive. I was wondering what I can do with them. I&amp;#39;ve considered using them for a linux boot but I don&amp;#39;t think I&amp;#39;ll use it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve a USB set up to store my music and as a Win10 install drive so I&amp;#39;m covered there. Aside from giving them to charity, does anyone else have any other ideas what I can use them for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ari8m", "is_robot_indexable": true, "report_reasons": null, "author": "Libertyforzombies", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ari8m/use_for_old_usb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ari8m/use_for_old_usb_drives/", "subreddit_subscribers": 716362, "created_utc": 1701716793.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}