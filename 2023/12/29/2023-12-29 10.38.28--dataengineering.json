{"kind": "Listing", "data": {"after": "t3_18szyuz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "what are the differentiators when both of them are encroaching into each other's territories ", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who would emerge as the winner between Databricks and Snowflake in the race of all things Data and AI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18svyhx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703785056.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703775868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;what are the differentiators when both of them are encroaching into each other&amp;#39;s territories &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18svyhx", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18svyhx/who_would_emerge_as_the_winner_between_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18svyhx/who_would_emerge_as_the_winner_between_databricks/", "subreddit_subscribers": 149174, "created_utc": 1703775868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Isn\u2019t there any more consumable book than Designing Data Intensive Applications That will help me move to data engineering from data analysis ? Everytime I try to absorb it I feel like I\u2019m stupid \ud83d\ude02", "author_fullname": "t2_t3nz93za", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DDIA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t52ro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703798890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Isn\u2019t there any more consumable book than Designing Data Intensive Applications That will help me move to data engineering from data analysis ? Everytime I try to absorb it I feel like I\u2019m stupid \ud83d\ude02&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18t52ro", "is_robot_indexable": true, "report_reasons": null, "author": "Repulsive-Ad7769", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t52ro/ddia/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t52ro/ddia/", "subreddit_subscribers": 149174, "created_utc": 1703798890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The biggest blockade to me learning an orchestration framework is the aversion that they have to \u201cstatefulness.\u201d\n  \nI\u2019m not asking HOW do pass \u201cstate\u201d between tasks (I know that it\u2019s XComs in Airflow and IOManagers in Dagster), but WHY does it force us to do it in a roundabout way?\n  \nMost everything that I\u2019ve ever coded has been written in a pretty stateful, object-oriented manner, so switching to this roundabout, functional approach has been difficult to grasp. What\u2019s the intuition behind this design pattern?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why can\u2019t I pass context between tasks in Airflow/Dagster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18sz7ta", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703784129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The biggest blockade to me learning an orchestration framework is the aversion that they have to \u201cstatefulness.\u201d&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not asking HOW do pass \u201cstate\u201d between tasks (I know that it\u2019s XComs in Airflow and IOManagers in Dagster), but WHY does it force us to do it in a roundabout way?&lt;/p&gt;\n\n&lt;p&gt;Most everything that I\u2019ve ever coded has been written in a pretty stateful, object-oriented manner, so switching to this roundabout, functional approach has been difficult to grasp. What\u2019s the intuition behind this design pattern?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18sz7ta", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18sz7ta/why_cant_i_pass_context_between_tasks_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18sz7ta/why_cant_i_pass_context_between_tasks_in/", "subreddit_subscribers": 149174, "created_utc": 1703784129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nWe\u2019re using a service that allows us to develop ETL pipelines and schedule jobs for automated runs. \n\nThe current setup thoroughly transforms the data step by step, but there are multiple buckets and other locations our pipelines are dropping data into. \n\nAs I continue development, what is a good way to begin documenting the ETL architecture, tools, and how do I go about documenting what is going o? \n\nHow detailed do I need to be when documenting this for new joiners? For now, I will not being using sphinx so I will be using docstrings to comment in, and creating documents myself typed out. \n\n\nThanks", "author_fullname": "t2_oc5syk6jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I document ETL/ELT pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t3ect", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703794654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;We\u2019re using a service that allows us to develop ETL pipelines and schedule jobs for automated runs. &lt;/p&gt;\n\n&lt;p&gt;The current setup thoroughly transforms the data step by step, but there are multiple buckets and other locations our pipelines are dropping data into. &lt;/p&gt;\n\n&lt;p&gt;As I continue development, what is a good way to begin documenting the ETL architecture, tools, and how do I go about documenting what is going o? &lt;/p&gt;\n\n&lt;p&gt;How detailed do I need to be when documenting this for new joiners? For now, I will not being using sphinx so I will be using docstrings to comment in, and creating documents myself typed out. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18t3ect", "is_robot_indexable": true, "report_reasons": null, "author": "tryingmybest200000", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t3ect/how_do_i_document_etlelt_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t3ect/how_do_i_document_etlelt_pipelines/", "subreddit_subscribers": 149174, "created_utc": 1703794654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\n\nMy team is scoping some new design patterns for coordinating file/data movement to/from SFTP servers and from API's.  These eventually will get consumed by PySpark/Glue and stored as Parquet in our Data Lake, but we are exploring the best ways to coordinate all of this upstream ingestion.  We have several ideas about patterns that will fit, but I wanted to pulse the community to see if anyone has something we haven't considered or ideas that could help us refine our approaches.\n\nSome background\n\n&amp;#x200B;\n\n* Need to integrate with several things like SFTP, API's for downloading files and also API's that send JSON payloads of data.\n* Have to connect to a few SASS offerings like Hubspot, but some of these are not API's that will be on a marketplace and will need to be custom integrated.\n* In a regulated environment (Healthcare).\n* Current setup is Terraform, AWS Glue, S3, RDS\n\nGeneral patterns we are weighing are:  \n\n\n* Modules in Terraform: We use terraform for IAC on AWS, so modularization of infrastructure using terraform could solve our issues entirely.  However, this requires possibly writing a fair bit of terraform code to coordinate all of this.  Not really an issue, but we'd currently don't have the best practices around deploying and managing Lambdas and other AWS technologies so theres some legwork there to improve our over all patterns.  Not to mention, our general fluency on Terraform isn't amazing right now on the team.\n* Framework like Chalice or Zappa: We have been exploring this possibility more broadly than this use case, so possibly deploying an app on one of these frameworks would allow us to handle the deployment of multiple AWS infrastructure items in an easier way than only leveraging Terraform modules.  Additionally, there are adaptors that translate Chalice to Terraform, so we can leverage our current deployment patterns still.\n* Other options:  Theres a ton of \"SASS\" type solutions for connectors and file movement.  While we aren't super thrilled to possibly pay for another SASS solution, if its monumentally easier to implement we may consider it.  I know tools like Airbyte have an SFTP connector and some other connectors we might need, but we also have some more complicated API's we'd need to integrate with that these types of SASS solutions wont solve for.\n\n&amp;#x200B;\n\nThanks everyone! ", "author_fullname": "t2_3tfgc8z0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Design patterns for coordinating data from files/API's to S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18stkc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703769014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;My team is scoping some new design patterns for coordinating file/data movement to/from SFTP servers and from API&amp;#39;s.  These eventually will get consumed by PySpark/Glue and stored as Parquet in our Data Lake, but we are exploring the best ways to coordinate all of this upstream ingestion.  We have several ideas about patterns that will fit, but I wanted to pulse the community to see if anyone has something we haven&amp;#39;t considered or ideas that could help us refine our approaches.&lt;/p&gt;\n\n&lt;p&gt;Some background&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Need to integrate with several things like SFTP, API&amp;#39;s for downloading files and also API&amp;#39;s that send JSON payloads of data.&lt;/li&gt;\n&lt;li&gt;Have to connect to a few SASS offerings like Hubspot, but some of these are not API&amp;#39;s that will be on a marketplace and will need to be custom integrated.&lt;/li&gt;\n&lt;li&gt;In a regulated environment (Healthcare).&lt;/li&gt;\n&lt;li&gt;Current setup is Terraform, AWS Glue, S3, RDS&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;General patterns we are weighing are:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Modules in Terraform: We use terraform for IAC on AWS, so modularization of infrastructure using terraform could solve our issues entirely.  However, this requires possibly writing a fair bit of terraform code to coordinate all of this.  Not really an issue, but we&amp;#39;d currently don&amp;#39;t have the best practices around deploying and managing Lambdas and other AWS technologies so theres some legwork there to improve our over all patterns.  Not to mention, our general fluency on Terraform isn&amp;#39;t amazing right now on the team.&lt;/li&gt;\n&lt;li&gt;Framework like Chalice or Zappa: We have been exploring this possibility more broadly than this use case, so possibly deploying an app on one of these frameworks would allow us to handle the deployment of multiple AWS infrastructure items in an easier way than only leveraging Terraform modules.  Additionally, there are adaptors that translate Chalice to Terraform, so we can leverage our current deployment patterns still.&lt;/li&gt;\n&lt;li&gt;Other options:  Theres a ton of &amp;quot;SASS&amp;quot; type solutions for connectors and file movement.  While we aren&amp;#39;t super thrilled to possibly pay for another SASS solution, if its monumentally easier to implement we may consider it.  I know tools like Airbyte have an SFTP connector and some other connectors we might need, but we also have some more complicated API&amp;#39;s we&amp;#39;d need to integrate with that these types of SASS solutions wont solve for.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18stkc3", "is_robot_indexable": true, "report_reasons": null, "author": "deepeyesmusic", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18stkc3/design_patterns_for_coordinating_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18stkc3/design_patterns_for_coordinating_data_from/", "subreddit_subscribers": 149174, "created_utc": 1703769014.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a PMP (Project Management Professional) from a previous professional career. I know that it is meaningless in tech and anyone who has a PMP will tell you what a joke it is and that forcing new employees to get their PMP is often used as a hazing ritual. However, the company that sponsors the cert spends a lot of money on marketing and thus a PMP is held is very high esteem by traditional boomer management and by recruiters.  \n\nShould I put my PMP on my resume? I am in DE now and just looking to change companies.", "author_fullname": "t2_o1c691xd6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Put PMP on resume?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18sslei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703765803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a PMP (Project Management Professional) from a previous professional career. I know that it is meaningless in tech and anyone who has a PMP will tell you what a joke it is and that forcing new employees to get their PMP is often used as a hazing ritual. However, the company that sponsors the cert spends a lot of money on marketing and thus a PMP is held is very high esteem by traditional boomer management and by recruiters.  &lt;/p&gt;\n\n&lt;p&gt;Should I put my PMP on my resume? I am in DE now and just looking to change companies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18sslei", "is_robot_indexable": true, "report_reasons": null, "author": "Impressive-One6226", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18sslei/put_pmp_on_resume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18sslei/put_pmp_on_resume/", "subreddit_subscribers": 149174, "created_utc": 1703765803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there resources for best ways to go about *documenting* database designs? \n\nI realize that one can export a database schema, add some comments to it, and use diagrams to represent it visually. But, these seem deficient in that they do not capture the semantics or rationale for the underlying fields/tables. I\u2019m especially curious about established resources that lay this out in detail, especially ones that I can show to others in support of best practices (a textbook would be ideal, but I realize that\u2019s a tall order for what I\u2019m looking for).", "author_fullname": "t2_i90lr7q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for documenting database design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tgg69", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703831292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there resources for best ways to go about &lt;em&gt;documenting&lt;/em&gt; database designs? &lt;/p&gt;\n\n&lt;p&gt;I realize that one can export a database schema, add some comments to it, and use diagrams to represent it visually. But, these seem deficient in that they do not capture the semantics or rationale for the underlying fields/tables. I\u2019m especially curious about established resources that lay this out in detail, especially ones that I can show to others in support of best practices (a textbook would be ideal, but I realize that\u2019s a tall order for what I\u2019m looking for).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tgg69", "is_robot_indexable": true, "report_reasons": null, "author": "rubikol", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tgg69/best_practices_for_documenting_database_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tgg69/best_practices_for_documenting_database_design/", "subreddit_subscribers": 149174, "created_utc": 1703831292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bit of a challenge where I am taking a sharded mysql database, combining all like tables and loading them into Snowflake.  We have over 30,000 different database shards with each containing the same 200 tables.  \n\nCurrently I have a python script that loops through all shards running a select \\* and appending data to 200 different csv files that are then pushed to S3.  I then use a Lambda function and the Copy Into statement to create the 200 final tables within snowflake.  \n\nIs there a better way to do this?  I currently have to run the entire job each morning and do a rip and replace.  ", "author_fullname": "t2_2qnlv1dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading a sharded MYSQL database into Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t98ob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703809643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bit of a challenge where I am taking a sharded mysql database, combining all like tables and loading them into Snowflake.  We have over 30,000 different database shards with each containing the same 200 tables.  &lt;/p&gt;\n\n&lt;p&gt;Currently I have a python script that loops through all shards running a select * and appending data to 200 different csv files that are then pushed to S3.  I then use a Lambda function and the Copy Into statement to create the 200 final tables within snowflake.  &lt;/p&gt;\n\n&lt;p&gt;Is there a better way to do this?  I currently have to run the entire job each morning and do a rip and replace.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18t98ob", "is_robot_indexable": true, "report_reasons": null, "author": "themooseCS", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t98ob/loading_a_sharded_mysql_database_into_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t98ob/loading_a_sharded_mysql_database_into_snowflake/", "subreddit_subscribers": 149174, "created_utc": 1703809643.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI\u2019m trying to figure out a good system for securely using and managing secrets. I\u2019d like to hear about what everyone else does. I\u2019ll also share my early idea for a system I think may some merit to it.\n\nThis approach leverages AWS Secrets Manager, Gitea, Age encryption protocol, and Mozilla SOPS. It attempts to provide robust and secure secrets management while minimizing cost,\n\n**Key Components:**\n1. **Infrastructure:**\n   - Gitea hosted on a private cloud subnet, accessed via a bastion server.\n   - AWS Secrets Manager for secure storage of encryption keys.\n\n2. **Secrets Encryption:**\n   - Age encryption protocol for encryption keys.\n   - Mozilla SOPS to locally encrypt secrets in a `secrets.json` file.\n\n3. **Remote Machine Usage**\n     - Cloud machines perform `git pull` every time secrets are needed to retrieve updated secrets.\n     - Fetch the new decryption key from AWS Secrets.\n     - Decrypt secrets in memory using Mozilla SOPS.\n     - Discard old decryption key and decrypted secrets from memory.\n     - Discard decrypted secrets from memory after use\n\n4. **Key Rotation Process:**\n   - **Generate New Key**\n     - Utilize Age encryption protocol to create a fresh encryption key.\n     - Upload the encryption key to AWS secrets for later use\n\n   - **Rotate Secrets**\n     - Use AWS Secrets API to collect the old encryption key.\n     - `git pull` all repositories that use the encryption key for their secrets\n     - Decrypt all secrets with the old encryption key\n     - Encrypt all secrets with Mozilla SOPS using the new key.\n     - Embed AWS Secrets ID in the git push for future reference.\n     - `git push` the updated `secrets.json` files to Secrets-Gitea.\n     - Update AWS Secrets to include the new decryption key.\n\n**Capabilities:**\n- **Traceability:**\n  - Each `secrets.json` file is associated with its encryption key through AWS Secrets ID.\n\n- **Key Rotation:**\n  - Enables regular updates for enhanced security.\n\n- **Cost Savings:**\n  - Effective use of cloud resources. Secrets management is expensive.\n\n- **Version Control:**\n  - Leverages Gitea for versioning of secrets, aiding in auditing and rollbacks.\n\n**Considerations:**\n- **Centralization Drawbacks:**\n  - Potential single point of attack\n\nEdit: perhaps with git-remote-gcrypt", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you securely manage remotely available secrets at scale in a cost effective way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tdkfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703823878.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703822035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to figure out a good system for securely using and managing secrets. I\u2019d like to hear about what everyone else does. I\u2019ll also share my early idea for a system I think may some merit to it.&lt;/p&gt;\n\n&lt;p&gt;This approach leverages AWS Secrets Manager, Gitea, Age encryption protocol, and Mozilla SOPS. It attempts to provide robust and secure secrets management while minimizing cost,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;\n1. &lt;strong&gt;Infrastructure:&lt;/strong&gt;\n   - Gitea hosted on a private cloud subnet, accessed via a bastion server.\n   - AWS Secrets Manager for secure storage of encryption keys.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Secrets Encryption:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Age encryption protocol for encryption keys.&lt;/li&gt;\n&lt;li&gt;Mozilla SOPS to locally encrypt secrets in a &lt;code&gt;secrets.json&lt;/code&gt; file.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Remote Machine Usage&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cloud machines perform &lt;code&gt;git pull&lt;/code&gt; every time secrets are needed to retrieve updated secrets.&lt;/li&gt;\n&lt;li&gt;Fetch the new decryption key from AWS Secrets.&lt;/li&gt;\n&lt;li&gt;Decrypt secrets in memory using Mozilla SOPS.&lt;/li&gt;\n&lt;li&gt;Discard old decryption key and decrypted secrets from memory.&lt;/li&gt;\n&lt;li&gt;Discard decrypted secrets from memory after use&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Key Rotation Process:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Generate New Key&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Utilize Age encryption protocol to create a fresh encryption key.&lt;/li&gt;\n&lt;li&gt;Upload the encryption key to AWS secrets for later use&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Rotate Secrets&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use AWS Secrets API to collect the old encryption key.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;git pull&lt;/code&gt; all repositories that use the encryption key for their secrets&lt;/li&gt;\n&lt;li&gt;Decrypt all secrets with the old encryption key&lt;/li&gt;\n&lt;li&gt;Encrypt all secrets with Mozilla SOPS using the new key.&lt;/li&gt;\n&lt;li&gt;Embed AWS Secrets ID in the git push for future reference.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;git push&lt;/code&gt; the updated &lt;code&gt;secrets.json&lt;/code&gt; files to Secrets-Gitea.&lt;/li&gt;\n&lt;li&gt;Update AWS Secrets to include the new decryption key.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Capabilities:&lt;/strong&gt;\n- &lt;strong&gt;Traceability:&lt;/strong&gt;\n  - Each &lt;code&gt;secrets.json&lt;/code&gt; file is associated with its encryption key through AWS Secrets ID.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Key Rotation:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Enables regular updates for enhanced security.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cost Savings:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Effective use of cloud resources. Secrets management is expensive.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Version Control:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Leverages Gitea for versioning of secrets, aiding in auditing and rollbacks.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Considerations:&lt;/strong&gt;\n- &lt;strong&gt;Centralization Drawbacks:&lt;/strong&gt;\n  - Potential single point of attack&lt;/p&gt;\n\n&lt;p&gt;Edit: perhaps with git-remote-gcrypt&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18tdkfv", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tdkfv/how_do_you_securely_manage_remotely_available/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tdkfv/how_do_you_securely_manage_remotely_available/", "subreddit_subscribers": 149174, "created_utc": 1703822035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! \n\nWe are implementing Delta table logic on an on-prem HDFS cluster using Spark for data processing. We append daily partitions to BRONZE layer which contain new records and updates from the previous day. We want to upsert a daily partition from BRONZE to SILVER layer, but it takes an unreasonable amount of time. If we simply append the data to SILVER, the whole ETL process is around 7 minutes (the write iself around 4 minutes) for a daily partition, which contains roughly 1 million rows. If we switch to merge write, then the runtime grows to roughly 30 minutes (read + transform only takes a few minutes, rest is the merge write step). I get that the merge operation is slower because of the join, but I wouldn't expect it to be this slow for only 1 million rows. Keep in mind that these performance tests were done with only a few daily partitions in the tables. We had the same performance when we worked with only one partition in the target table. Do you guys have any idea what we should do to optimize the performance? \n\nThe things we have tried so far without any significant result:\n- Add more resource to the Spark job\n- Run OPTIMIZE command on target table\n- Tried creating the delta tables with explicit and implicit partitioning\n- By default, Spark does a SortMergeJoin at the MERGE step. We coerced other kinds of joins, but SortMergeJoin seemed to be the fastest one\n\nThe merge itself is fairly simple. We merge to the target table using a daily partition from BRONZE (roughly 1 million rows) on one matching ID of string type. If we have a match, we update the whole set (UPDATE SET *), otherwise INSERT *.\nWe will expect a relatively big amount of updates later, but right now we are working with test data which contains a couple of updates maximum per daily batch.", "author_fullname": "t2_vun99h9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta merge very slow for small amount of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18sr2k6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703760102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! &lt;/p&gt;\n\n&lt;p&gt;We are implementing Delta table logic on an on-prem HDFS cluster using Spark for data processing. We append daily partitions to BRONZE layer which contain new records and updates from the previous day. We want to upsert a daily partition from BRONZE to SILVER layer, but it takes an unreasonable amount of time. If we simply append the data to SILVER, the whole ETL process is around 7 minutes (the write iself around 4 minutes) for a daily partition, which contains roughly 1 million rows. If we switch to merge write, then the runtime grows to roughly 30 minutes (read + transform only takes a few minutes, rest is the merge write step). I get that the merge operation is slower because of the join, but I wouldn&amp;#39;t expect it to be this slow for only 1 million rows. Keep in mind that these performance tests were done with only a few daily partitions in the tables. We had the same performance when we worked with only one partition in the target table. Do you guys have any idea what we should do to optimize the performance? &lt;/p&gt;\n\n&lt;p&gt;The things we have tried so far without any significant result:\n- Add more resource to the Spark job\n- Run OPTIMIZE command on target table\n- Tried creating the delta tables with explicit and implicit partitioning\n- By default, Spark does a SortMergeJoin at the MERGE step. We coerced other kinds of joins, but SortMergeJoin seemed to be the fastest one&lt;/p&gt;\n\n&lt;p&gt;The merge itself is fairly simple. We merge to the target table using a daily partition from BRONZE (roughly 1 million rows) on one matching ID of string type. If we have a match, we update the whole set (UPDATE SET *), otherwise INSERT *.\nWe will expect a relatively big amount of updates later, but right now we are working with test data which contains a couple of updates maximum per daily batch.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18sr2k6", "is_robot_indexable": true, "report_reasons": null, "author": "justadataengineer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18sr2k6/delta_merge_very_slow_for_small_amount_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18sr2k6/delta_merge_very_slow_for_small_amount_of_data/", "subreddit_subscribers": 149174, "created_utc": 1703760102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need to SFTP files generated every 15 minutes up to an external SAAS product.\n\nProblem is the app generating these files has a hard lock on them.  It monitors realtime and keeps appending.  The target system doesn't actually need those files.  But it does need all the other files in the folder.\n\nThe files don't have an extension so I can't pattern match the ones to copy up.\n\nIs there anyway to do this in ADF natively?\n\nOr if it can't do that can I run a python script or powershell and pass the file list I want it to send back to ADF?\n\nThe files have RTA in the name so I'm sure I can regex that string in python.  Its the passing that list back to ADF for the pipeline job is the bit I have no clue.", "author_fullname": "t2_21umeu7n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I exclude files in ADF?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tbxq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703817432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703817247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need to SFTP files generated every 15 minutes up to an external SAAS product.&lt;/p&gt;\n\n&lt;p&gt;Problem is the app generating these files has a hard lock on them.  It monitors realtime and keeps appending.  The target system doesn&amp;#39;t actually need those files.  But it does need all the other files in the folder.&lt;/p&gt;\n\n&lt;p&gt;The files don&amp;#39;t have an extension so I can&amp;#39;t pattern match the ones to copy up.&lt;/p&gt;\n\n&lt;p&gt;Is there anyway to do this in ADF natively?&lt;/p&gt;\n\n&lt;p&gt;Or if it can&amp;#39;t do that can I run a python script or powershell and pass the file list I want it to send back to ADF?&lt;/p&gt;\n\n&lt;p&gt;The files have RTA in the name so I&amp;#39;m sure I can regex that string in python.  Its the passing that list back to ADF for the pipeline job is the bit I have no clue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tbxq3", "is_robot_indexable": true, "report_reasons": null, "author": "ComfortAndSpeed", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tbxq3/how_do_i_exclude_files_in_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tbxq3/how_do_i_exclude_files_in_adf/", "subreddit_subscribers": 149174, "created_utc": 1703817247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u8kebhp6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\ude80 Introducing UCX v0.8.0: Powerful New Features to Streamline Your Unity Catalog Migration \ud83d\udcbc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18t3fpz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/0vOipl6id0Dt5BoRaINYq-IycgPnXLmGQqa84Wup6T4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703794754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/databricks-labs/introducing-ucx-v0-8-0-powerful-new-features-to-streamline-your-unity-catalog-migration-828d8c9b7aef", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?auto=webp&amp;s=1ee66d9440f6393fb76721d5ac3a6d41378d6c00", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c14ff71deedc0b9e6d99c4868c355533aad38da1", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=005d1db282e67060cb29f400c5274bca7c234e2b", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e740a84e06100104ccb48bf1b13a295cd774223d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=217fb4df0068cfbdf085e50b031e2b5b5fd2c13b", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2ccdc11219f16683c0052c570f00d298faa3c417", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e24c939caf8950e761c054e9cfc300f3769fb00", "width": 1080, "height": 1080}], "variants": {}, "id": "zTYfMjHl7ivk4KOBIcyxhYx1j3k-NSCJh5fT9TZlG9M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18t3fpz", "is_robot_indexable": true, "report_reasons": null, "author": "serge_databricks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t3fpz/introducing_ucx_v080_powerful_new_features_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/databricks-labs/introducing-ucx-v0-8-0-powerful-new-features-to-streamline-your-unity-catalog-migration-828d8c9b7aef", "subreddit_subscribers": 149174, "created_utc": 1703794754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I just got hired into a solar sales company, I'm one of two technical roles in the business. My career up to now has been in data analysis and data science, but I've been tasked with creating an entire data pipe to take our data from Salesforce to something automatable/queryable in a BI tool. This means I'm working way upstream of everything I know, but I was told the company had to pick between an engineer and an analyst, and chose an analyst. \n\nAfter researching I think I want to put as much as I can into one cloud service, so AWS seems good enough for that. The company is pretty small, so I don't need to do anything that scales incredibly well, once we get a real data engineer in a year or two I'll likely work with them on transitioning to something that works better. I'm currently prioritizing getting something usable but not perfect off the ground ASAP.\n\nWith my limited research, I believe the best way to do this is as follows:\n\nSalesforce -&gt; Amazon Appflow -&gt; S3 -&gt; AWS Glue for ETL -&gt; Redshift\n\nThere will also be data coming into S3 from other sources, some of which will also go into Redshift (ex: payment data).\n\nIs there a simpler path that I could be following to get to a warehouse entity, or is this as bare bones as it gets?\n\nAdditionally, is anyone aware of good tutorials on how to chain AWS tools together? I find great resources for individual tools but not generally how to use them in tandem. Other resources that you've liked that covers high-level data engineering would also be welcome.\n\nThanks in advance!", "author_fullname": "t2_ob5cp2nhs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to Create MVP AWS Data Pipeline from Salesforce to Redshift, for Beginner", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t0suy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703788025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I just got hired into a solar sales company, I&amp;#39;m one of two technical roles in the business. My career up to now has been in data analysis and data science, but I&amp;#39;ve been tasked with creating an entire data pipe to take our data from Salesforce to something automatable/queryable in a BI tool. This means I&amp;#39;m working way upstream of everything I know, but I was told the company had to pick between an engineer and an analyst, and chose an analyst. &lt;/p&gt;\n\n&lt;p&gt;After researching I think I want to put as much as I can into one cloud service, so AWS seems good enough for that. The company is pretty small, so I don&amp;#39;t need to do anything that scales incredibly well, once we get a real data engineer in a year or two I&amp;#39;ll likely work with them on transitioning to something that works better. I&amp;#39;m currently prioritizing getting something usable but not perfect off the ground ASAP.&lt;/p&gt;\n\n&lt;p&gt;With my limited research, I believe the best way to do this is as follows:&lt;/p&gt;\n\n&lt;p&gt;Salesforce -&amp;gt; Amazon Appflow -&amp;gt; S3 -&amp;gt; AWS Glue for ETL -&amp;gt; Redshift&lt;/p&gt;\n\n&lt;p&gt;There will also be data coming into S3 from other sources, some of which will also go into Redshift (ex: payment data).&lt;/p&gt;\n\n&lt;p&gt;Is there a simpler path that I could be following to get to a warehouse entity, or is this as bare bones as it gets?&lt;/p&gt;\n\n&lt;p&gt;Additionally, is anyone aware of good tutorials on how to chain AWS tools together? I find great resources for individual tools but not generally how to use them in tandem. Other resources that you&amp;#39;ve liked that covers high-level data engineering would also be welcome.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18t0suy", "is_robot_indexable": true, "report_reasons": null, "author": "Clockworked47", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t0suy/best_way_to_create_mvp_aws_data_pipeline_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t0suy/best_way_to_create_mvp_aws_data_pipeline_from/", "subreddit_subscribers": 149174, "created_utc": 1703788025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distributed Training with Ray Train and MinIO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 41, "top_awarded_type": null, "hide_score": false, "name": "t3_18t0d7o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pXB1rtyTSkezkckvRPpVBRBheU46VB6zOkxCTcYrptY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703786948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/distributed-training-with-ray-train-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=distributed_training_ray_train", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?auto=webp&amp;s=f680295353da4f7a0719aec553d414776a06ec8e", "width": 1200, "height": 359}, "resolutions": [{"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a4478915d12439a59e1d2b63dfe1f8375fc0bf5", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2d770520fcb9e3c577c921c0bb4bdbae5956799", "width": 216, "height": 64}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51659b839a56a92b71c44c92567b0d863aae6bf6", "width": 320, "height": 95}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df14f4043708b02b89059c9812a69b0dea9c5412", "width": 640, "height": 191}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e23bf2170a5565b664f959ba1819f2d531e68fa3", "width": 960, "height": 287}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bc807b0bbfad25f9b27697284195896d37d93bd", "width": 1080, "height": 323}], "variants": {}, "id": "1qHNuiFMkCibNNa1eJMHDYp4L_pBECFWE3FCV56M8Mo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18t0d7o", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t0d7o/distributed_training_with_ray_train_and_minio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.min.io/distributed-training-with-ray-train-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=distributed_training_ray_train", "subreddit_subscribers": 149174, "created_utc": 1703786948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New to this, I was checking the certification and it is asking for company email?", "author_fullname": "t2_153hv15z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can u get databricks lakehouse fundamentals if you are not working for a company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ssp71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703766158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to this, I was checking the certification and it is asking for company email?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ssp71", "is_robot_indexable": true, "report_reasons": null, "author": "hayleybts", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ssp71/can_u_get_databricks_lakehouse_fundamentals_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ssp71/can_u_get_databricks_lakehouse_fundamentals_if/", "subreddit_subscribers": 149174, "created_utc": 1703766158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_49cfbl1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Productionizing Jupyter Notebooks with Versatile Data Kit (VDK)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_18srpzd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hGxnGSA7jk4a0Y_ztroxbp7mbXKk89XC3eZ3AzKzKSM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703762624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/versatile-data-kit/productionizing-jupyter-notebooks-with-versatile-data-kit-vdk-ec5824d31b77", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5h4o3ZdEzAcNlfPHCi-d05lnlhPRRLPoekJDhz8Jjj4.jpg?auto=webp&amp;s=2e8caf5a2c4797cf40de240469a67f9b1cc0c60a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/5h4o3ZdEzAcNlfPHCi-d05lnlhPRRLPoekJDhz8Jjj4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53300dee93c4fc589e35b1d090234f8a0fc99fb8", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/5h4o3ZdEzAcNlfPHCi-d05lnlhPRRLPoekJDhz8Jjj4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b580813ab95367b6df0eaf65e67a79c5b6733499", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/5h4o3ZdEzAcNlfPHCi-d05lnlhPRRLPoekJDhz8Jjj4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6672fe6cff422da5f0e99c9fdd082c1340c8ed0f", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/5h4o3ZdEzAcNlfPHCi-d05lnlhPRRLPoekJDhz8Jjj4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=00fcee8da15e51419025aceb9f9deb393a26733f", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/5h4o3ZdEzAcNlfPHCi-d05lnlhPRRLPoekJDhz8Jjj4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8582180f9e5a9a40f69ffc30807137488c47653f", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/5h4o3ZdEzAcNlfPHCi-d05lnlhPRRLPoekJDhz8Jjj4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ee64fb2fad82d0c86d9200d4c3d13d7dab58b4ac", "width": 1080, "height": 720}], "variants": {}, "id": "TBVSJ3r1aaIX6zc3PW78ubLcDKNcOqTRDgaSGqozetw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18srpzd", "is_robot_indexable": true, "report_reasons": null, "author": "zverulacis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18srpzd/productionizing_jupyter_notebooks_with_versatile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/versatile-data-kit/productionizing-jupyter-notebooks-with-versatile-data-kit-vdk-ec5824d31b77", "subreddit_subscribers": 149174, "created_utc": 1703762624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_qvzmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zendesk Moves from DynamoDB to MySQL and S3 to Save over 80% in Costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_18tjgp8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/MTka0EXAzqPNnMEHfn9nAO8RwoLOYivouYWdxwOALLs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703843034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoq.com/news/2023/12/zendesk-dynamodb-mysql-s3-cost/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?auto=webp&amp;s=611d84ebb2bb3a9ad062890da1b4f47ab4298102", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01c327d65beccee01d820dd00f255c7f7bf1935c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9666d9ea0e68b65667b668c34fc9ec760b425f0d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f8f1538db45c07dca09b843c450983ec84a7eb0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=02413dedb861ac5101df9446c09bb064fec135ca", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3d2a57cf7a5999ddb10ea6b5f0736ae4eafb4e6", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb48b64a8d45b4d554080505920bc38b75431b32", "width": 1080, "height": 567}], "variants": {}, "id": "7yyKoKkfxVkqP3uSULJ_K5wy-_bRYEEmjcZkyfNDPxY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18tjgp8", "is_robot_indexable": true, "report_reasons": null, "author": "rgancarz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tjgp8/zendesk_moves_from_dynamodb_to_mysql_and_s3_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoq.com/news/2023/12/zendesk-dynamodb-mysql-s3-cost/", "subreddit_subscribers": 149174, "created_utc": 1703843034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone\n\nI really would appreciate some guidance. From what I've read tools like Fivetran, Airflow, and Airbyte and on paper they sound interesting because you can code and more flexibility customize your pipeline. btw I've experience with Talend, Informatica PowerCenter, and SSIS .\n\nBut i be honest the prospect of coding in Python within an ETL context really piques my interest but i have concerns Do these modern ETL tools (or as what they call it the new wave) support CDC and Incremental load ?\n\nOur DWH is SingleStoreDB On-Premises. which ETL tool would you recommend for me to push my skills and keep up with those new tools\n\nEdit: we have very large data like like one table has 25M records so are the modern tool can handle those large records faster then the traditional ETL tools like power center.. etc", "author_fullname": "t2_h7fs687a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Next-Gen ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tho6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703838530.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703835856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone&lt;/p&gt;\n\n&lt;p&gt;I really would appreciate some guidance. From what I&amp;#39;ve read tools like Fivetran, Airflow, and Airbyte and on paper they sound interesting because you can code and more flexibility customize your pipeline. btw I&amp;#39;ve experience with Talend, Informatica PowerCenter, and SSIS .&lt;/p&gt;\n\n&lt;p&gt;But i be honest the prospect of coding in Python within an ETL context really piques my interest but i have concerns Do these modern ETL tools (or as what they call it the new wave) support CDC and Incremental load ?&lt;/p&gt;\n\n&lt;p&gt;Our DWH is SingleStoreDB On-Premises. which ETL tool would you recommend for me to push my skills and keep up with those new tools&lt;/p&gt;\n\n&lt;p&gt;Edit: we have very large data like like one table has 25M records so are the modern tool can handle those large records faster then the traditional ETL tools like power center.. etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18tho6r", "is_robot_indexable": true, "report_reasons": null, "author": "thebatman7727", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tho6r/nextgen_etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tho6r/nextgen_etl_tools/", "subreddit_subscribers": 149174, "created_utc": 1703835856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the best way to clean up messy customer address data and names? Right now, the data is landing into snowflake from Fivetran.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Messy Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tf85z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703827217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the best way to clean up messy customer address data and names? Right now, the data is landing into snowflake from Fivetran.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tf85z", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tf85z/messy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tf85z/messy_data/", "subreddit_subscribers": 149174, "created_utc": 1703827217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a project that will include some Linux boxes for servers and want to have essentially a basic S3-esque layer on top of them. Is Minio my best choice? Most objects will be 50-250MB in size and I don\u2019t need a data lake/advanced partitioning of any sort", "author_fullname": "t2_4ffbvgzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mimic Blob Storage on Linux Box(es)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tcx87", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703820110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a project that will include some Linux boxes for servers and want to have essentially a basic S3-esque layer on top of them. Is Minio my best choice? Most objects will be 50-250MB in size and I don\u2019t need a data lake/advanced partitioning of any sort&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tcx87", "is_robot_indexable": true, "report_reasons": null, "author": "ReporterNervous6822", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tcx87/mimic_blob_storage_on_linux_boxes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tcx87/mimic_blob_storage_on_linux_boxes/", "subreddit_subscribers": 149174, "created_utc": 1703820110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm like DA, it's my first job in BI, in a retail company, it has 92 stores, 3 distribution plants, 1000 microstrategy licenses\n\nThe fact is that we have many tables, many attributes and metrics, many users when validating in microstrategy do not know how to cross-reference the attributes and metrics and the new reports they want to create are broken, is there some practical way/template or tool that allows them to instruct them with documentation so they know how to cross-reference the attributes and metrics without breaking the reports", "author_fullname": "t2_9pzeqq4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "documentation attributes and metrics in microstrategy / snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t5qul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703800562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m like DA, it&amp;#39;s my first job in BI, in a retail company, it has 92 stores, 3 distribution plants, 1000 microstrategy licenses&lt;/p&gt;\n\n&lt;p&gt;The fact is that we have many tables, many attributes and metrics, many users when validating in microstrategy do not know how to cross-reference the attributes and metrics and the new reports they want to create are broken, is there some practical way/template or tool that allows them to instruct them with documentation so they know how to cross-reference the attributes and metrics without breaking the reports&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18t5qul", "is_robot_indexable": true, "report_reasons": null, "author": "Icy_Cricket_779", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t5qul/documentation_attributes_and_metrics_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t5qul/documentation_attributes_and_metrics_in/", "subreddit_subscribers": 149174, "created_utc": 1703800562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone.\n\nI'm working on an OpenSource project in the data engineering field.\n\nHonestly, I was tired of FiveTran and their pricing at my previous job so I want to create something that people can use on their own.\n\nI already managed to accomplish the core of the system and create basic connectors like PG Logical Replication into Kafka, WebSockets, and Postgres.\n\nI wonder what data source and destination pairs people need. Like what third-party platforms you want to bring the data from and store it in the database, etc.\n\nFeel free to share your thoughts on that :)  \n", "author_fullname": "t2_lgjjnb5r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Data Integrations are needed for your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t58n3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703799289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on an OpenSource project in the data engineering field.&lt;/p&gt;\n\n&lt;p&gt;Honestly, I was tired of FiveTran and their pricing at my previous job so I want to create something that people can use on their own.&lt;/p&gt;\n\n&lt;p&gt;I already managed to accomplish the core of the system and create basic connectors like PG Logical Replication into Kafka, WebSockets, and Postgres.&lt;/p&gt;\n\n&lt;p&gt;I wonder what data source and destination pairs people need. Like what third-party platforms you want to bring the data from and store it in the database, etc.&lt;/p&gt;\n\n&lt;p&gt;Feel free to share your thoughts on that :)  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18t58n3", "is_robot_indexable": true, "report_reasons": null, "author": "warphere", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t58n3/what_data_integrations_are_needed_for_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t58n3/what_data_integrations_are_needed_for_your_job/", "subreddit_subscribers": 149174, "created_utc": 1703799289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://eash98.medium.com/debunking-5-myths-on-adopting-databricks-18140cc3d183", "author_fullname": "t2_15opju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An article on adopting Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18szr7u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703785453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://eash98.medium.com/debunking-5-myths-on-adopting-databricks-18140cc3d183\"&gt;https://eash98.medium.com/debunking-5-myths-on-adopting-databricks-18140cc3d183&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UUDJZB8xbXnOrvGUfFAIpnmWNq5ITjRG2m2U-rxdRyc.jpg?auto=webp&amp;s=9e5d2c1683c85385941635fec64eec342fb396db", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/UUDJZB8xbXnOrvGUfFAIpnmWNq5ITjRG2m2U-rxdRyc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2197c78f927364ebbb3f3f7dd972c0afae43b83", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/UUDJZB8xbXnOrvGUfFAIpnmWNq5ITjRG2m2U-rxdRyc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77cb6b9a6f2d363f9f27b59c416402c91bb544c0", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/UUDJZB8xbXnOrvGUfFAIpnmWNq5ITjRG2m2U-rxdRyc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ecb8e3790ad8ffcb19bc659cb1fe799fd5062b6", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/UUDJZB8xbXnOrvGUfFAIpnmWNq5ITjRG2m2U-rxdRyc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db967054ecd0770990fc3d9ed481543022722e4b", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/UUDJZB8xbXnOrvGUfFAIpnmWNq5ITjRG2m2U-rxdRyc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=539af44070a74c2d3aefacc1346cb8a348eac4e6", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/UUDJZB8xbXnOrvGUfFAIpnmWNq5ITjRG2m2U-rxdRyc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=17a06a5652bd0bfe5eef28d7195dad685f8d0016", "width": 1080, "height": 720}], "variants": {}, "id": "vylC4vmEtiWDdOM4__p_JX4NIAZwtyVx3edHg4idIb0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18szr7u", "is_robot_indexable": true, "report_reasons": null, "author": "eash_98", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18szr7u/an_article_on_adopting_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18szr7u/an_article_on_adopting_databricks/", "subreddit_subscribers": 149174, "created_utc": 1703785453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an on-premise Apache Superset 2.1.0, it works in a docker container. It's basically in a PoC stage, where we are trying to assess if Superset is the right fit for our purposes.\n\nOne requirement that we have is to be able to share created dashboards with people without accounts. Someone in the higher management, for example. \n\nIdeally, it would work in a user-friendly way, when dashboard creator can make it public on his own. Without having to provide access to the underlying datasets to a specific role or having us do that for him. Just a simple \"make it public\" kind of a button :)\n\nI've tried multiple things with the help of Google Bard, none of which worked.\n\nI've then tried [this guide](https://github.com/apache/superset/discussions/25299), but it didn't work either. In general, searching for help on Superset is frustrating, because it seems that everyone uses a different version with different options on basically anything.\n\nCan you guys show me the way? How tf do I make these dashboards easily available to people without logins?\n\n*Bonus question: Redis container is a part of the package, but superset init shows that it uses local cache for some reason. Am I doing something wrong here?*", "author_fullname": "t2_6n04s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Superset help needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18stjub", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703768970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an on-premise Apache Superset 2.1.0, it works in a docker container. It&amp;#39;s basically in a PoC stage, where we are trying to assess if Superset is the right fit for our purposes.&lt;/p&gt;\n\n&lt;p&gt;One requirement that we have is to be able to share created dashboards with people without accounts. Someone in the higher management, for example. &lt;/p&gt;\n\n&lt;p&gt;Ideally, it would work in a user-friendly way, when dashboard creator can make it public on his own. Without having to provide access to the underlying datasets to a specific role or having us do that for him. Just a simple &amp;quot;make it public&amp;quot; kind of a button :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried multiple things with the help of Google Bard, none of which worked.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve then tried &lt;a href=\"https://github.com/apache/superset/discussions/25299\"&gt;this guide&lt;/a&gt;, but it didn&amp;#39;t work either. In general, searching for help on Superset is frustrating, because it seems that everyone uses a different version with different options on basically anything.&lt;/p&gt;\n\n&lt;p&gt;Can you guys show me the way? How tf do I make these dashboards easily available to people without logins?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Bonus question: Redis container is a part of the package, but superset init shows that it uses local cache for some reason. Am I doing something wrong here?&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hEgqhIbmxNjumxj84yLjyXG9zhd2oxNHwz41c2m2fO8.jpg?auto=webp&amp;s=26f8bdf747636c8495bbb8c93635125c8c64ae4b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hEgqhIbmxNjumxj84yLjyXG9zhd2oxNHwz41c2m2fO8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff185993d7cd9a682d80d237fb1645786d1ada7f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hEgqhIbmxNjumxj84yLjyXG9zhd2oxNHwz41c2m2fO8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70959835ac078c408d4892ab09cdcf2af256093a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hEgqhIbmxNjumxj84yLjyXG9zhd2oxNHwz41c2m2fO8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa153c6b5bc8eefb0d907d8722541ecc5e8cb2b5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hEgqhIbmxNjumxj84yLjyXG9zhd2oxNHwz41c2m2fO8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bced786232bbbf930522136421cd27ac632bd4b5", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hEgqhIbmxNjumxj84yLjyXG9zhd2oxNHwz41c2m2fO8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=df64af16268a4ac0cdde14bcac3afca2057b42a2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/hEgqhIbmxNjumxj84yLjyXG9zhd2oxNHwz41c2m2fO8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8c91fb009ff6f185989c225df09b520b166cf265", "width": 1080, "height": 540}], "variants": {}, "id": "16eJ5hLQsuM8YctxRwpxnWB8Rtb_KGeyMayQ6P0noBc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18stjub", "is_robot_indexable": true, "report_reasons": null, "author": "zlobendog", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18stjub/apache_superset_help_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18stjub/apache_superset_help_needed/", "subreddit_subscribers": 149174, "created_utc": 1703768970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://substack.com/@datasketch](https://substack.com/@datasketch)\n\nI write about my learnings in data world in quest to help others.", "author_fullname": "t2_q3nzgktvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "check out my new sub stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18szyuz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703785976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://substack.com/@datasketch\"&gt;https://substack.com/@datasketch&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I write about my learnings in data world in quest to help others.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/K57aZZkzqhSyDIneiPrzuSEbbtIZnmPWxGxzpjW6y48.jpg?auto=webp&amp;s=92c57c3a1defad23f51945b66cac1689dc98276d", "width": 3024, "height": 4032}, "resolutions": [{"url": "https://external-preview.redd.it/K57aZZkzqhSyDIneiPrzuSEbbtIZnmPWxGxzpjW6y48.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=def47b6cf968530a09c7b5cc10e161ff42089bb7", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/K57aZZkzqhSyDIneiPrzuSEbbtIZnmPWxGxzpjW6y48.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36b52d18b6c496d9e8cfd8b93e157a4e4e7840d7", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/K57aZZkzqhSyDIneiPrzuSEbbtIZnmPWxGxzpjW6y48.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=07fc1aa52df8d8f84e750b763f897530b0c39611", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/K57aZZkzqhSyDIneiPrzuSEbbtIZnmPWxGxzpjW6y48.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb7e9de0320ef81b26b8c3f39ec8d0172ad9c199", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/K57aZZkzqhSyDIneiPrzuSEbbtIZnmPWxGxzpjW6y48.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4aae428e9fb9e5236bc540c9802770fb360e7df1", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/K57aZZkzqhSyDIneiPrzuSEbbtIZnmPWxGxzpjW6y48.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6a6c40b80e20145dbab4494ee676bf0bafaa82b", "width": 1080, "height": 1440}], "variants": {}, "id": "wvvQKXA_i3KizWZEX596BfSLFrzYyb35RFR68HvHbMQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18szyuz", "is_robot_indexable": true, "report_reasons": null, "author": "photon223", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18szyuz/check_out_my_new_sub_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18szyuz/check_out_my_new_sub_stack/", "subreddit_subscribers": 149174, "created_utc": 1703785976.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}