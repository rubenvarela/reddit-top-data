{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Isn\u2019t there any more consumable book than Designing Data Intensive Applications That will help me move to data engineering from data analysis ? Everytime I try to absorb it I feel like I\u2019m stupid \ud83d\ude02", "author_fullname": "t2_t3nz93za", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DDIA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t52ro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703798890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Isn\u2019t there any more consumable book than Designing Data Intensive Applications That will help me move to data engineering from data analysis ? Everytime I try to absorb it I feel like I\u2019m stupid \ud83d\ude02&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18t52ro", "is_robot_indexable": true, "report_reasons": null, "author": "Repulsive-Ad7769", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t52ro/ddia/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t52ro/ddia/", "subreddit_subscribers": 149240, "created_utc": 1703798890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_qvzmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zendesk Moves from DynamoDB to MySQL and S3 to Save over 80% in Costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18tjgp8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/MTka0EXAzqPNnMEHfn9nAO8RwoLOYivouYWdxwOALLs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703843034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoq.com/news/2023/12/zendesk-dynamodb-mysql-s3-cost/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?auto=webp&amp;s=611d84ebb2bb3a9ad062890da1b4f47ab4298102", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01c327d65beccee01d820dd00f255c7f7bf1935c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9666d9ea0e68b65667b668c34fc9ec760b425f0d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f8f1538db45c07dca09b843c450983ec84a7eb0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=02413dedb861ac5101df9446c09bb064fec135ca", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3d2a57cf7a5999ddb10ea6b5f0736ae4eafb4e6", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/X9yCopKqzGwuPDo1OW5TNnyyXPox61ZP-cbkmbc5n0s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb48b64a8d45b4d554080505920bc38b75431b32", "width": 1080, "height": 567}], "variants": {}, "id": "7yyKoKkfxVkqP3uSULJ_K5wy-_bRYEEmjcZkyfNDPxY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18tjgp8", "is_robot_indexable": true, "report_reasons": null, "author": "rgancarz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tjgp8/zendesk_moves_from_dynamodb_to_mysql_and_s3_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoq.com/news/2023/12/zendesk-dynamodb-mysql-s3-cost/", "subreddit_subscribers": 149240, "created_utc": 1703843034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nWe\u2019re using a service that allows us to develop ETL pipelines and schedule jobs for automated runs. \n\nThe current setup thoroughly transforms the data step by step, but there are multiple buckets and other locations our pipelines are dropping data into. \n\nAs I continue development, what is a good way to begin documenting the ETL architecture, tools, and how do I go about documenting what is going o? \n\nHow detailed do I need to be when documenting this for new joiners? For now, I will not being using sphinx so I will be using docstrings to comment in, and creating documents myself typed out. \n\n\nThanks", "author_fullname": "t2_oc5syk6jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I document ETL/ELT pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t3ect", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703794654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;We\u2019re using a service that allows us to develop ETL pipelines and schedule jobs for automated runs. &lt;/p&gt;\n\n&lt;p&gt;The current setup thoroughly transforms the data step by step, but there are multiple buckets and other locations our pipelines are dropping data into. &lt;/p&gt;\n\n&lt;p&gt;As I continue development, what is a good way to begin documenting the ETL architecture, tools, and how do I go about documenting what is going o? &lt;/p&gt;\n\n&lt;p&gt;How detailed do I need to be when documenting this for new joiners? For now, I will not being using sphinx so I will be using docstrings to comment in, and creating documents myself typed out. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18t3ect", "is_robot_indexable": true, "report_reasons": null, "author": "tryingmybest200000", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t3ect/how_do_i_document_etlelt_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t3ect/how_do_i_document_etlelt_pipelines/", "subreddit_subscribers": 149240, "created_utc": 1703794654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there resources for best ways to go about *documenting* database designs? \n\nI realize that one can export a database schema, add some comments to it, and use diagrams to represent it visually. But, these seem deficient in that they do not capture the semantics or rationale for the underlying fields/tables. I\u2019m especially curious about established resources that lay this out in detail, especially ones that I can show to others in support of best practices (a textbook would be ideal, but I realize that\u2019s a tall order for what I\u2019m looking for).", "author_fullname": "t2_i90lr7q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for documenting database design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tgg69", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703831292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there resources for best ways to go about &lt;em&gt;documenting&lt;/em&gt; database designs? &lt;/p&gt;\n\n&lt;p&gt;I realize that one can export a database schema, add some comments to it, and use diagrams to represent it visually. But, these seem deficient in that they do not capture the semantics or rationale for the underlying fields/tables. I\u2019m especially curious about established resources that lay this out in detail, especially ones that I can show to others in support of best practices (a textbook would be ideal, but I realize that\u2019s a tall order for what I\u2019m looking for).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tgg69", "is_robot_indexable": true, "report_reasons": null, "author": "rubikol", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tgg69/best_practices_for_documenting_database_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tgg69/best_practices_for_documenting_database_design/", "subreddit_subscribers": 149240, "created_utc": 1703831292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone\n\nI really would appreciate some guidance. From what I've read tools like Fivetran, Airflow, and Airbyte and on paper they sound interesting because you can code and more flexibility customize your pipeline. btw I've experience with Talend, Informatica PowerCenter, and SSIS .\n\nBut i be honest the prospect of coding in Python within an ETL context really piques my interest but i have concerns Do these modern ETL tools (or as what they call it the new wave) support CDC and Incremental load ?\n\nOur DWH is SingleStoreDB On-Premises. which ETL tool would you recommend for me to push my skills and keep up with those new tools\n\nEdit: we have very large data like like one table has 25M records so are the modern tool can handle those large records faster then the traditional ETL tools like power center.. etc", "author_fullname": "t2_h7fs687a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Next-Gen ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tho6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703838530.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703835856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone&lt;/p&gt;\n\n&lt;p&gt;I really would appreciate some guidance. From what I&amp;#39;ve read tools like Fivetran, Airflow, and Airbyte and on paper they sound interesting because you can code and more flexibility customize your pipeline. btw I&amp;#39;ve experience with Talend, Informatica PowerCenter, and SSIS .&lt;/p&gt;\n\n&lt;p&gt;But i be honest the prospect of coding in Python within an ETL context really piques my interest but i have concerns Do these modern ETL tools (or as what they call it the new wave) support CDC and Incremental load ?&lt;/p&gt;\n\n&lt;p&gt;Our DWH is SingleStoreDB On-Premises. which ETL tool would you recommend for me to push my skills and keep up with those new tools&lt;/p&gt;\n\n&lt;p&gt;Edit: we have very large data like like one table has 25M records so are the modern tool can handle those large records faster then the traditional ETL tools like power center.. etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18tho6r", "is_robot_indexable": true, "report_reasons": null, "author": "thebatman7727", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tho6r/nextgen_etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tho6r/nextgen_etl_tools/", "subreddit_subscribers": 149240, "created_utc": 1703835856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI\u2019m trying to figure out a good system for securely using and managing secrets. I\u2019d like to hear about what everyone else does. I\u2019ll also share my early idea for a system I think may some merit to it.\n\nThis approach leverages AWS Secrets Manager, Gitea, Age encryption protocol, and Mozilla SOPS. It attempts to provide robust and secure secrets management while minimizing cost,\n\n**Key Components:**\n1. **Infrastructure:**\n   - Gitea hosted on a private cloud subnet, accessed via a bastion server.\n   - AWS Secrets Manager for secure storage of encryption keys.\n\n2. **Secrets Encryption:**\n   - Age encryption protocol for encryption keys.\n   - Mozilla SOPS to locally encrypt secrets in a `secrets.json` file.\n\n3. **Remote Machine Usage**\n     - Cloud machines perform `git pull` every time secrets are needed to retrieve updated secrets.\n     - Fetch the new decryption key from AWS Secrets.\n     - Decrypt secrets in memory using Mozilla SOPS.\n     - Discard old decryption key and decrypted secrets from memory.\n     - Discard decrypted secrets from memory after use\n\n4. **Key Rotation Process:**\n   - **Generate New Key**\n     - Utilize Age encryption protocol to create a fresh encryption key.\n     - Upload the encryption key to AWS secrets for later use\n\n   - **Rotate Secrets**\n     - Use AWS Secrets API to collect the old encryption key.\n     - `git pull` all repositories that use the encryption key for their secrets\n     - Decrypt all secrets with the old encryption key\n     - Encrypt all secrets with Mozilla SOPS using the new key.\n     - Embed AWS Secrets ID in the git push for future reference.\n     - `git push` the updated `secrets.json` files to Secrets-Gitea.\n     - Update AWS Secrets to include the new decryption key.\n\n**Capabilities:**\n- **Traceability:**\n  - Each `secrets.json` file is associated with its encryption key through AWS Secrets ID.\n\n- **Key Rotation:**\n  - Enables regular updates for enhanced security.\n\n- **Cost Savings:**\n  - Effective use of cloud resources. Secrets management is expensive.\n\n- **Version Control:**\n  - Leverages Gitea for versioning of secrets, aiding in auditing and rollbacks.\n\n**Considerations:**\n- **Centralization Drawbacks:**\n  - Potential single point of attack\n\nEdit: perhaps with git-remote-gcrypt", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you securely manage remotely available secrets at scale in a cost effective way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tdkfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703823878.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703822035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to figure out a good system for securely using and managing secrets. I\u2019d like to hear about what everyone else does. I\u2019ll also share my early idea for a system I think may some merit to it.&lt;/p&gt;\n\n&lt;p&gt;This approach leverages AWS Secrets Manager, Gitea, Age encryption protocol, and Mozilla SOPS. It attempts to provide robust and secure secrets management while minimizing cost,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;\n1. &lt;strong&gt;Infrastructure:&lt;/strong&gt;\n   - Gitea hosted on a private cloud subnet, accessed via a bastion server.\n   - AWS Secrets Manager for secure storage of encryption keys.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Secrets Encryption:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Age encryption protocol for encryption keys.&lt;/li&gt;\n&lt;li&gt;Mozilla SOPS to locally encrypt secrets in a &lt;code&gt;secrets.json&lt;/code&gt; file.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Remote Machine Usage&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cloud machines perform &lt;code&gt;git pull&lt;/code&gt; every time secrets are needed to retrieve updated secrets.&lt;/li&gt;\n&lt;li&gt;Fetch the new decryption key from AWS Secrets.&lt;/li&gt;\n&lt;li&gt;Decrypt secrets in memory using Mozilla SOPS.&lt;/li&gt;\n&lt;li&gt;Discard old decryption key and decrypted secrets from memory.&lt;/li&gt;\n&lt;li&gt;Discard decrypted secrets from memory after use&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Key Rotation Process:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Generate New Key&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Utilize Age encryption protocol to create a fresh encryption key.&lt;/li&gt;\n&lt;li&gt;Upload the encryption key to AWS secrets for later use&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Rotate Secrets&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Use AWS Secrets API to collect the old encryption key.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;git pull&lt;/code&gt; all repositories that use the encryption key for their secrets&lt;/li&gt;\n&lt;li&gt;Decrypt all secrets with the old encryption key&lt;/li&gt;\n&lt;li&gt;Encrypt all secrets with Mozilla SOPS using the new key.&lt;/li&gt;\n&lt;li&gt;Embed AWS Secrets ID in the git push for future reference.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;git push&lt;/code&gt; the updated &lt;code&gt;secrets.json&lt;/code&gt; files to Secrets-Gitea.&lt;/li&gt;\n&lt;li&gt;Update AWS Secrets to include the new decryption key.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Capabilities:&lt;/strong&gt;\n- &lt;strong&gt;Traceability:&lt;/strong&gt;\n  - Each &lt;code&gt;secrets.json&lt;/code&gt; file is associated with its encryption key through AWS Secrets ID.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Key Rotation:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Enables regular updates for enhanced security.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cost Savings:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Effective use of cloud resources. Secrets management is expensive.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Version Control:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Leverages Gitea for versioning of secrets, aiding in auditing and rollbacks.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Considerations:&lt;/strong&gt;\n- &lt;strong&gt;Centralization Drawbacks:&lt;/strong&gt;\n  - Potential single point of attack&lt;/p&gt;\n\n&lt;p&gt;Edit: perhaps with git-remote-gcrypt&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18tdkfv", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tdkfv/how_do_you_securely_manage_remotely_available/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tdkfv/how_do_you_securely_manage_remotely_available/", "subreddit_subscribers": 149240, "created_utc": 1703822035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bit of a challenge where I am taking a sharded mysql database, combining all like tables and loading them into Snowflake.  We have over 30,000 different database shards with each containing the same 200 tables.  \n\nCurrently I have a python script that loops through all shards running a select \\* and appending data to 200 different csv files that are then pushed to S3.  I then use a Lambda function and the Copy Into statement to create the 200 final tables within snowflake.  \n\nIs there a better way to do this?  I currently have to run the entire job each morning and do a rip and replace.  ", "author_fullname": "t2_2qnlv1dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading a sharded MYSQL database into Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t98ob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703809643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bit of a challenge where I am taking a sharded mysql database, combining all like tables and loading them into Snowflake.  We have over 30,000 different database shards with each containing the same 200 tables.  &lt;/p&gt;\n\n&lt;p&gt;Currently I have a python script that loops through all shards running a select * and appending data to 200 different csv files that are then pushed to S3.  I then use a Lambda function and the Copy Into statement to create the 200 final tables within snowflake.  &lt;/p&gt;\n\n&lt;p&gt;Is there a better way to do this?  I currently have to run the entire job each morning and do a rip and replace.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18t98ob", "is_robot_indexable": true, "report_reasons": null, "author": "themooseCS", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t98ob/loading_a_sharded_mysql_database_into_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t98ob/loading_a_sharded_mysql_database_into_snowflake/", "subreddit_subscribers": 149240, "created_utc": 1703809643.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got hired as a data engineer and will likely start in 2 weeks. I have my theory ok, some personal projects and a certification, but I want to know how can I prepare for the day to day work. Bootcamps and solo projects don't fill much on what skills you will need in a real job I think. For some context, I have 4 yoe as a scientist (geologist).\n\nThanks!", "author_fullname": "t2_d9cmx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to know before my first job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18tr905", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703867945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got hired as a data engineer and will likely start in 2 weeks. I have my theory ok, some personal projects and a certification, but I want to know how can I prepare for the day to day work. Bootcamps and solo projects don&amp;#39;t fill much on what skills you will need in a real job I think. For some context, I have 4 yoe as a scientist (geologist).&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tr905", "is_robot_indexable": true, "report_reasons": null, "author": "sebakjal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tr905/what_to_know_before_my_first_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tr905/what_to_know_before_my_first_job/", "subreddit_subscribers": 149240, "created_utc": 1703867945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need to SFTP files generated every 15 minutes up to an external SAAS product.\n\nProblem is the app generating these files has a hard lock on them.  It monitors realtime and keeps appending.  The target system doesn't actually need those files.  But it does need all the other files in the folder.\n\nThe files don't have an extension so I can't pattern match the ones to copy up.\n\nIs there anyway to do this in ADF natively?\n\nOr if it can't do that can I run a python script or powershell and pass the file list I want it to send back to ADF?\n\nThe files have RTA in the name so I'm sure I can regex that string in python.  Its the passing that list back to ADF for the pipeline job is the bit I have no clue.", "author_fullname": "t2_21umeu7n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I exclude files in ADF?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tbxq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703817432.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703817247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need to SFTP files generated every 15 minutes up to an external SAAS product.&lt;/p&gt;\n\n&lt;p&gt;Problem is the app generating these files has a hard lock on them.  It monitors realtime and keeps appending.  The target system doesn&amp;#39;t actually need those files.  But it does need all the other files in the folder.&lt;/p&gt;\n\n&lt;p&gt;The files don&amp;#39;t have an extension so I can&amp;#39;t pattern match the ones to copy up.&lt;/p&gt;\n\n&lt;p&gt;Is there anyway to do this in ADF natively?&lt;/p&gt;\n\n&lt;p&gt;Or if it can&amp;#39;t do that can I run a python script or powershell and pass the file list I want it to send back to ADF?&lt;/p&gt;\n\n&lt;p&gt;The files have RTA in the name so I&amp;#39;m sure I can regex that string in python.  Its the passing that list back to ADF for the pipeline job is the bit I have no clue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tbxq3", "is_robot_indexable": true, "report_reasons": null, "author": "ComfortAndSpeed", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tbxq3/how_do_i_exclude_files_in_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tbxq3/how_do_i_exclude_files_in_adf/", "subreddit_subscribers": 149240, "created_utc": 1703817247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u8kebhp6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\ude80 Introducing UCX v0.8.0: Powerful New Features to Streamline Your Unity Catalog Migration \ud83d\udcbc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18t3fpz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/0vOipl6id0Dt5BoRaINYq-IycgPnXLmGQqa84Wup6T4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703794754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/databricks-labs/introducing-ucx-v0-8-0-powerful-new-features-to-streamline-your-unity-catalog-migration-828d8c9b7aef", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?auto=webp&amp;s=1ee66d9440f6393fb76721d5ac3a6d41378d6c00", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c14ff71deedc0b9e6d99c4868c355533aad38da1", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=005d1db282e67060cb29f400c5274bca7c234e2b", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e740a84e06100104ccb48bf1b13a295cd774223d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=217fb4df0068cfbdf085e50b031e2b5b5fd2c13b", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2ccdc11219f16683c0052c570f00d298faa3c417", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/A4XG2GSH0XTUZWv6wo1_qDF4ZGhvLcgFia05jKfaK3s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e24c939caf8950e761c054e9cfc300f3769fb00", "width": 1080, "height": 1080}], "variants": {}, "id": "zTYfMjHl7ivk4KOBIcyxhYx1j3k-NSCJh5fT9TZlG9M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18t3fpz", "is_robot_indexable": true, "report_reasons": null, "author": "serge_databricks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t3fpz/introducing_ucx_v080_powerful_new_features_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/databricks-labs/introducing-ucx-v0-8-0-powerful-new-features-to-streamline-your-unity-catalog-migration-828d8c9b7aef", "subreddit_subscribers": 149240, "created_utc": 1703794754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I just got hired into a solar sales company, I'm one of two technical roles in the business. My career up to now has been in data analysis and data science, but I've been tasked with creating an entire data pipe to take our data from Salesforce to something automatable/queryable in a BI tool. This means I'm working way upstream of everything I know, but I was told the company had to pick between an engineer and an analyst, and chose an analyst. \n\nAfter researching I think I want to put as much as I can into one cloud service, so AWS seems good enough for that. The company is pretty small, so I don't need to do anything that scales incredibly well, once we get a real data engineer in a year or two I'll likely work with them on transitioning to something that works better. I'm currently prioritizing getting something usable but not perfect off the ground ASAP.\n\nWith my limited research, I believe the best way to do this is as follows:\n\nSalesforce -&gt; Amazon Appflow -&gt; S3 -&gt; AWS Glue for ETL -&gt; Redshift\n\nThere will also be data coming into S3 from other sources, some of which will also go into Redshift (ex: payment data).\n\nIs there a simpler path that I could be following to get to a warehouse entity, or is this as bare bones as it gets?\n\nAdditionally, is anyone aware of good tutorials on how to chain AWS tools together? I find great resources for individual tools but not generally how to use them in tandem. Other resources that you've liked that covers high-level data engineering would also be welcome.\n\nThanks in advance!", "author_fullname": "t2_ob5cp2nhs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to Create MVP AWS Data Pipeline from Salesforce to Redshift, for Beginner", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t0suy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703788025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I just got hired into a solar sales company, I&amp;#39;m one of two technical roles in the business. My career up to now has been in data analysis and data science, but I&amp;#39;ve been tasked with creating an entire data pipe to take our data from Salesforce to something automatable/queryable in a BI tool. This means I&amp;#39;m working way upstream of everything I know, but I was told the company had to pick between an engineer and an analyst, and chose an analyst. &lt;/p&gt;\n\n&lt;p&gt;After researching I think I want to put as much as I can into one cloud service, so AWS seems good enough for that. The company is pretty small, so I don&amp;#39;t need to do anything that scales incredibly well, once we get a real data engineer in a year or two I&amp;#39;ll likely work with them on transitioning to something that works better. I&amp;#39;m currently prioritizing getting something usable but not perfect off the ground ASAP.&lt;/p&gt;\n\n&lt;p&gt;With my limited research, I believe the best way to do this is as follows:&lt;/p&gt;\n\n&lt;p&gt;Salesforce -&amp;gt; Amazon Appflow -&amp;gt; S3 -&amp;gt; AWS Glue for ETL -&amp;gt; Redshift&lt;/p&gt;\n\n&lt;p&gt;There will also be data coming into S3 from other sources, some of which will also go into Redshift (ex: payment data).&lt;/p&gt;\n\n&lt;p&gt;Is there a simpler path that I could be following to get to a warehouse entity, or is this as bare bones as it gets?&lt;/p&gt;\n\n&lt;p&gt;Additionally, is anyone aware of good tutorials on how to chain AWS tools together? I find great resources for individual tools but not generally how to use them in tandem. Other resources that you&amp;#39;ve liked that covers high-level data engineering would also be welcome.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18t0suy", "is_robot_indexable": true, "report_reasons": null, "author": "Clockworked47", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t0suy/best_way_to_create_mvp_aws_data_pipeline_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t0suy/best_way_to_create_mvp_aws_data_pipeline_from/", "subreddit_subscribers": 149240, "created_utc": 1703788025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distributed Training with Ray Train and MinIO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 41, "top_awarded_type": null, "hide_score": false, "name": "t3_18t0d7o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pXB1rtyTSkezkckvRPpVBRBheU46VB6zOkxCTcYrptY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703786948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/distributed-training-with-ray-train-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=distributed_training_ray_train", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?auto=webp&amp;s=f680295353da4f7a0719aec553d414776a06ec8e", "width": 1200, "height": 359}, "resolutions": [{"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a4478915d12439a59e1d2b63dfe1f8375fc0bf5", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2d770520fcb9e3c577c921c0bb4bdbae5956799", "width": 216, "height": 64}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51659b839a56a92b71c44c92567b0d863aae6bf6", "width": 320, "height": 95}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df14f4043708b02b89059c9812a69b0dea9c5412", "width": 640, "height": 191}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e23bf2170a5565b664f959ba1819f2d531e68fa3", "width": 960, "height": 287}, {"url": "https://external-preview.redd.it/usfqNvzII57kX_Fua5j1tgLS8sBS8X-G9dRNh_9A99k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bc807b0bbfad25f9b27697284195896d37d93bd", "width": 1080, "height": 323}], "variants": {}, "id": "1qHNuiFMkCibNNa1eJMHDYp4L_pBECFWE3FCV56M8Mo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18t0d7o", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t0d7o/distributed_training_with_ray_train_and_minio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.min.io/distributed-training-with-ray-train-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=distributed_training_ray_train", "subreddit_subscribers": 149240, "created_utc": 1703786948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the best way to clean up messy customer address data and names? Right now, the data is landing into snowflake from Fivetran.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Messy Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tf85z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703827217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the best way to clean up messy customer address data and names? Right now, the data is landing into snowflake from Fivetran.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tf85z", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tf85z/messy_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tf85z/messy_data/", "subreddit_subscribers": 149240, "created_utc": 1703827217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a project that will include some Linux boxes for servers and want to have essentially a basic S3-esque layer on top of them. Is Minio my best choice? Most objects will be 50-250MB in size and I don\u2019t need a data lake/advanced partitioning of any sort", "author_fullname": "t2_4ffbvgzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mimic Blob Storage on Linux Box(es)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18tcx87", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703820110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a project that will include some Linux boxes for servers and want to have essentially a basic S3-esque layer on top of them. Is Minio my best choice? Most objects will be 50-250MB in size and I don\u2019t need a data lake/advanced partitioning of any sort&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18tcx87", "is_robot_indexable": true, "report_reasons": null, "author": "ReporterNervous6822", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18tcx87/mimic_blob_storage_on_linux_boxes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18tcx87/mimic_blob_storage_on_linux_boxes/", "subreddit_subscribers": 149240, "created_utc": 1703820110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm like DA, it's my first job in BI, in a retail company, it has 92 stores, 3 distribution plants, 1000 microstrategy licenses\n\nThe fact is that we have many tables, many attributes and metrics, many users when validating in microstrategy do not know how to cross-reference the attributes and metrics and the new reports they want to create are broken, is there some practical way/template or tool that allows them to instruct them with documentation so they know how to cross-reference the attributes and metrics without breaking the reports", "author_fullname": "t2_9pzeqq4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "documentation attributes and metrics in microstrategy / snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t5qul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703800562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m like DA, it&amp;#39;s my first job in BI, in a retail company, it has 92 stores, 3 distribution plants, 1000 microstrategy licenses&lt;/p&gt;\n\n&lt;p&gt;The fact is that we have many tables, many attributes and metrics, many users when validating in microstrategy do not know how to cross-reference the attributes and metrics and the new reports they want to create are broken, is there some practical way/template or tool that allows them to instruct them with documentation so they know how to cross-reference the attributes and metrics without breaking the reports&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18t5qul", "is_robot_indexable": true, "report_reasons": null, "author": "Icy_Cricket_779", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t5qul/documentation_attributes_and_metrics_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t5qul/documentation_attributes_and_metrics_in/", "subreddit_subscribers": 149240, "created_utc": 1703800562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone.\n\nI'm working on an OpenSource project in the data engineering field.\n\nHonestly, I was tired of FiveTran and their pricing at my previous job so I want to create something that people can use on their own.\n\nI already managed to accomplish the core of the system and create basic connectors like PG Logical Replication into Kafka, WebSockets, and Postgres.\n\nI wonder what data source and destination pairs people need. Like what third-party platforms you want to bring the data from and store it in the database, etc.\n\nFeel free to share your thoughts on that :)  \n", "author_fullname": "t2_lgjjnb5r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Data Integrations are needed for your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18t58n3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703799289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on an OpenSource project in the data engineering field.&lt;/p&gt;\n\n&lt;p&gt;Honestly, I was tired of FiveTran and their pricing at my previous job so I want to create something that people can use on their own.&lt;/p&gt;\n\n&lt;p&gt;I already managed to accomplish the core of the system and create basic connectors like PG Logical Replication into Kafka, WebSockets, and Postgres.&lt;/p&gt;\n\n&lt;p&gt;I wonder what data source and destination pairs people need. Like what third-party platforms you want to bring the data from and store it in the database, etc.&lt;/p&gt;\n\n&lt;p&gt;Feel free to share your thoughts on that :)  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18t58n3", "is_robot_indexable": true, "report_reasons": null, "author": "warphere", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18t58n3/what_data_integrations_are_needed_for_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18t58n3/what_data_integrations_are_needed_for_your_job/", "subreddit_subscribers": 149240, "created_utc": 1703799289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You are given a pair of datasets each api call and you're asked to link each record from file 1 to similar record from file 2 in real time without any metadata about the columns. How would you approach this problem?", "author_fullname": "t2_7zvlhn0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Record Linkage problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18torlm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703861333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You are given a pair of datasets each api call and you&amp;#39;re asked to link each record from file 1 to similar record from file 2 in real time without any metadata about the columns. How would you approach this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18torlm", "is_robot_indexable": true, "report_reasons": null, "author": "Omart__", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18torlm/record_linkage_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18torlm/record_linkage_problem/", "subreddit_subscribers": 149240, "created_utc": 1703861333.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}