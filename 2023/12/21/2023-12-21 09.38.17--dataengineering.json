{"kind": "Listing", "data": {"after": "t3_18n0bpf", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 Proven Steps to Build a Data Platform Without Breaking The Bank", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18mvojy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4Netwg6C0SfHub8Ar7o7IvyA2fb-t0j_pUoFZLz1RD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703082011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datagibberish.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datagibberish.com/p/6-proven-steps-to-build-a-data-platform", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bhbgU0DwjjSfAuJqEKFiQge13ausXeA-B7SgwS_ZjJw.jpg?auto=webp&amp;s=b4e01d833ae239137948b7014a5c2d50667b63c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/bhbgU0DwjjSfAuJqEKFiQge13ausXeA-B7SgwS_ZjJw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23cdc9e2ae3d462dafa76c4991b14aba71a5e6b5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bhbgU0DwjjSfAuJqEKFiQge13ausXeA-B7SgwS_ZjJw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36390021811a702409538d0cba4c4ccc56b1dbb7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/bhbgU0DwjjSfAuJqEKFiQge13ausXeA-B7SgwS_ZjJw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0de18994a004f1eca6d09c3be22f3b76d67a17c1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/bhbgU0DwjjSfAuJqEKFiQge13ausXeA-B7SgwS_ZjJw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=38442c64ab97d1229e85f0bc63d987907f72eaaa", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/bhbgU0DwjjSfAuJqEKFiQge13ausXeA-B7SgwS_ZjJw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=971d240f60986c83ffaa1374fab9dccefac61f4d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/bhbgU0DwjjSfAuJqEKFiQge13ausXeA-B7SgwS_ZjJw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=11ab64642b6c199f76dfa82aea920b996c883e3a", "width": 1080, "height": 540}], "variants": {}, "id": "JYcV74sgLPj23ZyktvPLScZjhEMsmv1zE4r4gGvRlwg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18mvojy", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mvojy/6_proven_steps_to_build_a_data_platform_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datagibberish.com/p/6-proven-steps-to-build-a-data-platform", "subreddit_subscribers": 147320, "created_utc": 1703082011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say i know how to connect to external databases and do dataframe transformations using pyspark in databricks. do i \"know\" spark? from my understanding, databricks handles all the other spark stuff like cores/threads etc.", "author_fullname": "t2_o21w1ya01", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how can a DE say that they \"know\" spark/pyspark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n9wnm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703119056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say i know how to connect to external databases and do dataframe transformations using pyspark in databricks. do i &amp;quot;know&amp;quot; spark? from my understanding, databricks handles all the other spark stuff like cores/threads etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18n9wnm", "is_robot_indexable": true, "report_reasons": null, "author": "300A24", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n9wnm/how_can_a_de_say_that_they_know_sparkpyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n9wnm/how_can_a_de_say_that_they_know_sparkpyspark/", "subreddit_subscribers": 147320, "created_utc": 1703119056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to pursue a career in DE, I have 2 YOE as Data Analyst, my college major is Corporate Finance, what I use the most are Spreadsheets, SQL, Python for data manipulation, and some reporting tools such as Power BI. As you can see I have some technical knowledge but am not even close to asserting I can transition easily into a DE role.\n\nBased on what I researched I need to learn topics such as Snowflake, Airflow, dbt, Spark, and Cloud services, just to mention a few.\n\nI am here to ask for advice on how to learn DE by doing while on a budget, this means I would like to gain as much hands-on experience for free. I have read that running locally (I believe I have a capable pc) might be the most cost-effective option, but not sure if this is the right way to learn since Cloud appears to be the standard.\n\nThank you, I appreciate any guidance", "author_fullname": "t2_4iz2pipg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the most cost-effective way to learn DE in the upcoming 2024?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n22z7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703098280.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to pursue a career in DE, I have 2 YOE as Data Analyst, my college major is Corporate Finance, what I use the most are Spreadsheets, SQL, Python for data manipulation, and some reporting tools such as Power BI. As you can see I have some technical knowledge but am not even close to asserting I can transition easily into a DE role.&lt;/p&gt;\n\n&lt;p&gt;Based on what I researched I need to learn topics such as Snowflake, Airflow, dbt, Spark, and Cloud services, just to mention a few.&lt;/p&gt;\n\n&lt;p&gt;I am here to ask for advice on how to learn DE by doing while on a budget, this means I would like to gain as much hands-on experience for free. I have read that running locally (I believe I have a capable pc) might be the most cost-effective option, but not sure if this is the right way to learn since Cloud appears to be the standard.&lt;/p&gt;\n\n&lt;p&gt;Thank you, I appreciate any guidance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18n22z7", "is_robot_indexable": true, "report_reasons": null, "author": "_Lavender_Brown_", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n22z7/whats_the_most_costeffective_way_to_learn_de_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n22z7/whats_the_most_costeffective_way_to_learn_de_in/", "subreddit_subscribers": 147320, "created_utc": 1703098280.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I used to work as a data engineer at a large enterprise and we built pretty cool stuff, such as ML pipelines or microservices containing business logic or routing to other ML endpoints, or data pipelines that would process a bunch of images and use cognitive services. We would use mostly open source tech and host everything on Kubernetes. I really learned a lot and had the feeling I was really \u2018engineering\u2019 projects. I left this company because the company was very political and our data/ML products were hardly used by the business. \n\nNow I joined a much smaller scaleup, also as a data engineer. They had a couple of consultants for a year who built up a lakehouse architecture in Databricks and made some dashboards, reports etc. also in Databricks. Now I\u2019m the first person joining the in-house team and my job is basically to take over from the consultants while the company is hiring more engineers/data scientists. Up until now I was a bit shocked by the work that was now on my plate. Currently I\u2019m either debugging data pipelines and trying to understand the decisions that were taken, or building dashboards, or fixing reports. All the work happens in Databricks so I hardly use my code editor, which is really frustrating because I\u2019m missing out on my developer tools. \n\nClearly, the data engineering position in the second company differs a lot from the same position in the first company and I\u2019m contemplating if data engineering is for me or that I should look for a software engineering role because it\u2019s more technical. I was wondering if others had the same feeling somewhere throughout their career.", "author_fullname": "t2_sazjm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can the same role differ so much per company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n6m5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703109996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to work as a data engineer at a large enterprise and we built pretty cool stuff, such as ML pipelines or microservices containing business logic or routing to other ML endpoints, or data pipelines that would process a bunch of images and use cognitive services. We would use mostly open source tech and host everything on Kubernetes. I really learned a lot and had the feeling I was really \u2018engineering\u2019 projects. I left this company because the company was very political and our data/ML products were hardly used by the business. &lt;/p&gt;\n\n&lt;p&gt;Now I joined a much smaller scaleup, also as a data engineer. They had a couple of consultants for a year who built up a lakehouse architecture in Databricks and made some dashboards, reports etc. also in Databricks. Now I\u2019m the first person joining the in-house team and my job is basically to take over from the consultants while the company is hiring more engineers/data scientists. Up until now I was a bit shocked by the work that was now on my plate. Currently I\u2019m either debugging data pipelines and trying to understand the decisions that were taken, or building dashboards, or fixing reports. All the work happens in Databricks so I hardly use my code editor, which is really frustrating because I\u2019m missing out on my developer tools. &lt;/p&gt;\n\n&lt;p&gt;Clearly, the data engineering position in the second company differs a lot from the same position in the first company and I\u2019m contemplating if data engineering is for me or that I should look for a software engineering role because it\u2019s more technical. I was wondering if others had the same feeling somewhere throughout their career.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18n6m5g", "is_robot_indexable": true, "report_reasons": null, "author": "Danielloesoe", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n6m5g/how_can_the_same_role_differ_so_much_per_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n6m5g/how_can_the_same_role_differ_so_much_per_company/", "subreddit_subscribers": 147320, "created_utc": 1703109996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I joined the team as a Data Engineer, and recently someone (BI Developer) got fired and another BI Developer got on medical leave for 2 months. The team was short staffed, they asked me to help out and I understand. However, the BI Developer got back from leave and they are still asking me to do their BI Development work. My manager manages the BI and Data Engineering team so we are in the same team, but I really hate that they are dragging me into BI and I feel that I no longer do Data Engineering works. I am all day doing SQL, Tableau and helping them to look into their Report SQL issue and it really pisses me off.\n\nEdit: I left a data analyst job at a very good company to join this lousy company as a data engineer, in hope of doing more engineering and less analytics.", "author_fullname": "t2_cjk2xje6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick rant: why does the team have me do BI developer job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n0515", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703093544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I joined the team as a Data Engineer, and recently someone (BI Developer) got fired and another BI Developer got on medical leave for 2 months. The team was short staffed, they asked me to help out and I understand. However, the BI Developer got back from leave and they are still asking me to do their BI Development work. My manager manages the BI and Data Engineering team so we are in the same team, but I really hate that they are dragging me into BI and I feel that I no longer do Data Engineering works. I am all day doing SQL, Tableau and helping them to look into their Report SQL issue and it really pisses me off.&lt;/p&gt;\n\n&lt;p&gt;Edit: I left a data analyst job at a very good company to join this lousy company as a data engineer, in hope of doing more engineering and less analytics.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18n0515", "is_robot_indexable": true, "report_reasons": null, "author": "highlifeed", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n0515/quick_rant_why_does_the_team_have_me_do_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n0515/quick_rant_why_does_the_team_have_me_do_bi/", "subreddit_subscribers": 147320, "created_utc": 1703093544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One Big Table (OBT) vs Star Schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18mvgwh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/SDiI0dJHoQiFl4ZHeBXKKH3nhQ9LGcUr6ghvdhSl1OE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703081426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/one-big-table-obt-vs-star-schema?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/y9YQAvSKRjZjDWqiTthBJWdrSIIHO8h33H1-uxhGPS0.jpg?auto=webp&amp;s=40246754099f7147bd393b58ba56c85999ce917e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/y9YQAvSKRjZjDWqiTthBJWdrSIIHO8h33H1-uxhGPS0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=baeeae35275fbd06beabb50b83a222dd9759d932", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/y9YQAvSKRjZjDWqiTthBJWdrSIIHO8h33H1-uxhGPS0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9ba127d6353c89fc6ac0654854ff99dc4247636", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/y9YQAvSKRjZjDWqiTthBJWdrSIIHO8h33H1-uxhGPS0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c913f7677d189b7a86a612cd2e1303f708c770a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/y9YQAvSKRjZjDWqiTthBJWdrSIIHO8h33H1-uxhGPS0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=604f040974b73ba51a5cb02a89c2c06a063e33a0", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/y9YQAvSKRjZjDWqiTthBJWdrSIIHO8h33H1-uxhGPS0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bb727cb6de7fbf08a558e0d86b1a1326e1678ff", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/y9YQAvSKRjZjDWqiTthBJWdrSIIHO8h33H1-uxhGPS0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14fdab292ae7a8973c3ff46673ca819a231f27f0", "width": 1080, "height": 540}], "variants": {}, "id": "mSCJirLB7sQgTvveMXLZYbzdEJUC2xqJdbayVMzrg6A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18mvgwh", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mvgwh/one_big_table_obt_vs_star_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/one-big-table-obt-vs-star-schema?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 147320, "created_utc": 1703081426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the outlook of both the fields in next 5 years?\nWhat\u2019s the difference in compensation between the two roles? Are Cyber Sec professionals paid way more than Data Engineers for the same experience?", "author_fullname": "t2_gxqyzp06l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cyber Sec vs Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n5sqj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703107862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the outlook of both the fields in next 5 years?\nWhat\u2019s the difference in compensation between the two roles? Are Cyber Sec professionals paid way more than Data Engineers for the same experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18n5sqj", "is_robot_indexable": true, "report_reasons": null, "author": "BullfrogMotor2634", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n5sqj/cyber_sec_vs_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n5sqj/cyber_sec_vs_data_engineer/", "subreddit_subscribers": 147320, "created_utc": 1703107862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data Analyst with 5 years experience. I currently make 88k USD. I was offered a Jr. Data Engineering position at another company making 70k USD. I didn't meet the expecations on the regular DE role, but they really liked me so thought I'd be a better fit for the Jr. Role.\n\n&amp;#x200B;\n\nMy questions\n\n&amp;#x200B;\n\n1.) Would you take this pay cut for the DE experience? I am 30... so I am wondering if this would pay off in a few years. I seem to be paid at the upper range for a Data Analyst.\n\n2.) My current job is remote, and this other job is remote too. I am bored to tears at my current role and have asked for different projections, went above and beyond with documentation, building out things, and doing everything I know how. My bosses just kinda shrug and say \"do what you can.\" Do you think I would be able to pull off both jobs remotely, even for a few months? Or is that a super bad idea.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_qcl3n475s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you take this pay cut for DE and another weird question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mw7iq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703083461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data Analyst with 5 years experience. I currently make 88k USD. I was offered a Jr. Data Engineering position at another company making 70k USD. I didn&amp;#39;t meet the expecations on the regular DE role, but they really liked me so thought I&amp;#39;d be a better fit for the Jr. Role.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My questions&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1.) Would you take this pay cut for the DE experience? I am 30... so I am wondering if this would pay off in a few years. I seem to be paid at the upper range for a Data Analyst.&lt;/p&gt;\n\n&lt;p&gt;2.) My current job is remote, and this other job is remote too. I am bored to tears at my current role and have asked for different projections, went above and beyond with documentation, building out things, and doing everything I know how. My bosses just kinda shrug and say &amp;quot;do what you can.&amp;quot; Do you think I would be able to pull off both jobs remotely, even for a few months? Or is that a super bad idea.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18mw7iq", "is_robot_indexable": true, "report_reasons": null, "author": "Elderberry_OG", "discussion_type": null, "num_comments": 25, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mw7iq/would_you_take_this_pay_cut_for_de_and_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mw7iq/would_you_take_this_pay_cut_for_de_and_another/", "subreddit_subscribers": 147320, "created_utc": 1703083461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to get a gauge for how much a DE should reasonably expect to have to define transformations or say, configure SCD logic as a part of their job duties. or ideally, too", "author_fullname": "t2_hqp95mxij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL development as a % of DE workload", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n85ia", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703114094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to get a gauge for how much a DE should reasonably expect to have to define transformations or say, configure SCD logic as a part of their job duties. or ideally, too&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18n85ia", "is_robot_indexable": true, "report_reasons": null, "author": "midnightrambling", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n85ia/etl_development_as_a_of_de_workload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n85ia/etl_development_as_a_of_de_workload/", "subreddit_subscribers": 147320, "created_utc": 1703114094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Assuming Databricks on Azure in a larger corporation. Depending on the data project, there exist separate teams for development and ops (so not really DevOps). Databricks is the standard way for LT (not ELT) in that company, so it's also used for projects with quite small amount of data, not time-critical, going to Power BI.\n\nLet's also assume the dev team likes to play with new technology and ops wants to have it as stable as possible.\n\n&amp;#x200B;\n\nHow would you make sure the ops team can handle the output of the dev team?\n\nI'm thinking of something like:\n\n* reducing the number of languages in notebooks (Python + SQL + Spark all mixed = hard to maintain)\n* linting of the code (but how to do that when Databricks does not have it build-in?)\n* have data quality handling somehow\n* documentation of course\n\n&amp;#x200B;\n\nI'm not thinking of:\n\n* org changes (DevOps and alike)\n\n&amp;#x200B;\n\nAny ideas appreciated.\n\n&amp;#x200B;\n\nThx alot", "author_fullname": "t2_3uhf3z0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to keep Databricks maintainable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mxhyh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703086828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Assuming Databricks on Azure in a larger corporation. Depending on the data project, there exist separate teams for development and ops (so not really DevOps). Databricks is the standard way for LT (not ELT) in that company, so it&amp;#39;s also used for projects with quite small amount of data, not time-critical, going to Power BI.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s also assume the dev team likes to play with new technology and ops wants to have it as stable as possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How would you make sure the ops team can handle the output of the dev team?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of something like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;reducing the number of languages in notebooks (Python + SQL + Spark all mixed = hard to maintain)&lt;/li&gt;\n&lt;li&gt;linting of the code (but how to do that when Databricks does not have it build-in?)&lt;/li&gt;\n&lt;li&gt;have data quality handling somehow&lt;/li&gt;\n&lt;li&gt;documentation of course&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not thinking of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;org changes (DevOps and alike)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any ideas appreciated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thx alot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18mxhyh", "is_robot_indexable": true, "report_reasons": null, "author": "blackitgreenit", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mxhyh/how_to_keep_databricks_maintainable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mxhyh/how_to_keep_databricks_maintainable/", "subreddit_subscribers": 147320, "created_utc": 1703086828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, i work as in a beginner level. I gave a requirement to data engineering lead to update redshift table records. It is basically update statements. My de lead told me that redshift does not support update statements, write everything on select query. \n\nI want to know in what case redshift won\u2019t accept update statements", "author_fullname": "t2_7ui8gfy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift update statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ner3i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703134360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, i work as in a beginner level. I gave a requirement to data engineering lead to update redshift table records. It is basically update statements. My de lead told me that redshift does not support update statements, write everything on select query. &lt;/p&gt;\n\n&lt;p&gt;I want to know in what case redshift won\u2019t accept update statements&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ner3i", "is_robot_indexable": true, "report_reasons": null, "author": "Negi_DA", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ner3i/redshift_update_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ner3i/redshift_update_statement/", "subreddit_subscribers": 147320, "created_utc": 1703134360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I\u2019m a 5th year data engineer. Have any of you ever published to mongodb from the databricks feature store? I want to do this. If you read the databricks document, it says that the third-party online store only supports Amazon DynamoDB, Amazon Aurora, and Amazon RDS MySQL.\n\n[https://docs.databricks.com/en/machine-learning/feature-store/online-feature-stores.html](https://docs.databricks.com/en/machine-learning/feature-store/online-feature-stores.html)\n\nHowever I heard from a databricks's manager that many customers also using mongodb a lot. However, I don\u2019t know the exact method. Anyone who knows how to publish with mongodb in the feature store, please help.\n\n(\bI can ask the databricks's manager first, but I'm just comfortable here. Please help me.)", "author_fullname": "t2_lvw025ht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "from databricks feature store to mongodb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ncnse", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703127589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019m a 5th year data engineer. Have any of you ever published to mongodb from the databricks feature store? I want to do this. If you read the databricks document, it says that the third-party online store only supports Amazon DynamoDB, Amazon Aurora, and Amazon RDS MySQL.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.databricks.com/en/machine-learning/feature-store/online-feature-stores.html\"&gt;https://docs.databricks.com/en/machine-learning/feature-store/online-feature-stores.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However I heard from a databricks&amp;#39;s manager that many customers also using mongodb a lot. However, I don\u2019t know the exact method. Anyone who knows how to publish with mongodb in the feature store, please help.&lt;/p&gt;\n\n&lt;p&gt;(I can ask the databricks&amp;#39;s manager first, but I&amp;#39;m just comfortable here. Please help me.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ncnse", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious_Sense_108", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ncnse/from_databricks_feature_store_to_mongodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ncnse/from_databricks_feature_store_to_mongodb/", "subreddit_subscribers": 147320, "created_utc": 1703127589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This article is about , \"Understanding Azure Data Lake Storage Gen2\" This article will cover: \ud83d\udca1\n\n1- Why Azure Data Lake Storage Gen2\n\n2- How to enable Azure Data Lake Storage Gen2\n\n3- Azure Data Lake Gen2 vs Azure Blob Storage Gen2\n\nIf you are interested to understand Azure Data Lake Storage Gen2 you can access the full article here: [https://devblogit.com/understand-azure-data-lake-storage-gen2/](https://devblogit.com/understand-azure-data-lake-storage-gen2/)\n\nDon't miss out on this opportunity to transform your data practices and stay ahead of the competition. Read the article today and unlock the power of Azure Data Lake Storage Gen2! \ud83d\udcaa[\\#Azure](https://www.linkedin.com/feed/hashtag/?keywords=azure&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\\#DataManagement](https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\\#Analytics](https://www.linkedin.com/feed/hashtag/?keywords=analytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\\#DataLake](https://www.linkedin.com/feed/hashtag/?keywords=datalake&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672)", "author_fullname": "t2_blyyz3sy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Lake Storage Gen2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18naxfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703122164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This article is about , &amp;quot;Understanding Azure Data Lake Storage Gen2&amp;quot; This article will cover: \ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;1- Why Azure Data Lake Storage Gen2&lt;/p&gt;\n\n&lt;p&gt;2- How to enable Azure Data Lake Storage Gen2&lt;/p&gt;\n\n&lt;p&gt;3- Azure Data Lake Gen2 vs Azure Blob Storage Gen2&lt;/p&gt;\n\n&lt;p&gt;If you are interested to understand Azure Data Lake Storage Gen2 you can access the full article here: &lt;a href=\"https://devblogit.com/understand-azure-data-lake-storage-gen2/\"&gt;https://devblogit.com/understand-azure-data-lake-storage-gen2/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t miss out on this opportunity to transform your data practices and stay ahead of the competition. Read the article today and unlock the power of Azure Data Lake Storage Gen2! \ud83d\udcaa&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=azure&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672\"&gt;#Azure&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672\"&gt;#DataManagement&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=analytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672\"&gt;#Analytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datalake&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672\"&gt;#DataLake&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18naxfs", "is_robot_indexable": true, "report_reasons": null, "author": "Bubbly_Bed_4478", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18naxfs/azure_data_lake_storage_gen2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18naxfs/azure_data_lake_storage_gen2/", "subreddit_subscribers": 147320, "created_utc": 1703122164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, is there any way to insert data into Iceberg table or generate Iceberg file without Spark/Glue or staging? There seems to be JAVA API in official doc but I can't find any examples. ", "author_fullname": "t2_13qnch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inserting data into Iceberg table in AWS from Java application without Glue/Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n9584", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703116822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, is there any way to insert data into Iceberg table or generate Iceberg file without Spark/Glue or staging? There seems to be JAVA API in official doc but I can&amp;#39;t find any examples. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18n9584", "is_robot_indexable": true, "report_reasons": null, "author": "eyes1216", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n9584/inserting_data_into_iceberg_table_in_aws_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n9584/inserting_data_into_iceberg_table_in_aws_from/", "subreddit_subscribers": 147320, "created_utc": 1703116822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So currently I\u2019m somewhat of a BI Analyst at a Tech Company my day-to-day involves \n\nBuilding re-usable queries to transform data housed in Oracle and Snowflake and providing these datasets in Excel Files / Google Sheets files to end users, as well as building the assets from the datasets as well.\n\nExample 1: A team focusing on third-party partner sales wants some tables to see which of their customers to target; I\u2019ll ask the requirements of the fields they need pull in 2-4 data tabs in google sheets, create a bunch of tables which are Sumifs (calculated from raw data) and schedule Python / Airflow to refresh the data in the data tabs as well as handle any non-SQL friendly transformations.\n\nExample 2: Another team wants a dashboard showing license usage, I do research, gather requirements and build a bunch of SQL queries and then schedule them to refresh on tableau server with tableau prep, from there I build out a tableau workbook comprising of multiple dashboards.\n\nExample 3: it\u2019s quarterly business review season multiple teams that I support need data for their decks, so I provide 6-7 tabs of data in gsheets for each team based on their region, industry, etc. (about 5 teams) and schedule them to refresh with airflow (I do this 5 days before quarter end and stop 2-3 days after when data is considered \u201cfinal\u201d)\n\nI wanted to transition an Analytics Engineer or Front-End DE (forgive if I\u2019m not using that term right) do you think the skills below (may already use some above) are all that\u2019s required or what you recommend that\u2019s actually practical for a newbie (don\u2019t want to get down rabbit holes and be stuck in tutorial hell).\n\n1. Python to Ingest, Parse, and sometimes transform data (SQL &amp; DBT can handle) - maybe some packages like pandas, numpy?\n2. Data Modeling \n3. Airflow for Orchestration\n4. SQL and DbT as well as a cloud warehouse (Snowflake)\n5. LEETCODE SQL and Python", "author_fullname": "t2_oasgyrv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics Engineering / \u201cFront-end\u201d DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n2bs9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703099154.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703098889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So currently I\u2019m somewhat of a BI Analyst at a Tech Company my day-to-day involves &lt;/p&gt;\n\n&lt;p&gt;Building re-usable queries to transform data housed in Oracle and Snowflake and providing these datasets in Excel Files / Google Sheets files to end users, as well as building the assets from the datasets as well.&lt;/p&gt;\n\n&lt;p&gt;Example 1: A team focusing on third-party partner sales wants some tables to see which of their customers to target; I\u2019ll ask the requirements of the fields they need pull in 2-4 data tabs in google sheets, create a bunch of tables which are Sumifs (calculated from raw data) and schedule Python / Airflow to refresh the data in the data tabs as well as handle any non-SQL friendly transformations.&lt;/p&gt;\n\n&lt;p&gt;Example 2: Another team wants a dashboard showing license usage, I do research, gather requirements and build a bunch of SQL queries and then schedule them to refresh on tableau server with tableau prep, from there I build out a tableau workbook comprising of multiple dashboards.&lt;/p&gt;\n\n&lt;p&gt;Example 3: it\u2019s quarterly business review season multiple teams that I support need data for their decks, so I provide 6-7 tabs of data in gsheets for each team based on their region, industry, etc. (about 5 teams) and schedule them to refresh with airflow (I do this 5 days before quarter end and stop 2-3 days after when data is considered \u201cfinal\u201d)&lt;/p&gt;\n\n&lt;p&gt;I wanted to transition an Analytics Engineer or Front-End DE (forgive if I\u2019m not using that term right) do you think the skills below (may already use some above) are all that\u2019s required or what you recommend that\u2019s actually practical for a newbie (don\u2019t want to get down rabbit holes and be stuck in tutorial hell).&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Python to Ingest, Parse, and sometimes transform data (SQL &amp;amp; DBT can handle) - maybe some packages like pandas, numpy?&lt;/li&gt;\n&lt;li&gt;Data Modeling &lt;/li&gt;\n&lt;li&gt;Airflow for Orchestration&lt;/li&gt;\n&lt;li&gt;SQL and DbT as well as a cloud warehouse (Snowflake)&lt;/li&gt;\n&lt;li&gt;LEETCODE SQL and Python&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18n2bs9", "is_robot_indexable": true, "report_reasons": null, "author": "Weary-Individual-309", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n2bs9/analytics_engineering_frontend_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n2bs9/analytics_engineering_frontend_de/", "subreddit_subscribers": 147320, "created_utc": 1703098889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle API rate limitations with a queue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18mwhxl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mcs3w2DkkiX1dzBpMx4FQPx0UGgYV33CItkhdVAaITQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703084276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memphis.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memphis.dev/blog/how-to-handle-api-rate-limitations-with-a-queue/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DmoEiy3pq0PSJvZg4Lj1zH6J9rgD4aULztZ9AjoRMUU.jpg?auto=webp&amp;s=b46325fec5aa5c62fa2a5746b770394a81489d54", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/DmoEiy3pq0PSJvZg4Lj1zH6J9rgD4aULztZ9AjoRMUU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c6fa67be008f68785f264cafe5ef9b1499760c8", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/DmoEiy3pq0PSJvZg4Lj1zH6J9rgD4aULztZ9AjoRMUU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=35f0b2ae2413d45c6af8328a926c154728946b44", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/DmoEiy3pq0PSJvZg4Lj1zH6J9rgD4aULztZ9AjoRMUU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df7ea8eb5e4fa409fbb3192942b5ab87491ba880", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/DmoEiy3pq0PSJvZg4Lj1zH6J9rgD4aULztZ9AjoRMUU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2b85275282ef8df5b557ffc0aeb611ef8c496a5a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/DmoEiy3pq0PSJvZg4Lj1zH6J9rgD4aULztZ9AjoRMUU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33e0ab12fa05b28f1032cf48be1ac22b5deeb51a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/DmoEiy3pq0PSJvZg4Lj1zH6J9rgD4aULztZ9AjoRMUU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c05a78ffbf6bd38eaa065ffd5564c199a18fe022", "width": 1080, "height": 607}], "variants": {}, "id": "cG0STmsO4c6fm2evftInDHBLwhsdU1EGvhbt3ixrVpg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18mwhxl", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mwhxl/how_to_handle_api_rate_limitations_with_a_queue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memphis.dev/blog/how-to-handle-api-rate-limitations-with-a-queue/", "subreddit_subscribers": 147320, "created_utc": 1703084276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been trying to convert json data to parquet by storing the json data in Python list -&gt; Pandas Dataframe -&gt; Parquet.\n\nThe main issue is when i convert the python list to dataframe, it took too long to process. Is there any better way to do this?", "author_fullname": "t2_dgqi4197", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Convert json data to parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nduxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703131360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been trying to convert json data to parquet by storing the json data in Python list -&amp;gt; Pandas Dataframe -&amp;gt; Parquet.&lt;/p&gt;\n\n&lt;p&gt;The main issue is when i convert the python list to dataframe, it took too long to process. Is there any better way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18nduxn", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Criticism-8127", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nduxn/convert_json_data_to_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nduxn/convert_json_data_to_parquet/", "subreddit_subscribers": 147320, "created_utc": 1703131360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some context, I work at a school district and we get a lot of different files. We had some process that would take a file, pull out the columns we want, and upload them into our student information system. There was a few problems with this, one is we had certain people come back and want a column we were not capturing. This would cause us to have to update and re-run files through the process and time and effort matching again.  The other is columns in files change all the time. It is easy for say SAT to have a new file format every single year\u2026SAT and ACT are fixed width files that have just recently done csv types. \n\nA few years ago we created a \u201cdata lake\u201d in MS SQL Server.  We have a table with generic columns and a table that houses the column names for those.  The issue now, is some of the files are getting to and making a tow error out in size in MS SQL. \n\nIs there any decent free data lake software that can be hosted on site?  I have seen some demos of Azure where you pick a file and it sort of reads the file into its own table, and you can flow it as you want.  It would be nice to be able to have something like that.", "author_fullname": "t2_4uiot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lake Alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nabs4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703120342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some context, I work at a school district and we get a lot of different files. We had some process that would take a file, pull out the columns we want, and upload them into our student information system. There was a few problems with this, one is we had certain people come back and want a column we were not capturing. This would cause us to have to update and re-run files through the process and time and effort matching again.  The other is columns in files change all the time. It is easy for say SAT to have a new file format every single year\u2026SAT and ACT are fixed width files that have just recently done csv types. &lt;/p&gt;\n\n&lt;p&gt;A few years ago we created a \u201cdata lake\u201d in MS SQL Server.  We have a table with generic columns and a table that houses the column names for those.  The issue now, is some of the files are getting to and making a tow error out in size in MS SQL. &lt;/p&gt;\n\n&lt;p&gt;Is there any decent free data lake software that can be hosted on site?  I have seen some demos of Azure where you pick a file and it sort of reads the file into its own table, and you can flow it as you want.  It would be nice to be able to have something like that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nabs4", "is_robot_indexable": true, "report_reasons": null, "author": "jezter24", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nabs4/data_lake_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nabs4/data_lake_alternatives/", "subreddit_subscribers": 147320, "created_utc": 1703120342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nI just joined a team in an industry I\u2019m not very familiar with. So far I\u2019ve been here for 2 months and im still learning the ropes especially since I was technically a bit more hands-off in my last position. I wanted to help out with some work and hopefully brush up and get up to speed .\n\nWhen my boss and lead discuss our etl development I tend to ask a lot of questions like \u201c Okay so if we build it this way, we\u2019d eventually have to\u2026\u201d\n\nMost of the time my boss will cut me off and say \u201cdon\u2019t worry about that now\u201d. The thing is, im not worrying or trying to make it a big deal, I just wanted to discuss the task and where it could lead to. I\u2019m just curious. \n\nInstead of my boss taking my questions as a sign of interest in the field and eager to help, he more than likely thinks im annoying. I just don\u2019t get it, so I just take orders and never say anything? \n\nOne thing im really big about is documentation, I noticed that we were working on this task and there was no intermediate between the tech lead and I and the stakeholder. I assumed that position and finalized some requirements.\n\nI\u2019m not a fan of rushing into development when we don\u2019t have the necessary pieces. So when others have PRs in already or a merge has been done, I try to step through code to learn and get a sense of the style of docsstrings if there are any. I also help myself brush up on things. While im brushing up on things and stepping through the code, im also creating data dictionaries and just documentation on the etl process. \n\nMy boss doesn\u2019t care about that. He seems annoyed that I even ask if we have documentation on a particular project or task. I usually ask to make sure if I can update it myself or to get a sense of the template. \n\nFor the current task, no documentation has been written. I started doing that myself. No one has contributed to that documentation, rather, they\u2019re focusing on just code commenting. \n\nI just don\u2019t believe in doing that. But my boss doesn\u2019t care about that and just finds me annoying. How can I be helpful without being pesky? I just don\u2019t get why we\u2019re building this for a team and we\u2019re not documenting _nothing_. When I ask I usually plan on planning to work with someone on it or I end up doing it myself. \n\nThoughts?", "author_fullname": "t2_oc5syk6jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I improve this communication?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n9033", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703116416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I just joined a team in an industry I\u2019m not very familiar with. So far I\u2019ve been here for 2 months and im still learning the ropes especially since I was technically a bit more hands-off in my last position. I wanted to help out with some work and hopefully brush up and get up to speed .&lt;/p&gt;\n\n&lt;p&gt;When my boss and lead discuss our etl development I tend to ask a lot of questions like \u201c Okay so if we build it this way, we\u2019d eventually have to\u2026\u201d&lt;/p&gt;\n\n&lt;p&gt;Most of the time my boss will cut me off and say \u201cdon\u2019t worry about that now\u201d. The thing is, im not worrying or trying to make it a big deal, I just wanted to discuss the task and where it could lead to. I\u2019m just curious. &lt;/p&gt;\n\n&lt;p&gt;Instead of my boss taking my questions as a sign of interest in the field and eager to help, he more than likely thinks im annoying. I just don\u2019t get it, so I just take orders and never say anything? &lt;/p&gt;\n\n&lt;p&gt;One thing im really big about is documentation, I noticed that we were working on this task and there was no intermediate between the tech lead and I and the stakeholder. I assumed that position and finalized some requirements.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not a fan of rushing into development when we don\u2019t have the necessary pieces. So when others have PRs in already or a merge has been done, I try to step through code to learn and get a sense of the style of docsstrings if there are any. I also help myself brush up on things. While im brushing up on things and stepping through the code, im also creating data dictionaries and just documentation on the etl process. &lt;/p&gt;\n\n&lt;p&gt;My boss doesn\u2019t care about that. He seems annoyed that I even ask if we have documentation on a particular project or task. I usually ask to make sure if I can update it myself or to get a sense of the template. &lt;/p&gt;\n\n&lt;p&gt;For the current task, no documentation has been written. I started doing that myself. No one has contributed to that documentation, rather, they\u2019re focusing on just code commenting. &lt;/p&gt;\n\n&lt;p&gt;I just don\u2019t believe in doing that. But my boss doesn\u2019t care about that and just finds me annoying. How can I be helpful without being pesky? I just don\u2019t get why we\u2019re building this for a team and we\u2019re not documenting &lt;em&gt;nothing&lt;/em&gt;. When I ask I usually plan on planning to work with someone on it or I end up doing it myself. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18n9033", "is_robot_indexable": true, "report_reasons": null, "author": "tryingmybest200000", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n9033/how_do_i_improve_this_communication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n9033/how_do_i_improve_this_communication/", "subreddit_subscribers": 147320, "created_utc": 1703116416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI receive daily reports. For each report I include both the report name and the date that the report was received. Should I also include the time as well or just limit it to the date? \n\nThe lowest level of granularity for our use case is daily, but I'm not sure what best practice is. ", "author_fullname": "t2_muluj9r1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I include time of report in the file name of daily reports?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n3qxx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703102521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I receive daily reports. For each report I include both the report name and the date that the report was received. Should I also include the time as well or just limit it to the date? &lt;/p&gt;\n\n&lt;p&gt;The lowest level of granularity for our use case is daily, but I&amp;#39;m not sure what best practice is. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18n3qxx", "is_robot_indexable": true, "report_reasons": null, "author": "PowerBIFriend", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n3qxx/should_i_include_time_of_report_in_the_file_name/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n3qxx/should_i_include_time_of_report_in_the_file_name/", "subreddit_subscribers": 147320, "created_utc": 1703102521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there everyone, as the title says, currently some filtering logic is stored in a postgresql function and the backend is in ruby.\n\nETL is also done in ruby &amp; SQL at the moment and I'm in the process of moving these to dbt.\n\nHowever, the function is called with parameters through the current API to query the final transformed data and I can't exactly manage that through dbt (I'm doing the transformations in a dwh, not pgsql).\n\nI'm thinking that this would be better managed through an ORM, but maybe there's a better way? Or should I just stick to functions?", "author_fullname": "t2_6d2ltdsv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Move from postgresql function to ruby ORM?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n1a6q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703096306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there everyone, as the title says, currently some filtering logic is stored in a postgresql function and the backend is in ruby.&lt;/p&gt;\n\n&lt;p&gt;ETL is also done in ruby &amp;amp; SQL at the moment and I&amp;#39;m in the process of moving these to dbt.&lt;/p&gt;\n\n&lt;p&gt;However, the function is called with parameters through the current API to query the final transformed data and I can&amp;#39;t exactly manage that through dbt (I&amp;#39;m doing the transformations in a dwh, not pgsql).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking that this would be better managed through an ORM, but maybe there&amp;#39;s a better way? Or should I just stick to functions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18n1a6q", "is_robot_indexable": true, "report_reasons": null, "author": "No_Potential6129", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n1a6q/move_from_postgresql_function_to_ruby_orm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n1a6q/move_from_postgresql_function_to_ruby_orm/", "subreddit_subscribers": 147320, "created_utc": 1703096306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a request to help a group replace some excel macros with something more modern and secure.  They\u2019re just writing excel files to a folder on OneDrive (could use sharepoint folders if needed) and a couple times a day at most somebody runs the macro and it does some light transformation and saves the new file to a sibling directory in Onedrive.   So it\u2019s not really ETL but ET then write back to OneDrive or SP.    \n\n\nI was thinking maybe an azure function and using graph API to read/write to OneDrive could be a decent low maintenance secure option.    \n\n\nBut I feel like it may be missing some easier way to go (not that azure functions are hard but it\u2019s also not totally trivial).    \n\n\nWas looking a bit at whether power query online plus power automate could do it, and it probably can, even just plain power automate, but transforming data in power automate is so painful in my limited experience with that platform.  I\u2019d really rather just be able to write code to do that part.  Plus not sure what licensing they have and if OneDrive might be a premium connector.  And all the MS fabric stuff seems based around bringing data into their stack.  And it\u2019s also not free.  Too heavy duty for this IMO.    \n\n\nThis org doesn\u2019t have a warehouse or advanced IT staff, they\u2019re small, so saying \u201cjust put it in with their other data factory and ADLS stuff\u201d isn\u2019t an option.  Trying to stay with fairly light weight and cheap options.  It\u2019s only like 100 rows and a few files per week.  This is a tiny need from the data standpoint.  ", "author_fullname": "t2_156kui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lightweight approach to excel file transformations using MS tooling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mx23i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703085689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a request to help a group replace some excel macros with something more modern and secure.  They\u2019re just writing excel files to a folder on OneDrive (could use sharepoint folders if needed) and a couple times a day at most somebody runs the macro and it does some light transformation and saves the new file to a sibling directory in Onedrive.   So it\u2019s not really ETL but ET then write back to OneDrive or SP.    &lt;/p&gt;\n\n&lt;p&gt;I was thinking maybe an azure function and using graph API to read/write to OneDrive could be a decent low maintenance secure option.    &lt;/p&gt;\n\n&lt;p&gt;But I feel like it may be missing some easier way to go (not that azure functions are hard but it\u2019s also not totally trivial).    &lt;/p&gt;\n\n&lt;p&gt;Was looking a bit at whether power query online plus power automate could do it, and it probably can, even just plain power automate, but transforming data in power automate is so painful in my limited experience with that platform.  I\u2019d really rather just be able to write code to do that part.  Plus not sure what licensing they have and if OneDrive might be a premium connector.  And all the MS fabric stuff seems based around bringing data into their stack.  And it\u2019s also not free.  Too heavy duty for this IMO.    &lt;/p&gt;\n\n&lt;p&gt;This org doesn\u2019t have a warehouse or advanced IT staff, they\u2019re small, so saying \u201cjust put it in with their other data factory and ADLS stuff\u201d isn\u2019t an option.  Trying to stay with fairly light weight and cheap options.  It\u2019s only like 100 rows and a few files per week.  This is a tiny need from the data standpoint.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mx23i", "is_robot_indexable": true, "report_reasons": null, "author": "reelznfeelz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mx23i/lightweight_approach_to_excel_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mx23i/lightweight_approach_to_excel_file/", "subreddit_subscribers": 147320, "created_utc": 1703085689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys , I'm working on an aws environment ,we are changing from a **RDS** Database to a **Redshift** Database , previously we were using a lambda carrying a python script using psycopg2 which used the command `copy_from` to insert csv file to a **RDS** database,now our use-case as changed and we need to load parquet files on **Redshift** Database.  \n\n\nMy first thought which need the minimal change to be implemented:\n\n* Using one more time our **lambda** \\+ psycopg to creating a connection with **Redshift** and send an sql query \"**copy**\" [https://docs.aws.amazon.com/fr\\_fr/redshift/latest/dg/r\\_COPY.html](https://docs.aws.amazon.com/fr_fr/redshift/latest/dg/r_COPY.html) with the information needed  , the problem is that the lambda will be up until the transaction is ended and my fear is that some copy necessitate more than 15 minute of process.(for now our parquet doesn't exceed 100mb)\n\n&amp;#x200B;\n\nDo someone have a similar use-case and use lambda to do such ingestion? \n\n&amp;#x200B;\n\nthanks !", "author_fullname": "t2_o7szslyxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to insert data to a database redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mtbcg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703074784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys , I&amp;#39;m working on an aws environment ,we are changing from a &lt;strong&gt;RDS&lt;/strong&gt; Database to a &lt;strong&gt;Redshift&lt;/strong&gt; Database , previously we were using a lambda carrying a python script using psycopg2 which used the command &lt;code&gt;copy_from&lt;/code&gt; to insert csv file to a &lt;strong&gt;RDS&lt;/strong&gt; database,now our use-case as changed and we need to load parquet files on &lt;strong&gt;Redshift&lt;/strong&gt; Database.  &lt;/p&gt;\n\n&lt;p&gt;My first thought which need the minimal change to be implemented:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Using one more time our &lt;strong&gt;lambda&lt;/strong&gt; + psycopg to creating a connection with &lt;strong&gt;Redshift&lt;/strong&gt; and send an sql query &amp;quot;&lt;strong&gt;copy&lt;/strong&gt;&amp;quot; &lt;a href=\"https://docs.aws.amazon.com/fr_fr/redshift/latest/dg/r_COPY.html\"&gt;https://docs.aws.amazon.com/fr_fr/redshift/latest/dg/r_COPY.html&lt;/a&gt; with the information needed  , the problem is that the lambda will be up until the transaction is ended and my fear is that some copy necessitate more than 15 minute of process.(for now our parquet doesn&amp;#39;t exceed 100mb)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do someone have a similar use-case and use lambda to do such ingestion? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18mtbcg", "is_robot_indexable": true, "report_reasons": null, "author": "OkPrinciple5462", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mtbcg/best_way_to_insert_data_to_a_database_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mtbcg/best_way_to_insert_data_to_a_database_redshift/", "subreddit_subscribers": 147320, "created_utc": 1703074784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI do have a chance to swap to data engineering since my current company is looking to build a data department (consisting of 1x DE, 1x Analyst).\n\nSome backstory: Im currently in a  digital marketing / seo analyst role and I built a SEO data warehouse this year using GCP (cloud functions, cloud scheduler, bigquery). I had tons of fun doing so. I also built some data pipelines for private projects as well as freelancing marketing projects, got quite some experience with APIs as well at this point. Used python and SQL so far.\n\nI dont have a CS degree background. masters in marketing as well as humanities. \n\nNow my company wants to built a company-wide data warehouse and I am really considering swapping roles. I do have some questions where Id love to get some input from experienced DEs:\n\n* I'd be the only DE in the company. So no Senior level person to learn from. Is this a red flag? Im pretty much self-taught on anything I do in regards to programming and DE so very familiar with \"learning by doing\".\n* Currently we do not have any data pipelines or even any \"sort of\" data warehouse. So i'd be able to build it from scratch, which seems like a great opportunity to me. Asking the more experienced DEs out here: Do you think this could be a great opportunity?\n* What are the \"must have\"-skill besides the experiences / skills I already mentioned when going into this role? I figured that combining data from like 50+ data sources for many departments could be too much for a simple \"cloud functions + scheduled sql queries + bigquery into looker studio\"-pipeline (from an orchestration point of view). So I guess I would look into something like airflow?\n\nReally looking forward to any input you have for me. :)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_hieu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Advice: Swap to DE from SEO, shall I do it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n3mzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703102251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I do have a chance to swap to data engineering since my current company is looking to build a data department (consisting of 1x DE, 1x Analyst).&lt;/p&gt;\n\n&lt;p&gt;Some backstory: Im currently in a  digital marketing / seo analyst role and I built a SEO data warehouse this year using GCP (cloud functions, cloud scheduler, bigquery). I had tons of fun doing so. I also built some data pipelines for private projects as well as freelancing marketing projects, got quite some experience with APIs as well at this point. Used python and SQL so far.&lt;/p&gt;\n\n&lt;p&gt;I dont have a CS degree background. masters in marketing as well as humanities. &lt;/p&gt;\n\n&lt;p&gt;Now my company wants to built a company-wide data warehouse and I am really considering swapping roles. I do have some questions where Id love to get some input from experienced DEs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;d be the only DE in the company. So no Senior level person to learn from. Is this a red flag? Im pretty much self-taught on anything I do in regards to programming and DE so very familiar with &amp;quot;learning by doing&amp;quot;.&lt;/li&gt;\n&lt;li&gt;Currently we do not have any data pipelines or even any &amp;quot;sort of&amp;quot; data warehouse. So i&amp;#39;d be able to build it from scratch, which seems like a great opportunity to me. Asking the more experienced DEs out here: Do you think this could be a great opportunity?&lt;/li&gt;\n&lt;li&gt;What are the &amp;quot;must have&amp;quot;-skill besides the experiences / skills I already mentioned when going into this role? I figured that combining data from like 50+ data sources for many departments could be too much for a simple &amp;quot;cloud functions + scheduled sql queries + bigquery into looker studio&amp;quot;-pipeline (from an orchestration point of view). So I guess I would look into something like airflow?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Really looking forward to any input you have for me. :)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18n3mzw", "is_robot_indexable": true, "report_reasons": null, "author": "vimen", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n3mzw/career_advice_swap_to_de_from_seo_shall_i_do_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n3mzw/career_advice_swap_to_de_from_seo_shall_i_do_it/", "subreddit_subscribers": 147320, "created_utc": 1703102251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I'm setting up a new/old computer and installing various software. I'd like to practice more on data engineering tasks and etl tools. What are few applications that can replicate cloud environments like airflow, databricks? Open source environments are preferable. I'd like to build an attractive portfolio so if there are any other relevant ideas drop them down too. I already have Anaconda/jupyter to practice my python based etl notebooks and sql server to practice my SQL.\nI'm looking to learn more about software used in the industry or anything close to them. \n\nFor anyone in the future (and me) looking for any reference portfolios, drop your favorite/personal portfolio links please.", "author_fullname": "t2_hqiwxblm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On premises software to practice data engineering and pipeline development?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n0bpf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703094010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m setting up a new/old computer and installing various software. I&amp;#39;d like to practice more on data engineering tasks and etl tools. What are few applications that can replicate cloud environments like airflow, databricks? Open source environments are preferable. I&amp;#39;d like to build an attractive portfolio so if there are any other relevant ideas drop them down too. I already have Anaconda/jupyter to practice my python based etl notebooks and sql server to practice my SQL.\nI&amp;#39;m looking to learn more about software used in the industry or anything close to them. &lt;/p&gt;\n\n&lt;p&gt;For anyone in the future (and me) looking for any reference portfolios, drop your favorite/personal portfolio links please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18n0bpf", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Reason59", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n0bpf/on_premises_software_to_practice_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n0bpf/on_premises_software_to_practice_data_engineering/", "subreddit_subscribers": 147320, "created_utc": 1703094010.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}