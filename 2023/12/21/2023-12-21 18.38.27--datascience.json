{"kind": "Listing", "data": {"after": null, "dist": 11, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In my own experience, data science managers and analytics managers are very toxic and condescending to people that don't have the highest level of knowledge and experience. This is evident in both interviews, and working in the industry personally. I have 5 years of experience using SQL and Python automating things, doing things like training ML models, regression analysis, creating analytics using matplot lib and seaborn, I've used R extensively for years. I've created Python APIs to connect different data sources together to merge together massive data in pandas, using numpy.... \n\n\nBut it always seems like someone thinks they are better, or they look down on you for trying to learn new things. For example, I was learning TensorFlow And I did a whole Python project using it, as well as some exploratory data analysis on Kaggle for the Titanic in space predicting survival project. When I brought that up as a way of showing that I am very interested in learning more about data science and advancing my skills and experience, It was met with mockery and a very condescending attitude. \"That's a cute pet project, but we are really looking for someone a lot more motivated and experienced\". Like, I tried to express that I'm learning new skills, and trying to advance my data science skill sets, and that's the way I get treated.\n\n\n\n**I also get the impression that people think analysts are not real data scientists**. I met a data scientist who had almost no experience in SQL, just a statistics background, he had no idea how to use Python so I had to explain a bunch of things to him about SQL, Python, hell, I was even teaching them about Excel!! But I'm not a real data scientist, I'm just an analyst, and they are qualified to be a real data scientist. But if I were to apply to that position, I would be grilled about having a non-statistic background, and I haven't worked in a job where my actual title is data scientist...\n\n\nThe heck is wrong with this industry lately?", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I truly hate how toxic and condescending the Data Science industry is", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nnniz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": "#dadada", "subreddit_type": "public", "ups": 252, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "d898d418-70eb-11ee-81d7-36bf4b44216b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 252, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703167139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my own experience, data science managers and analytics managers are very toxic and condescending to people that don&amp;#39;t have the highest level of knowledge and experience. This is evident in both interviews, and working in the industry personally. I have 5 years of experience using SQL and Python automating things, doing things like training ML models, regression analysis, creating analytics using matplot lib and seaborn, I&amp;#39;ve used R extensively for years. I&amp;#39;ve created Python APIs to connect different data sources together to merge together massive data in pandas, using numpy.... &lt;/p&gt;\n\n&lt;p&gt;But it always seems like someone thinks they are better, or they look down on you for trying to learn new things. For example, I was learning TensorFlow And I did a whole Python project using it, as well as some exploratory data analysis on Kaggle for the Titanic in space predicting survival project. When I brought that up as a way of showing that I am very interested in learning more about data science and advancing my skills and experience, It was met with mockery and a very condescending attitude. &amp;quot;That&amp;#39;s a cute pet project, but we are really looking for someone a lot more motivated and experienced&amp;quot;. Like, I tried to express that I&amp;#39;m learning new skills, and trying to advance my data science skill sets, and that&amp;#39;s the way I get treated.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I also get the impression that people think analysts are not real data scientists&lt;/strong&gt;. I met a data scientist who had almost no experience in SQL, just a statistics background, he had no idea how to use Python so I had to explain a bunch of things to him about SQL, Python, hell, I was even teaching them about Excel!! But I&amp;#39;m not a real data scientist, I&amp;#39;m just an analyst, and they are qualified to be a real data scientist. But if I were to apply to that position, I would be grilled about having a non-statistic background, and I haven&amp;#39;t worked in a job where my actual title is data scientist...&lt;/p&gt;\n\n&lt;p&gt;The heck is wrong with this industry lately?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "18nnniz", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 188, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/18nnniz/i_truly_hate_how_toxic_and_condescending_the_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18nnniz/i_truly_hate_how_toxic_and_condescending_the_data/", "subreddit_subscribers": 1197257, "created_utc": 1703167139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "UPDATE TO TITLE: I meant, \u201cheading down the management path.\u201d \n\nIt\u2019s been 6 months since starting a data science management role, and now have been laid off. \n\nThe role was sold as a data science manager, yet ended up doing admin work and touched on very small amounts of actual data science projects. \n\nI was upset about the role but my boss assured me there were \u201cbig things\u201d in the pipeline. \n\nI got my break and finally took responsibility for a project and coordinated with a team from the US. Things went smoothly, and as a native English speaker (I\u2019m EU based), I sat in on more global meetings. \n\nFinally, things seemed to take off and work well\u2026 until I got fired today. \n\nThe result was: \u201cbusiness wasn\u2019t happy with your results.\u201d (We acted as data science manager consultants for different Business Units within the company). I asked why, the only feedback was a comment about a presentation not working out. \n\nBusiness never communicated with me directly, only with my boss. There was a layer of reasoning missing. \n\nSo, here I am: questions unanswered, out of a job, and feel like I\u2019ve missed my chance at DS management. \n\nI have one week left. What could I best do with this time?\n\nThanks all :)", "author_fullname": "t2_mzwvpkmks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got laid off as I was heading the management path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18natsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703121843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;UPDATE TO TITLE: I meant, \u201cheading down the management path.\u201d &lt;/p&gt;\n\n&lt;p&gt;It\u2019s been 6 months since starting a data science management role, and now have been laid off. &lt;/p&gt;\n\n&lt;p&gt;The role was sold as a data science manager, yet ended up doing admin work and touched on very small amounts of actual data science projects. &lt;/p&gt;\n\n&lt;p&gt;I was upset about the role but my boss assured me there were \u201cbig things\u201d in the pipeline. &lt;/p&gt;\n\n&lt;p&gt;I got my break and finally took responsibility for a project and coordinated with a team from the US. Things went smoothly, and as a native English speaker (I\u2019m EU based), I sat in on more global meetings. &lt;/p&gt;\n\n&lt;p&gt;Finally, things seemed to take off and work well\u2026 until I got fired today. &lt;/p&gt;\n\n&lt;p&gt;The result was: \u201cbusiness wasn\u2019t happy with your results.\u201d (We acted as data science manager consultants for different Business Units within the company). I asked why, the only feedback was a comment about a presentation not working out. &lt;/p&gt;\n\n&lt;p&gt;Business never communicated with me directly, only with my boss. There was a layer of reasoning missing. &lt;/p&gt;\n\n&lt;p&gt;So, here I am: questions unanswered, out of a job, and feel like I\u2019ve missed my chance at DS management. &lt;/p&gt;\n\n&lt;p&gt;I have one week left. What could I best do with this time?&lt;/p&gt;\n\n&lt;p&gt;Thanks all :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18natsq", "is_robot_indexable": true, "report_reasons": null, "author": "Hot-hentai-cum-papi", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18natsq/just_got_laid_off_as_i_was_heading_the_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18natsq/just_got_laid_off_as_i_was_heading_the_management/", "subreddit_subscribers": 1197257, "created_utc": 1703121843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "  This article will explain how to use [Pipeline](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com) and [Transformers](https://scikit-learn.org/stable/data_transforms.html?ref=dataleadsfuture.com) correctly in Scikit-Learn (sklearn) projects to speed up and reuse our model training process.\n\nThis piece complements and clarifies the official documentation on Pipeline examples and some common misunderstandings.\n\nI hope that after reading this, you'll be able to use the Pipeline, an excellent design, to better complete your machine learning tasks.\n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/) \n\n### Why use a Pipeline\n\nAs mentioned earlier, in a machine learning task, we often need to use various Transformers for data scaling and feature dimensionality reduction before training a model.\n\nThis presents several challenges:\n\n* **Code complexity**: For each use of a Transformer, we have to go through initialization, `fit_transform`, and `transform` steps. Missing one step during a transformation could derail the entire training process.\n* **Data leakage**: As we discussed, for each Transformer, we fit with train data and then transform both train and test data. We must avoid letting the distribution of the test data leak into the train data.\n* **Code reusability**: A machine learning model includes not only the trained Estimator for prediction but also the data preprocessing steps. Therefore, a machine learning task comprising Transformers and an Estimator should be atomic and indivisible.\n* **Hyperparameter tuning**: After setting up the steps of machine learning, we need to adjust hyperparameters to find the best combination of Transformer parameter values.\n\nScikit-Learn introduced the `Pipeline` module to solve these issues.\n\n### What is a Pipeline\n\nA `Pipeline` is a module in Scikit-Learn that implements the chain of responsibility design pattern.\n\nWhen creating a Pipeline, we use the `steps` parameter to chain together multiple Transformers for initialization:\n\n    from sklearn.pipeline import Pipeline\n    from sklearn.decomposition import PCA\n    from sklearn.ensemble import RandomForestClassifier\n    \n    pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('pca', PCA(n_components=2, random_state=42)),\n                               ('estimator', RandomForestClassifier(n_estimators=3, max_depth=5))])\n\nThe [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline) points out that the last Transformer must be an Estimator.\n\nIf you don't need to specify each Transformer's name, you can simplify the creation of a Pipeline with `make_pipeline`:\n\n    from sklearn.pipeline import make_pipeline\n    \n    pipeline_2 = make_pipeline(StandardScaler(),\n                               PCA(n_components=2, random_state=42),\n                               RandomForestClassifier(n_estimators=3, max_depth=5))\n\n Understanding the Pipeline's mechanism from the source code\n\nWe've mentioned the importance of not letting test data variables leak into training data when using each Transformer.\n\nThis principle is relatively easy to ensure when each data preprocessing step is independent.\n\nBut what if we integrate these steps using a Pipeline?\n\nIf we look at the [official documentation](https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline), we find it simply uses the fit  \n method on the entire dataset without explaining how to handle train and test data separately.\n\nWith this question in mind, I dived into the Pipeline's source code to find the answer.\n\nReading the source code revealed that although Pipeline implements `fit`, `fit_transform`, and `predict` methods, they work differently from regular Transformers.\n\nTake the following Pipeline creation process as an example:\n\n    from sklearn.pipeline import Pipeline\n    from sklearn.decomposition import PCA\n    from sklearn.ensemble import RandomForestClassifier\n    \n    pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('pca', PCA(n_components=2, random_state=42)),\n                               ('estimator', RandomForestClassifier(n_estimators=3, max_depth=5))])\n\n The internal implementation can be represented by the following diagram: \n\n[ Internal implementation of the fit and predict methods when called. Image by Author ](https://preview.redd.it/okhwyg75gl7c1.png?width=684&amp;format=png&amp;auto=webp&amp;s=8106360fcaeb17deea2adf04fc34228dd31a9fd7)\n\nAs you can see, when we call the `fit` method, Pipeline first separates Transformers from the Estimator.\n\nFor each Transformer, Pipeline checks if there's a `fit_transform` method; if so, it calls it; otherwise, it calls `fit`.\n\nFor the Estimator, it calls `fit` directly.\n\nFor the `predict` method, Pipeline separates Transformers from the Estimator.\n\nPipeline calls each Transformer's `transform` method in sequence, followed by the Estimator's `predict`  \n method.\n\nTherefore, when using a Pipeline, we still need to split train and test data. Then we simply call `fit` on the train data and `predict` on the test data.\n\nThere's a special case when combining Pipeline with `GridSearchCV` for hyperparameter tuning: you don't need to manually split train and test data. I'll explain this in more detail in the best practices section.\n\n## Best Practices for Using Transformers and Pipeline in Actual Applications\n\nNow that we've discussed the working principles of Transformers and Pipeline, it's time to fulfill the promise made in the title and talk about the best practices when combining Transformers with Pipeline in real projects.\n\n### Combining Pipeline with GridSearchCV for hyperparameter tuning\n\nIn a machine learning project, selecting the right dataset processing and algorithm is one aspect. After debugging the initial steps, it's time for parameter optimization.\n\nUsing `GridSearchCV` or `RandomizedSearchCV`, you can try different parameters for the Estimator to find the best fit:\n\n    import time\n    \n    from sklearn.model_selection import GridSearchCV\n    \n    pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('pca', PCA()),\n                               ('estimator', RandomForestClassifier())])\n    param_grid = {'pca__n_components': [2, 'mle'],\n                  'estimator__n_estimators': [3, 5, 7],\n                  'estimator__max_depth': [3, 5]}\n    \n    start = time.perf_counter()\n    clf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=4)\n    clf.fit(X, y)\n    \n    # It takes 2.39 seconds to finish the search on my laptop.\n    print(f\"It takes {time.perf_counter() - start} seconds to finish the search.\")\n\n But in machine learning, hyperparameter tuning is not limited to Estimator parameters; it also involves combinations of Transformer parameters.\n\nIntegrating all steps with Pipeline allows for hyperparameter tuning of every element with different parameter combinations.\n\nNote that during hyperparameter tuning, we no longer need to manually split train and test data. `GridSearchCV` will split the data into training and validation sets using [StratifiedKFold](https://scikit-learn.org/stable/modules/cross_validation.html?ref=dataleadsfuture.com#stratified-k-fold), which implemented a k-fold cross validation mechanism.\n\n[ StratifiedKFold iterative process of splitting train data and test data. Image by Author ](https://preview.redd.it/uqnuo8fpgl7c1.png?width=681&amp;format=png&amp;auto=webp&amp;s=bd6b5f7edb2d4f5e71786fd6c0f45745ea2095d9)\n\n We can also set the number of folds for cross-validation and choose how many workers to use. The tuning process is illustrated in the following diagram: \n\n[ Internal implementation of GridSearchCV hyperparameter tuning. Image by Author ](https://preview.redd.it/piqqv9nrgl7c1.png?width=699&amp;format=png&amp;auto=webp&amp;s=1bb2a2d4739e8100e71559fb9f9e0cfaa4846803)\n\n Due to space constraints, I won't go into detail about `GridSearchCV` and `RandomizedSearchCV` here. If you're interested, I can write another article explaining them next time. \n\n### Using the memory parameter to cache Transformer outputs\n\nOf course, hyperparameter tuning with `GridSearchCV` can be slow, but that's no worry, Pipeline provides a caching mechanism to speed up the tuning efficiency by caching the results of intermediate steps.\n\nWhen initializing a Pipeline, you can pass in a memory parameter, which will cache the results after the first call to `fit` and `transform` for each transformer.\n\nIf subsequent calls to fit and `transform` use the same parameters, which is very likely during hyperparameter tuning, these steps will directly read the results from the cache instead of recalculating, significantly speeding up the efficiency when running the same Transformer repeatedly.\n\nThe `memory` parameter can accept the following values:\n\n* The default is None: caching is not used.\n* A string: providing a path to store the cached results.\n* A `joblib.Memory` object: allows for finer-grained control, such as configuring the storage backend for the cache.\n\nNext, let's use the previous `GridSearchCV` example, this time adding `memory` to the Pipeline to see how much speed can be improved:\n\n    pipeline_m = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('pca', PCA()),\n                               ('estimator', RandomForestClassifier())],\n                          memory='./cache')\n    start = time.perf_counter()\n    clf_m = GridSearchCV(pipeline_m, param_grid=param_grid, cv=5, n_jobs=4)\n    clf_m.fit(X, y)\n    \n    # It takes 0.22 seconds to finish the search with memory parameter.\n    print(f\"It takes {time.perf_counter() - start} seconds to finish the search with memory.\")\n\nAs shown, with caching, the tuning process only takes 0.2 seconds, a significant speed increase from the previous 2.4 seconds.\n\n### How to debug Scikit-Learn Pipeline\n\nAfter integrating Transformers into a Pipeline, the entire preprocessing and transformation process becomes a black box. It can be difficult to understand which step the process is currently on.\n\nFortunately, we can solve this problem by adding logging to the Pipeline.  \nWe need to create custom transformers to add logging at each step of data transformation.\n\nHere's an example of adding logging with Python's standard logging library:\n\nFirst, you need to configure a logger:\n\n    import logging\n    \n    from sklearn.base import BaseEstimator, TransformerMixin\n    \n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger()\n\n Next, you can create a custom Transformer and add logging within its methods: \n\n    class LoggingTransformer(BaseEstimator, TransformerMixin):\n        def __init__(self, transformer):\n            self.transformer = transformer\n            self.real_name = self.transformer.__class__.__name__\n    \n        def fit(self, X, y=None):\n            logging.info(f\"Begin fit: {self.real_name}\")\n            self.transformer.fit(X, y)\n            logging.info(f\"End fit: {self.real_name}\")\n            return self\n    \n        def fit_transform(self, X, y=None):\n            logging.info(f\"Begin fit_transform: {self.real_name}\")\n            X_fit_transformed = self.transformer.fit_transform(X, y)\n            logging.info(f\"End fit_transform: {self.real_name}\")\n            return X_fit_transformed\n    \n        def transform(self, X):\n            logging.info(f\"Begin transform: {self.real_name}\")\n            X_transformed = self.transformer.transform(X)\n            logging.info(f\"End transform: {self.real_name}\")\n            return X_transformed\n\n Then you can use this `LoggingTransformer` when creating your Pipeline: \n\n    pipeline_logging = Pipeline(steps=[('scaler', LoggingTransformer(StandardScaler())),\n                                 ('pca', LoggingTransformer(PCA(n_components=2))),\n                                 ('estimator', RandomForestClassifier(n_estimators=5, max_depth=3))])\n    pipeline_logging.fit(X_train, y_train)\n\n[ The effect after adding the LoggingTransformer. Image by Author ](https://preview.redd.it/53kl3padhl7c1.png?width=664&amp;format=png&amp;auto=webp&amp;s=a3e3d9dcf25f0ee8dc7234e1db9280ce1cd15b0d)\n\nWhen you use `pipeline.fit`, it will call the `fit` and `transform` methods for each step in turn and log the appropriate messages.\n\n### Use passthrough in Scikit-Learn Pipeline\n\nIn a Pipeline, a step can be set to `'passthrough`', which means that for this specific step, the input data will pass through unchanged to the next step.\n\nThis is useful when you want to selectively enable/disable certain steps in a complex pipeline.\n\nTaking the code example above, we know that when using `DecisionTree` or `RandomForest`, standardizing the data is unnecessary, so we can use `passthrough` to skip this step.\n\nAn example would be as follows:\n\n    param_grid = {'scaler': ['passthrough'],\n                  'pca__n_components': [2, 'mle'],\n                  'estimator__n_estimators': [3, 5, 7],\n                  'estimator__max_depth': [3, 5]}\n    clf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=4)\n    clf.fit(X, y)\n\n Reusing the Pipeline\n\nAfter a journey of trials and tribulations, we finally have a well-performing machine learning model.\n\nNow, you might consider how to reuse this model, share it with colleagues, or deploy it in a production environment.\n\nHowever, the result of a model's training includes not only the model itself but also the various data processing steps, which all need to be saved.\n\nUsing `joblib` and Pipeline, we can save the entire training process for later use. The following code provides a simple example:\n\n    from joblib import dump, load\n    \n    # save pipeline\n    dump(pipeline, 'model_pipeline.joblib')\n    \n    # load pipeline\n    loaded_pipeline = load('model_pipeline.joblib')\n    \n    # predict with loaded pipeline\n    loaded_predictions = loaded_pipeline.predict(X_test)\n\n This article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/) \n\n&amp;#x200B;", "author_fullname": "t2_9r8ft2a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to correctly use sklearn Transformers in a Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"piqqv9nrgl7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 97, "x": 108, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc712dc1ff6400f697a3881d51f8d3570f829238"}, {"y": 194, "x": 216, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3a36b89acd98a75d521128b625e47005add048a"}, {"y": 288, "x": 320, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03780e7cf2e0d41698ce9f4e0ce50d301171ebb0"}, {"y": 577, "x": 640, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f33f9a7028cf3468f80742501736390b58e76681"}], "s": {"y": 631, "x": 699, "u": "https://preview.redd.it/piqqv9nrgl7c1.png?width=699&amp;format=png&amp;auto=webp&amp;s=1bb2a2d4739e8100e71559fb9f9e0cfaa4846803"}, "id": "piqqv9nrgl7c1"}, "uqnuo8fpgl7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b370e612070428374a207b597a92aff82d8a147"}, {"y": 108, "x": 216, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=94c2adf95ce18b60f16bc2cc8abc2a6c14563d8f"}, {"y": 160, "x": 320, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=84db99e434cb5e4d32032a6ae0d7446b5193e10a"}, {"y": 320, "x": 640, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad714328a71791ed1f8ded1f1e5f63e03f1ccbaa"}], "s": {"y": 341, "x": 681, "u": "https://preview.redd.it/uqnuo8fpgl7c1.png?width=681&amp;format=png&amp;auto=webp&amp;s=bd6b5f7edb2d4f5e71786fd6c0f45745ea2095d9"}, "id": "uqnuo8fpgl7c1"}, "okhwyg75gl7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a3723b91df001b47a713b573280c2632814bff9"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=318fb9cf0ea6f735eaa74c3ee9f46c0a1a31d5d1"}, {"y": 196, "x": 320, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a9cd4c403b9f61f1ee9176cdfdb693a946dbe2b"}, {"y": 393, "x": 640, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1eac9f48da6a1da4c9a130d67dbc6580769e103e"}], "s": {"y": 421, "x": 684, "u": "https://preview.redd.it/okhwyg75gl7c1.png?width=684&amp;format=png&amp;auto=webp&amp;s=8106360fcaeb17deea2adf04fc34228dd31a9fd7"}, "id": "okhwyg75gl7c1"}, "53kl3padhl7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ccc85545041b723d70529bd68170283417122e9"}, {"y": 126, "x": 216, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd832e4183f64bc5cb636ae98f3cf166d672ded4"}, {"y": 187, "x": 320, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b39ac233ad51443069ed58202070dc9749e37e96"}, {"y": 375, "x": 640, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5dc09cc5b6c6fc6fd89507b8a1fbef3e85099e61"}], "s": {"y": 390, "x": 664, "u": "https://preview.redd.it/53kl3padhl7c1.png?width=664&amp;format=png&amp;auto=webp&amp;s=a3e3d9dcf25f0ee8dc7234e1db9280ce1cd15b0d"}, "id": "53kl3padhl7c1"}}, "name": "t3_18ngsgv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Coding", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/R6xMJtVPodH3aN_i-OC2a0zZZJv0utQLFd8yB5_cBeU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1703141418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This article will explain how to use &lt;a href=\"https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com\"&gt;Pipeline&lt;/a&gt; and &lt;a href=\"https://scikit-learn.org/stable/data_transforms.html?ref=dataleadsfuture.com\"&gt;Transformers&lt;/a&gt; correctly in Scikit-Learn (sklearn) projects to speed up and reuse our model training process.&lt;/p&gt;\n\n&lt;p&gt;This piece complements and clarifies the official documentation on Pipeline examples and some common misunderstandings.&lt;/p&gt;\n\n&lt;p&gt;I hope that after reading this, you&amp;#39;ll be able to use the Pipeline, an excellent design, to better complete your machine learning tasks.&lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n\n&lt;h3&gt;Why use a Pipeline&lt;/h3&gt;\n\n&lt;p&gt;As mentioned earlier, in a machine learning task, we often need to use various Transformers for data scaling and feature dimensionality reduction before training a model.&lt;/p&gt;\n\n&lt;p&gt;This presents several challenges:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Code complexity&lt;/strong&gt;: For each use of a Transformer, we have to go through initialization, &lt;code&gt;fit_transform&lt;/code&gt;, and &lt;code&gt;transform&lt;/code&gt; steps. Missing one step during a transformation could derail the entire training process.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data leakage&lt;/strong&gt;: As we discussed, for each Transformer, we fit with train data and then transform both train and test data. We must avoid letting the distribution of the test data leak into the train data.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Code reusability&lt;/strong&gt;: A machine learning model includes not only the trained Estimator for prediction but also the data preprocessing steps. Therefore, a machine learning task comprising Transformers and an Estimator should be atomic and indivisible.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hyperparameter tuning&lt;/strong&gt;: After setting up the steps of machine learning, we need to adjust hyperparameters to find the best combination of Transformer parameter values.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Scikit-Learn introduced the &lt;code&gt;Pipeline&lt;/code&gt; module to solve these issues.&lt;/p&gt;\n\n&lt;h3&gt;What is a Pipeline&lt;/h3&gt;\n\n&lt;p&gt;A &lt;code&gt;Pipeline&lt;/code&gt; is a module in Scikit-Learn that implements the chain of responsibility design pattern.&lt;/p&gt;\n\n&lt;p&gt;When creating a Pipeline, we use the &lt;code&gt;steps&lt;/code&gt; parameter to chain together multiple Transformers for initialization:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, StandardScaler()),\n                           (&amp;#39;pca&amp;#39;, PCA(n_components=2, random_state=42)),\n                           (&amp;#39;estimator&amp;#39;, RandomForestClassifier(n_estimators=3, max_depth=5))])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The &lt;a href=\"https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline\"&gt;official documentation&lt;/a&gt; points out that the last Transformer must be an Estimator.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t need to specify each Transformer&amp;#39;s name, you can simplify the creation of a Pipeline with &lt;code&gt;make_pipeline&lt;/code&gt;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.pipeline import make_pipeline\n\npipeline_2 = make_pipeline(StandardScaler(),\n                           PCA(n_components=2, random_state=42),\n                           RandomForestClassifier(n_estimators=3, max_depth=5))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Understanding the Pipeline&amp;#39;s mechanism from the source code&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve mentioned the importance of not letting test data variables leak into training data when using each Transformer.&lt;/p&gt;\n\n&lt;p&gt;This principle is relatively easy to ensure when each data preprocessing step is independent.&lt;/p&gt;\n\n&lt;p&gt;But what if we integrate these steps using a Pipeline?&lt;/p&gt;\n\n&lt;p&gt;If we look at the &lt;a href=\"https://scikit-learn.org/stable/modules/compose.html?ref=dataleadsfuture.com#pipeline\"&gt;official documentation&lt;/a&gt;, we find it simply uses the fit&lt;br/&gt;\n method on the entire dataset without explaining how to handle train and test data separately.&lt;/p&gt;\n\n&lt;p&gt;With this question in mind, I dived into the Pipeline&amp;#39;s source code to find the answer.&lt;/p&gt;\n\n&lt;p&gt;Reading the source code revealed that although Pipeline implements &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;fit_transform&lt;/code&gt;, and &lt;code&gt;predict&lt;/code&gt; methods, they work differently from regular Transformers.&lt;/p&gt;\n\n&lt;p&gt;Take the following Pipeline creation process as an example:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, StandardScaler()),\n                           (&amp;#39;pca&amp;#39;, PCA(n_components=2, random_state=42)),\n                           (&amp;#39;estimator&amp;#39;, RandomForestClassifier(n_estimators=3, max_depth=5))])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The internal implementation can be represented by the following diagram: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/okhwyg75gl7c1.png?width=684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8106360fcaeb17deea2adf04fc34228dd31a9fd7\"&gt; Internal implementation of the fit and predict methods when called. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As you can see, when we call the &lt;code&gt;fit&lt;/code&gt; method, Pipeline first separates Transformers from the Estimator.&lt;/p&gt;\n\n&lt;p&gt;For each Transformer, Pipeline checks if there&amp;#39;s a &lt;code&gt;fit_transform&lt;/code&gt; method; if so, it calls it; otherwise, it calls &lt;code&gt;fit&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;For the Estimator, it calls &lt;code&gt;fit&lt;/code&gt; directly.&lt;/p&gt;\n\n&lt;p&gt;For the &lt;code&gt;predict&lt;/code&gt; method, Pipeline separates Transformers from the Estimator.&lt;/p&gt;\n\n&lt;p&gt;Pipeline calls each Transformer&amp;#39;s &lt;code&gt;transform&lt;/code&gt; method in sequence, followed by the Estimator&amp;#39;s &lt;code&gt;predict&lt;/code&gt;&lt;br/&gt;\n method.&lt;/p&gt;\n\n&lt;p&gt;Therefore, when using a Pipeline, we still need to split train and test data. Then we simply call &lt;code&gt;fit&lt;/code&gt; on the train data and &lt;code&gt;predict&lt;/code&gt; on the test data.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a special case when combining Pipeline with &lt;code&gt;GridSearchCV&lt;/code&gt; for hyperparameter tuning: you don&amp;#39;t need to manually split train and test data. I&amp;#39;ll explain this in more detail in the best practices section.&lt;/p&gt;\n\n&lt;h2&gt;Best Practices for Using Transformers and Pipeline in Actual Applications&lt;/h2&gt;\n\n&lt;p&gt;Now that we&amp;#39;ve discussed the working principles of Transformers and Pipeline, it&amp;#39;s time to fulfill the promise made in the title and talk about the best practices when combining Transformers with Pipeline in real projects.&lt;/p&gt;\n\n&lt;h3&gt;Combining Pipeline with GridSearchCV for hyperparameter tuning&lt;/h3&gt;\n\n&lt;p&gt;In a machine learning project, selecting the right dataset processing and algorithm is one aspect. After debugging the initial steps, it&amp;#39;s time for parameter optimization.&lt;/p&gt;\n\n&lt;p&gt;Using &lt;code&gt;GridSearchCV&lt;/code&gt; or &lt;code&gt;RandomizedSearchCV&lt;/code&gt;, you can try different parameters for the Estimator to find the best fit:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import time\n\nfrom sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, StandardScaler()),\n                           (&amp;#39;pca&amp;#39;, PCA()),\n                           (&amp;#39;estimator&amp;#39;, RandomForestClassifier())])\nparam_grid = {&amp;#39;pca__n_components&amp;#39;: [2, &amp;#39;mle&amp;#39;],\n              &amp;#39;estimator__n_estimators&amp;#39;: [3, 5, 7],\n              &amp;#39;estimator__max_depth&amp;#39;: [3, 5]}\n\nstart = time.perf_counter()\nclf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=4)\nclf.fit(X, y)\n\n# It takes 2.39 seconds to finish the search on my laptop.\nprint(f&amp;quot;It takes {time.perf_counter() - start} seconds to finish the search.&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But in machine learning, hyperparameter tuning is not limited to Estimator parameters; it also involves combinations of Transformer parameters.&lt;/p&gt;\n\n&lt;p&gt;Integrating all steps with Pipeline allows for hyperparameter tuning of every element with different parameter combinations.&lt;/p&gt;\n\n&lt;p&gt;Note that during hyperparameter tuning, we no longer need to manually split train and test data. &lt;code&gt;GridSearchCV&lt;/code&gt; will split the data into training and validation sets using &lt;a href=\"https://scikit-learn.org/stable/modules/cross_validation.html?ref=dataleadsfuture.com#stratified-k-fold\"&gt;StratifiedKFold&lt;/a&gt;, which implemented a k-fold cross validation mechanism.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uqnuo8fpgl7c1.png?width=681&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd6b5f7edb2d4f5e71786fd6c0f45745ea2095d9\"&gt; StratifiedKFold iterative process of splitting train data and test data. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We can also set the number of folds for cross-validation and choose how many workers to use. The tuning process is illustrated in the following diagram: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/piqqv9nrgl7c1.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bb2a2d4739e8100e71559fb9f9e0cfaa4846803\"&gt; Internal implementation of GridSearchCV hyperparameter tuning. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Due to space constraints, I won&amp;#39;t go into detail about &lt;code&gt;GridSearchCV&lt;/code&gt; and &lt;code&gt;RandomizedSearchCV&lt;/code&gt; here. If you&amp;#39;re interested, I can write another article explaining them next time. &lt;/p&gt;\n\n&lt;h3&gt;Using the memory parameter to cache Transformer outputs&lt;/h3&gt;\n\n&lt;p&gt;Of course, hyperparameter tuning with &lt;code&gt;GridSearchCV&lt;/code&gt; can be slow, but that&amp;#39;s no worry, Pipeline provides a caching mechanism to speed up the tuning efficiency by caching the results of intermediate steps.&lt;/p&gt;\n\n&lt;p&gt;When initializing a Pipeline, you can pass in a memory parameter, which will cache the results after the first call to &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; for each transformer.&lt;/p&gt;\n\n&lt;p&gt;If subsequent calls to fit and &lt;code&gt;transform&lt;/code&gt; use the same parameters, which is very likely during hyperparameter tuning, these steps will directly read the results from the cache instead of recalculating, significantly speeding up the efficiency when running the same Transformer repeatedly.&lt;/p&gt;\n\n&lt;p&gt;The &lt;code&gt;memory&lt;/code&gt; parameter can accept the following values:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The default is None: caching is not used.&lt;/li&gt;\n&lt;li&gt;A string: providing a path to store the cached results.&lt;/li&gt;\n&lt;li&gt;A &lt;code&gt;joblib.Memory&lt;/code&gt; object: allows for finer-grained control, such as configuring the storage backend for the cache.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Next, let&amp;#39;s use the previous &lt;code&gt;GridSearchCV&lt;/code&gt; example, this time adding &lt;code&gt;memory&lt;/code&gt; to the Pipeline to see how much speed can be improved:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipeline_m = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, StandardScaler()),\n                           (&amp;#39;pca&amp;#39;, PCA()),\n                           (&amp;#39;estimator&amp;#39;, RandomForestClassifier())],\n                      memory=&amp;#39;./cache&amp;#39;)\nstart = time.perf_counter()\nclf_m = GridSearchCV(pipeline_m, param_grid=param_grid, cv=5, n_jobs=4)\nclf_m.fit(X, y)\n\n# It takes 0.22 seconds to finish the search with memory parameter.\nprint(f&amp;quot;It takes {time.perf_counter() - start} seconds to finish the search with memory.&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;As shown, with caching, the tuning process only takes 0.2 seconds, a significant speed increase from the previous 2.4 seconds.&lt;/p&gt;\n\n&lt;h3&gt;How to debug Scikit-Learn Pipeline&lt;/h3&gt;\n\n&lt;p&gt;After integrating Transformers into a Pipeline, the entire preprocessing and transformation process becomes a black box. It can be difficult to understand which step the process is currently on.&lt;/p&gt;\n\n&lt;p&gt;Fortunately, we can solve this problem by adding logging to the Pipeline.&lt;br/&gt;\nWe need to create custom transformers to add logging at each step of data transformation.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an example of adding logging with Python&amp;#39;s standard logging library:&lt;/p&gt;\n\n&lt;p&gt;First, you need to configure a logger:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import logging\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nlogging.basicConfig(level=logging.INFO, format=&amp;#39;%(asctime)s - %(levelname)s - %(message)s&amp;#39;)\nlogger = logging.getLogger()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Next, you can create a custom Transformer and add logging within its methods: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class LoggingTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, transformer):\n        self.transformer = transformer\n        self.real_name = self.transformer.__class__.__name__\n\n    def fit(self, X, y=None):\n        logging.info(f&amp;quot;Begin fit: {self.real_name}&amp;quot;)\n        self.transformer.fit(X, y)\n        logging.info(f&amp;quot;End fit: {self.real_name}&amp;quot;)\n        return self\n\n    def fit_transform(self, X, y=None):\n        logging.info(f&amp;quot;Begin fit_transform: {self.real_name}&amp;quot;)\n        X_fit_transformed = self.transformer.fit_transform(X, y)\n        logging.info(f&amp;quot;End fit_transform: {self.real_name}&amp;quot;)\n        return X_fit_transformed\n\n    def transform(self, X):\n        logging.info(f&amp;quot;Begin transform: {self.real_name}&amp;quot;)\n        X_transformed = self.transformer.transform(X)\n        logging.info(f&amp;quot;End transform: {self.real_name}&amp;quot;)\n        return X_transformed\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then you can use this &lt;code&gt;LoggingTransformer&lt;/code&gt; when creating your Pipeline: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipeline_logging = Pipeline(steps=[(&amp;#39;scaler&amp;#39;, LoggingTransformer(StandardScaler())),\n                             (&amp;#39;pca&amp;#39;, LoggingTransformer(PCA(n_components=2))),\n                             (&amp;#39;estimator&amp;#39;, RandomForestClassifier(n_estimators=5, max_depth=3))])\npipeline_logging.fit(X_train, y_train)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/53kl3padhl7c1.png?width=664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3e3d9dcf25f0ee8dc7234e1db9280ce1cd15b0d\"&gt; The effect after adding the LoggingTransformer. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;When you use &lt;code&gt;pipeline.fit&lt;/code&gt;, it will call the &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; methods for each step in turn and log the appropriate messages.&lt;/p&gt;\n\n&lt;h3&gt;Use passthrough in Scikit-Learn Pipeline&lt;/h3&gt;\n\n&lt;p&gt;In a Pipeline, a step can be set to &lt;code&gt;&amp;#39;passthrough&lt;/code&gt;&amp;#39;, which means that for this specific step, the input data will pass through unchanged to the next step.&lt;/p&gt;\n\n&lt;p&gt;This is useful when you want to selectively enable/disable certain steps in a complex pipeline.&lt;/p&gt;\n\n&lt;p&gt;Taking the code example above, we know that when using &lt;code&gt;DecisionTree&lt;/code&gt; or &lt;code&gt;RandomForest&lt;/code&gt;, standardizing the data is unnecessary, so we can use &lt;code&gt;passthrough&lt;/code&gt; to skip this step.&lt;/p&gt;\n\n&lt;p&gt;An example would be as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;param_grid = {&amp;#39;scaler&amp;#39;: [&amp;#39;passthrough&amp;#39;],\n              &amp;#39;pca__n_components&amp;#39;: [2, &amp;#39;mle&amp;#39;],\n              &amp;#39;estimator__n_estimators&amp;#39;: [3, 5, 7],\n              &amp;#39;estimator__max_depth&amp;#39;: [3, 5]}\nclf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=4)\nclf.fit(X, y)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Reusing the Pipeline&lt;/p&gt;\n\n&lt;p&gt;After a journey of trials and tribulations, we finally have a well-performing machine learning model.&lt;/p&gt;\n\n&lt;p&gt;Now, you might consider how to reuse this model, share it with colleagues, or deploy it in a production environment.&lt;/p&gt;\n\n&lt;p&gt;However, the result of a model&amp;#39;s training includes not only the model itself but also the various data processing steps, which all need to be saved.&lt;/p&gt;\n\n&lt;p&gt;Using &lt;code&gt;joblib&lt;/code&gt; and Pipeline, we can save the entire training process for later use. The following code provides a simple example:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from joblib import dump, load\n\n# save pipeline\ndump(pipeline, &amp;#39;model_pipeline.joblib&amp;#39;)\n\n# load pipeline\nloaded_pipeline = load(&amp;#39;model_pipeline.joblib&amp;#39;)\n\n# predict with loaded pipeline\nloaded_predictions = loaded_pipeline.predict(X_test)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/ensuring-correct-use-of-transformers-in-scikit-learn-pipeline/\"&gt;Data Leads Future.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3QeLrvmrkTlith0rWod1RvJb84uIyQ_ooxhR4GAxxyk.jpg?auto=webp&amp;s=0898caa546f8aacdf897f436bb227a3544bf2c83", "width": 160, "height": 58}, "resolutions": [{"url": "https://external-preview.redd.it/3QeLrvmrkTlith0rWod1RvJb84uIyQ_ooxhR4GAxxyk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54fa7103ac1fa7c5fa4f7488aed2f8f183533aa7", "width": 108, "height": 39}], "variants": {}, "id": "aRDTtkw0_GBCGb9E7JLBzDXeEp2pWa53pBC0AILtbPw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4ab9c418-70eb-11ee-8a37-4a495429ae82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18ngsgv", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18ngsgv/how_to_correctly_use_sklearn_transformers_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18ngsgv/how_to_correctly_use_sklearn_transformers_in_a/", "subreddit_subscribers": 1197257, "created_utc": 1703141418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve written a couple of times about my manager and how toxic it has been on the team. I had a chance to re-org to a different team, but he made it difficult to change and kept putting so many hoops.\n\nAnd then he became interim AVP because AVP got fed up and left, which meant that I wasn\u2019t switching teams.\n\nHowever, I had a random chance meeting with the someone higher and I spoke to them about everything, which he wasn\u2019t aware of. They have a brand new team that I\u2019ll be a part of. \n\nAnd best of all, my current manager can\u2019t say anything and I\u2019ll be leaving! However, since it\u2019s internal, there\u2019ll be transition time since there is no team. There\u2019s no one to replace me on my team, unfortunately.\n\nHowever, the new role comes with a promotion and a 20% salary increase, so this is truly the best Christmas gift ever! The projects won\u2019t be as interesting, but I want to take a break from LLMs and stick to classification. LLMs in a corporation is really\u2026 political and the data sucks.\n\nAnd then I can look for a new role externally with the title change, so definitely excited! I definitely have imposter syndrome about the new role, but very excited to start having more than 4 hours of sleep and not waking up to a million rude messages from my manager. It\u2019s ridiculously hard to prepare for a new job when your current job is so toxic and you\u2019re so burnt out.\n\nThanks so much for the advice and encouragement!!", "author_fullname": "t2_6lukipdd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thank you for the advice on my manager! I\u2019m finally switching teams!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18ns3lu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703183371.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703179234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve written a couple of times about my manager and how toxic it has been on the team. I had a chance to re-org to a different team, but he made it difficult to change and kept putting so many hoops.&lt;/p&gt;\n\n&lt;p&gt;And then he became interim AVP because AVP got fed up and left, which meant that I wasn\u2019t switching teams.&lt;/p&gt;\n\n&lt;p&gt;However, I had a random chance meeting with the someone higher and I spoke to them about everything, which he wasn\u2019t aware of. They have a brand new team that I\u2019ll be a part of. &lt;/p&gt;\n\n&lt;p&gt;And best of all, my current manager can\u2019t say anything and I\u2019ll be leaving! However, since it\u2019s internal, there\u2019ll be transition time since there is no team. There\u2019s no one to replace me on my team, unfortunately.&lt;/p&gt;\n\n&lt;p&gt;However, the new role comes with a promotion and a 20% salary increase, so this is truly the best Christmas gift ever! The projects won\u2019t be as interesting, but I want to take a break from LLMs and stick to classification. LLMs in a corporation is really\u2026 political and the data sucks.&lt;/p&gt;\n\n&lt;p&gt;And then I can look for a new role externally with the title change, so definitely excited! I definitely have imposter syndrome about the new role, but very excited to start having more than 4 hours of sleep and not waking up to a million rude messages from my manager. It\u2019s ridiculously hard to prepare for a new job when your current job is so toxic and you\u2019re so burnt out.&lt;/p&gt;\n\n&lt;p&gt;Thanks so much for the advice and encouragement!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ns3lu", "is_robot_indexable": true, "report_reasons": null, "author": "Much-Focus-1408", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18ns3lu/thank_you_for_the_advice_on_my_manager_im_finally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18ns3lu/thank_you_for_the_advice_on_my_manager_im_finally/", "subreddit_subscribers": 1197257, "created_utc": 1703179234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm doing an excercise for an interview process and I'm no used to working on open source projects so I'm supposed to extract a csv and a Json and do some cleaning. I uploaded the files on a public github repository and did the extraction, cleaning and intial modeling on a jupyter notebook. so far so good.\n\nThe next step is to do some SQL queries to analize data but I'm wondering how can I set everything up so that the recruiter will be able to connect and run my queries?\n\n1. Where and how should I output my jupyter created dataframes so that anyone can connect to them \n2. Which software could be used to query the data without having to set up a connection\n\n&amp;#x200B;\n\nThanks a lot", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coding Excercise question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n9928", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703117107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m doing an excercise for an interview process and I&amp;#39;m no used to working on open source projects so I&amp;#39;m supposed to extract a csv and a Json and do some cleaning. I uploaded the files on a public github repository and did the extraction, cleaning and intial modeling on a jupyter notebook. so far so good.&lt;/p&gt;\n\n&lt;p&gt;The next step is to do some SQL queries to analize data but I&amp;#39;m wondering how can I set everything up so that the recruiter will be able to connect and run my queries?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Where and how should I output my jupyter created dataframes so that anyone can connect to them &lt;/li&gt;\n&lt;li&gt;Which software could be used to query the data without having to set up a connection&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "18n9928", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18n9928/coding_excercise_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18n9928/coding_excercise_question/", "subreddit_subscribers": 1197257, "created_utc": 1703117107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey data science people, \n\nI've been a data scientist now for several years, and am currently in somewhat of a leadership position. I work for a smaller company, for which I was their very first data scientist. Overall, I'd describe my situation as somewhat of a massive double-edged sword. I've developed all their infrastructure and have really set the scope of data science at the organization. This last year, I started a push into actual data products. In that sense, it's been very rewarding getting to experience multiple hats, mostly be my own boss, and to be able to create something across not only the modeling side, but also the infrastructure and product management sides. \n\nThat said, the other side of the sword is all that comes with being the first data scientist at a smaller organization. The being a one-man startup within a company thing can definitely be a fair amount of exhausting and isolating. Hiring other positions itself has been very tricky. Company buy-in has been a weird mix... they definitely make me feel heavily valued, and I'm not afraid of losing my job. The last presentation I gave, the COO was practically salivating. Yet, I also feel like they haven't committed too much buy-in to data science in general. I've told my boss repeatedly how it all can feel really tenuous. Even scheduling meetings to go over data science success with various company stakeholders, feels like they really have to fit it in. \n\nWith all that said, my boss has for a long time been very interested in scaling up data science at the organization. He recently informally offered me a title bump up to Director of Data Science, and I'm considering it. I've also been thinking about changing companies to experience how others do it, or else taking somewhat of a sabbatical to think out the next stage of my life, what direction I want to go in. Overall I'm torn about the limitations of my current company... had originally gotten into data science to do NLP, and now that LLMs are taking off, I feel like I'm falling farther and farther away from that working here. I'm also wondering on whether it would be smart to take on a director role from the first company I started my ds career with. I'm a little worried I would get shoehorned from now on into director roles, when I'm not sure I'm ready to give up a more raw position. That said, my company has overall treated me well, and it IS a director role, which essentially I'm already doing. \n\nSo yeah, overall I think taking the director role would be an interesting experience and might play into my underlying motivations around creativity, BUT I'm also wary of the limitations of my own company in the field, and whether it overall aligns with my interests in things like LLMs. \n\nWhat do you guys think? :) ", "author_fullname": "t2_q91gszwod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Taking a Director of Data Science Position from First Company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nfk9b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703137081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data science people, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been a data scientist now for several years, and am currently in somewhat of a leadership position. I work for a smaller company, for which I was their very first data scientist. Overall, I&amp;#39;d describe my situation as somewhat of a massive double-edged sword. I&amp;#39;ve developed all their infrastructure and have really set the scope of data science at the organization. This last year, I started a push into actual data products. In that sense, it&amp;#39;s been very rewarding getting to experience multiple hats, mostly be my own boss, and to be able to create something across not only the modeling side, but also the infrastructure and product management sides. &lt;/p&gt;\n\n&lt;p&gt;That said, the other side of the sword is all that comes with being the first data scientist at a smaller organization. The being a one-man startup within a company thing can definitely be a fair amount of exhausting and isolating. Hiring other positions itself has been very tricky. Company buy-in has been a weird mix... they definitely make me feel heavily valued, and I&amp;#39;m not afraid of losing my job. The last presentation I gave, the COO was practically salivating. Yet, I also feel like they haven&amp;#39;t committed too much buy-in to data science in general. I&amp;#39;ve told my boss repeatedly how it all can feel really tenuous. Even scheduling meetings to go over data science success with various company stakeholders, feels like they really have to fit it in. &lt;/p&gt;\n\n&lt;p&gt;With all that said, my boss has for a long time been very interested in scaling up data science at the organization. He recently informally offered me a title bump up to Director of Data Science, and I&amp;#39;m considering it. I&amp;#39;ve also been thinking about changing companies to experience how others do it, or else taking somewhat of a sabbatical to think out the next stage of my life, what direction I want to go in. Overall I&amp;#39;m torn about the limitations of my current company... had originally gotten into data science to do NLP, and now that LLMs are taking off, I feel like I&amp;#39;m falling farther and farther away from that working here. I&amp;#39;m also wondering on whether it would be smart to take on a director role from the first company I started my ds career with. I&amp;#39;m a little worried I would get shoehorned from now on into director roles, when I&amp;#39;m not sure I&amp;#39;m ready to give up a more raw position. That said, my company has overall treated me well, and it IS a director role, which essentially I&amp;#39;m already doing. &lt;/p&gt;\n\n&lt;p&gt;So yeah, overall I think taking the director role would be an interesting experience and might play into my underlying motivations around creativity, BUT I&amp;#39;m also wary of the limitations of my own company in the field, and whether it overall aligns with my interests in things like LLMs. &lt;/p&gt;\n\n&lt;p&gt;What do you guys think? :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nfk9b", "is_robot_indexable": true, "report_reasons": null, "author": "dsthrowaway1337", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18nfk9b/taking_a_director_of_data_science_position_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18nfk9b/taking_a_director_of_data_science_position_from/", "subreddit_subscribers": 1197257, "created_utc": 1703137081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I\u2019m an undergraduate senior who has been job-hunting for the past couple of months for a role after I graduate in the spring. While I\u2019m sad I\u2019m not landing anything huge like $100k+, I\u2019m grateful to still have gotten offers in this job market. I narrowed it down to two that I\u2019m choosing between (both are data scientist/adjacent roles):\n\nOffer 1:\n- Research institution (think Los Alamos, Oak Ridge, JHU APL, MIT Lincoln)\n- L/MCOL, office located in the suburbs\n- $95k salary\n- Much of the work is for DOD (offer is contingent on security clearance)\n- Employer would pay for a masters degree\n\nOffer 2:\n- Small/mid-sized advertising and marketing firm\n- Fully remote with offices in big cities\n- $75k with 15-25% target bonus\n- I\u2019ve interned with the firm before\n- Entry-level people tend to get promoted quickly (within 6-18 months)\n\nI\u2019m leaning towards taking offer 2 even though it pays less.\n\nMy hesitance with offer 1 comes from the fact that I don\u2019t want to have to drive / make a reverse commute to the suburbs. I don\u2019t judge anyone who does work in defense but I also feel ethically conflicted in doing work towards the war industry. My offer also depends on me getting granted the security clearance, which I\u2019m not sure I\u2019d get because of a number of reasons.\n\nI\u2019m attracted to offer 2 because I already know and like the culture of the team/company and being fully remote allows me to be flexible in where I\u2019m living. I\u2019ve been aiming to live in New York and this role would allow me to do that although the salary makes me nervous about budgeting in nyc. However, I\u2019ve also recently started thinking about exploring other cities with a lower COL (ex. Chicago, Austin) that would allow me to save more of my money.\n\nAm I crazy for not wanting to take offer 1 because I don\u2019t like the location and I\u2019m unsure if I can mentally grapple with the impact of the work?", "author_fullname": "t2_kax6yfnsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a new grad job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n5zuk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703111851.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703108390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019m an undergraduate senior who has been job-hunting for the past couple of months for a role after I graduate in the spring. While I\u2019m sad I\u2019m not landing anything huge like $100k+, I\u2019m grateful to still have gotten offers in this job market. I narrowed it down to two that I\u2019m choosing between (both are data scientist/adjacent roles):&lt;/p&gt;\n\n&lt;p&gt;Offer 1:\n- Research institution (think Los Alamos, Oak Ridge, JHU APL, MIT Lincoln)\n- L/MCOL, office located in the suburbs\n- $95k salary\n- Much of the work is for DOD (offer is contingent on security clearance)\n- Employer would pay for a masters degree&lt;/p&gt;\n\n&lt;p&gt;Offer 2:\n- Small/mid-sized advertising and marketing firm\n- Fully remote with offices in big cities\n- $75k with 15-25% target bonus\n- I\u2019ve interned with the firm before\n- Entry-level people tend to get promoted quickly (within 6-18 months)&lt;/p&gt;\n\n&lt;p&gt;I\u2019m leaning towards taking offer 2 even though it pays less.&lt;/p&gt;\n\n&lt;p&gt;My hesitance with offer 1 comes from the fact that I don\u2019t want to have to drive / make a reverse commute to the suburbs. I don\u2019t judge anyone who does work in defense but I also feel ethically conflicted in doing work towards the war industry. My offer also depends on me getting granted the security clearance, which I\u2019m not sure I\u2019d get because of a number of reasons.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m attracted to offer 2 because I already know and like the culture of the team/company and being fully remote allows me to be flexible in where I\u2019m living. I\u2019ve been aiming to live in New York and this role would allow me to do that although the salary makes me nervous about budgeting in nyc. However, I\u2019ve also recently started thinking about exploring other cities with a lower COL (ex. Chicago, Austin) that would allow me to save more of my money.&lt;/p&gt;\n\n&lt;p&gt;Am I crazy for not wanting to take offer 1 because I don\u2019t like the location and I\u2019m unsure if I can mentally grapple with the impact of the work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18n5zuk", "is_robot_indexable": true, "report_reasons": null, "author": "Unfair-Muscle3393", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18n5zuk/choosing_a_new_grad_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18n5zuk/choosing_a_new_grad_job/", "subreddit_subscribers": 1197257, "created_utc": 1703108390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, looking at a career transition out of the implementation side and into the sales / pre-sales side.\n\nMany folks here who are in that space? If so I'd love to ask some questions and get the vibe. I have done quite a bit of the analytics side of DS and been involved in projects to do with Gen AI.\n\nIs this type of thing best left to pure tech sales folks? One of the commonalities I hear is that pure tech sales folks really struggle to understand or sell in the AI/ML space. So I figured I'd look at moving over.", "author_fullname": "t2_7jjayp3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking at sales", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n9g8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703117702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, looking at a career transition out of the implementation side and into the sales / pre-sales side.&lt;/p&gt;\n\n&lt;p&gt;Many folks here who are in that space? If so I&amp;#39;d love to ask some questions and get the vibe. I have done quite a bit of the analytics side of DS and been involved in projects to do with Gen AI.&lt;/p&gt;\n\n&lt;p&gt;Is this type of thing best left to pure tech sales folks? One of the commonalities I hear is that pure tech sales folks really struggle to understand or sell in the AI/ML space. So I figured I&amp;#39;d look at moving over.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18n9g8m", "is_robot_indexable": true, "report_reasons": null, "author": "quantpsychguy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18n9g8m/looking_at_sales/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18n9g8m/looking_at_sales/", "subreddit_subscribers": 1197257, "created_utc": 1703117702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Title says it all: why SQLAlchemy over traditional SQL?\n\nedit: did not realize I would get so much backlash for asking a question. I am great with SQL, was recently introed to SQLAlchemy, and was pretty much like\u2026 why? I kinda see if you\u2019re querying multiple different kinds of databases that are all similar enough to use the same structure it would be useful\u2026(I.e different hosts for databases that act as repos for same software family), but I struggle to understand how this could possibly be better or easier than just using SQL to the point where it seems like instead of teaching SQL a class would teach SQLAlchemy.  SQL is one of the easiest most human readable languages in the world, arguably even more so than python(definitely less versatile than it).  The point of using your own SQL is only with it can you truly make accommodations for the database structure and calculation schedules.  I get that you can write SQL in SQLAlchemy if called for, but that just makes it seem like an extra step.  \n\nI got some of the answers, I like the SINGLE one where they argue for 3 different efficiencies, but I doubt\u2026 need to do more research ig on efficiency, but most responses seem more backlash for asking a vagueish Question than anything.", "author_fullname": "t2_14wbpk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why SQLAlchemy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n5ylh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.51, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703133669.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703108298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title says it all: why SQLAlchemy over traditional SQL?&lt;/p&gt;\n\n&lt;p&gt;edit: did not realize I would get so much backlash for asking a question. I am great with SQL, was recently introed to SQLAlchemy, and was pretty much like\u2026 why? I kinda see if you\u2019re querying multiple different kinds of databases that are all similar enough to use the same structure it would be useful\u2026(I.e different hosts for databases that act as repos for same software family), but I struggle to understand how this could possibly be better or easier than just using SQL to the point where it seems like instead of teaching SQL a class would teach SQLAlchemy.  SQL is one of the easiest most human readable languages in the world, arguably even more so than python(definitely less versatile than it).  The point of using your own SQL is only with it can you truly make accommodations for the database structure and calculation schedules.  I get that you can write SQL in SQLAlchemy if called for, but that just makes it seem like an extra step.  &lt;/p&gt;\n\n&lt;p&gt;I got some of the answers, I like the SINGLE one where they argue for 3 different efficiencies, but I doubt\u2026 need to do more research ig on efficiency, but most responses seem more backlash for asking a vagueish Question than anything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "18n5ylh", "is_robot_indexable": true, "report_reasons": null, "author": "kater543", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18n5ylh/why_sqlalchemy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18n5ylh/why_sqlalchemy/", "subreddit_subscribers": 1197257, "created_utc": 1703108298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What would be the best practice to create a data warehouse out dataframes that were created in a juoyter notebook? \nShould I use some magic commands to run sql within the notebook itself or should I export the dataframes and import them to another software? \nThe goal is to use the normalized tables in PBI", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Juoyter notebook to SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n3hij", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703101850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What would be the best practice to create a data warehouse out dataframes that were created in a juoyter notebook? \nShould I use some magic commands to run sql within the notebook itself or should I export the dataframes and import them to another software? \nThe goal is to use the normalized tables in PBI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "18n3hij", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18n3hij/juoyter_notebook_to_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18n3hij/juoyter_notebook_to_sql/", "subreddit_subscribers": 1197257, "created_utc": 1703101850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi. I have the opportunity to define my own data role at a very very large company. I'm currently a Sr. DS and I manage a team at a startup. This seems like a trick - the company is tech and data heavy and they have data scientists on staff, in abundance - *I'm* not that great.    \n\n Is this a **trap**? ", "author_fullname": "t2_4ciy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice needed, (not a humble brag, I swear)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nccs1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703126609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I have the opportunity to define my own data role at a very very large company. I&amp;#39;m currently a Sr. DS and I manage a team at a startup. This seems like a trick - the company is tech and data heavy and they have data scientists on staff, in abundance - &lt;em&gt;I&amp;#39;m&lt;/em&gt; not that great.    &lt;/p&gt;\n\n&lt;p&gt;Is this a &lt;strong&gt;trap&lt;/strong&gt;? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nccs1", "is_robot_indexable": true, "report_reasons": null, "author": "tmotytmoty", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18nccs1/advice_needed_not_a_humble_brag_i_swear/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18nccs1/advice_needed_not_a_humble_brag_i_swear/", "subreddit_subscribers": 1197257, "created_utc": 1703126609.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}