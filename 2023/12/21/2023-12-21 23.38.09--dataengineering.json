{"kind": "Listing", "data": {"after": "t3_18ng961", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say i know how to connect to external databases and do dataframe transformations using pyspark in databricks. do i \"know\" spark? from my understanding, databricks handles all the other spark stuff like cores/threads etc.", "author_fullname": "t2_o21w1ya01", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how can a DE say that they \"know\" spark/pyspark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n9wnm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703119056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say i know how to connect to external databases and do dataframe transformations using pyspark in databricks. do i &amp;quot;know&amp;quot; spark? from my understanding, databricks handles all the other spark stuff like cores/threads etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18n9wnm", "is_robot_indexable": true, "report_reasons": null, "author": "300A24", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n9wnm/how_can_a_de_say_that_they_know_sparkpyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n9wnm/how_can_a_de_say_that_they_know_sparkpyspark/", "subreddit_subscribers": 147539, "created_utc": 1703119056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I thought I was an intermediate user of SQL until I started this current job. In the past, I just did simple joins, translations using coalesque, trims, uppercase, CTE things like that. Occasionally I'd use a window function. I was using BigQuery, SQL server, DB2. \n\n\n**My current company is astronomically different**. I am just a data analyst. We use a flavor of SQL called Teradata. when we are starting a massive project with a billion rows of data, we pull each section of data into smaller pieces using volatile table. So for example, employees are put into an employee volatile table, orders are put into their own volatile table. Then we merge the volatile tables together and restrict them using where clauses for region, department, if we are looking at a specific hierarchy under a single VP.\n\n\nThe part that really gets me is that we are not allowed to use window functions or CTE. Everything has to be a temporary table. Then, after it's all done, we just select from the temporary table and insert it into an actual database table that we create. Keep in mind that I am just a data analyst, so I'm doing some pretty hardcore transformations and ETL, and creating my own tables, updating tables, having to manage primary indexes, query optimization and table enhancement... It's pretty crazy. \n\n\nWhat I want to know though is why are CTE's bad and tempt tables are the solution to everything?", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My company's ETL process is extremely confusing to me", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nmv06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703164777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought I was an intermediate user of SQL until I started this current job. In the past, I just did simple joins, translations using coalesque, trims, uppercase, CTE things like that. Occasionally I&amp;#39;d use a window function. I was using BigQuery, SQL server, DB2. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My current company is astronomically different&lt;/strong&gt;. I am just a data analyst. We use a flavor of SQL called Teradata. when we are starting a massive project with a billion rows of data, we pull each section of data into smaller pieces using volatile table. So for example, employees are put into an employee volatile table, orders are put into their own volatile table. Then we merge the volatile tables together and restrict them using where clauses for region, department, if we are looking at a specific hierarchy under a single VP.&lt;/p&gt;\n\n&lt;p&gt;The part that really gets me is that we are not allowed to use window functions or CTE. Everything has to be a temporary table. Then, after it&amp;#39;s all done, we just select from the temporary table and insert it into an actual database table that we create. Keep in mind that I am just a data analyst, so I&amp;#39;m doing some pretty hardcore transformations and ETL, and creating my own tables, updating tables, having to manage primary indexes, query optimization and table enhancement... It&amp;#39;s pretty crazy. &lt;/p&gt;\n\n&lt;p&gt;What I want to know though is why are CTE&amp;#39;s bad and tempt tables are the solution to everything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nmv06", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nmv06/my_companys_etl_process_is_extremely_confusing_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nmv06/my_companys_etl_process_is_extremely_confusing_to/", "subreddit_subscribers": 147539, "created_utc": 1703164777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, the company where I am working currently, are offering AWS Certifications, for their cloud related projects. And I have showed interest in doing that. As my plan is to do the certifications, get hands-on with some projects, meanwhile learning other data engineering related skills on my own, and then make a transition towards a data engineering role. \n\nIs this a good approach? What do you guys think?", "author_fullname": "t2_pt8qqht7l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can a Cloud Engineer with Solution Architect Associate Certification, get a job as a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nkbl0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703155850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, the company where I am working currently, are offering AWS Certifications, for their cloud related projects. And I have showed interest in doing that. As my plan is to do the certifications, get hands-on with some projects, meanwhile learning other data engineering related skills on my own, and then make a transition towards a data engineering role. &lt;/p&gt;\n\n&lt;p&gt;Is this a good approach? What do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nkbl0", "is_robot_indexable": true, "report_reasons": null, "author": "Abdullah6600", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nkbl0/can_a_cloud_engineer_with_solution_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nkbl0/can_a_cloud_engineer_with_solution_architect/", "subreddit_subscribers": 147539, "created_utc": 1703155850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "has anyone worked on both dbt and Delta Live Tables with Databricks? What's the pros/cons?\n\nRecently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline\n\nWe had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo'd us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)", "author_fullname": "t2_ghlgg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: dbt or Delta Live Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18ny8tf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703195187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;has anyone worked on both dbt and Delta Live Tables with Databricks? What&amp;#39;s the pros/cons?&lt;/p&gt;\n\n&lt;p&gt;Recently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline&lt;/p&gt;\n\n&lt;p&gt;We had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo&amp;#39;d us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ny8tf", "is_robot_indexable": true, "report_reasons": null, "author": "y45hiro", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ny8tf/databricks_dbt_or_delta_live_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ny8tf/databricks_dbt_or_delta_live_tables/", "subreddit_subscribers": 147539, "created_utc": 1703195187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, if Apache Flink and Stream Processing is your jam, Ververica just pushed all the session videos from Flink Forward Seattle from November.  You can watch them [here](https://www.ververica.academy/app/videos).", "author_fullname": "t2_8k4wqq0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flink Forward 2023 Session Videos are LIVE!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ntdgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703182539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, if Apache Flink and Stream Processing is your jam, Ververica just pushed all the session videos from Flink Forward Seattle from November.  You can watch them &lt;a href=\"https://www.ververica.academy/app/videos\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?auto=webp&amp;s=587f667acd6f8dc058ad81caa52f478370bec78f", "width": 288, "height": 72}, "resolutions": [{"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc40a585a78df51a77b29097e282812a54342421", "width": 108, "height": 27}, {"url": "https://external-preview.redd.it/UiPs3RQdsaYAwAlRlUKbVoCQI8EZdYvPve0bj0dsIJc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7db7b8904146e8109c4834cdfae1b8fb7b494500", "width": 216, "height": 54}], "variants": {}, "id": "YFXgszUcZBc4zgvyAizrbeETuOI6zuEJCxmOQS9yBBM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ntdgo", "is_robot_indexable": true, "report_reasons": null, "author": "wildbreaker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ntdgo/flink_forward_2023_session_videos_are_live/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ntdgo/flink_forward_2023_session_videos_are_live/", "subreddit_subscribers": 147539, "created_utc": 1703182539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to get a gauge for how much a DE should reasonably expect to have to define transformations or say, configure SCD logic as a part of their job duties. or ideally, too", "author_fullname": "t2_hqp95mxij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL development as a % of DE workload", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n85ia", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703114094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to get a gauge for how much a DE should reasonably expect to have to define transformations or say, configure SCD logic as a part of their job duties. or ideally, too&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18n85ia", "is_robot_indexable": true, "report_reasons": null, "author": "midnightrambling", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n85ia/etl_development_as_a_of_de_workload/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n85ia/etl_development_as_a_of_de_workload/", "subreddit_subscribers": 147539, "created_utc": 1703114094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datafusion SQL CLI - Look Ma, I made a new ETL tool.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18nxjfu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wHTKvyWGxYvyFCZodMyJnMQjzmMoZMB_jKzsGGrWrcU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703193326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "confessionsofadataguy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.confessionsofadataguy.com/datafusion-sql-cli-look-ma-i-made-a-new-etl-tool/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?auto=webp&amp;s=41423b348e6ad563cd9ec3c93bb2ef568d601d94", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6770d83ea8f1b52b4202f91465721a61ab368c3c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f52f431ce010bf1d78004cd087fb77ff52933da0", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35c938305c576391e1e83f1d8ba5ae5f653d4605", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30da6f3130dcc719059d8f1e646c19d64c041394", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/lNC38_yBC3Lldf6ZazKHLcsfJcb8NsH6sJbVsYvNmY4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b7bdb03463a1206d2006d958847620dccc9f170", "width": 960, "height": 960}], "variants": {}, "id": "yA9pk-IMBm6egneN5vdpyDs0g2ml4ezsDBMsp_nGF0w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nxjfu", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nxjfu/datafusion_sql_cli_look_ma_i_made_a_new_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.confessionsofadataguy.com/datafusion-sql-cli-look-ma-i-made-a-new-etl-tool/", "subreddit_subscribers": 147539, "created_utc": 1703193326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does databricks have its own operational database? It primarily plays in the analytical space, right?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nvfch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703187807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nvfch", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nvfch/does_databricks_have_its_own_operational_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nvfch/does_databricks_have_its_own_operational_database/", "subreddit_subscribers": 147539, "created_utc": 1703187807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, Parquet is well known for allowing projection and predicate pushdown. I know most parquet readers implement them. However, parquet readers read the data. What is I just want to reorganize the files based on predicates and columns, without reading them... is that possible?  \n\nFor example, suppose we have 2 files and I want to reorganize them into 5 smaller files that each will contain a subset of rows and a subset of columns.\n\nIf I use a parquet reader, I will need to read, reorganize in memory and write as I wish. I'll benefit from pushdown, but still will need to read. But considering I don't need to work on the data, only to reorganize it in different files based on predicates and columns, if there is a way to do so better, it will save a lot of IO and memory. \n\nAppreciate any resource.", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to reorganize parquet files with predicate and projections pushdown?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nravf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703177109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, Parquet is well known for allowing projection and predicate pushdown. I know most parquet readers implement them. However, parquet readers read the data. What is I just want to reorganize the files based on predicates and columns, without reading them... is that possible?  &lt;/p&gt;\n\n&lt;p&gt;For example, suppose we have 2 files and I want to reorganize them into 5 smaller files that each will contain a subset of rows and a subset of columns.&lt;/p&gt;\n\n&lt;p&gt;If I use a parquet reader, I will need to read, reorganize in memory and write as I wish. I&amp;#39;ll benefit from pushdown, but still will need to read. But considering I don&amp;#39;t need to work on the data, only to reorganize it in different files based on predicates and columns, if there is a way to do so better, it will save a lot of IO and memory. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any resource.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18nravf", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nravf/how_to_reorganize_parquet_files_with_predicate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nravf/how_to_reorganize_parquet_files_with_predicate/", "subreddit_subscribers": 147539, "created_utc": 1703177109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\n[Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial](https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=a4ddbb06ac319a3328da49a7ee41596b857b9b9a)\n\n[Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial](https://spatial-dev.guru/2023/12/17/exploring-3d-terrain-visualization-with-python-a-dem-and-pyvista-tutorial/)", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 61, "top_awarded_type": null, "hide_score": false, "media_metadata": {"j5mpe1ek1o7c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 47, "x": 108, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb749d4d6b158b3920467e93ca12992f0ebe54d0"}, {"y": 94, "x": 216, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06bd8bb9610be8fb37ad4db5ae52175c6accc8b1"}, {"y": 139, "x": 320, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=70c9f275871fd8d7b414b8a457b917c1ec87d7a4"}, {"y": 279, "x": 640, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=220c4b38d1bb2f118bc5020c2d6ad8a344363aab"}, {"y": 419, "x": 960, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=750caa82e1f98abf161376941f287f071430b39a"}], "s": {"y": 447, "x": 1024, "u": "https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=a4ddbb06ac319a3328da49a7ee41596b857b9b9a"}, "id": "j5mpe1ek1o7c1"}}, "name": "t3_18npho9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FEzntdzYQD8bvhSuHxJuzmCzLpmmw6MzWeeVZq1FV8c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1703172273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a4ddbb06ac319a3328da49a7ee41596b857b9b9a\"&gt;Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2023/12/17/exploring-3d-terrain-visualization-with-python-a-dem-and-pyvista-tutorial/\"&gt;Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?auto=webp&amp;s=d716dbc723271fd0bd0c981e2d46b69434e0ebaa", "width": 1023, "height": 447}, "resolutions": [{"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b88e41bb4f458332ca9644d2332e8b253147d295", "width": 108, "height": 47}, {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2efd502ef60fe672163b69d418a7e64f55de8c6c", "width": 216, "height": 94}, {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60b06a376720168c101f25c2f3b2e41b9bbfd6cb", "width": 320, "height": 139}, {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69ccf8a73d405e774792ae0ce5b5d75d7631e0a5", "width": 640, "height": 279}, {"url": "https://external-preview.redd.it/VElpAv9emfMpm6PBlM7VhVJl2DA0kbphYPnjWAVKZ-g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3dc3567266d66203f81a095e60bb56ad5d0467f6", "width": 960, "height": 419}], "variants": {}, "id": "u9N9iTbyonfgAIhUyGLDZmp072_g2o3C4fBK-A8sQEc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18npho9", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18npho9/exploring_3d_terrain_visualization_with_python_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18npho9/exploring_3d_terrain_visualization_with_python_a/", "subreddit_subscribers": 147539, "created_utc": 1703172273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, i work as in a beginner level. I gave a requirement to data engineering lead to update redshift table records. It is basically update statements. My de lead told me that redshift does not support update statements, write everything on select query. \n\nI want to know in what case redshift won\u2019t accept update statements", "author_fullname": "t2_7ui8gfy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift update statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ner3i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703134360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, i work as in a beginner level. I gave a requirement to data engineering lead to update redshift table records. It is basically update statements. My de lead told me that redshift does not support update statements, write everything on select query. &lt;/p&gt;\n\n&lt;p&gt;I want to know in what case redshift won\u2019t accept update statements&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ner3i", "is_robot_indexable": true, "report_reasons": null, "author": "Negi_DA", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ner3i/redshift_update_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ner3i/redshift_update_statement/", "subreddit_subscribers": 147539, "created_utc": 1703134360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been trying to convert json data to parquet by storing the json data in Python list -&gt; Pandas Dataframe -&gt; Parquet.\n\nThe main issue is when i convert the python list to dataframe, it took too long to process. Is there any better way to do this?", "author_fullname": "t2_dgqi4197", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Convert json data to parquet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nduxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703131360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been trying to convert json data to parquet by storing the json data in Python list -&amp;gt; Pandas Dataframe -&amp;gt; Parquet.&lt;/p&gt;\n\n&lt;p&gt;The main issue is when i convert the python list to dataframe, it took too long to process. Is there any better way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18nduxn", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Criticism-8127", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nduxn/convert_json_data_to_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nduxn/convert_json_data_to_parquet/", "subreddit_subscribers": 147539, "created_utc": 1703131360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I\u2019m a 5th year data engineer. Have any of you ever published to mongodb from the databricks feature store? I want to do this. If you read the databricks document, it says that the third-party online store only supports Amazon DynamoDB, Amazon Aurora, and Amazon RDS MySQL.\n\n[https://docs.databricks.com/en/machine-learning/feature-store/online-feature-stores.html](https://docs.databricks.com/en/machine-learning/feature-store/online-feature-stores.html)\n\nHowever I heard from a databricks's manager that many customers also using mongodb a lot. However, I don\u2019t know the exact method. Anyone who knows how to publish with mongodb in the feature store, please help.\n\n(\bI can ask the databricks's manager first, but I'm just comfortable here. Please help me.)", "author_fullname": "t2_lvw025ht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "from databricks feature store to mongodb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ncnse", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703127589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019m a 5th year data engineer. Have any of you ever published to mongodb from the databricks feature store? I want to do this. If you read the databricks document, it says that the third-party online store only supports Amazon DynamoDB, Amazon Aurora, and Amazon RDS MySQL.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.databricks.com/en/machine-learning/feature-store/online-feature-stores.html\"&gt;https://docs.databricks.com/en/machine-learning/feature-store/online-feature-stores.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However I heard from a databricks&amp;#39;s manager that many customers also using mongodb a lot. However, I don\u2019t know the exact method. Anyone who knows how to publish with mongodb in the feature store, please help.&lt;/p&gt;\n\n&lt;p&gt;(I can ask the databricks&amp;#39;s manager first, but I&amp;#39;m just comfortable here. Please help me.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ncnse", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious_Sense_108", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ncnse/from_databricks_feature_store_to_mongodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ncnse/from_databricks_feature_store_to_mongodb/", "subreddit_subscribers": 147539, "created_utc": 1703127589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some context, I work at a school district and we get a lot of different files. We had some process that would take a file, pull out the columns we want, and upload them into our student information system. There was a few problems with this, one is we had certain people come back and want a column we were not capturing. This would cause us to have to update and re-run files through the process and time and effort matching again.  The other is columns in files change all the time. It is easy for say SAT to have a new file format every single year\u2026SAT and ACT are fixed width files that have just recently done csv types. \n\nA few years ago we created a \u201cdata lake\u201d in MS SQL Server.  We have a table with generic columns and a table that houses the column names for those.  The issue now, is some of the files are getting to and making a tow error out in size in MS SQL. \n\nIs there any decent free data lake software that can be hosted on site?  I have seen some demos of Azure where you pick a file and it sort of reads the file into its own table, and you can flow it as you want.  It would be nice to be able to have something like that.", "author_fullname": "t2_4uiot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lake Alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nabs4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703120342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some context, I work at a school district and we get a lot of different files. We had some process that would take a file, pull out the columns we want, and upload them into our student information system. There was a few problems with this, one is we had certain people come back and want a column we were not capturing. This would cause us to have to update and re-run files through the process and time and effort matching again.  The other is columns in files change all the time. It is easy for say SAT to have a new file format every single year\u2026SAT and ACT are fixed width files that have just recently done csv types. &lt;/p&gt;\n\n&lt;p&gt;A few years ago we created a \u201cdata lake\u201d in MS SQL Server.  We have a table with generic columns and a table that houses the column names for those.  The issue now, is some of the files are getting to and making a tow error out in size in MS SQL. &lt;/p&gt;\n\n&lt;p&gt;Is there any decent free data lake software that can be hosted on site?  I have seen some demos of Azure where you pick a file and it sort of reads the file into its own table, and you can flow it as you want.  It would be nice to be able to have something like that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nabs4", "is_robot_indexable": true, "report_reasons": null, "author": "jezter24", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nabs4/data_lake_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nabs4/data_lake_alternatives/", "subreddit_subscribers": 147539, "created_utc": 1703120342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, is there any way to insert data into Iceberg table or generate Iceberg file without Spark/Glue or staging? There seems to be JAVA API in official doc but I can't find any examples. ", "author_fullname": "t2_13qnch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inserting data into Iceberg table in AWS from Java application without Glue/Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n9584", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703116822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, is there any way to insert data into Iceberg table or generate Iceberg file without Spark/Glue or staging? There seems to be JAVA API in official doc but I can&amp;#39;t find any examples. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18n9584", "is_robot_indexable": true, "report_reasons": null, "author": "eyes1216", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n9584/inserting_data_into_iceberg_table_in_aws_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n9584/inserting_data_into_iceberg_table_in_aws_from/", "subreddit_subscribers": 147539, "created_utc": 1703116822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I \u201clearned\u201d pySpark as part of my masters\u2018 cloud computing class but I didn\u2019t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark\u2019s. I don\u2019t know what pySpark is really. It isn\u2019t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven\u2019t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.", "author_fullname": "t2_6jnra7z5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Differences between just python and pySpark? What is pySpark even and do I need it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18nzg9f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.3, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703198939.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703198436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I \u201clearned\u201d pySpark as part of my masters\u2018 cloud computing class but I didn\u2019t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark\u2019s. I don\u2019t know what pySpark is really. It isn\u2019t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven\u2019t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18nzg9f", "is_robot_indexable": true, "report_reasons": null, "author": "Ngachate", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nzg9f/differences_between_just_python_and_pyspark_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nzg9f/differences_between_just_python_and_pyspark_what/", "subreddit_subscribers": 147539, "created_utc": 1703198436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our most recent blog on Real-time Change Data Capture from Postgres 16 Read Replicas [https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas](https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas)  \nStart replicating data from Postgres to Data Warehouses, Queues and Storage using Read Replicas instead of Primaries. No worry of additional load or outages of the Primary database.\n\nWhile building this feature, a few of our learnings on logical decoding on Postgres Read Replicas:  \n1\ufe0f\u20e3 CREATE\\_REPLICATION\\_SLOT works as expected  \n2\ufe0f\u20e3 SNAPSHOT can be created and used on Read Replicas  \n3\ufe0f\u20e3 START\\_REPLICATION worked as expected  \n4\ufe0f\u20e3 Publications cannot be created on the Replica. They need to be created on the primary.  \n5\ufe0f\u20e3 WAL control functions cannot run on Read Replicas. Use pg\\_last\\_wal\\_receive\\_lsn() instead of pg\\_current\\_wal\\_lsn()", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Change Data Capture from Postgres 16 Read Replicas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18nzdi8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703198239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our most recent blog on Real-time Change Data Capture from Postgres 16 Read Replicas &lt;a href=\"https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas\"&gt;https://blog.peerdb.io/real-time-change-data-capture-from-postgres-16-read-replicas&lt;/a&gt;&lt;br/&gt;\nStart replicating data from Postgres to Data Warehouses, Queues and Storage using Read Replicas instead of Primaries. No worry of additional load or outages of the Primary database.&lt;/p&gt;\n\n&lt;p&gt;While building this feature, a few of our learnings on logical decoding on Postgres Read Replicas:&lt;br/&gt;\n1\ufe0f\u20e3 CREATE_REPLICATION_SLOT works as expected&lt;br/&gt;\n2\ufe0f\u20e3 SNAPSHOT can be created and used on Read Replicas&lt;br/&gt;\n3\ufe0f\u20e3 START_REPLICATION worked as expected&lt;br/&gt;\n4\ufe0f\u20e3 Publications cannot be created on the Replica. They need to be created on the primary.&lt;br/&gt;\n5\ufe0f\u20e3 WAL control functions cannot run on Read Replicas. Use pg_last_wal_receive_lsn() instead of pg_current_wal_lsn()&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?auto=webp&amp;s=2b914bd51e6fe4fee7388cd4feab65531c9c1376", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd79830ce32aad0a840914f23c6003fe0c7c1ae6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a206c8f190043c41f3128dd1b71d29de6c21f112", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d552aa43457c0da8f5013ebe653391036f5332a", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d6d36f451b4354a0aa9dc47d5753f1edfc2a73b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cdfed35c434a9c8f014e7ee8eba77ad545bf114", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/SERlHjFA0WSb_NQy4u0XDQNww3F8-iEfLeigCfxBVaI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec5d413b0521c2eec24d14cf32571b0179e4ef0a", "width": 1080, "height": 567}], "variants": {}, "id": "xCM7b2TzQAsK8mVeahKijq6AT8KMhyAVbQr5BhJjngE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nzdi8", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nzdi8/realtime_change_data_capture_from_postgres_16/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nzdi8/realtime_change_data_capture_from_postgres_16/", "subreddit_subscribers": 147539, "created_utc": 1703198239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looks like a pretty cool new project - curious if anyone here is using it   \n[https://prql-lang.org/](https://prql-lang.org/)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using PRQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18nyvxt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703196901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like a pretty cool new project - curious if anyone here is using it&lt;br/&gt;\n&lt;a href=\"https://prql-lang.org/\"&gt;https://prql-lang.org/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nyvxt", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nyvxt/anyone_using_prql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nyvxt/anyone_using_prql/", "subreddit_subscribers": 147539, "created_utc": 1703196901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4s3pxn3ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source: Apache Flink Snowflake Connector", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18nu9le", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kLq3s-CLXUH2VOW7K8zgUhCQUC3JJ3SaTTnBE9UouDw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703184848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/deltastreaminc/flink-connector-snowflake", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?auto=webp&amp;s=ede12520963f3fd8dfac3d1c268b1a96d24aa28b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=94e9e8dbccb6777b133732cf88f0a5ddd24f82f6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=42b7c35aa1a79b17830e1a332d86ea5f7274c542", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ecd5cff96be211c37660d91770726f11999c9b4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d811589af1fa84d7df29106efdee027355e87b8e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f1c2b48e3bd28a1f71b9f98069b886ae9f72ee5b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/KsYxK6A-ai2zqEzYyF7DY0QVHLAaf-OKFboZ4Uch_3Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1924e218a53fcc3a1dd802c00d16c3bf4ab257ec", "width": 1080, "height": 540}], "variants": {}, "id": "RGBgY8rhE9yMTve-s0JAWriC0RRtN-gWb_G5M5rDRuI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18nu9le", "is_robot_indexable": true, "report_reasons": null, "author": "DeltaStream_io", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nu9le/open_source_apache_flink_snowflake_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/deltastreaminc/flink-connector-snowflake", "subreddit_subscribers": 147539, "created_utc": 1703184848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " The  decimator is a function that removes points in the plot while keeping  all the \"value/information\" of a chart. The post features examples with  times series and clustering.\n\n[https://www.taipy.io/posts/big-data-charting-strategies-in-python](https://www.taipy.io/posts/big-data-charting-strategies-in-python)", "author_fullname": "t2_tfe7ylgn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plotting Big Data in Python with the Decimator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nqchi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703174552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The  decimator is a function that removes points in the plot while keeping  all the &amp;quot;value/information&amp;quot; of a chart. The post features examples with  times series and clustering.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.taipy.io/posts/big-data-charting-strategies-in-python\"&gt;https://www.taipy.io/posts/big-data-charting-strategies-in-python&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18nqchi", "is_robot_indexable": true, "report_reasons": null, "author": "quicklyalienated76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nqchi/plotting_big_data_in_python_with_the_decimator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nqchi/plotting_big_data_in_python_with_the_decimator/", "subreddit_subscribers": 147539, "created_utc": 1703174552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This article is about , \"Understanding Azure Data Lake Storage Gen2\" This article will cover: \ud83d\udca1\n\n1- Why Azure Data Lake Storage Gen2\n\n2- How to enable Azure Data Lake Storage Gen2\n\n3- Azure Data Lake Gen2 vs Azure Blob Storage Gen2\n\nIf you are interested to understand Azure Data Lake Storage Gen2 you can access the full article here: [https://devblogit.com/understand-azure-data-lake-storage-gen2/](https://devblogit.com/understand-azure-data-lake-storage-gen2/)\n\nDon't miss out on this opportunity to transform your data practices and stay ahead of the competition. Read the article today and unlock the power of Azure Data Lake Storage Gen2! \ud83d\udcaa[\\#Azure](https://www.linkedin.com/feed/hashtag/?keywords=azure&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\\#DataManagement](https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\\#Analytics](https://www.linkedin.com/feed/hashtag/?keywords=analytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\\#DataLake](https://www.linkedin.com/feed/hashtag/?keywords=datalake&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672)", "author_fullname": "t2_blyyz3sy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Lake Storage Gen2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18naxfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703122164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This article is about , &amp;quot;Understanding Azure Data Lake Storage Gen2&amp;quot; This article will cover: \ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;1- Why Azure Data Lake Storage Gen2&lt;/p&gt;\n\n&lt;p&gt;2- How to enable Azure Data Lake Storage Gen2&lt;/p&gt;\n\n&lt;p&gt;3- Azure Data Lake Gen2 vs Azure Blob Storage Gen2&lt;/p&gt;\n\n&lt;p&gt;If you are interested to understand Azure Data Lake Storage Gen2 you can access the full article here: &lt;a href=\"https://devblogit.com/understand-azure-data-lake-storage-gen2/\"&gt;https://devblogit.com/understand-azure-data-lake-storage-gen2/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t miss out on this opportunity to transform your data practices and stay ahead of the competition. Read the article today and unlock the power of Azure Data Lake Storage Gen2! \ud83d\udcaa&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=azure&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672\"&gt;#Azure&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672\"&gt;#DataManagement&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=analytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672\"&gt;#Analytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datalake&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672\"&gt;#DataLake&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18naxfs", "is_robot_indexable": true, "report_reasons": null, "author": "Bubbly_Bed_4478", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18naxfs/azure_data_lake_storage_gen2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18naxfs/azure_data_lake_storage_gen2/", "subreddit_subscribers": 147539, "created_utc": 1703122164.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nI just joined a team in an industry I\u2019m not very familiar with. So far I\u2019ve been here for 2 months and im still learning the ropes especially since I was technically a bit more hands-off in my last position. I wanted to help out with some work and hopefully brush up and get up to speed .\n\nWhen my boss and lead discuss our etl development I tend to ask a lot of questions like \u201c Okay so if we build it this way, we\u2019d eventually have to\u2026\u201d\n\nMost of the time my boss will cut me off and say \u201cdon\u2019t worry about that now\u201d. The thing is, im not worrying or trying to make it a big deal, I just wanted to discuss the task and where it could lead to. I\u2019m just curious. \n\nInstead of my boss taking my questions as a sign of interest in the field and eager to help, he more than likely thinks im annoying. I just don\u2019t get it, so I just take orders and never say anything? \n\nOne thing im really big about is documentation, I noticed that we were working on this task and there was no intermediate between the tech lead and I and the stakeholder. I assumed that position and finalized some requirements.\n\nI\u2019m not a fan of rushing into development when we don\u2019t have the necessary pieces. So when others have PRs in already or a merge has been done, I try to step through code to learn and get a sense of the style of docsstrings if there are any. I also help myself brush up on things. While im brushing up on things and stepping through the code, im also creating data dictionaries and just documentation on the etl process. \n\nMy boss doesn\u2019t care about that. He seems annoyed that I even ask if we have documentation on a particular project or task. I usually ask to make sure if I can update it myself or to get a sense of the template. \n\nFor the current task, no documentation has been written. I started doing that myself. No one has contributed to that documentation, rather, they\u2019re focusing on just code commenting. \n\nI just don\u2019t believe in doing that. But my boss doesn\u2019t care about that and just finds me annoying. How can I be helpful without being pesky? I just don\u2019t get why we\u2019re building this for a team and we\u2019re not documenting _nothing_. When I ask I usually plan on planning to work with someone on it or I end up doing it myself. \n\nThoughts?", "author_fullname": "t2_oc5syk6jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I improve this communication?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n9033", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703116416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I just joined a team in an industry I\u2019m not very familiar with. So far I\u2019ve been here for 2 months and im still learning the ropes especially since I was technically a bit more hands-off in my last position. I wanted to help out with some work and hopefully brush up and get up to speed .&lt;/p&gt;\n\n&lt;p&gt;When my boss and lead discuss our etl development I tend to ask a lot of questions like \u201c Okay so if we build it this way, we\u2019d eventually have to\u2026\u201d&lt;/p&gt;\n\n&lt;p&gt;Most of the time my boss will cut me off and say \u201cdon\u2019t worry about that now\u201d. The thing is, im not worrying or trying to make it a big deal, I just wanted to discuss the task and where it could lead to. I\u2019m just curious. &lt;/p&gt;\n\n&lt;p&gt;Instead of my boss taking my questions as a sign of interest in the field and eager to help, he more than likely thinks im annoying. I just don\u2019t get it, so I just take orders and never say anything? &lt;/p&gt;\n\n&lt;p&gt;One thing im really big about is documentation, I noticed that we were working on this task and there was no intermediate between the tech lead and I and the stakeholder. I assumed that position and finalized some requirements.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not a fan of rushing into development when we don\u2019t have the necessary pieces. So when others have PRs in already or a merge has been done, I try to step through code to learn and get a sense of the style of docsstrings if there are any. I also help myself brush up on things. While im brushing up on things and stepping through the code, im also creating data dictionaries and just documentation on the etl process. &lt;/p&gt;\n\n&lt;p&gt;My boss doesn\u2019t care about that. He seems annoyed that I even ask if we have documentation on a particular project or task. I usually ask to make sure if I can update it myself or to get a sense of the template. &lt;/p&gt;\n\n&lt;p&gt;For the current task, no documentation has been written. I started doing that myself. No one has contributed to that documentation, rather, they\u2019re focusing on just code commenting. &lt;/p&gt;\n\n&lt;p&gt;I just don\u2019t believe in doing that. But my boss doesn\u2019t care about that and just finds me annoying. How can I be helpful without being pesky? I just don\u2019t get why we\u2019re building this for a team and we\u2019re not documenting &lt;em&gt;nothing&lt;/em&gt;. When I ask I usually plan on planning to work with someone on it or I end up doing it myself. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18n9033", "is_robot_indexable": true, "report_reasons": null, "author": "tryingmybest200000", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n9033/how_do_i_improve_this_communication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n9033/how_do_i_improve_this_communication/", "subreddit_subscribers": 147539, "created_utc": 1703116416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am doing a cert on my time off. I couldn\u2019t believe this page on Microsoft Learn. It describes a bogus way to create a list of dates with hard coded start and end dates, then has a disclaimer that the example may be complex and offers an EVEN worse solution of manually creating a file in Excel! It continues with a warning that you may need to update the solution as time goes on! \n\n\nhttps://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/4-load-time-dimension-tables", "author_fullname": "t2_q66b7x4ip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft Learn Fail", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18nfmg2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.42, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703137288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am doing a cert on my time off. I couldn\u2019t believe this page on Microsoft Learn. It describes a bogus way to create a list of dates with hard coded start and end dates, then has a disclaimer that the example may be complex and offers an EVEN worse solution of manually creating a file in Excel! It continues with a warning that you may need to update the solution as time goes on! &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/4-load-time-dimension-tables\"&gt;https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/4-load-time-dimension-tables&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?auto=webp&amp;s=41fa146938cd97da5abfeff0d092a2cc151e65fa", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3881e36da92b82c6947f6ca4ff3804ca47f2aea", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17b5b01e50a969ac9e2353bebb062cd52a99d108", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acadaf004e8aeb6919eabdb0d93065a34f7e89df", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=883009d39175a2f03b76275ed0f7c6011d94a3a7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cc62aef83f192d102fa78c83c8f4fcfa85057e3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ca6913f202be9a9f83b266dd459edc90adbf9dd", "width": 1080, "height": 567}], "variants": {}, "id": "RCFh0Kid3SAqWEkALMGNW1e9Vu6ayZpftekoayP00hY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18nfmg2", "is_robot_indexable": true, "report_reasons": null, "author": "UltraInstinctAussie", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18nfmg2/microsoft_learn_fail/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18nfmg2/microsoft_learn_fail/", "subreddit_subscribers": 147539, "created_utc": 1703137288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in open source analytics stack (in FAANG) want to understand what small/medium companies are using as their analytics stack (if they have one). \n\nDo they prefer open source and run it inhouse vs go open source cloud hosted vs self hosted solutions?\n\nAnd what choice determine them to choose one vs the other?\n\n&amp;#x200B;\n\nAsking as exploring the pain points that they might have which can be solved with some solution. I personally felt that running open source solution in house requires a team of experts to maintain it. And it's pain so run a complex system inhouse with limited expertise. \n\nRunning in cloud incurs lot of cost for small companies. But at the same time they may not prefer to spend money on a team to manage it if the usage is not significant but still not ignorable. \n\nHope to get some healthy discussion and identify some of the pain points. Happy to provide pointers on some solutions that i have seen at medium/large companies if that helps solve your problems. ", "author_fullname": "t2_1k30ntnf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question for Professional Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18n8arn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.13, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703114489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in open source analytics stack (in FAANG) want to understand what small/medium companies are using as their analytics stack (if they have one). &lt;/p&gt;\n\n&lt;p&gt;Do they prefer open source and run it inhouse vs go open source cloud hosted vs self hosted solutions?&lt;/p&gt;\n\n&lt;p&gt;And what choice determine them to choose one vs the other?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Asking as exploring the pain points that they might have which can be solved with some solution. I personally felt that running open source solution in house requires a team of experts to maintain it. And it&amp;#39;s pain so run a complex system inhouse with limited expertise. &lt;/p&gt;\n\n&lt;p&gt;Running in cloud incurs lot of cost for small companies. But at the same time they may not prefer to spend money on a team to manage it if the usage is not significant but still not ignorable. &lt;/p&gt;\n\n&lt;p&gt;Hope to get some healthy discussion and identify some of the pain points. Happy to provide pointers on some solutions that i have seen at medium/large companies if that helps solve your problems. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18n8arn", "is_robot_indexable": true, "report_reasons": null, "author": "swapsmagic", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18n8arn/question_for_professional_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18n8arn/question_for_professional_data_engineers/", "subreddit_subscribers": 147539, "created_utc": 1703114489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4hj6e483", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on this open source situationship? China\u2019s Baidu sponsored Apache Doris vs. Starrocks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18ng961", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.18, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LG63aDglc3-8qyYHLDSOFj1-m0GorVB5E0CC7N_zXss.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703139484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/7d18t7w3cl7c1.jpeg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/7d18t7w3cl7c1.jpeg?auto=webp&amp;s=3ce323cb2a11cacef6473f9efc5bd5033af2b01c", "width": 1284, "height": 2778}, "resolutions": [{"url": "https://preview.redd.it/7d18t7w3cl7c1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1cebdca27ae9116ca70eb11a114c02d757411811", "width": 108, "height": 216}, {"url": "https://preview.redd.it/7d18t7w3cl7c1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1ec476a5a63433f32739adc7b6f28153b0e0e33", "width": 216, "height": 432}, {"url": "https://preview.redd.it/7d18t7w3cl7c1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3518afdc2c7af7e48abfd11885e34160d84d945c", "width": 320, "height": 640}, {"url": "https://preview.redd.it/7d18t7w3cl7c1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e214ce8ac75d16dff2fbc74c663c23f0ce064b14", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/7d18t7w3cl7c1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=24c984aec76badbc98984d21aa6f2c8beeeff41e", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/7d18t7w3cl7c1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6eaa86a1e97bd109737fc277c1ffe71a6addc451", "width": 1080, "height": 2160}], "variants": {}, "id": "y0nvRx1iK4AJDKccLmXshhy4mZN7bYa-UncusLzC7xg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ng961", "is_robot_indexable": true, "report_reasons": null, "author": "Proper_Scholar4905", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ng961/thoughts_on_this_open_source_situationship_chinas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/7d18t7w3cl7c1.jpeg", "subreddit_subscribers": 147539, "created_utc": 1703139484.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}