{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm wondering what is the responsibility and daily work like.\n\nEdit: not Rockstar Games.", "author_fullname": "t2_xcba5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rockstar Data Engineers making big bucks: what are you doing exactly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dvjbf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702110843.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702066448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering what is the responsibility and daily work like.&lt;/p&gt;\n\n&lt;p&gt;Edit: not Rockstar Games.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18dvjbf", "is_robot_indexable": true, "report_reasons": null, "author": "mackbenc", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dvjbf/rockstar_data_engineers_making_big_bucks_what_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dvjbf/rockstar_data_engineers_making_big_bucks_what_are/", "subreddit_subscribers": 144789, "created_utc": 1702066448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a weird meeting where My department head pulled me into a meeting with HR and started it with \"why do you think youre a computer programmer\" \n\n\nAnd spent the next 30 minutes trying to say I don't program and never was paid to program but am instead, a scripter who only scripts. \n\n\n\nNow. My department head isn't a tech guy. He's a finance guy. My best guess is HR informed him of the California Computer Professional law and now he's trying to dismantle any beliefs that he owes me more money. \n\n\n\nSigh. I took a break from my government data engineering job (extended leave while they are building a new office) to try out Healthcare and I love the work. Its rewarding. I feel as though I've improved greatly. But it's very hard when one person above you constantly throws wrenches in the system.\n\n\nAs he puts it \"I don't understand it because it's just a glorified excel\"\n\n\nAm I annoyed? Yeah. More insulted then anything else. But, this is a side gig to help me improve my skill set so im not worried", "author_fullname": "t2_gzuzr62b1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Why do you think you're a programmer\" a rant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e8r3g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702109138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a weird meeting where My department head pulled me into a meeting with HR and started it with &amp;quot;why do you think youre a computer programmer&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;And spent the next 30 minutes trying to say I don&amp;#39;t program and never was paid to program but am instead, a scripter who only scripts. &lt;/p&gt;\n\n&lt;p&gt;Now. My department head isn&amp;#39;t a tech guy. He&amp;#39;s a finance guy. My best guess is HR informed him of the California Computer Professional law and now he&amp;#39;s trying to dismantle any beliefs that he owes me more money. &lt;/p&gt;\n\n&lt;p&gt;Sigh. I took a break from my government data engineering job (extended leave while they are building a new office) to try out Healthcare and I love the work. Its rewarding. I feel as though I&amp;#39;ve improved greatly. But it&amp;#39;s very hard when one person above you constantly throws wrenches in the system.&lt;/p&gt;\n\n&lt;p&gt;As he puts it &amp;quot;I don&amp;#39;t understand it because it&amp;#39;s just a glorified excel&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Am I annoyed? Yeah. More insulted then anything else. But, this is a side gig to help me improve my skill set so im not worried&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18e8r3g", "is_robot_indexable": true, "report_reasons": null, "author": "Not_Another_Cookbook", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e8r3g/why_do_you_think_youre_a_programmer_a_rant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e8r3g/why_do_you_think_youre_a_programmer_a_rant/", "subreddit_subscribers": 144789, "created_utc": 1702109138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently started a new contract as a data engineer and I'm the lead developer on a new Azure project. The existing team, who have all the permissions, don't want to setup any new resources and instead think it's easier (for them) if I use existing ones they have in place which include an Azure SQL Database and Data Factory. Resources are fine, the problem is they are completely against calling any stored procedures through ADF pipelines. They say it adds a barrier for users to understand the full extent of transformations as they would have to exit ADF and go view the procedure in the DB or repo. Instead, they use ADF data flows for all transformations that can't be handled via generic pipeline activities. Is it just me, or, is this a silly reason for not using any stored procedures whatsoever? From my personal experience, data flows are bulky and have a lot of limitations and are harder to test then SPs. I'm sure data flows have their benefits over SPs in some instances. What I'm trying to get at is the best tool for the job should be chosen not just based on \"we can't be bothered to go read the code\" but on a number of factors; speed, cost, robustness, scalability, etc.", "author_fullname": "t2_p60arri3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Team preventing any stored procedures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dvrd2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702067040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently started a new contract as a data engineer and I&amp;#39;m the lead developer on a new Azure project. The existing team, who have all the permissions, don&amp;#39;t want to setup any new resources and instead think it&amp;#39;s easier (for them) if I use existing ones they have in place which include an Azure SQL Database and Data Factory. Resources are fine, the problem is they are completely against calling any stored procedures through ADF pipelines. They say it adds a barrier for users to understand the full extent of transformations as they would have to exit ADF and go view the procedure in the DB or repo. Instead, they use ADF data flows for all transformations that can&amp;#39;t be handled via generic pipeline activities. Is it just me, or, is this a silly reason for not using any stored procedures whatsoever? From my personal experience, data flows are bulky and have a lot of limitations and are harder to test then SPs. I&amp;#39;m sure data flows have their benefits over SPs in some instances. What I&amp;#39;m trying to get at is the best tool for the job should be chosen not just based on &amp;quot;we can&amp;#39;t be bothered to go read the code&amp;quot; but on a number of factors; speed, cost, robustness, scalability, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18dvrd2", "is_robot_indexable": true, "report_reasons": null, "author": "08mccaca", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dvrd2/team_preventing_any_stored_procedures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dvrd2/team_preventing_any_stored_procedures/", "subreddit_subscribers": 144789, "created_utc": 1702067040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you\u2019ll employing data contracts in your data pipelines? I\u2019ve seen a lot of system diagrams and articles on it but want to see if theres someone who has implemented data contracts, how they are doing it, and maybe willing to share some of their experience.", "author_fullname": "t2_5cjr5v2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Contracts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dmdx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702040644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you\u2019ll employing data contracts in your data pipelines? I\u2019ve seen a lot of system diagrams and articles on it but want to see if theres someone who has implemented data contracts, how they are doing it, and maybe willing to share some of their experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18dmdx8", "is_robot_indexable": true, "report_reasons": null, "author": "captut", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dmdx8/data_contracts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dmdx8/data_contracts/", "subreddit_subscribers": 144789, "created_utc": 1702040644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Coming from AWS SAA-C03, it was surprisingly easy. Or maybe i overprepped because the AWS one was tricky. Hooray! Derar's course in udemy was super helpful. Exam Topics websites too!", "author_fullname": "t2_113zaf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I passed the databricks data engineer associate exam!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e6vcg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702101377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Coming from AWS SAA-C03, it was surprisingly easy. Or maybe i overprepped because the AWS one was tricky. Hooray! Derar&amp;#39;s course in udemy was super helpful. Exam Topics websites too!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18e6vcg", "is_robot_indexable": true, "report_reasons": null, "author": "omnipotentsoul", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e6vcg/i_passed_the_databricks_data_engineer_associate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e6vcg/i_passed_the_databricks_data_engineer_associate/", "subreddit_subscribers": 144789, "created_utc": 1702101377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey \ud83d\udc4b, what's up? I have uploaded a dagster tutorial video of building dagster job and schedule!", "author_fullname": "t2_h4j43yry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster Tutorial: Building Dagster Job &amp; Schedule", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18dzi59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/rWDcdl8SFAI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Dagster Tutorial: Building Dagster Job &amp;amp; Schedule\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Dagster Tutorial: Building Dagster Job &amp; Schedule", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/rWDcdl8SFAI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Dagster Tutorial: Building Dagster Job &amp;amp; Schedule\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/rWDcdl8SFAI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/rWDcdl8SFAI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Dagster Tutorial: Building Dagster Job &amp;amp; Schedule\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18dzi59", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rhfC1XteV5uWY3YN_VvHOKWzy_BWNe42KLcqvXU8-oQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702077325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey \ud83d\udc4b, what&amp;#39;s up? I have uploaded a dagster tutorial video of building dagster job and schedule!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/rWDcdl8SFAI", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DRdd2QwV2z91LE9q9ntU7WY0Bj5yj0aM27g79SiY9tw.jpg?auto=webp&amp;s=c7b8dc9fb4973f78c088ca8a313aaf3744321ddc", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/DRdd2QwV2z91LE9q9ntU7WY0Bj5yj0aM27g79SiY9tw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e97b7a8375129fc7c37231ff43e94accd270d0fc", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/DRdd2QwV2z91LE9q9ntU7WY0Bj5yj0aM27g79SiY9tw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3e1c399ff6d3d67c0b549eca6004276c0ff7b34", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/DRdd2QwV2z91LE9q9ntU7WY0Bj5yj0aM27g79SiY9tw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ae86c4985b0270f53bdc1ca0b1889dff32a4657", "width": 320, "height": 240}], "variants": {}, "id": "cdG1th2ehbRu-VX1KE3HQAEHPL4WzOY7Ovze_HUqyKs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18dzi59", "is_robot_indexable": true, "report_reasons": null, "author": "Coder2j", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dzi59/dagster_tutorial_building_dagster_job_schedule/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/rWDcdl8SFAI", "subreddit_subscribers": 144789, "created_utc": 1702077325.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Dagster Tutorial: Building Dagster Job &amp; Schedule", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/rWDcdl8SFAI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Dagster Tutorial: Building Dagster Job &amp;amp; Schedule\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/rWDcdl8SFAI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I'm a grad student who's interested in pursuing a career in data engineering. I've had experience working with data on multiple projects. What I've come to learn is that data engineering is often done on large scale data and requires paid services. As a broke student what are a few things I could do to actually get experience with data engineering that is similar to a workplace use case and put on my resume?\nI've tried using databricks SQL but it needed me to pay to use warehouse features.", "author_fullname": "t2_hqiwxblm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering at home?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e7qo9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702104905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m a grad student who&amp;#39;s interested in pursuing a career in data engineering. I&amp;#39;ve had experience working with data on multiple projects. What I&amp;#39;ve come to learn is that data engineering is often done on large scale data and requires paid services. As a broke student what are a few things I could do to actually get experience with data engineering that is similar to a workplace use case and put on my resume?\nI&amp;#39;ve tried using databricks SQL but it needed me to pay to use warehouse features.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18e7qo9", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Reason59", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e7qo9/data_engineering_at_home/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e7qo9/data_engineering_at_home/", "subreddit_subscribers": 144789, "created_utc": 1702104905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I made a [post](https://www.reddit.com/r/dataengineering/comments/18c6fnt/kimball_fact_tables/) a few days ago about fact tables, thank you all for contributing and sharing your thoughts. I think I have narrowed my confusion about fact tables down to a more specific topic - combining business processes. I am really trying to get a good understanding on how to implement dimensional modeling, but am struggling with the abstract nature of it. I have no mentor and am trying to learn what I can. I am just trying to wrap my head around implementing a dimensional model for problems beyond the over simplified \"Sales by &lt;some dimension&gt;\" example.\n\nKimball says that fact tables should not be created for a specific analysis or report. Instead, a fact table should represent detailed events about a business process; where a business process is defined as any low-level activity that is performed in an organization, such as paying an invoice, creating delivery documents, creating order documents, handling customer calls, or completing a step in a manufacturing process.\n\nIt is said that fact tables should not be joined. But what if a metric that the business wants to track requires data from multiple business processes? The example I provided in my previous post was \"On Time Deliveries\", which would require joining data from sales orders and a deliveries. According to Kimball, creating sales orders and deliveries would be two distinct business processes and should be represented as two fact tables in the warehouse - one fact table for orders and the other for deliveries. I also couldn't just join the data from the source to create a \"fact\\_on\\_time\\_deliveries\" table, because fact tables can't represent a specific analysis.\n\nIn the operational source system there is a *\"deliveries line item\"* table and a *\"sales document line item\"* table.\n\n* The sales document table tells me the order number, item number, quantity ordered, and the target delivery date.\n* The deliveries table tells me the delivery number, item number, quantity issued, date issued, and foreign key that references the sales document number and item number.\n\nI am very confused as to how dimensional modeling handles these type of metrics and reporting requirements. This is a very simple example, I could imagine far more complex scenarios such as also considering inventory levels on the day a delivery was supposed to be created to analyze why deliveries are missed. It is very difficult to even image a report that doesn't require combining data from multiple business processes.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Combining Fact Tables in Dimensional Modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dx2me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702070788.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702070594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I made a &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/18c6fnt/kimball_fact_tables/\"&gt;post&lt;/a&gt; a few days ago about fact tables, thank you all for contributing and sharing your thoughts. I think I have narrowed my confusion about fact tables down to a more specific topic - combining business processes. I am really trying to get a good understanding on how to implement dimensional modeling, but am struggling with the abstract nature of it. I have no mentor and am trying to learn what I can. I am just trying to wrap my head around implementing a dimensional model for problems beyond the over simplified &amp;quot;Sales by &amp;lt;some dimension&amp;gt;&amp;quot; example.&lt;/p&gt;\n\n&lt;p&gt;Kimball says that fact tables should not be created for a specific analysis or report. Instead, a fact table should represent detailed events about a business process; where a business process is defined as any low-level activity that is performed in an organization, such as paying an invoice, creating delivery documents, creating order documents, handling customer calls, or completing a step in a manufacturing process.&lt;/p&gt;\n\n&lt;p&gt;It is said that fact tables should not be joined. But what if a metric that the business wants to track requires data from multiple business processes? The example I provided in my previous post was &amp;quot;On Time Deliveries&amp;quot;, which would require joining data from sales orders and a deliveries. According to Kimball, creating sales orders and deliveries would be two distinct business processes and should be represented as two fact tables in the warehouse - one fact table for orders and the other for deliveries. I also couldn&amp;#39;t just join the data from the source to create a &amp;quot;fact_on_time_deliveries&amp;quot; table, because fact tables can&amp;#39;t represent a specific analysis.&lt;/p&gt;\n\n&lt;p&gt;In the operational source system there is a &lt;em&gt;&amp;quot;deliveries line item&amp;quot;&lt;/em&gt; table and a &lt;em&gt;&amp;quot;sales document line item&amp;quot;&lt;/em&gt; table.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The sales document table tells me the order number, item number, quantity ordered, and the target delivery date.&lt;/li&gt;\n&lt;li&gt;The deliveries table tells me the delivery number, item number, quantity issued, date issued, and foreign key that references the sales document number and item number.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am very confused as to how dimensional modeling handles these type of metrics and reporting requirements. This is a very simple example, I could imagine far more complex scenarios such as also considering inventory levels on the day a delivery was supposed to be created to analyze why deliveries are missed. It is very difficult to even image a report that doesn&amp;#39;t require combining data from multiple business processes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18dx2me", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dx2me/combining_fact_tables_in_dimensional_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dx2me/combining_fact_tables_in_dimensional_modeling/", "subreddit_subscribers": 144789, "created_utc": 1702070594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you were to begin building a data stack, what would your starting point be?\n\nI would assume the following:\n* Set up a system to pull data from sources (&amp; look up APIs &amp; access rules to gain access)\n* Set up a lake location to store raw files (ex: S3, etc)\n* Load data into warehouse\n* Get scheduler framework in place\n* Build data quality rules to check conditions at ingestion\n* Run data processing in stages to prep data \n* Develop data models in cooperation with business stakeholders\n* Manage stakeholders to ensure they understand how the work needs to work, and maintain buy-in\n* Check that final results achieve desired results\n\nJust trying to make sure I am not missing anything across the process.\n\nThe starting framework could be cheap &amp; simple tech to build the first draft with good business logic, and that a better version as things scale so long as the logic is fairly standard &amp; well-designed. But that the right conception of the project is critical at start to build to scale.\n\nI would assume the right start is more of a cowboy DA/DE skillset, because of the level of whitespace, and need to find &amp; prove value for the early project.\n\n-------\n\nIs this on the right track, or off the rails?\n\nWhere would people who have thought about scaling this work start?", "author_fullname": "t2_eblvtun2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Start a Data Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e7eeb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702103454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you were to begin building a data stack, what would your starting point be?&lt;/p&gt;\n\n&lt;p&gt;I would assume the following:\n* Set up a system to pull data from sources (&amp;amp; look up APIs &amp;amp; access rules to gain access)\n* Set up a lake location to store raw files (ex: S3, etc)\n* Load data into warehouse\n* Get scheduler framework in place\n* Build data quality rules to check conditions at ingestion\n* Run data processing in stages to prep data \n* Develop data models in cooperation with business stakeholders\n* Manage stakeholders to ensure they understand how the work needs to work, and maintain buy-in\n* Check that final results achieve desired results&lt;/p&gt;\n\n&lt;p&gt;Just trying to make sure I am not missing anything across the process.&lt;/p&gt;\n\n&lt;p&gt;The starting framework could be cheap &amp;amp; simple tech to build the first draft with good business logic, and that a better version as things scale so long as the logic is fairly standard &amp;amp; well-designed. But that the right conception of the project is critical at start to build to scale.&lt;/p&gt;\n\n&lt;p&gt;I would assume the right start is more of a cowboy DA/DE skillset, because of the level of whitespace, and need to find &amp;amp; prove value for the early project.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Is this on the right track, or off the rails?&lt;/p&gt;\n\n&lt;p&gt;Where would people who have thought about scaling this work start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18e7eeb", "is_robot_indexable": true, "report_reasons": null, "author": "Glotto_Gold", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e7eeb/how_to_start_a_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e7eeb/how_to_start_a_data_stack/", "subreddit_subscribers": 144789, "created_utc": 1702103454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I\u2019m currently trying to figure out a way to implement scd2 for my python crons which I will be refactoring for an orchestrator. \nI want to know how I can go about doing this and what are the best practices of writing a python cron via an orchestrator. We currently write modular applications spanning several python files which can be reused for other python crons. \nUsual folder structure contains a utils folder which contains python files containing functions which are called in a main.py file.", "author_fullname": "t2_6y1og8oq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice for implementing SCD2 on python for batch pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e6nc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702100495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I\u2019m currently trying to figure out a way to implement scd2 for my python crons which I will be refactoring for an orchestrator. \nI want to know how I can go about doing this and what are the best practices of writing a python cron via an orchestrator. We currently write modular applications spanning several python files which can be reused for other python crons. \nUsual folder structure contains a utils folder which contains python files containing functions which are called in a main.py file.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18e6nc5", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Ninja70", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e6nc5/best_practice_for_implementing_scd2_on_python_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e6nc5/best_practice_for_implementing_scd2_on_python_for/", "subreddit_subscribers": 144789, "created_utc": 1702100495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm a long-time dbt user, but my organisation is looking for something streamlined to help democratise data transformation, increase agility and enforce best-practices.\n\nWe're looking at [https://coalesce.io/](https://coalesce.io/) . Has anyone used this tool? It's a tad light on hard facts in the website. If so, what are your thoughts on its strengths / weaknesses in practice. Do you like it ? Are there any glaring gaps? Is it efficient, and does it still support complex transformations and code-first workflows when needed?\n\nReally appreciate your help and insights!!\n\nThanks,PB", "author_fullname": "t2_jkvzr8r3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coalesce.io - thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e4m70", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702093242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a long-time dbt user, but my organisation is looking for something streamlined to help democratise data transformation, increase agility and enforce best-practices.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re looking at &lt;a href=\"https://coalesce.io/\"&gt;https://coalesce.io/&lt;/a&gt; . Has anyone used this tool? It&amp;#39;s a tad light on hard facts in the website. If so, what are your thoughts on its strengths / weaknesses in practice. Do you like it ? Are there any glaring gaps? Is it efficient, and does it still support complex transformations and code-first workflows when needed?&lt;/p&gt;\n\n&lt;p&gt;Really appreciate your help and insights!!&lt;/p&gt;\n\n&lt;p&gt;Thanks,PB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18e4m70", "is_robot_indexable": true, "report_reasons": null, "author": "pbower2049", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e4m70/coalesceio_thoughts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e4m70/coalesceio_thoughts/", "subreddit_subscribers": 144789, "created_utc": 1702093242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi \n\nI would like to know in what feature store technology are you using ? And what technology to calculate the features ? Especially when we want to use the features in real time (streaming) inference with low latency. Mostly aggregation features in windows\n\nDoes anyone have an experience with materialized view for aggregation and what are the pros in using it and not doing the calculation outside of the store.", "author_fullname": "t2_j15inhpup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature store for real time inference", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dmyjr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702042543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;/p&gt;\n\n&lt;p&gt;I would like to know in what feature store technology are you using ? And what technology to calculate the features ? Especially when we want to use the features in real time (streaming) inference with low latency. Mostly aggregation features in windows&lt;/p&gt;\n\n&lt;p&gt;Does anyone have an experience with materialized view for aggregation and what are the pros in using it and not doing the calculation outside of the store.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18dmyjr", "is_robot_indexable": true, "report_reasons": null, "author": "springRock88", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dmyjr/feature_store_for_real_time_inference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dmyjr/feature_store_for_real_time_inference/", "subreddit_subscribers": 144789, "created_utc": 1702042543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, everyone! Data analyst-researcher here. \n\nI have the opportunity to create a data engineering mini-project that would greatly benefit me in my current position and is an excellent chance to learn more about this field. I would be delighted to gain some experience in this aspect.\n\nHowever, I need your help to come up with a plan on how to execute this and which (preferably free) tools to use.\n\nThe issue? At work, we perform certain, so-called, measurements twice a year for several years now. Essentially, it's monitoring, and the same variables are tracked. Measurements are conducted on samples of about 10,000 people across several countries.\n\nThis data is stored in a bunch of Excel files (raw data), and some aggregations and analytics are done manually (yeah, I know). Clearly, I would like to put all of this into a data engineering context and potentially speed up and simplify the entire process. It doesn't need to have an \"input\" for new data; I would work only with the existing data for practice.\n\nI would appreciate your help in figuring out how to conceptualize the whole thing:\n\n* Based on my modest knowledge, it would be good to convert all these Excel files into, for example, an SQL database. Am I right? There is already an assigned ID for each entity in the dataset, so I can track respondents over time. Still, I understand that it wouldn't hurt to assign an \"ID\" to each timestamp/measurement, making the main table essentially a \"merge\" of all existing Excel sheets?\n* Then, once the main database is created, I would like to design an ETL process and pipeline. Here, data transformation (formatting, loading) would be beneficial. I understand that Airflow could be useful for this purpose? I'm just not sure about the concept; would ETL extract data from the (initial) database and where would it store the transformed data and in what format? JSON response?\n* In the end, I understand that at least ad-hoc analytics would be useful (e.g., pandas scripts) that would perform specific aggregations and similar tasks, and matplotlib/seaborn for visualization.\n\nI hope this isn't too confusing. I would really appreciate guidance on the technical side and suggestions for tools I can use. I'm already working with Python and would love to take this opportunity to make my job easier and learn something new. Thank you very much!", "author_fullname": "t2_eom50v9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need guidance with my first data engineering project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dm0ls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702039396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everyone! Data analyst-researcher here. &lt;/p&gt;\n\n&lt;p&gt;I have the opportunity to create a data engineering mini-project that would greatly benefit me in my current position and is an excellent chance to learn more about this field. I would be delighted to gain some experience in this aspect.&lt;/p&gt;\n\n&lt;p&gt;However, I need your help to come up with a plan on how to execute this and which (preferably free) tools to use.&lt;/p&gt;\n\n&lt;p&gt;The issue? At work, we perform certain, so-called, measurements twice a year for several years now. Essentially, it&amp;#39;s monitoring, and the same variables are tracked. Measurements are conducted on samples of about 10,000 people across several countries.&lt;/p&gt;\n\n&lt;p&gt;This data is stored in a bunch of Excel files (raw data), and some aggregations and analytics are done manually (yeah, I know). Clearly, I would like to put all of this into a data engineering context and potentially speed up and simplify the entire process. It doesn&amp;#39;t need to have an &amp;quot;input&amp;quot; for new data; I would work only with the existing data for practice.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate your help in figuring out how to conceptualize the whole thing:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Based on my modest knowledge, it would be good to convert all these Excel files into, for example, an SQL database. Am I right? There is already an assigned ID for each entity in the dataset, so I can track respondents over time. Still, I understand that it wouldn&amp;#39;t hurt to assign an &amp;quot;ID&amp;quot; to each timestamp/measurement, making the main table essentially a &amp;quot;merge&amp;quot; of all existing Excel sheets?&lt;/li&gt;\n&lt;li&gt;Then, once the main database is created, I would like to design an ETL process and pipeline. Here, data transformation (formatting, loading) would be beneficial. I understand that Airflow could be useful for this purpose? I&amp;#39;m just not sure about the concept; would ETL extract data from the (initial) database and where would it store the transformed data and in what format? JSON response?&lt;/li&gt;\n&lt;li&gt;In the end, I understand that at least ad-hoc analytics would be useful (e.g., pandas scripts) that would perform specific aggregations and similar tasks, and matplotlib/seaborn for visualization.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I hope this isn&amp;#39;t too confusing. I would really appreciate guidance on the technical side and suggestions for tools I can use. I&amp;#39;m already working with Python and would love to take this opportunity to make my job easier and learn something new. Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18dm0ls", "is_robot_indexable": true, "report_reasons": null, "author": "g4nymede_", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dm0ls/need_guidance_with_my_first_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dm0ls/need_guidance_with_my_first_data_engineering/", "subreddit_subscribers": 144789, "created_utc": 1702039396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE community,\n\nI am looking for advice.\n\nI currently work as a 'Systems Specialist' for a well known private university in the United States.\n\nThis is my first IT role and I've been here a little over two years.\n\nIn that time, I've built a portal application with Django, I rebuilt the Service Portal for our instance of ServiceNow which required working with Javascript\\\\Angular, and most notably, I rebuilt 15 ETL processes that were originally managed in a SaaS product called Snaplogic.\n\nTo build these ETL pipelines, I used SSIS, SQL Server, and the Pandas module.  This project has opened my eyes up to the world of Data Engineering\\\\Data Science and I am determined to make this work my career.\n\nCompleting this project has saved my employer over $100k a year in SaaS costs and improved the performance of many of these processes by over 50%.\n\nCan anyone give me insight into whether the experience I've gained is enough to apply for real Data Engineering roles.\n\nI have an Associate Degree in Applied Science with a focus on programming. Will my lack of a four year degree or masters hinder me that much?  \n\nI am currently taking the IBM Data Science course on Coursera because it is a certification created and backed by IBM and I thought it would look better on my resume. \n\nI appreciate everyone's time and any advice.\n\nI apologize if this isn't the right place to ask for this advice.", "author_fullname": "t2_a5epg929", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I ready?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dv0di", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702065081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE community,&lt;/p&gt;\n\n&lt;p&gt;I am looking for advice.&lt;/p&gt;\n\n&lt;p&gt;I currently work as a &amp;#39;Systems Specialist&amp;#39; for a well known private university in the United States.&lt;/p&gt;\n\n&lt;p&gt;This is my first IT role and I&amp;#39;ve been here a little over two years.&lt;/p&gt;\n\n&lt;p&gt;In that time, I&amp;#39;ve built a portal application with Django, I rebuilt the Service Portal for our instance of ServiceNow which required working with Javascript\\Angular, and most notably, I rebuilt 15 ETL processes that were originally managed in a SaaS product called Snaplogic.&lt;/p&gt;\n\n&lt;p&gt;To build these ETL pipelines, I used SSIS, SQL Server, and the Pandas module.  This project has opened my eyes up to the world of Data Engineering\\Data Science and I am determined to make this work my career.&lt;/p&gt;\n\n&lt;p&gt;Completing this project has saved my employer over $100k a year in SaaS costs and improved the performance of many of these processes by over 50%.&lt;/p&gt;\n\n&lt;p&gt;Can anyone give me insight into whether the experience I&amp;#39;ve gained is enough to apply for real Data Engineering roles.&lt;/p&gt;\n\n&lt;p&gt;I have an Associate Degree in Applied Science with a focus on programming. Will my lack of a four year degree or masters hinder me that much?  &lt;/p&gt;\n\n&lt;p&gt;I am currently taking the IBM Data Science course on Coursera because it is a certification created and backed by IBM and I thought it would look better on my resume. &lt;/p&gt;\n\n&lt;p&gt;I appreciate everyone&amp;#39;s time and any advice.&lt;/p&gt;\n\n&lt;p&gt;I apologize if this isn&amp;#39;t the right place to ask for this advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18dv0di", "is_robot_indexable": true, "report_reasons": null, "author": "Independent_Tell_461", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dv0di/am_i_ready/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dv0di/am_i_ready/", "subreddit_subscribers": 144789, "created_utc": 1702065081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHow do we calculate, budget, or project the server costs for a data science platform?\n\nWe are currently tasked with budgeting and projecting the server costs for our upcoming data science platform. I was wondering if you could provide some guidance or recommend any tools or best practices that would help us estimate these costs effectively.\n\nWe are currently using/testing AWS, Google Cloud, Hugging Face, Lambda, and OpenAI with a small amount of data, and the cost falls within the range of hundreds of dollars. However, we want to estimate the potential long-term costs we may incur.", "author_fullname": "t2_m0r3pnug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Calculating Server Costs for Data Science Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e945b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702110658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do we calculate, budget, or project the server costs for a data science platform?&lt;/p&gt;\n\n&lt;p&gt;We are currently tasked with budgeting and projecting the server costs for our upcoming data science platform. I was wondering if you could provide some guidance or recommend any tools or best practices that would help us estimate these costs effectively.&lt;/p&gt;\n\n&lt;p&gt;We are currently using/testing AWS, Google Cloud, Hugging Face, Lambda, and OpenAI with a small amount of data, and the cost falls within the range of hundreds of dollars. However, we want to estimate the potential long-term costs we may incur.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18e945b", "is_robot_indexable": true, "report_reasons": null, "author": "Alertt_53", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e945b/calculating_server_costs_for_data_science_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e945b/calculating_server_costs_for_data_science_platform/", "subreddit_subscribers": 144789, "created_utc": 1702110658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nCurrently I have a task to ingest terabyte big table from onprem SQL Server to ADLS using ADF. After that Databricks will take care of it.\n\nI am wondering if someone had similar situation and had some advice to share when it comes to ADF (and Databricks). What would be dos and donts when it comes to copying this large table. I am mostly concerned how long will it take to ingest this. If this is not optimal, any different approaches to extracting data from this table to ADLS on a schedule? \n\nAny advice is appreciated!", "author_fullname": "t2_4c7udteq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory - Copy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e8i1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702108056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;Currently I have a task to ingest terabyte big table from onprem SQL Server to ADLS using ADF. After that Databricks will take care of it.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if someone had similar situation and had some advice to share when it comes to ADF (and Databricks). What would be dos and donts when it comes to copying this large table. I am mostly concerned how long will it take to ingest this. If this is not optimal, any different approaches to extracting data from this table to ADLS on a schedule? &lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18e8i1k", "is_robot_indexable": true, "report_reasons": null, "author": "MahoYami", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e8i1k/azure_data_factory_copy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e8i1k/azure_data_factory_copy/", "subreddit_subscribers": 144789, "created_utc": 1702108056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Check out my medium article on a beginner's introduction to Apache Airflow. I have made it fairly simple for aspiring data engineers/beginners DEs to setup Apache Airflow on their local system and to test how it works. \n\nPlease share your thoughts and concerns and how I can make it better.", "author_fullname": "t2_udxf1u9n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Apache Airflow: Part 1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 91, "top_awarded_type": null, "hide_score": false, "name": "t3_18dzj73", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QtfnxdhaumrDbWv5ekvOgB0UAJKGXa4QCif1O6xSieM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702077407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out my medium article on a beginner&amp;#39;s introduction to Apache Airflow. I have made it fairly simple for aspiring data engineers/beginners DEs to setup Apache Airflow on their local system and to test how it works. &lt;/p&gt;\n\n&lt;p&gt;Please share your thoughts and concerns and how I can make it better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@sanjairaj/demystifying-apache-airflow-essential-intro-for-data-engineers-part-1-d44509b84cef", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2w-iG31fuo-ZknxGC1mmE7al32EbfmCG2gtJwEY6FZY.jpg?auto=webp&amp;s=c18fb5fb4425d3055fa2e7eda5e8139b283beb62", "width": 1200, "height": 781}, "resolutions": [{"url": "https://external-preview.redd.it/2w-iG31fuo-ZknxGC1mmE7al32EbfmCG2gtJwEY6FZY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e36b4045cd90fa1e663439c5c479eb3641c1f96f", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/2w-iG31fuo-ZknxGC1mmE7al32EbfmCG2gtJwEY6FZY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=736ec842dcc0add34addc7017f7e068d4db181f3", "width": 216, "height": 140}, {"url": "https://external-preview.redd.it/2w-iG31fuo-ZknxGC1mmE7al32EbfmCG2gtJwEY6FZY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acdf05e39a7090af01207d2e445062457270823b", "width": 320, "height": 208}, {"url": "https://external-preview.redd.it/2w-iG31fuo-ZknxGC1mmE7al32EbfmCG2gtJwEY6FZY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9816d09286b8db1c57358e83e679f3192389f765", "width": 640, "height": 416}, {"url": "https://external-preview.redd.it/2w-iG31fuo-ZknxGC1mmE7al32EbfmCG2gtJwEY6FZY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ce1849eebed318d2e1924bfc26e55c3cd8438af", "width": 960, "height": 624}, {"url": "https://external-preview.redd.it/2w-iG31fuo-ZknxGC1mmE7al32EbfmCG2gtJwEY6FZY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b09da9bb38b8a21ec50564a2885ef7972503024c", "width": 1080, "height": 702}], "variants": {}, "id": "pTVymE_EIMYTVhHJZczOUU0egxVLqOsJYqhw727ttlI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18dzj73", "is_robot_indexable": true, "report_reasons": null, "author": "Legal_Key_7212", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dzj73/introduction_to_apache_airflow_part_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@sanjairaj/demystifying-apache-airflow-essential-intro-for-data-engineers-part-1-d44509b84cef", "subreddit_subscribers": 144789, "created_utc": 1702077407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi y'all,  \n\n\nI came across a volunteer data engineer position. They'd be willing to let me learn everything I need to satisfy the following task:  \n\n\n\"Data Engineering -- two CRMs w/ backend SQL mirrors that we need to create automated syncs between\"  \n\n\nWhat would this entail? What would I need to learn to create that? Python? Which concepts?", "author_fullname": "t2_rw4pz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dxavi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702071213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi y&amp;#39;all,  &lt;/p&gt;\n\n&lt;p&gt;I came across a volunteer data engineer position. They&amp;#39;d be willing to let me learn everything I need to satisfy the following task:  &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Data Engineering -- two CRMs w/ backend SQL mirrors that we need to create automated syncs between&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;What would this entail? What would I need to learn to create that? Python? Which concepts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18dxavi", "is_robot_indexable": true, "report_reasons": null, "author": "AP3340", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dxavi/question_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dxavi/question_for_data_engineers/", "subreddit_subscribers": 144789, "created_utc": 1702071213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a simple program that connects to my postgresDB and writes out the table into parquet files. When using 1k chunk\\_size the total size of the files is \\`6.45GB\\` where as the 10k \\`chunk\\_size\\` is \\`153.23GB\\`. What am I missing? How can I investigate this further? \n\n    # %%\n    import pyarrow as pa\n    import pyarrow.parquet as pq\n    ...\n    full_path = os.path.abspath(file_path)\n    chunk_size = 1000\n    os.makedirs(full_path, exist_ok=True)\n    l(f'Writing parquet files to {full_path} in chunks of {chunk_size} rows')\n    # Process in chunks\n    def write_table(chunk, chunk_count):\n        table = pa.Table.from_pandas(chunk)\n        pq.write_to_dataset(table, root_path=file_path, use_legacy_dataset=False)\n        l(f'Processed chunk {chunk_count}') if chunk_count % 10 == 0 else None\n    \n    def main():\n            # %% Setup required SQLAlachemy for pands\n        config = dotenv.dotenv_values('../.env')\n        db_url = config.get(\"DATABASE_URL\")\n        if db_url is None:\n            raise ValueError(\"DATABASE_URL not set in ../env file\")\n        sa_engine = create_engine(db_url).execution_options(stream_results=True)\n    \n        sacon = sa_engine.connect()\n    \n        chunks = 0\n        for chunk in pd.read_sql_query(gargantum_query, sacon, chunksize=chunk_size):\n            write_table(chunk, chunks)\n            chunks += 1\n        sacon.close()\n\nI've tried to use parquet\\_tools inspect and investigate the counts but nothing seems to be out of place:\n\n    DuckDB count, only 10% row count diff duplicated rows in the 10k one\n    D select count('*') from read_parquet('/\\./feed_entries_gargantum_10k.parquet/*')\n    &gt; ;\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 count('*') \u2502\n    \u2502   int64    \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502    1797173 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    D select count('*') from read_parquet('/Users/arkone/x/ai/crawl/crawly/z/.pvc/spark/feed_entries_gargantum_1k.parquet/*') ;\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 count('*') \u2502\n    \u2502   int64    \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502    1787173 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n&amp;#x200B;", "author_fullname": "t2_img2xgzp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20x jump in Parquet size between from 1k chunk_size and 10k chunk_size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dwuow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702070005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a simple program that connects to my postgresDB and writes out the table into parquet files. When using 1k chunk_size the total size of the files is `6.45GB` where as the 10k `chunk_size` is `153.23GB`. What am I missing? How can I investigate this further? &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# %%\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n...\nfull_path = os.path.abspath(file_path)\nchunk_size = 1000\nos.makedirs(full_path, exist_ok=True)\nl(f&amp;#39;Writing parquet files to {full_path} in chunks of {chunk_size} rows&amp;#39;)\n# Process in chunks\ndef write_table(chunk, chunk_count):\n    table = pa.Table.from_pandas(chunk)\n    pq.write_to_dataset(table, root_path=file_path, use_legacy_dataset=False)\n    l(f&amp;#39;Processed chunk {chunk_count}&amp;#39;) if chunk_count % 10 == 0 else None\n\ndef main():\n        # %% Setup required SQLAlachemy for pands\n    config = dotenv.dotenv_values(&amp;#39;../.env&amp;#39;)\n    db_url = config.get(&amp;quot;DATABASE_URL&amp;quot;)\n    if db_url is None:\n        raise ValueError(&amp;quot;DATABASE_URL not set in ../env file&amp;quot;)\n    sa_engine = create_engine(db_url).execution_options(stream_results=True)\n\n    sacon = sa_engine.connect()\n\n    chunks = 0\n    for chunk in pd.read_sql_query(gargantum_query, sacon, chunksize=chunk_size):\n        write_table(chunk, chunks)\n        chunks += 1\n    sacon.close()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to use parquet_tools inspect and investigate the counts but nothing seems to be out of place:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;DuckDB count, only 10% row count diff duplicated rows in the 10k one\nD select count(&amp;#39;*&amp;#39;) from read_parquet(&amp;#39;/\\./feed_entries_gargantum_10k.parquet/*&amp;#39;)\n&amp;gt; ;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count(&amp;#39;*&amp;#39;) \u2502\n\u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    1797173 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nD select count(&amp;#39;*&amp;#39;) from read_parquet(&amp;#39;/Users/arkone/x/ai/crawl/crawly/z/.pvc/spark/feed_entries_gargantum_1k.parquet/*&amp;#39;) ;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count(&amp;#39;*&amp;#39;) \u2502\n\u2502   int64    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    1787173 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18dwuow", "is_robot_indexable": true, "report_reasons": null, "author": "yonz-", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dwuow/20x_jump_in_parquet_size_between_from_1k_chunk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dwuow/20x_jump_in_parquet_size_between_from_1k_chunk/", "subreddit_subscribers": 144789, "created_utc": 1702070005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have an interview and need help preparing.", "author_fullname": "t2_jz4beane", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Data Leads or Architects that are in the ST.Louis Missouri, I would love to connect.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18du0xh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702062353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have an interview and need help preparing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18du0xh", "is_robot_indexable": true, "report_reasons": null, "author": "Beneficial_Wish_9791", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18du0xh/any_data_leads_or_architects_that_are_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18du0xh/any_data_leads_or_architects_that_are_in_the/", "subreddit_subscribers": 144789, "created_utc": 1702062353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n(this is my first post on this community!) \n\nI am new in DE and i just working in my final project to 'show it to the world. I know that I probably have flaws in my reasoning or data modeling principles, but I would appreciate anyone who can spare a few minutes to help me.\n\nMy (big) issue is (i am using dbt tool):\n\n1. My fact table \"fct\\_orders\" has a row per product\\_id per order\\_id and most of the sales were made in 2021-02-01, represented by the column \"created\\_at\"::timestamp.\n2. I have a table \"products\" with the columns: product\\_id, name, price\".\n3. I know the price of a product can change, so i want to implement a SCD2 snapshot to track changes in price of the table \"products\".\n4. My goal: display a column with the correct price of a product\\_id in my table \"fct\\_orders\". My plan was to LEFT JOIN the snapshot with the fact table using the following join conditions:\n   1. **\"left join snapshot\\_products f on f.product\\_id = a.product\\_id  AND a.created\\_at BETWEEN f.dbt\\_valid\\_from AND f.dbt\\_valid\\_to** \"\n5. Problem: the columns that the snapshot creates, specially \"dbt\\_valit\\_from\" has a date of 2023-11-11, so i can't use the approach i thougth. So there is a  dbt\\_Valid\\_From date discrepancy with the sales. What approach should i do? \n\nFinal details:\n\n1. i am using the snapshot strategy type \"timestamp\".\n2. there are some way to \"edit\" the column \"dbt\\_valid\\_from\" to start in 2021-01-01 (for example) or it is a wrong think to do?\n\nThanks for your time and patience reading all the post.", "author_fullname": "t2_9fw6zygj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using dbt Snapshot scd2 to allow correct product_price in fact table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dq4aq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702051702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;(this is my first post on this community!) &lt;/p&gt;\n\n&lt;p&gt;I am new in DE and i just working in my final project to &amp;#39;show it to the world. I know that I probably have flaws in my reasoning or data modeling principles, but I would appreciate anyone who can spare a few minutes to help me.&lt;/p&gt;\n\n&lt;p&gt;My (big) issue is (i am using dbt tool):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;My fact table &amp;quot;fct_orders&amp;quot; has a row per product_id per order_id and most of the sales were made in 2021-02-01, represented by the column &amp;quot;created_at&amp;quot;::timestamp.&lt;/li&gt;\n&lt;li&gt;I have a table &amp;quot;products&amp;quot; with the columns: product_id, name, price&amp;quot;.&lt;/li&gt;\n&lt;li&gt;I know the price of a product can change, so i want to implement a SCD2 snapshot to track changes in price of the table &amp;quot;products&amp;quot;.&lt;/li&gt;\n&lt;li&gt;My goal: display a column with the correct price of a product_id in my table &amp;quot;fct_orders&amp;quot;. My plan was to LEFT JOIN the snapshot with the fact table using the following join conditions:\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;&amp;quot;left join snapshot_products f on f.product_id = a.product_id  AND a.created_at BETWEEN f.dbt_valid_from AND f.dbt_valid_to&lt;/strong&gt; &amp;quot;&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Problem: the columns that the snapshot creates, specially &amp;quot;dbt_valit_from&amp;quot; has a date of 2023-11-11, so i can&amp;#39;t use the approach i thougth. So there is a  dbt_Valid_From date discrepancy with the sales. What approach should i do? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Final details:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;i am using the snapshot strategy type &amp;quot;timestamp&amp;quot;.&lt;/li&gt;\n&lt;li&gt;there are some way to &amp;quot;edit&amp;quot; the column &amp;quot;dbt_valid_from&amp;quot; to start in 2021-01-01 (for example) or it is a wrong think to do?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks for your time and patience reading all the post.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18dq4aq", "is_robot_indexable": true, "report_reasons": null, "author": "Beautiful_Place_9816", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dq4aq/using_dbt_snapshot_scd2_to_allow_correct_product/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dq4aq/using_dbt_snapshot_scd2_to_allow_correct_product/", "subreddit_subscribers": 144789, "created_utc": 1702051702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " IBM recently announced a break through in quantum computing. That got me thinking - even though normalized quantum computing is while out - what does data engineering look like in that new age? What are yall's predictions? ", "author_fullname": "t2_7lvmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quantum Computing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dt300", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702059815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;IBM recently announced a break through in quantum computing. That got me thinking - even though normalized quantum computing is while out - what does data engineering look like in that new age? What are yall&amp;#39;s predictions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18dt300", "is_robot_indexable": true, "report_reasons": null, "author": "claytonjr", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dt300/quantum_computing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dt300/quantum_computing/", "subreddit_subscribers": 144789, "created_utc": 1702059815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good evening everyone, I am new to DE amd I would like to know if there's anybook or material I can read to understand how to turn complicated jsons into a dataframe", "author_fullname": "t2_35f1rdk6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Api injestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dqs2z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702053493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening everyone, I am new to DE amd I would like to know if there&amp;#39;s anybook or material I can read to understand how to turn complicated jsons into a dataframe&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18dqs2z", "is_robot_indexable": true, "report_reasons": null, "author": "Rogie_88", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dqs2z/api_injestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dqs2z/api_injestion/", "subreddit_subscribers": 144789, "created_utc": 1702053493.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}