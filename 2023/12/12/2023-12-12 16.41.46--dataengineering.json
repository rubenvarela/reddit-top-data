{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just accepted a data engineering job. Talked about graph databases, unstructured data, python, DAGs, etc during the interview. \n\nDay one I'm told the job will be making dashboards in tableau under the direction of senior data scientist. \n\nWhy?", "author_fullname": "t2_769lgtz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do job descriptions demand skills that are not at all needed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gfpn9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702362999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just accepted a data engineering job. Talked about graph databases, unstructured data, python, DAGs, etc during the interview. &lt;/p&gt;\n\n&lt;p&gt;Day one I&amp;#39;m told the job will be making dashboards in tableau under the direction of senior data scientist. &lt;/p&gt;\n\n&lt;p&gt;Why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18gfpn9", "is_robot_indexable": true, "report_reasons": null, "author": "FisterAct", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gfpn9/why_do_job_descriptions_demand_skills_that_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gfpn9/why_do_job_descriptions_demand_skills_that_are/", "subreddit_subscribers": 145555, "created_utc": 1702362999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a few years of experience as a low level data analyst and a year of data engineering. I\u2019ve been doing leetcode to prepare for a move to SWE. Today my company announced a reorg back to data ops. We are expecting to train our replacements and get laid off. \nAs the junior dev market is dead right now should I consider going back to school for a BS in CS? I have a business degree and data engineering seems suboptimal compared to back end dev to me.", "author_fullname": "t2_cimoe09n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expecting a layoff - should I go back to school?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g8ey3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702339565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a few years of experience as a low level data analyst and a year of data engineering. I\u2019ve been doing leetcode to prepare for a move to SWE. Today my company announced a reorg back to data ops. We are expecting to train our replacements and get laid off. \nAs the junior dev market is dead right now should I consider going back to school for a BS in CS? I have a business degree and data engineering seems suboptimal compared to back end dev to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18g8ey3", "is_robot_indexable": true, "report_reasons": null, "author": "OkMacaron493", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g8ey3/expecting_a_layoff_should_i_go_back_to_school/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g8ey3/expecting_a_layoff_should_i_go_back_to_school/", "subreddit_subscribers": 145555, "created_utc": 1702339565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Doing a (light-hearted) roast at work where Data Scientists poke fun at Data Engineers and vice versa. Any suggestions for content that would roast Data Engineers? \n\nAs an example, I plan to poke some fun at all the lingo Data Engineers throw around at daily stand-ups from using 30 AWS services.", "author_fullname": "t2_mvj12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Roast Content", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g3odg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702327427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doing a (light-hearted) roast at work where Data Scientists poke fun at Data Engineers and vice versa. Any suggestions for content that would roast Data Engineers? &lt;/p&gt;\n\n&lt;p&gt;As an example, I plan to poke some fun at all the lingo Data Engineers throw around at daily stand-ups from using 30 AWS services.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18g3odg", "is_robot_indexable": true, "report_reasons": null, "author": "succulentBroccoli", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g3odg/de_roast_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g3odg/de_roast_content/", "subreddit_subscribers": 145555, "created_utc": 1702327427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on a global security team and am responsible for building a system that can be used to inform people of changes to different types of risk factors with latency on the order of 15 minutes.\n\nI probably don't need streaming, but, I still wonder if Kafka is something that I should have to make it easier to perform time-series data analytics compared to writing lots of one-off analytics jobs in Python.\n\nI can think of Kafka as being beneficial because it would allow me to create OLAP cubes with Pinot or perform streaming event correlation with Flink, but, since I don't need streaming, this might not be required at all - it's hard to say - just using dagster, dbt, DuckDB, and Superset is probably more than good enough.\n\nI need to track metrics such as when new vulnerabilities are found, when the risk score of a given vulnerability changes, when a vulnerability scanner finishes running, etc., and could probably do all of this with microbatching rather than streaming.\n\nBut, does it still make sense to use Kafka as a layer of abstraction over time-series data?\n\nHere are a few other problems that I work on:\n\n* Batch jobs with multiple levels of event correlation (e.g. a user executed a command, the command resulted in the invocation of (N) subprocesses, these subprocesses produced (M) alerts, etc., but, from the perspective of better understanding what's detectable and what's not - I'm not doing incident response - more data science-y stuff).\n* Data warehousing for time-series data (e.g. historical views into attack surface in different areas of a network)\n* Anomaly detection over time-series data (e.g. out of these 10,000 security vulnerabilities, which have experienced a significant increase in predicted exploitability, which security vulnerabilities have received a significant increase in risk score, etc.).\n* Identifying gaps in sensor coverage in between vulnerability scans that run as batch jobs.\n\nI'm building everything greenfield and am looking at the following stack:\n\n* **~~Presto~~** \\+ **DuckDB** \\+ **dbt** and/or **S3** \\+ **Athena** \\+ **Glue** with **Parquet** files\n* **~~SeaweedFS~~** ~~+~~ **~~RabbitMQ~~** ~~for triggering dbt jobs, since I'd like to avoid loading data into a database until query time, and don't want materialized views - I want lots and lots of column-oriented files~~\n* **TimescaleDB** for relational and pure time-series analytics (i.e. **Postgres**)\n* **Neo4j** for a graph database - I like how its schemaless\n* **Temporal** and/or **Dagster** for workflow orchestration\n* **Metaflow** for machine learning jobs, Pandas, Pola.rs, Prophet, etc.\n* **ElasticSearch**, **Logstash**, **Kibana** \\- data will be normalized into Elastic Common Schema (ECS) and/or STIX 2.1 format, and ECS would be a great format to use when performing serverless event correlation with DuckDB, etc.\n* **Loki** or **Fluentd** for logs\n* **Prometheus** and **Grafana** for metrics (not sure what to use for long-term metric storage, maybe TimescaleDB)\n* **Access,** **Superset, Jupyter notebooks**, and **Quarto** for data exploration and visualization\n* **PowerBI** for reporting, since it's built into Microsoft 365\n* **Vagrant**, **Ansible**, **~~Consul~~**, **Vault**, **Nomad**, **Boundary** for infrastructure\n\nAgents, sensors, and microservices are written in Go and I'm still trying to figure out which protocol to use for C2 communication - gRPC seems like a good fit, but, so does RabbitMQ with ephemeral queues.\n\nI'd like to keep things as serverless as possible, but, also need to work in a hybrid cloud environment, including in airgapped environments, since I deal with ransomware among other things.\n\nI like how the following companies structure their analytics stacks:\n\n* Uber\n* Netflix\n* AirBnB", "author_fullname": "t2_lwr1wvz8q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I deal with a few hundred million time-series data points but don't need streaming - should I still use Kafka?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g8z9k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702382545.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702341159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on a global security team and am responsible for building a system that can be used to inform people of changes to different types of risk factors with latency on the order of 15 minutes.&lt;/p&gt;\n\n&lt;p&gt;I probably don&amp;#39;t need streaming, but, I still wonder if Kafka is something that I should have to make it easier to perform time-series data analytics compared to writing lots of one-off analytics jobs in Python.&lt;/p&gt;\n\n&lt;p&gt;I can think of Kafka as being beneficial because it would allow me to create OLAP cubes with Pinot or perform streaming event correlation with Flink, but, since I don&amp;#39;t need streaming, this might not be required at all - it&amp;#39;s hard to say - just using dagster, dbt, DuckDB, and Superset is probably more than good enough.&lt;/p&gt;\n\n&lt;p&gt;I need to track metrics such as when new vulnerabilities are found, when the risk score of a given vulnerability changes, when a vulnerability scanner finishes running, etc., and could probably do all of this with microbatching rather than streaming.&lt;/p&gt;\n\n&lt;p&gt;But, does it still make sense to use Kafka as a layer of abstraction over time-series data?&lt;/p&gt;\n\n&lt;p&gt;Here are a few other problems that I work on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Batch jobs with multiple levels of event correlation (e.g. a user executed a command, the command resulted in the invocation of (N) subprocesses, these subprocesses produced (M) alerts, etc., but, from the perspective of better understanding what&amp;#39;s detectable and what&amp;#39;s not - I&amp;#39;m not doing incident response - more data science-y stuff).&lt;/li&gt;\n&lt;li&gt;Data warehousing for time-series data (e.g. historical views into attack surface in different areas of a network)&lt;/li&gt;\n&lt;li&gt;Anomaly detection over time-series data (e.g. out of these 10,000 security vulnerabilities, which have experienced a significant increase in predicted exploitability, which security vulnerabilities have received a significant increase in risk score, etc.).&lt;/li&gt;\n&lt;li&gt;Identifying gaps in sensor coverage in between vulnerability scans that run as batch jobs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m building everything greenfield and am looking at the following stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;del&gt;Presto&lt;/del&gt;&lt;/strong&gt; + &lt;strong&gt;DuckDB&lt;/strong&gt; + &lt;strong&gt;dbt&lt;/strong&gt; and/or &lt;strong&gt;S3&lt;/strong&gt; + &lt;strong&gt;Athena&lt;/strong&gt; + &lt;strong&gt;Glue&lt;/strong&gt; with &lt;strong&gt;Parquet&lt;/strong&gt; files&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;del&gt;SeaweedFS&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;+&lt;/del&gt; &lt;strong&gt;&lt;del&gt;RabbitMQ&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;for triggering dbt jobs, since I&amp;#39;d like to avoid loading data into a database until query time, and don&amp;#39;t want materialized views - I want lots and lots of column-oriented files&lt;/del&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;TimescaleDB&lt;/strong&gt; for relational and pure time-series analytics (i.e. &lt;strong&gt;Postgres&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Neo4j&lt;/strong&gt; for a graph database - I like how its schemaless&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Temporal&lt;/strong&gt; and/or &lt;strong&gt;Dagster&lt;/strong&gt; for workflow orchestration&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Metaflow&lt;/strong&gt; for machine learning jobs, Pandas, Pola.rs, Prophet, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ElasticSearch&lt;/strong&gt;, &lt;strong&gt;Logstash&lt;/strong&gt;, &lt;strong&gt;Kibana&lt;/strong&gt; - data will be normalized into Elastic Common Schema (ECS) and/or STIX 2.1 format, and ECS would be a great format to use when performing serverless event correlation with DuckDB, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Loki&lt;/strong&gt; or &lt;strong&gt;Fluentd&lt;/strong&gt; for logs&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Prometheus&lt;/strong&gt; and &lt;strong&gt;Grafana&lt;/strong&gt; for metrics (not sure what to use for long-term metric storage, maybe TimescaleDB)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Access,&lt;/strong&gt; &lt;strong&gt;Superset, Jupyter notebooks&lt;/strong&gt;, and &lt;strong&gt;Quarto&lt;/strong&gt; for data exploration and visualization&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PowerBI&lt;/strong&gt; for reporting, since it&amp;#39;s built into Microsoft 365&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Vagrant&lt;/strong&gt;, &lt;strong&gt;Ansible&lt;/strong&gt;, &lt;strong&gt;&lt;del&gt;Consul&lt;/del&gt;&lt;/strong&gt;, &lt;strong&gt;Vault&lt;/strong&gt;, &lt;strong&gt;Nomad&lt;/strong&gt;, &lt;strong&gt;Boundary&lt;/strong&gt; for infrastructure&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Agents, sensors, and microservices are written in Go and I&amp;#39;m still trying to figure out which protocol to use for C2 communication - gRPC seems like a good fit, but, so does RabbitMQ with ephemeral queues.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to keep things as serverless as possible, but, also need to work in a hybrid cloud environment, including in airgapped environments, since I deal with ransomware among other things.&lt;/p&gt;\n\n&lt;p&gt;I like how the following companies structure their analytics stacks:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Uber&lt;/li&gt;\n&lt;li&gt;Netflix&lt;/li&gt;\n&lt;li&gt;AirBnB&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18g8z9k", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Importance-1605", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18g8z9k/i_deal_with_a_few_hundred_million_timeseries_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g8z9k/i_deal_with_a_few_hundred_million_timeseries_data/", "subreddit_subscribers": 145555, "created_utc": 1702341159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just wanted to have a discussion with folks with experienced similar things but getting burned out working with messy data. There\u2019s always something that the business wants to add the data is either incomplete or it doesn\u2019t jam with the data model because it\u2019s at a different grain. You can speak up as much as you want but the business just claims it\u2019s super important and so you make an exception to the rule only to have more crappy data come in and screw up an entire data model that you\u2019ve built. \n\nNot looking for any advice, I would just like to hear everyone\u2019s story.", "author_fullname": "t2_5ukitegd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Burnout", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g6a5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702333835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wanted to have a discussion with folks with experienced similar things but getting burned out working with messy data. There\u2019s always something that the business wants to add the data is either incomplete or it doesn\u2019t jam with the data model because it\u2019s at a different grain. You can speak up as much as you want but the business just claims it\u2019s super important and so you make an exception to the rule only to have more crappy data come in and screw up an entire data model that you\u2019ve built. &lt;/p&gt;\n\n&lt;p&gt;Not looking for any advice, I would just like to hear everyone\u2019s story.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18g6a5o", "is_robot_indexable": true, "report_reasons": null, "author": "burningburnerbern", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g6a5o/data_burnout/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g6a5o/data_burnout/", "subreddit_subscribers": 145555, "created_utc": 1702333835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Although I work as a Machine Learning Engineer, sometimes I'm requested to build some queries, for instance for dashboarding purposes.\n\nHowever, I find it really tedious when working with SQL. My main reasons being:\n\n1. Most of the times we work with tables in the DataLake we don't own, hence it takes an awful lot of time to get an understanding of how that data is structured (and what kind of data problems it might have)\n2. I feel really unproductive having to wait for query results. It really slows down any kind of exploration one might want do with the data like one would with tools like Pandas or Polars (and loading data locally is not an option as we're talking of billions of records here, which we're handling with Spark)\n3. Sometimes queries grow to be extremely complex, which makes it harder for team mates to review\n4. Along the previous point, I really feel SQL is extremely unreadable as compared to a programming language", "author_fullname": "t2_21rg1aff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you make working with SQL enjoyable (or less tedious)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gn43u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702391339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Although I work as a Machine Learning Engineer, sometimes I&amp;#39;m requested to build some queries, for instance for dashboarding purposes.&lt;/p&gt;\n\n&lt;p&gt;However, I find it really tedious when working with SQL. My main reasons being:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Most of the times we work with tables in the DataLake we don&amp;#39;t own, hence it takes an awful lot of time to get an understanding of how that data is structured (and what kind of data problems it might have)&lt;/li&gt;\n&lt;li&gt;I feel really unproductive having to wait for query results. It really slows down any kind of exploration one might want do with the data like one would with tools like Pandas or Polars (and loading data locally is not an option as we&amp;#39;re talking of billions of records here, which we&amp;#39;re handling with Spark)&lt;/li&gt;\n&lt;li&gt;Sometimes queries grow to be extremely complex, which makes it harder for team mates to review&lt;/li&gt;\n&lt;li&gt;Along the previous point, I really feel SQL is extremely unreadable as compared to a programming language&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18gn43u", "is_robot_indexable": true, "report_reasons": null, "author": "barberogaston", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gn43u/how_do_you_make_working_with_sql_enjoyable_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gn43u/how_do_you_make_working_with_sql_enjoyable_or/", "subreddit_subscribers": 145555, "created_utc": 1702391339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been going through a few rounds of interviews for my first DE job, and while Im excited to break in to that 'role' from where I'm at now, I'm a bit concerned with the QoL the company seems to have. \n\nMost reviews Ive seen say its a grind, and that it's nonstop - but none of those kinds of reviews exist for the DE team I'd be on - A PS team serving clients data needs and projects. \n\nOn top of that, there are some red flags Ive seen, like the person who would be my manager telling me \"*Our first core tenant is that we don't hire assholes*\". They also 'live on the bleeding edge' and aren't afraid to pivot to new tech asap if old tech isn't working out. I asked how they every skill up and get used to the inner workings of tools they're using if theyre changing tech every 3 months, but he gave a real non-answer.\n\n\nIt has a weirdly 'bro-culture' vibe, and though the glassdoor reviews aren't overtly negative (~3.6), [and actually higher than my current company I work at and like] I still have a weird vibe about the place. I'm just on the fence about it because of the red flags.\n \n**I know DE is usually seen as an internal role to get data to different teams for reporting, so I'm curious if anyone else has worked in a similar client-facing DE role and could talk about it.** \n\nIt'd be like a ~40% raise to what Im making now (putting me at around 130,000) and they use the tools I want to learn that I can't use in my current day to day job (PySpark/Hadoop, PyTorch, and others) so I figure I can grind it out a year to get the skills then bounce - but Im someone who is very focused on maintaining a WLB and it sounds like they don't really have one.", "author_fullname": "t2_f6oir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone do DE on a 'professional services' client-facing team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fxx4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702310695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been going through a few rounds of interviews for my first DE job, and while Im excited to break in to that &amp;#39;role&amp;#39; from where I&amp;#39;m at now, I&amp;#39;m a bit concerned with the QoL the company seems to have. &lt;/p&gt;\n\n&lt;p&gt;Most reviews Ive seen say its a grind, and that it&amp;#39;s nonstop - but none of those kinds of reviews exist for the DE team I&amp;#39;d be on - A PS team serving clients data needs and projects. &lt;/p&gt;\n\n&lt;p&gt;On top of that, there are some red flags Ive seen, like the person who would be my manager telling me &amp;quot;&lt;em&gt;Our first core tenant is that we don&amp;#39;t hire assholes&lt;/em&gt;&amp;quot;. They also &amp;#39;live on the bleeding edge&amp;#39; and aren&amp;#39;t afraid to pivot to new tech asap if old tech isn&amp;#39;t working out. I asked how they every skill up and get used to the inner workings of tools they&amp;#39;re using if theyre changing tech every 3 months, but he gave a real non-answer.&lt;/p&gt;\n\n&lt;p&gt;It has a weirdly &amp;#39;bro-culture&amp;#39; vibe, and though the glassdoor reviews aren&amp;#39;t overtly negative (~3.6), [and actually higher than my current company I work at and like] I still have a weird vibe about the place. I&amp;#39;m just on the fence about it because of the red flags.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I know DE is usually seen as an internal role to get data to different teams for reporting, so I&amp;#39;m curious if anyone else has worked in a similar client-facing DE role and could talk about it.&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;d be like a ~40% raise to what Im making now (putting me at around 130,000) and they use the tools I want to learn that I can&amp;#39;t use in my current day to day job (PySpark/Hadoop, PyTorch, and others) so I figure I can grind it out a year to get the skills then bounce - but Im someone who is very focused on maintaining a WLB and it sounds like they don&amp;#39;t really have one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18fxx4a", "is_robot_indexable": true, "report_reasons": null, "author": "XxNerdAtHeartxX", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fxx4a/does_anyone_do_de_on_a_professional_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fxx4a/does_anyone_do_de_on_a_professional_services/", "subreddit_subscribers": 145555, "created_utc": 1702310695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My firm is looking to fill a new position. Because this person manipulates data without modeling it, the HIPPOS currently think the correct title is Data Engineer. I think this title is wrong and will make it harder to find the right person.\n\nThis job will: (1) gather requirements from the scientists (2) understand what's in the data stores (3) write study-specific Python/R pipelines to turn the data from the stores into the (generally highly specified) format the scientists need (4) make sure all the data from studies is put away safely unto future generations.\n\nSeems to me the following titles might be better:\n\n\\- Research software engineer -- except there aren't releases really, just pipelines that are sort of project-specific\n\n\\- Algorithm engineer -- covers that they will be manipulating numeric data and such. But, this often means fancy stuff like ML algorithms which won't be happening\n\n\\- Data analyst -- except the final analysis is by the scientists\n\n\\- Python/R software engineer -- I am leaning to this one although again, software products won't be shipping\n\nThank you for any thoughts and have a pleasant day.  \n", "author_fullname": "t2_2egw0f8r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling with title for \"data pipeline\" job position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g3xxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702328101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My firm is looking to fill a new position. Because this person manipulates data without modeling it, the HIPPOS currently think the correct title is Data Engineer. I think this title is wrong and will make it harder to find the right person.&lt;/p&gt;\n\n&lt;p&gt;This job will: (1) gather requirements from the scientists (2) understand what&amp;#39;s in the data stores (3) write study-specific Python/R pipelines to turn the data from the stores into the (generally highly specified) format the scientists need (4) make sure all the data from studies is put away safely unto future generations.&lt;/p&gt;\n\n&lt;p&gt;Seems to me the following titles might be better:&lt;/p&gt;\n\n&lt;p&gt;- Research software engineer -- except there aren&amp;#39;t releases really, just pipelines that are sort of project-specific&lt;/p&gt;\n\n&lt;p&gt;- Algorithm engineer -- covers that they will be manipulating numeric data and such. But, this often means fancy stuff like ML algorithms which won&amp;#39;t be happening&lt;/p&gt;\n\n&lt;p&gt;- Data analyst -- except the final analysis is by the scientists&lt;/p&gt;\n\n&lt;p&gt;- Python/R software engineer -- I am leaning to this one although again, software products won&amp;#39;t be shipping&lt;/p&gt;\n\n&lt;p&gt;Thank you for any thoughts and have a pleasant day.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18g3xxk", "is_robot_indexable": true, "report_reasons": null, "author": "edinburghpotsdam", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g3xxk/struggling_with_title_for_data_pipeline_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g3xxk/struggling_with_title_for_data_pipeline_job/", "subreddit_subscribers": 145555, "created_utc": 1702328101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data in S3 like this (for example)\n\n&amp;#x200B;\n\n|NAME|COUNTRY|SOMETHING|OPERATION|TIMESTAMP|\n|:-|:-|:-|:-|:-|\n|steve|usa|hello|insert|1702390270|\n|joe|canada|hi|insert|1702390271|\n|steve|usa|howdy|update|1702390272|\n\nWhat is the fastest way that you would get this data into Redshit BUT with the final table \"deduplicated\"? (that is, where Steve only appears once and with value for \\`something\\` field being \\`howdy\\`)\n\nEDIT: This may seem like something I can google, but it's not. We actually already achieve this in a couple of ways, but we are not happy with latency. This question was more to see how *you* achieve something like this. Please and thank you.\n\nEDIT: Thanks to a commenter, I realize that I failed to include a timestamp column in my example, making this question very confusing. I'm sorry.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data from S3 to Redshift, but Deduplicated", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g1mam", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702390286.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702322434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data in S3 like this (for example)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;NAME&lt;/th&gt;\n&lt;th align=\"left\"&gt;COUNTRY&lt;/th&gt;\n&lt;th align=\"left\"&gt;SOMETHING&lt;/th&gt;\n&lt;th align=\"left\"&gt;OPERATION&lt;/th&gt;\n&lt;th align=\"left\"&gt;TIMESTAMP&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;steve&lt;/td&gt;\n&lt;td align=\"left\"&gt;usa&lt;/td&gt;\n&lt;td align=\"left\"&gt;hello&lt;/td&gt;\n&lt;td align=\"left\"&gt;insert&lt;/td&gt;\n&lt;td align=\"left\"&gt;1702390270&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;joe&lt;/td&gt;\n&lt;td align=\"left\"&gt;canada&lt;/td&gt;\n&lt;td align=\"left\"&gt;hi&lt;/td&gt;\n&lt;td align=\"left\"&gt;insert&lt;/td&gt;\n&lt;td align=\"left\"&gt;1702390271&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;steve&lt;/td&gt;\n&lt;td align=\"left\"&gt;usa&lt;/td&gt;\n&lt;td align=\"left\"&gt;howdy&lt;/td&gt;\n&lt;td align=\"left\"&gt;update&lt;/td&gt;\n&lt;td align=\"left\"&gt;1702390272&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;What is the fastest way that you would get this data into Redshit BUT with the final table &amp;quot;deduplicated&amp;quot;? (that is, where Steve only appears once and with value for `something` field being `howdy`)&lt;/p&gt;\n\n&lt;p&gt;EDIT: This may seem like something I can google, but it&amp;#39;s not. We actually already achieve this in a couple of ways, but we are not happy with latency. This question was more to see how &lt;em&gt;you&lt;/em&gt; achieve something like this. Please and thank you.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thanks to a commenter, I realize that I failed to include a timestamp column in my example, making this question very confusing. I&amp;#39;m sorry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18g1mam", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g1mam/data_from_s3_to_redshift_but_deduplicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g1mam/data_from_s3_to_redshift_but_deduplicated/", "subreddit_subscribers": 145555, "created_utc": 1702322434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m helping a friend who\u2019s writing a paper on a hospital dataset he found. I promised him that would help do some data analysis (I have a data science background), but didn\u2019t expect the dataset to be so big. It\u2019s basically 4 csv files all zipped. Unzipped 3 of them are roughly 20GB each, while the fourth one was only 1GB. Usually when I had to deal with datasets this large it was already in sql so it was much easier to look at. At the moment I can\u2019t even unzip the files on my computer. Any takes on how to get started? I have an AWS account (from my startup) with some credit I can use. I already uploaded the 1GB files into s3 then amazon redshift. Would love to do the same for the other files but open to more ideas", "author_fullname": "t2_tm2yzhjm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opening a giant dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gb7dp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702347853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m helping a friend who\u2019s writing a paper on a hospital dataset he found. I promised him that would help do some data analysis (I have a data science background), but didn\u2019t expect the dataset to be so big. It\u2019s basically 4 csv files all zipped. Unzipped 3 of them are roughly 20GB each, while the fourth one was only 1GB. Usually when I had to deal with datasets this large it was already in sql so it was much easier to look at. At the moment I can\u2019t even unzip the files on my computer. Any takes on how to get started? I have an AWS account (from my startup) with some credit I can use. I already uploaded the 1GB files into s3 then amazon redshift. Would love to do the same for the other files but open to more ideas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18gb7dp", "is_robot_indexable": true, "report_reasons": null, "author": "vald_eagle", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gb7dp/opening_a_giant_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gb7dp/opening_a_giant_dataset/", "subreddit_subscribers": 145555, "created_utc": 1702347853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So i'm currently in my fifth year of school with a economics and cs dual degree. I work as a data analyst (not intern) at a pharmaceutical consulting company. I do a lot of excel, power BI, data processing, and help broker data deals between data vendors and buyers. I had a good connection so thats how I landed this job while still in school. I am just nervous that I am not getting the practical experience or learning the skills I should be doing to land a DE job. I am not sure what jobs/internships I should be looking for to put me on the path to become a DE. However, my boss has recently told me to experimenting with python as a tool for data analytics and how we can implement different strategies into the company. I get paid for this time and have been learning a lot which is good. I have been also learning SQL. I was thinking about bringing up to him the fact that I want to be a DE and how I can learn while working with the company to do this and possibly change my job title in the future. However, I feel like I would not be learning very good as we do not have any DE already. Should I stay at this role or start looking at applying for other jobs? Would it be a good idea to develop my skills and experience than start looking at DE roles? ", "author_fullname": "t2_2z2wymyd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some guidance on what path I should take", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g6eub", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702334171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i&amp;#39;m currently in my fifth year of school with a economics and cs dual degree. I work as a data analyst (not intern) at a pharmaceutical consulting company. I do a lot of excel, power BI, data processing, and help broker data deals between data vendors and buyers. I had a good connection so thats how I landed this job while still in school. I am just nervous that I am not getting the practical experience or learning the skills I should be doing to land a DE job. I am not sure what jobs/internships I should be looking for to put me on the path to become a DE. However, my boss has recently told me to experimenting with python as a tool for data analytics and how we can implement different strategies into the company. I get paid for this time and have been learning a lot which is good. I have been also learning SQL. I was thinking about bringing up to him the fact that I want to be a DE and how I can learn while working with the company to do this and possibly change my job title in the future. However, I feel like I would not be learning very good as we do not have any DE already. Should I stay at this role or start looking at applying for other jobs? Would it be a good idea to develop my skills and experience than start looking at DE roles? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18g6eub", "is_robot_indexable": true, "report_reasons": null, "author": "cailloudragonballs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g6eub/need_some_guidance_on_what_path_i_should_take/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g6eub/need_some_guidance_on_what_path_i_should_take/", "subreddit_subscribers": 145555, "created_utc": 1702334171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is not entirely dedicated to data engineering, but it has touch points.\n\n **Some context:**\n\nIn our small to medium-sized enterprise operating in a production environment with a myriad of applications, the majority of which have point-to-point interfaces with our ERP (a local SQL database), and a considerable amount of legacy systems, the IT team, consisting of just two members, manages all applications and integrations. It's noteworthy that we lack in-house developers. Many of our integrations rely on XML files for data exchange, employing SQL and mapping tools like Altova and sometimes even BizTalk.\n\nLooking ahead, by the end of 2024, we plan to implement a new ERP, likely Dynamics 365 F&amp;O. This transition necessitates the rebuilding of numerous integrations to align with the new ERP. Ideally, we aim to centralize these integrations for improved oversight, addressing issues proactively rather than reactively as is often the case currently.\n\nOur challenges are further compounded when dealing with modern web applications utilizing RESTAPI, especially with our legacy ERP. Typically, we resort to scripting PowerShell and requesting vendors to set up an SFTP server for XML file exchange, incurring additional costs for their extra efforts. To address these issues, we have explored integration platforms such as Mulesoft, Workato, and Boomi.\n\nBoomi was discarded after practical testing due to perceived shortcomings. \n\nMulesoft proved comprehensible, enabling successful use cases, yet its deployment and maintenance seemed burdensome. \n\nWorkato, while lacking robust debugging and offering basic logging, demonstrated ease of use in creating integrations, aligning well with the simplicity of our integration needs. Its variety of connectors reduces the need for custom wrappers or connectors, and it presents automation possibilities.\n\nSome of the use cases that I finalized in the 2 last tools are:  \n\\- I was able to build an extensive integration between ERP and Salesforce, \n\n\\- create an API based on openapi specs and send data between ERP and the web app\n\n\\- do data transformation into an XML file with iterations \n\n**The question at hand** is whether an Integration Platform as a Service (iPaaS) is a suitable solution for our scenario, given that we do not anticipate hiring an in-house developer. Additionally, are there other platforms or tools we might be overlooking? We are seeking insights from those with experience in the listed tools, with Workato emerging as our preferred choice due to its balance of simplicity and functionality, even though Mulesoft offers more robust features. The overarching question is whether the additional functionalities of Mulesoft are necessary for our specific requirements.\n\nTL;DR: We're a small-to-medium enterprise handling various applications with point-to-point interfaces and legacy systems. Two-person IT team, no in-house developers. Looking to rebuild integrations for a new ERP (probably Dynamics 365 F&amp;O) by 2024. Currently considering Workato, Mulesoft, and Boomi. Workato seems suitable due to simplicity and variety of connectors. Wondering if Integration Platform as a Service (iPaaS) is the right solution for us and seeking insights on other tools or experiences with the mentioned platforms. Open to suggestions given our context and the absence of in-house developers.", "author_fullname": "t2_8mnqz0ek", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about IPAAS platforms (Mulesoft, Workato,...)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gm0jn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702388156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is not entirely dedicated to data engineering, but it has touch points.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Some context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In our small to medium-sized enterprise operating in a production environment with a myriad of applications, the majority of which have point-to-point interfaces with our ERP (a local SQL database), and a considerable amount of legacy systems, the IT team, consisting of just two members, manages all applications and integrations. It&amp;#39;s noteworthy that we lack in-house developers. Many of our integrations rely on XML files for data exchange, employing SQL and mapping tools like Altova and sometimes even BizTalk.&lt;/p&gt;\n\n&lt;p&gt;Looking ahead, by the end of 2024, we plan to implement a new ERP, likely Dynamics 365 F&amp;amp;O. This transition necessitates the rebuilding of numerous integrations to align with the new ERP. Ideally, we aim to centralize these integrations for improved oversight, addressing issues proactively rather than reactively as is often the case currently.&lt;/p&gt;\n\n&lt;p&gt;Our challenges are further compounded when dealing with modern web applications utilizing RESTAPI, especially with our legacy ERP. Typically, we resort to scripting PowerShell and requesting vendors to set up an SFTP server for XML file exchange, incurring additional costs for their extra efforts. To address these issues, we have explored integration platforms such as Mulesoft, Workato, and Boomi.&lt;/p&gt;\n\n&lt;p&gt;Boomi was discarded after practical testing due to perceived shortcomings. &lt;/p&gt;\n\n&lt;p&gt;Mulesoft proved comprehensible, enabling successful use cases, yet its deployment and maintenance seemed burdensome. &lt;/p&gt;\n\n&lt;p&gt;Workato, while lacking robust debugging and offering basic logging, demonstrated ease of use in creating integrations, aligning well with the simplicity of our integration needs. Its variety of connectors reduces the need for custom wrappers or connectors, and it presents automation possibilities.&lt;/p&gt;\n\n&lt;p&gt;Some of the use cases that I finalized in the 2 last tools are:&lt;br/&gt;\n- I was able to build an extensive integration between ERP and Salesforce, &lt;/p&gt;\n\n&lt;p&gt;- create an API based on openapi specs and send data between ERP and the web app&lt;/p&gt;\n\n&lt;p&gt;- do data transformation into an XML file with iterations &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The question at hand&lt;/strong&gt; is whether an Integration Platform as a Service (iPaaS) is a suitable solution for our scenario, given that we do not anticipate hiring an in-house developer. Additionally, are there other platforms or tools we might be overlooking? We are seeking insights from those with experience in the listed tools, with Workato emerging as our preferred choice due to its balance of simplicity and functionality, even though Mulesoft offers more robust features. The overarching question is whether the additional functionalities of Mulesoft are necessary for our specific requirements.&lt;/p&gt;\n\n&lt;p&gt;TL;DR: We&amp;#39;re a small-to-medium enterprise handling various applications with point-to-point interfaces and legacy systems. Two-person IT team, no in-house developers. Looking to rebuild integrations for a new ERP (probably Dynamics 365 F&amp;amp;O) by 2024. Currently considering Workato, Mulesoft, and Boomi. Workato seems suitable due to simplicity and variety of connectors. Wondering if Integration Platform as a Service (iPaaS) is the right solution for us and seeking insights on other tools or experiences with the mentioned platforms. Open to suggestions given our context and the absence of in-house developers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18gm0jn", "is_robot_indexable": true, "report_reasons": null, "author": "LangeHamburger", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gm0jn/question_about_ipaas_platforms_mulesoft_workato/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gm0jn/question_about_ipaas_platforms_mulesoft_workato/", "subreddit_subscribers": 145555, "created_utc": 1702388156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm looking to test Airbyte OSS and have deployed the product locally on WSL using the instructions [here](https://docs.airbyte.com/deploying-airbyte/local-deployment). This has create a local repo with all the Airbyte files.\n\nI am looking to use these files to create a connector to ADLS, which is a standard connector on Airbyte, however, all of the documentation is about using Airbyte Cloud and the UI, which I don't want to do. I want to deploy an ADLS connector locally then use Docker to publish to Azure Container Registry, then self-host this container in a VM.\n\nI know how to use Docker and Azure, but am stumped with the first stage. Is there any documentation about how to create a connector using OSS rather than the Cloud version of the product?\n\nThanks, ", "author_fullname": "t2_ocur3kkm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to create connectors for Self-Hosted Airbyte?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gky16", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702384695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to test Airbyte OSS and have deployed the product locally on WSL using the instructions &lt;a href=\"https://docs.airbyte.com/deploying-airbyte/local-deployment\"&gt;here&lt;/a&gt;. This has create a local repo with all the Airbyte files.&lt;/p&gt;\n\n&lt;p&gt;I am looking to use these files to create a connector to ADLS, which is a standard connector on Airbyte, however, all of the documentation is about using Airbyte Cloud and the UI, which I don&amp;#39;t want to do. I want to deploy an ADLS connector locally then use Docker to publish to Azure Container Registry, then self-host this container in a VM.&lt;/p&gt;\n\n&lt;p&gt;I know how to use Docker and Azure, but am stumped with the first stage. Is there any documentation about how to create a connector using OSS rather than the Cloud version of the product?&lt;/p&gt;\n\n&lt;p&gt;Thanks, &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18gky16", "is_robot_indexable": true, "report_reasons": null, "author": "OutlandishnessOdd695", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gky16/how_to_create_connectors_for_selfhosted_airbyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gky16/how_to_create_connectors_for_selfhosted_airbyte/", "subreddit_subscribers": 145555, "created_utc": 1702384695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_e2nemx2x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Text Editor Data Structures: Rethinking Undo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18gkb4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/3XhzYyw8teAAoZteZx93zMB8muM72xfcryWrIFISW94.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702382410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cdacamar.github.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cdacamar.github.io/data%20structures/algorithms/benchmarking/text%20editors/c++/rethinking-undo/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_O1NAjuFVHPWSQrxpn9UxuSTuWqP-gDytGjdnGiG5LA.jpg?auto=webp&amp;s=dc979df84fccce2dd97f6684151bc6706e699844", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/_O1NAjuFVHPWSQrxpn9UxuSTuWqP-gDytGjdnGiG5LA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b79a1d8c7c180efe26989df2f93f864bf0e11179", "width": 108, "height": 108}], "variants": {}, "id": "054kh5x7H-LaMjWU8y5O6b3KQqF24aoISBGMUIydDbQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18gkb4r", "is_robot_indexable": true, "report_reasons": null, "author": "askredtoy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gkb4r/text_editor_data_structures_rethinking_undo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cdacamar.github.io/data%20structures/algorithms/benchmarking/text%20editors/c++/rethinking-undo/", "subreddit_subscribers": 145555, "created_utc": 1702382410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Kind of a noob in Snowflake and need some help. The situation is a little more complicated but I\u2019ll keep it simple. We have a table that was built in Database A with Role X. The privileges for Role X were changed to no longer include access to Database A, and it says Role X is the owner of the table still. The only role Role X has been granted to is ACCOUNTADMIN. Is getting someone with ACCOUNTADMIN access our only option to update schema/delete these tables?", "author_fullname": "t2_oaue187y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Roles/Privileges", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gcwvj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702353247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kind of a noob in Snowflake and need some help. The situation is a little more complicated but I\u2019ll keep it simple. We have a table that was built in Database A with Role X. The privileges for Role X were changed to no longer include access to Database A, and it says Role X is the owner of the table still. The only role Role X has been granted to is ACCOUNTADMIN. Is getting someone with ACCOUNTADMIN access our only option to update schema/delete these tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18gcwvj", "is_robot_indexable": true, "report_reasons": null, "author": "Big_Pomegranate8943", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gcwvj/snowflake_rolesprivileges/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gcwvj/snowflake_rolesprivileges/", "subreddit_subscribers": 145555, "created_utc": 1702353247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a freelancer and this is my first job and I don\u2019t know how to collect the data the client needs. Is it something that can be done? \n\nHere is whole story\n\u201cI'm looking for a person who can do a very specific task. Your profile tells me that you may be the right person. In principle it is like this: Messages are posted in a Telegram channel (always in the same format). These messages contain information in the form of numbers and possibly text. This information should be extracted as soon as the post enters the channel and entered into a database. This database should then be calculated and evaluated so that a live time web dashboard shows insights. The customers who are on my website should be able to view the dashboard on my website. Of course, the dashboard should always be up to date. The amount of data is extremely small and the statistical analysis requires little effort, as it only involves simple insights. Am I at the right place with you or should I look for someone else?\u201d \n\nHow can I start ?", "author_fullname": "t2_fxlwknu3u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help/ my first project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g3tu5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702327813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a freelancer and this is my first job and I don\u2019t know how to collect the data the client needs. Is it something that can be done? &lt;/p&gt;\n\n&lt;p&gt;Here is whole story\n\u201cI&amp;#39;m looking for a person who can do a very specific task. Your profile tells me that you may be the right person. In principle it is like this: Messages are posted in a Telegram channel (always in the same format). These messages contain information in the form of numbers and possibly text. This information should be extracted as soon as the post enters the channel and entered into a database. This database should then be calculated and evaluated so that a live time web dashboard shows insights. The customers who are on my website should be able to view the dashboard on my website. Of course, the dashboard should always be up to date. The amount of data is extremely small and the statistical analysis requires little effort, as it only involves simple insights. Am I at the right place with you or should I look for someone else?\u201d &lt;/p&gt;\n\n&lt;p&gt;How can I start ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18g3tu5", "is_robot_indexable": true, "report_reasons": null, "author": "AvailableField7708", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g3tu5/help_my_first_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g3tu5/help_my_first_project/", "subreddit_subscribers": 145555, "created_utc": 1702327813.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an analytics engineer working in Snowflake/dbt and part of my role is supporting two business analysts who build Tableau dashboards. Small data team, so I'm the Tableau admin as I used to be the one and only analyst. Right now, I build them whatever data sources they need in a presentation-layer Snowflake database. They connect Tableau to Snowflake and do an extract. While this works, it seems to lead to lots of extract duplication as they will use the same tables over and over for different dashboards. My thought is that I could create a single extract for each Presentation-layer table via a Tableau Virtual Connection (or whatever PBI equivalent is, chance we may end up switching next year). With this approach, I'd cut down on duplicate extracts and could even link to dbt documentation in the data source description. Using a VC, they can still join tables together within Tableau.\n\nI'm curious how everyone else handles this as BI scales within an organization, especially a non-technical one.", "author_fullname": "t2_ee8slplxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for exposing DWH tables to Tableau/Power BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g30px", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702325815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an analytics engineer working in Snowflake/dbt and part of my role is supporting two business analysts who build Tableau dashboards. Small data team, so I&amp;#39;m the Tableau admin as I used to be the one and only analyst. Right now, I build them whatever data sources they need in a presentation-layer Snowflake database. They connect Tableau to Snowflake and do an extract. While this works, it seems to lead to lots of extract duplication as they will use the same tables over and over for different dashboards. My thought is that I could create a single extract for each Presentation-layer table via a Tableau Virtual Connection (or whatever PBI equivalent is, chance we may end up switching next year). With this approach, I&amp;#39;d cut down on duplicate extracts and could even link to dbt documentation in the data source description. Using a VC, they can still join tables together within Tableau.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious how everyone else handles this as BI scales within an organization, especially a non-technical one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18g30px", "is_robot_indexable": true, "report_reasons": null, "author": "Known-Huckleberry-55", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g30px/best_practices_for_exposing_dwh_tables_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g30px/best_practices_for_exposing_dwh_tables_to/", "subreddit_subscribers": 145555, "created_utc": 1702325815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nI am no newbie to data migration using AWS tech and more. However, I would love to pick the brain of someone who is working with a reliable stack that serves a Business Intelligence team or Data Warehouse population.\n\nWould anyone care to meet on discord to discuss such topics? If you're company is using a combination of AWS and open-source tools to get the job done, and the job IS getting done, I really want to chat with you. The one constraint I would add is that I am not interested in any tooling that requires sending your data to a third-party network for treatment.\n\nTo be clear: I am not looking for work, I am not selling anything, and I am not a beginner. I am sincerely looking for advice and use cases to enrich my understanding in this field.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Request for mentor in AWS Land (for intermediate-advanced topics)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18g00ax", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702317252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I am no newbie to data migration using AWS tech and more. However, I would love to pick the brain of someone who is working with a reliable stack that serves a Business Intelligence team or Data Warehouse population.&lt;/p&gt;\n\n&lt;p&gt;Would anyone care to meet on discord to discuss such topics? If you&amp;#39;re company is using a combination of AWS and open-source tools to get the job done, and the job IS getting done, I really want to chat with you. The one constraint I would add is that I am not interested in any tooling that requires sending your data to a third-party network for treatment.&lt;/p&gt;\n\n&lt;p&gt;To be clear: I am not looking for work, I am not selling anything, and I am not a beginner. I am sincerely looking for advice and use cases to enrich my understanding in this field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18g00ax", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g00ax/request_for_mentor_in_aws_land_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g00ax/request_for_mentor_in_aws_land_for/", "subreddit_subscribers": 145555, "created_utc": 1702317252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here is a good video on how EARLIER really works in SSAS or Power BI. Learning this vintage function have more benefits than the funtion itself.\n\n[https://www.youtube.com/watch?v=q8zsWqcd1vM](https://www.youtube.com/watch?v=q8zsWqcd1vM)", "author_fullname": "t2_hef0vmnb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EARLIER a vintage DAX", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18gnv5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702393357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a good video on how EARLIER really works in SSAS or Power BI. Learning this vintage function have more benefits than the funtion itself.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=q8zsWqcd1vM\"&gt;https://www.youtube.com/watch?v=q8zsWqcd1vM&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UOq9aBM5uJgFcFmb8yTOREjW9aY7N_Yy_2d-Kn2qCR4.jpg?auto=webp&amp;s=23e320986cdd831dcd5e6739a3badbc537145181", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/UOq9aBM5uJgFcFmb8yTOREjW9aY7N_Yy_2d-Kn2qCR4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a4cea66290c0a59cdbd1f46d6cbcdea2fad8f85", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/UOq9aBM5uJgFcFmb8yTOREjW9aY7N_Yy_2d-Kn2qCR4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=32c386882f19a036d2e053185a57d1e878c2d9bc", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/UOq9aBM5uJgFcFmb8yTOREjW9aY7N_Yy_2d-Kn2qCR4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51b513c4e7ac4fa6031c3d3b16aa00c6f7e2ae56", "width": 320, "height": 240}], "variants": {}, "id": "NtlWFciM2skBwE4MLoqEv2tH0EKK0sNVdcntqQeN63Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18gnv5s", "is_robot_indexable": true, "report_reasons": null, "author": "Status-Cap-5236", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gnv5s/earlier_a_vintage_dax/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gnv5s/earlier_a_vintage_dax/", "subreddit_subscribers": 145555, "created_utc": 1702393357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wi0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "High Performance Airbyte Alternative (With marketplace &amp; rev-share option).", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_18gocy7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RXisw1vYyrryutL98IO-maYwVLytLI2DsmM2KmTLwJY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702394682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/cloudquery/cloudquery", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZZLJ03hmDhSN9K9jMakkbWQPxx6jxa-FCr0AVzpqQFo.jpg?auto=webp&amp;s=dd143a7f9b6a9015459c3dc772da9e98db7b6f47", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZZLJ03hmDhSN9K9jMakkbWQPxx6jxa-FCr0AVzpqQFo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=147893b3db6dd4cae150c8886e4e7626cfbbad73", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ZZLJ03hmDhSN9K9jMakkbWQPxx6jxa-FCr0AVzpqQFo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1a14790a267d49a600b795d0e69becaa0f880bb", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ZZLJ03hmDhSN9K9jMakkbWQPxx6jxa-FCr0AVzpqQFo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89f0f40761cef4a98448da8d705fc0f7ac8da098", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ZZLJ03hmDhSN9K9jMakkbWQPxx6jxa-FCr0AVzpqQFo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ea4fd467efb9e12fdf42002ceadf2610ce7bb27", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ZZLJ03hmDhSN9K9jMakkbWQPxx6jxa-FCr0AVzpqQFo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c47a44b980bda39bba79f42cced4432f1d67a1e7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ZZLJ03hmDhSN9K9jMakkbWQPxx6jxa-FCr0AVzpqQFo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9817a5e9a3d1d6e3e2cd0fb13006fde1cb8be7a", "width": 1080, "height": 540}], "variants": {}, "id": "PYdEfT0qlVl6WVtoPac0-CRAKGdhmLIlFgFnNSh3Qcw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18gocy7", "is_robot_indexable": true, "report_reasons": null, "author": "jekapats", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gocy7/high_performance_airbyte_alternative_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/cloudquery/cloudquery", "subreddit_subscribers": 145555, "created_utc": 1702394682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Client gives some business rules to follow, me do that, boss revamps the requirements, me modify existing. Client screams, me wtf. ( caveman lang )", "author_fullname": "t2_76x4aitl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wtf", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18gk992", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.3, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702382207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Client gives some business rules to follow, me do that, boss revamps the requirements, me modify existing. Client screams, me wtf. ( caveman lang )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18gk992", "is_robot_indexable": true, "report_reasons": null, "author": "Scratch_that_Iich", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18gk992/wtf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18gk992/wtf/", "subreddit_subscribers": 145555, "created_utc": 1702382207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been studying the capabilities of generative AI recently, and it is really stronger than I imagined, it is even shocking. I have made several attempts to handle data queries with GPT:\n\n* You can completely use natural language to generate the SQL you want, with an accuracy of over 95%. It can even generate functions that we don\u2019t often use.As long as you tell it the structure in advance, it will remember it completely and generate production-ready sql.\n* I tried to associate multiple tables, and it can fully implement various join operations and return what we want.\n* It can even generate field comments based on your table header.\n* It can fully optimize and interpret the SQL you write and combine it with plugins to generate reports on the data. And it is compatible with any type of databases or big data platform which I think nobody can do this.\n* More importantly, CRUD code can be quickly generated based on the table structure. Typically, the ORM or class structure can be generated, which is not only used by data engineers, but also by developers.\n\nIf you don't believe, I have made [a small tool](https://tablechatai.com/) where you can try the generative pairing capabilities for free.", "author_fullname": "t2_n76m41g6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I feel that generative AI will bring evolution to data engineers. Is SQL statement still important?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18glq6q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.1, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702390674.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702387278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been studying the capabilities of generative AI recently, and it is really stronger than I imagined, it is even shocking. I have made several attempts to handle data queries with GPT:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You can completely use natural language to generate the SQL you want, with an accuracy of over 95%. It can even generate functions that we don\u2019t often use.As long as you tell it the structure in advance, it will remember it completely and generate production-ready sql.&lt;/li&gt;\n&lt;li&gt;I tried to associate multiple tables, and it can fully implement various join operations and return what we want.&lt;/li&gt;\n&lt;li&gt;It can even generate field comments based on your table header.&lt;/li&gt;\n&lt;li&gt;It can fully optimize and interpret the SQL you write and combine it with plugins to generate reports on the data. And it is compatible with any type of databases or big data platform which I think nobody can do this.&lt;/li&gt;\n&lt;li&gt;More importantly, CRUD code can be quickly generated based on the table structure. Typically, the ORM or class structure can be generated, which is not only used by data engineers, but also by developers.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you don&amp;#39;t believe, I have made &lt;a href=\"https://tablechatai.com/\"&gt;a small tool&lt;/a&gt; where you can try the generative pairing capabilities for free.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18glq6q", "is_robot_indexable": true, "report_reasons": null, "author": "RichaelMusk", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18glq6q/i_feel_that_generative_ai_will_bring_evolution_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18glq6q/i_feel_that_generative_ai_will_bring_evolution_to/", "subreddit_subscribers": 145555, "created_utc": 1702387278.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}