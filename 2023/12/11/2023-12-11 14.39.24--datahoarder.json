{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There\u2019s another thread asking folks about the 3-2-1 strategy and where they keep their offsite backup, and it seems like tons of people keep a copy of their data at a friend or family member\u2019s house.\n\nI doubt that most of the people in my family keep backups of their data at all, let alone have a 3-2-1 strategy. How did you go about convincing relatives to let you keep an online NAS at their house, an external drive that they mail to you you regularly, etc\u2026?\n\nDid there just happen to be other data hoarders in your family as well? Or did you introduce them to backing up data? Or did they just go along with it easily? Outside of this sub I\u2019ve never heard of folks doing this so am just curious. Thanks.", "author_fullname": "t2_hylomcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is keeping a backup of your data at a relative\u2019s house a common thing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f61f2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702222251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There\u2019s another thread asking folks about the 3-2-1 strategy and where they keep their offsite backup, and it seems like tons of people keep a copy of their data at a friend or family member\u2019s house.&lt;/p&gt;\n\n&lt;p&gt;I doubt that most of the people in my family keep backups of their data at all, let alone have a 3-2-1 strategy. How did you go about convincing relatives to let you keep an online NAS at their house, an external drive that they mail to you you regularly, etc\u2026?&lt;/p&gt;\n\n&lt;p&gt;Did there just happen to be other data hoarders in your family as well? Or did you introduce them to backing up data? Or did they just go along with it easily? Outside of this sub I\u2019ve never heard of folks doing this so am just curious. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18f61f2", "is_robot_indexable": true, "report_reasons": null, "author": "Celcius_87", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18f61f2/is_keeping_a_backup_of_your_data_at_a_relatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18f61f2/is_keeping_a_backup_of_your_data_at_a_relatives/", "subreddit_subscribers": 717892, "created_utc": 1702222251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My server has two plastic space fillers and empty bays. What\u2019s the most Data-Hoarder-y thing to do with that space?", "author_fullname": "t2_11dcf8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data hoarders with servers that have 5.25 inch bays. What do you put in those things?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fisxb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702257752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My server has two plastic space fillers and empty bays. What\u2019s the most Data-Hoarder-y thing to do with that space?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fisxb", "is_robot_indexable": true, "report_reasons": null, "author": "wonka88", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fisxb/data_hoarders_with_servers_that_have_525_inch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fisxb/data_hoarders_with_servers_that_have_525_inch/", "subreddit_subscribers": 717892, "created_utc": 1702257752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used to do this when I had limited data, downloaded at home so I\u2019ve always got something to listen to when I\u2019m out. \n\nData hasn\u2019t been a problem for a while, but habits die hard. \n\nI have almost 3 month\u2019s worth if I listened to them 12 hours a day. All curated. No batch downloads.", "author_fullname": "t2_q3sqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anybody else hoard podcasts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 64, "top_awarded_type": null, "hide_score": false, "name": "t3_18fkm1y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/c9sLf2hDI8MXFG0zmMCvKpIbHG5yK16cQ6GBdzPmFq8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702263475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to do this when I had limited data, downloaded at home so I\u2019ve always got something to listen to when I\u2019m out. &lt;/p&gt;\n\n&lt;p&gt;Data hasn\u2019t been a problem for a while, but habits die hard. &lt;/p&gt;\n\n&lt;p&gt;I have almost 3 month\u2019s worth if I listened to them 12 hours a day. All curated. No batch downloads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ae192t6azk5c1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ae192t6azk5c1.jpg?auto=webp&amp;s=cf86641a93382be1c3335b8cd7dbd285d077b13b", "width": 841, "height": 386}, "resolutions": [{"url": "https://preview.redd.it/ae192t6azk5c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c2acf33c979ade4bcd7c2ff14b11a4ce673e87a", "width": 108, "height": 49}, {"url": "https://preview.redd.it/ae192t6azk5c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0aa815551556ed2f08ddbc48b18ebea3462a9578", "width": 216, "height": 99}, {"url": "https://preview.redd.it/ae192t6azk5c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=07a3641edb559c961237c986b510f6ce3cd85837", "width": 320, "height": 146}, {"url": "https://preview.redd.it/ae192t6azk5c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd233f581333f3ec51f6b3f8438be367a3746cfc", "width": 640, "height": 293}], "variants": {}, "id": "Hkl9kwR_6CbIAsrQ45NAnVqhzk1HwjdlqjDErIlf_CE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18fkm1y", "is_robot_indexable": true, "report_reasons": null, "author": "glytxh", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fkm1y/does_anybody_else_hoard_podcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ae192t6azk5c1.jpg", "subreddit_subscribers": 717892, "created_utc": 1702263475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wondering if anyone has had success archiving accounts from social media (Facebook/Instagram) in a solution that is still usable (i.e. is navigable in some similar way to the original site).\n\nFor a bit of background, I've been feeling like perhaps my social media (SM) use has been getting a bit out of hand of late, so wanted to stop using it. I'd stop entirely, but there's still accounts I don't want to miss content of. My biggest issue really is social media's innate need to push new accounts in front of you all the time. TBH if they dropped that entirely, along with ads, I'd feel less overwhelmed by the whole thing.\n\nFor me accounts falls into two categories; there's the accounts I still want to actively follow, that actively bring something to the table for me regularly. Then there's the accounts I don't really want to see as often, but would feel more comfortable knowing they're being archived, so there's no FOMO kicking in.\n\nSo my plan was, what if I could self host a SM clone? One that I can set up to archive the stuff I care about, or just don't want to miss out on. That's browsable (even if only through a web browser (no apps even better lol). Well then I can dump native SM entirely.\n\nAny advice?\\*  \n(Tech advice. Yes I get it. SM is the devil. I should just drop it if it's impacting my life lol)", "author_fullname": "t2_x9cjat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving social media in a usable fashion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f4aom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702217057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if anyone has had success archiving accounts from social media (Facebook/Instagram) in a solution that is still usable (i.e. is navigable in some similar way to the original site).&lt;/p&gt;\n\n&lt;p&gt;For a bit of background, I&amp;#39;ve been feeling like perhaps my social media (SM) use has been getting a bit out of hand of late, so wanted to stop using it. I&amp;#39;d stop entirely, but there&amp;#39;s still accounts I don&amp;#39;t want to miss content of. My biggest issue really is social media&amp;#39;s innate need to push new accounts in front of you all the time. TBH if they dropped that entirely, along with ads, I&amp;#39;d feel less overwhelmed by the whole thing.&lt;/p&gt;\n\n&lt;p&gt;For me accounts falls into two categories; there&amp;#39;s the accounts I still want to actively follow, that actively bring something to the table for me regularly. Then there&amp;#39;s the accounts I don&amp;#39;t really want to see as often, but would feel more comfortable knowing they&amp;#39;re being archived, so there&amp;#39;s no FOMO kicking in.&lt;/p&gt;\n\n&lt;p&gt;So my plan was, what if I could self host a SM clone? One that I can set up to archive the stuff I care about, or just don&amp;#39;t want to miss out on. That&amp;#39;s browsable (even if only through a web browser (no apps even better lol). Well then I can dump native SM entirely.&lt;/p&gt;\n\n&lt;p&gt;Any advice?*&lt;br/&gt;\n(Tech advice. Yes I get it. SM is the devil. I should just drop it if it&amp;#39;s impacting my life lol)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18f4aom", "is_robot_indexable": true, "report_reasons": null, "author": "te5s3rakt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18f4aom/archiving_social_media_in_a_usable_fashion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18f4aom/archiving_social_media_in_a_usable_fashion/", "subreddit_subscribers": 717892, "created_utc": 1702217057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there guys,I'm about getting a new,maybe, couple of Hdds, for my daily PC , there's a drawback using the Seagate exos lineup? Specifically this one's:\n\nSeagate Exos 7E8 ST8000NM004A 8TB 512e Enterprise SATA Hard Drive\n\nEnterprise C EXOS X18 14TB 3.5IN 7200RPM SATA Helium 512E\n\nI know the exos in general are a little bit more louder and as little bit more power consumption, but aside from that, there's any big drawback on using them on Daily PC?\n\nThanks", "author_fullname": "t2_q09rbmy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any drawback using Seagate exos on daily PC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fojml", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702277582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there guys,I&amp;#39;m about getting a new,maybe, couple of Hdds, for my daily PC , there&amp;#39;s a drawback using the Seagate exos lineup? Specifically this one&amp;#39;s:&lt;/p&gt;\n\n&lt;p&gt;Seagate Exos 7E8 ST8000NM004A 8TB 512e Enterprise SATA Hard Drive&lt;/p&gt;\n\n&lt;p&gt;Enterprise C EXOS X18 14TB 3.5IN 7200RPM SATA Helium 512E&lt;/p&gt;\n\n&lt;p&gt;I know the exos in general are a little bit more louder and as little bit more power consumption, but aside from that, there&amp;#39;s any big drawback on using them on Daily PC?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fojml", "is_robot_indexable": true, "report_reasons": null, "author": "SkyBk", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fojml/any_drawback_using_seagate_exos_on_daily_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fojml/any_drawback_using_seagate_exos_on_daily_pc/", "subreddit_subscribers": 717892, "created_utc": 1702277582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It's not officially supported (I'd pay for it if that was an option), but I *can* individually open each photo in full quality and save to disk (they even provide a download button). So I'm just looking for a downloader tool or other way to automate that process to extract all photos. EXIF data is preserved so no other metadata is needed, just the JPEG files.\n\nAny help appreciated, especially if you have any experience with this particular site!", "author_fullname": "t2_4cmkl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any suggestions for downloading years' worth of photos from Family Album (mitene.us)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fhju3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702253914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not officially supported (I&amp;#39;d pay for it if that was an option), but I &lt;em&gt;can&lt;/em&gt; individually open each photo in full quality and save to disk (they even provide a download button). So I&amp;#39;m just looking for a downloader tool or other way to automate that process to extract all photos. EXIF data is preserved so no other metadata is needed, just the JPEG files.&lt;/p&gt;\n\n&lt;p&gt;Any help appreciated, especially if you have any experience with this particular site!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "120MB SCSI", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fhju3", "is_robot_indexable": true, "report_reasons": null, "author": "yParticle", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18fhju3/any_suggestions_for_downloading_years_worth_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fhju3/any_suggestions_for_downloading_years_worth_of/", "subreddit_subscribers": 717892, "created_utc": 1702253914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it possible to get decent NAS setup for under $300? I've been looking at 1 and 2 bay setups and they always end up closer to $500+. Unless I go with a buffalo setup, I don't see a way to do this. Buffalo doesn't seem to be a \"good\" option though for long term use. At this point I've been looking all day but I feel I'm a bit out of my depth. Please help with any recommendations?\n\nThis is primarily for my wifes use. My wife is not tech savvy at all, and I'm not much better. We just want an accessible spot to backup family photos and some documents that doesn't take a subscription. My wife takes a lot of photos... Probably best to start with at least a couple TB.", "author_fullname": "t2_lb2ja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I setup NAS for under $300?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fhcgt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702253309.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to get decent NAS setup for under $300? I&amp;#39;ve been looking at 1 and 2 bay setups and they always end up closer to $500+. Unless I go with a buffalo setup, I don&amp;#39;t see a way to do this. Buffalo doesn&amp;#39;t seem to be a &amp;quot;good&amp;quot; option though for long term use. At this point I&amp;#39;ve been looking all day but I feel I&amp;#39;m a bit out of my depth. Please help with any recommendations?&lt;/p&gt;\n\n&lt;p&gt;This is primarily for my wifes use. My wife is not tech savvy at all, and I&amp;#39;m not much better. We just want an accessible spot to backup family photos and some documents that doesn&amp;#39;t take a subscription. My wife takes a lot of photos... Probably best to start with at least a couple TB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fhcgt", "is_robot_indexable": true, "report_reasons": null, "author": "MalkavTepes", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fhcgt/can_i_setup_nas_for_under_300/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fhcgt/can_i_setup_nas_for_under_300/", "subreddit_subscribers": 717892, "created_utc": 1702253309.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello y all, I am trying to download a website. That has a index page containing all the pages of the websites. \nI do not understand if there is a way to download this page and then all the links would be converted to local links after the website has been downloaded. \nIs there a way to achieve this ? \nThanks for your help", "author_fullname": "t2_24wxjalc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recursive Website Archiver", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fqr6a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702287239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello y all, I am trying to download a website. That has a index page containing all the pages of the websites. \nI do not understand if there is a way to download this page and then all the links would be converted to local links after the website has been downloaded. \nIs there a way to achieve this ? \nThanks for your help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fqr6a", "is_robot_indexable": true, "report_reasons": null, "author": "Heewllett", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fqr6a/recursive_website_archiver/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fqr6a/recursive_website_archiver/", "subreddit_subscribers": 717892, "created_utc": 1702287239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "M.2 ssd prices keep rising here so these are the only sane options \n\nLaptop doesnt support internal discs of any kind \n\nIm thinking about getting the SD600Q although the SE880 would have a lot better speeds although its still dramless so is it really worth it\n\nSU650 would probably be better than the SU630 but they will need an adapter which would make it more expensive than the SD600Q\n\n[View Poll](https://www.reddit.com/poll/18fpnju)", "author_fullname": "t2_k7tqpvpg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing between an external ssd", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fpnju", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702282428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;M.2 ssd prices keep rising here so these are the only sane options &lt;/p&gt;\n\n&lt;p&gt;Laptop doesnt support internal discs of any kind &lt;/p&gt;\n\n&lt;p&gt;Im thinking about getting the SD600Q although the SE880 would have a lot better speeds although its still dramless so is it really worth it&lt;/p&gt;\n\n&lt;p&gt;SU650 would probably be better than the SU630 but they will need an adapter which would make it more expensive than the SD600Q&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/18fpnju\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fpnju", "is_robot_indexable": true, "report_reasons": null, "author": "kairukar", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1702455228767, "options": [{"text": "Adata GD600Q 1TB (59\u20ac)", "id": "26335367"}, {"text": "Adata SE880 1TB (69\u20ac)", "id": "26335368"}, {"text": "Adata SU630/SU650 (54\u20ac) + USB adapter (10\u20ac) [64\u20ac]", "id": "26335369"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 3, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fpnju/choosing_between_an_external_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/18fpnju/choosing_between_an_external_ssd/", "subreddit_subscribers": 717892, "created_utc": 1702282428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Most of my data is normal data. But some folders have large amount of small files ( 10kb~20kb  nearly million image files.  I tried to copy 500GB to a 1TB exfat formatted external ssd, but disk became full before copy finished.) \n\nUpon searching online, I found I need block suballocation or tail merging in file system, which exfat does not support, but zfs, btrfs support. Btrfs is still experimental and many user faced data corruption.\n\nCurrently, I am building a homelab nas. I would like to maximize available disk space.\n\nI have 3 2TB SSD. I am planning to use 1 SSD in exfat fs, another in zfs or both of them in zfs separately ( to maximize available disk space). Combine them using mergerfs. Use another 2tb SSD as parity using Snapraid. Whole system brought together using open media vault. Is this a valid / good approach. Please suggest \ud83d\ude4f", "author_fullname": "t2_b0glcuhi9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using ZFS on single disks, combining them with mergerfs, and paritizing them with Snapraid", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f6qkw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702224296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of my data is normal data. But some folders have large amount of small files ( 10kb~20kb  nearly million image files.  I tried to copy 500GB to a 1TB exfat formatted external ssd, but disk became full before copy finished.) &lt;/p&gt;\n\n&lt;p&gt;Upon searching online, I found I need block suballocation or tail merging in file system, which exfat does not support, but zfs, btrfs support. Btrfs is still experimental and many user faced data corruption.&lt;/p&gt;\n\n&lt;p&gt;Currently, I am building a homelab nas. I would like to maximize available disk space.&lt;/p&gt;\n\n&lt;p&gt;I have 3 2TB SSD. I am planning to use 1 SSD in exfat fs, another in zfs or both of them in zfs separately ( to maximize available disk space). Combine them using mergerfs. Use another 2tb SSD as parity using Snapraid. Whole system brought together using open media vault. Is this a valid / good approach. Please suggest \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18f6qkw", "is_robot_indexable": true, "report_reasons": null, "author": "sisalpino", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18f6qkw/using_zfs_on_single_disks_combining_them_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18f6qkw/using_zfs_on_single_disks_combining_them_with/", "subreddit_subscribers": 717892, "created_utc": 1702224296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need a DAS to connect to my Windows PC. After unsatisfactory experiences with USB DAS devices, I'm looking for something faster and more stable. QNAP has the TL-D400S, a four-bay DAS enclosure which connects to an included ASM1164-based PCI-E card via an SFF-8088 cable.\n\nThere are very few user reviews or discussions about this product to be found online, which makes me a little wary of it. I'm also not particularly excited about giving QNAP more of my money after my recent experience with the TR-002. That was an overpriced, underperforming product and QNAP's support wasn't great either. The TL-D400S even seems to use the same flimsy drive trays as the TR-002.\n\nIs anyone using the TL-D400S, especially with Windows? How has it been?\n\nAlso, are there any other non-USB DAS products similar to the TL-D400S that will work with Windows?\n\n&amp;#x200B;", "author_fullname": "t2_72fm42nb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using the QNAP TL-D400S SFF-8088 DAS? Any recommended alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18ftn7r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702298399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a DAS to connect to my Windows PC. After unsatisfactory experiences with USB DAS devices, I&amp;#39;m looking for something faster and more stable. QNAP has the TL-D400S, a four-bay DAS enclosure which connects to an included ASM1164-based PCI-E card via an SFF-8088 cable.&lt;/p&gt;\n\n&lt;p&gt;There are very few user reviews or discussions about this product to be found online, which makes me a little wary of it. I&amp;#39;m also not particularly excited about giving QNAP more of my money after my recent experience with the TR-002. That was an overpriced, underperforming product and QNAP&amp;#39;s support wasn&amp;#39;t great either. The TL-D400S even seems to use the same flimsy drive trays as the TR-002.&lt;/p&gt;\n\n&lt;p&gt;Is anyone using the TL-D400S, especially with Windows? How has it been?&lt;/p&gt;\n\n&lt;p&gt;Also, are there any other non-USB DAS products similar to the TL-D400S that will work with Windows?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ftn7r", "is_robot_indexable": true, "report_reasons": null, "author": "ScrioteMyRewquards", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ftn7r/anyone_using_the_qnap_tld400s_sff8088_das_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ftn7r/anyone_using_the_qnap_tld400s_sff8088_das_any/", "subreddit_subscribers": 717892, "created_utc": 1702298399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5hleg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CaptureGem: a multi-threaded app for saving recordings from a variety of adult cam sites including VR recordings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 115, "top_awarded_type": null, "hide_score": false, "name": "t3_18fksgp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702264036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "capturegem.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.capturegem.com/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": true, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?auto=webp&amp;s=7f01090d5ba915f52bcac63bab3a0cf2e7ff92fb", "width": 1632, "height": 1350}, "resolutions": [{"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d481d9d8c7c8749d34807ca8bf67ac600b273dc2", "width": 108, "height": 89}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=24804cde631ca9ed128cbb7d799c2d66f8f29bed", "width": 216, "height": 178}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a897777650ad729c866b6cdaec14cff274c4699e", "width": 320, "height": 264}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=35997cec4985405a5fc2ae1343ac54bb2ef661d8", "width": 640, "height": 529}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3161916dca47bf02b6bc252cf47f2774be9aa7b", "width": 960, "height": 794}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da02f44d9eae8ff1aee5652b922fc3e57bb06ad4", "width": 1080, "height": 893}], "variants": {"obfuscated": {"source": {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=2d608208d9025bfb26c00bc3c51855fbd764c665", "width": 1632, "height": 1350}, "resolutions": [{"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=0996c2c10b4b54fd6a79356216c660632d87062b", "width": 108, "height": 89}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=b5b8d5c44ebe159ac017d4735ef819c6a8588c1b", "width": 216, "height": 178}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=49116012f94430606e0cac3541af656dbbdf0a3d", "width": 320, "height": 264}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=2e4e1ab6b8841f750fba9fea015ca27e5a718245", "width": 640, "height": 529}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=960&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=bf65b432151942087b700aa6a5bcb092c5ebaa12", "width": 960, "height": 794}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=1080&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=f61d4c92201b15af49e310d11ed54063e3982a9f", "width": 1080, "height": 893}]}, "nsfw": {"source": {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=2d608208d9025bfb26c00bc3c51855fbd764c665", "width": 1632, "height": 1350}, "resolutions": [{"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=0996c2c10b4b54fd6a79356216c660632d87062b", "width": 108, "height": 89}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=b5b8d5c44ebe159ac017d4735ef819c6a8588c1b", "width": 216, "height": 178}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=49116012f94430606e0cac3541af656dbbdf0a3d", "width": 320, "height": 264}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=2e4e1ab6b8841f750fba9fea015ca27e5a718245", "width": 640, "height": 529}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=960&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=bf65b432151942087b700aa6a5bcb092c5ebaa12", "width": 960, "height": 794}, {"url": "https://external-preview.redd.it/vAr_4cgB4zwncW4onNc5hcBz6l4ft_xiFtc2t5DqQnU.jpg?width=1080&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=f61d4c92201b15af49e310d11ed54063e3982a9f", "width": 1080, "height": 893}]}}, "id": "GuVxVKhxVeaoc19kPZfUDv8DNLEiDZPuP2XmGNsxcA8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fksgp", "is_robot_indexable": true, "report_reasons": null, "author": "lordofindia", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fksgp/capturegem_a_multithreaded_app_for_saving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.capturegem.com/", "subreddit_subscribers": 717892, "created_utc": 1702264036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've researched and found about pigz/parallelLZMA which is good for Multi-CPU servers, is it possible to combine, instead of CPUs, multiple servers to compress files?\n\nI'm using Ceph and I need to compress heavy files (&gt;50GB), as I need speed + good compression to hoard the most possible, using a single CPU takes a few hours, it's too slow and each server got a powerful CPU that's being 'wasted'. does a software exist for multi-server compression?", "author_fullname": "t2_3gdxkeob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Compress files with multiple servers? Is this possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fhi0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702254127.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702253763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve researched and found about pigz/parallelLZMA which is good for Multi-CPU servers, is it possible to combine, instead of CPUs, multiple servers to compress files?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Ceph and I need to compress heavy files (&amp;gt;50GB), as I need speed + good compression to hoard the most possible, using a single CPU takes a few hours, it&amp;#39;s too slow and each server got a powerful CPU that&amp;#39;s being &amp;#39;wasted&amp;#39;. does a software exist for multi-server compression?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "42TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fhi0g", "is_robot_indexable": true, "report_reasons": null, "author": "JoaGamo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18fhi0g/compress_files_with_multiple_servers_is_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fhi0g/compress_files_with_multiple_servers_is_this/", "subreddit_subscribers": 717892, "created_utc": 1702253763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys and gals, long time lurker first time poster. I am going to try to include as much information as possible and be as clear as I can. If something doesn\u2019t make sense I apologize, let me know and I\u2019ll try to clarify. \n\nI\u2019m a photographer who also makes YouTube videos, I use a MacBook Pro M3 with a 1TB SSD onboard. I purchased an OWC Thunderbay Mini to use as primary storage when I\u2019m docked, it is setup with 4, 4TB SSD\u2019s and each mount as a separate storage volume. One is setup for RAW image intake and the second is setup for finished edits and their original RAW file counterparts. The other two are setup the same way for video. I didn\u2019t want to setup RAID on it, I wanted to use each drive separately to keep the data separated.\n\nThe OWC backs up to Backblaze, it is backed up to the cloud as I work. When I\u2019m finished with an edit I keep a copy of the edit and original RAW files together in a folder on the OWC and then copy the same folder to an offsite Synology DS220+ that also backs up to Backblaze. The images that were not selected to be edited also stay on the primary OWC on a separate drive. I basically want to take a second OWC Mini with 4, 2TB SSD\u2019s setup as 2 - 4TB pools daisychained to the primary one and use it to backup the drive holding the final photo edits, and the other drive holding the original photos. The intended purpose of the second OWC is to have a secondary physical backup onsite. \n\nMy question is this: can I setup the second Mini to backup the first using my Mac, or do I need to use a special software for that? And those of you who have more experience with this, is this a viable backup solution? Any recommendations? \n\nThanks in advance!", "author_fullname": "t2_jl7hvcb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OWC Thunderbay Mini Daisychain Backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fh4xy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702252722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys and gals, long time lurker first time poster. I am going to try to include as much information as possible and be as clear as I can. If something doesn\u2019t make sense I apologize, let me know and I\u2019ll try to clarify. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a photographer who also makes YouTube videos, I use a MacBook Pro M3 with a 1TB SSD onboard. I purchased an OWC Thunderbay Mini to use as primary storage when I\u2019m docked, it is setup with 4, 4TB SSD\u2019s and each mount as a separate storage volume. One is setup for RAW image intake and the second is setup for finished edits and their original RAW file counterparts. The other two are setup the same way for video. I didn\u2019t want to setup RAID on it, I wanted to use each drive separately to keep the data separated.&lt;/p&gt;\n\n&lt;p&gt;The OWC backs up to Backblaze, it is backed up to the cloud as I work. When I\u2019m finished with an edit I keep a copy of the edit and original RAW files together in a folder on the OWC and then copy the same folder to an offsite Synology DS220+ that also backs up to Backblaze. The images that were not selected to be edited also stay on the primary OWC on a separate drive. I basically want to take a second OWC Mini with 4, 2TB SSD\u2019s setup as 2 - 4TB pools daisychained to the primary one and use it to backup the drive holding the final photo edits, and the other drive holding the original photos. The intended purpose of the second OWC is to have a secondary physical backup onsite. &lt;/p&gt;\n\n&lt;p&gt;My question is this: can I setup the second Mini to backup the first using my Mac, or do I need to use a special software for that? And those of you who have more experience with this, is this a viable backup solution? Any recommendations? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fh4xy", "is_robot_indexable": true, "report_reasons": null, "author": "Nicholas-Albert", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fh4xy/owc_thunderbay_mini_daisychain_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fh4xy/owc_thunderbay_mini_daisychain_backup/", "subreddit_subscribers": 717892, "created_utc": 1702252722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "1 x Synology DS223j\n\n1 x Synology Plus Series HAT3300 12TB Drive\n\nI only have about 3-4 TB data at the moment, and the DS223j with 2 bays should be enough for quite a while. I would only use the 1 drive to start, as this is for convenience rather than backup.\n\nThis is for my own, little home network, just to store movies and shows. This is not for plex streaming.", "author_fullname": "t2_7cerf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this an okay place to start? [hardware question]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f9bza", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1702234146.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702231450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;1 x Synology DS223j&lt;/p&gt;\n\n&lt;p&gt;1 x Synology Plus Series HAT3300 12TB Drive&lt;/p&gt;\n\n&lt;p&gt;I only have about 3-4 TB data at the moment, and the DS223j with 2 bays should be enough for quite a while. I would only use the 1 drive to start, as this is for convenience rather than backup.&lt;/p&gt;\n\n&lt;p&gt;This is for my own, little home network, just to store movies and shows. This is not for plex streaming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18f9bza", "is_robot_indexable": true, "report_reasons": null, "author": "Lundorff", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18f9bza/is_this_an_okay_place_to_start_hardware_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18f9bza/is_this_an_okay_place_to_start_hardware_question/", "subreddit_subscribers": 717892, "created_utc": 1702231450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Hoarders,\n\n&amp;#x200B;\n\nI'm joining the community with a D6000 and 40 6Tb drives. I want to connect the HPE D6000 to a Dell poweredge T330.\n\nI'm looking into the right IT mode raid controller to hook the drives.\n\nThe D6000 have 2 6GB IO Modules. In term of raid controller I had a PERC h310 that I liked quite a lot.\n\n&amp;#x200B;\n\nWhat would be your advice for a cheap IT mode raid controller?", "author_fullname": "t2_v5ocv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HPE D6000 40 drives setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f4pgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702218346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Hoarders,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m joining the community with a D6000 and 40 6Tb drives. I want to connect the HPE D6000 to a Dell poweredge T330.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking into the right IT mode raid controller to hook the drives.&lt;/p&gt;\n\n&lt;p&gt;The D6000 have 2 6GB IO Modules. In term of raid controller I had a PERC h310 that I liked quite a lot.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What would be your advice for a cheap IT mode raid controller?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18f4pgm", "is_robot_indexable": true, "report_reasons": null, "author": "MarTn_IV", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18f4pgm/hpe_d6000_40_drives_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18f4pgm/hpe_d6000_40_drives_setup/", "subreddit_subscribers": 717892, "created_utc": 1702218346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a LaCie 2big RAID 12 TB; 2x 6 TB IronWolf Pro(?)\n\nDriveDx gave me a warning now that it is \"FAILING\", because the  \"Reallocated Sector Count\" increased from 8 to 16.\n\nDon't NAS drives have error correction?\n\nDo I need to worry? I do have everything backed up tho. But I'd like to avoid buying replacement drives now when it actually would last a few more months.\n\n&amp;#x200B;\n\nReport:\n\n`Application Name                     : DriveDx`\n\n`Application Version                  : 1.12.1.760`\n\n`Application SubBuild                 : 0`\n\n`Application Edition                  : Standalone`\n\n`Application Website                  :` [`https://binaryfruit.com/drivedx`](https://binaryfruit.com/drivedx)\n\n`DriveDx Knowledge Base Revision      : 103/103`\n\n&amp;#x200B;\n\n`Computer Model                       : MacBookPro18,3`\n\n&amp;#x200B;\n\n`OS Boot Time                         : 2023-12-11T10:37:22`\n\n`Time Since Boot                      : 02h 37m 18s`\n\n`OS Name                              : macOS`\n\n`OS Version                           : 14.1.2`\n\n`OS Build                             : 23B92`\n\n`OS Kernel Version                    : Darwin 23.1.0`\n\n&amp;#x200B;\n\n`SAT SMART Driver Version             : 0.10.3s`\n\n`ATA Command Support Tolerance        : verypermissive`\n\n`N of drives in report                : 1`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n`### DRIVE 1 OF 1 ###`\n\n`Last Checked                         : 11. Dezember 2023 10:46:02 MEZ`\n\n`Last Checked (ISO 8601 format)       : 2023-12-11T10:46:02`\n\n&amp;#x200B;\n\n`Advanced SMART Status                : FAILING`\n\n`Overall Health Rating                : N/A 100%`\n\n`Overall Performance Rating           : GOOD 100%`\n\n`Issues found                         : 2`\n\n&amp;#x200B;\n\n`Serial Number                        : ZAD266H1`\n\n`WWN Id                               : 5 000c50 0a43e9761`\n\n`Volumes                              : Edit`\n\n`Device Path                          : /dev/disk14`\n\n`Total Capacity                       : 12.0 TB (12.002.350.243.840 Bytes)`\n\n`Model Family                         : LaCie (Seagate-based) HDDs`\n\n`Model                                : LaCie   2big Dock Thunderbolt 3  Raid 0`\n\n`Form Factor                          : 3.5 inches`\n\n`Firmware Version                     : EN02`\n\n`Drive Type                           : HDD 7200 rpm`\n\n&amp;#x200B;\n\n`Power On Time                        : 23.885 hours (33 months 5 days 5 hours)`\n\n`Power Cycles Count                   : 8.097`\n\n`Current Power Cycle Time             : 2.6 hours`\n\n&amp;#x200B;\n\n`=== DEVICE CAPABILITIES ===`\n\n`S.M.A.R.T. support enabled           : yes`\n\n`DriveDx Active Diagnostic Config     : LaCie (Seagate-based) HDDs config [hdd.seagate.lacie]`\n\n`Sector Logical Size                  : 512`\n\n`Sector Physical Size                 : 4096`\n\n`Physical Interconnect                : SATA`\n\n`Logical Protocol                     : SATA`\n\n`Removable                            : no`\n\n`Ejectable                            : no`\n\n`ATA Version                          : ACS-3 T13/2161-D revision 5`\n\n`SATA Version                         : SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)`\n\n`I/O Path                             : IOService:/AppleARMPE/arm-io/AppleT600xIO/apciec1@30000000/AppleT6000PCIeC/pcic1-bridge@0/IOPP/pci-bridge@0/IOPP/pci-bridge@1/IOPP/pci1b21,625@0/AppleAHCI/PRT0@0/IOAHCIDevice@0/AppleAHCIDiskDriver/IOAHCIBlockStorageDevice`\n\n`Attributes Data Structure Revision   : 10`\n\n`SMART Command Transport (SCT) flags  : 0x50bd`\n\n`SCT Status supported                 : yes`\n\n`SCT Feature Control supported        : yes`\n\n`SCT Data Table supported             : yes`\n\n`Error logging capabilities           : 0x1`\n\n`Self-tests supported                 : yes`\n\n`Offline Data Collection capabilities : 0x7b`\n\n`Offline Data Collection status       : 0x82`\n\n`Auto Offline Data Collection flags   : 0x1`\n\n`[Known device                       ]: yes`\n\n`[Drive State Flags                  ]: 0x42000000`\n\n&amp;#x200B;\n\n`=== CURRENT POWER CYCLE STATISTICS ===`\n\n`Time since computer startup         : 2 hours`\n\n`Data Read                           : 12.982.272 bytes (13.0 MB)`\n\n`Data Written                        : 0 bytes (0 B)`\n\n`Data Read/hour                      : 5.0 MB/hour`\n\n`Data Write/hour                     : 0 B/hour`\n\n&amp;#x200B;\n\n`Operations (Read)                   : 1.744`\n\n`Read IOPS                           : 849`\n\n`Operations (Write)                  : 0`\n\n`Write IOPS                          : 0`\n\n`Throughput per operation (Read)     : 7.4 KB/Op`\n\n&amp;#x200B;\n\n`Latency Time (Read)                 : 0 ns`\n\n`Latency Time (Write)                : 0 ns`\n\n`Retries (Read)                      : 0`\n\n`Retries (Write)                     : 0`\n\n`Errors (Read)                       : 0`\n\n`Errors (Write)                      : 0`\n\n&amp;#x200B;\n\n`=== PROBLEMS SUMMARY ===`\n\n`Failed Indicators (life-span / pre-fail)  : 0 (0 / 0)`\n\n`Failing Indicators (life-span / pre-fail) : 1 (0 / 1)`\n\n`Warnings (life-span / pre-fail)           : 1 (1 / 0)`\n\n`Recently failed Self-tests (Short / Full) : 0 (0 / 0)`\n\n`I/O Error Count                           : 0 (0 / 0)`\n\n`Time in Under temperature                 : 0 minutes`\n\n`Time in Over temperature                  : 0 minutes`\n\n&amp;#x200B;\n\n`=== IMPORTANT HEALTH INDICATORS ===`\n\n`ID  NAME                                         RAW VALUE                  STATUS`\n\n  `5 Reallocated Sector Count                     16                         100% Failing`\n\n`187 Reported Uncorrectable Errors                0                          100% OK`\n\n`197 Current Pending Sector Count                 0                          100% OK`\n\n`198 Offline Uncorrectable Sector Count           0                          100% OK`\n\n`199 UDMA CRC Error Count                         0                          100% OK`\n\n`241 Total LBAs Written                           481.141.734.088 (246.3 TB) 100% OK`", "author_fullname": "t2_165juo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IronWolf Pro RAID FAILING: \"Reallocated Sector Count\" 16 ??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ftdgr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702297466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a LaCie 2big RAID 12 TB; 2x 6 TB IronWolf Pro(?)&lt;/p&gt;\n\n&lt;p&gt;DriveDx gave me a warning now that it is &amp;quot;FAILING&amp;quot;, because the  &amp;quot;Reallocated Sector Count&amp;quot; increased from 8 to 16.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t NAS drives have error correction?&lt;/p&gt;\n\n&lt;p&gt;Do I need to worry? I do have everything backed up tho. But I&amp;#39;d like to avoid buying replacement drives now when it actually would last a few more months.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Report:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Application Name                     : DriveDx&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Application Version                  : 1.12.1.760&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Application SubBuild                 : 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Application Edition                  : Standalone&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Application Website                  :&lt;/code&gt; &lt;a href=\"https://binaryfruit.com/drivedx\"&gt;&lt;code&gt;https://binaryfruit.com/drivedx&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;DriveDx Knowledge Base Revision      : 103/103&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Computer Model                       : MacBookPro18,3&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;OS Boot Time                         : 2023-12-11T10:37:22&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Time Since Boot                      : 02h 37m 18s&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;OS Name                              : macOS&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;OS Version                           : 14.1.2&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;OS Build                             : 23B92&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;OS Kernel Version                    : Darwin 23.1.0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SAT SMART Driver Version             : 0.10.3s&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ATA Command Support Tolerance        : verypermissive&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;N of drives in report                : 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;### DRIVE 1 OF 1 ###&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Last Checked                         : 11. Dezember 2023 10:46:02 MEZ&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Last Checked (ISO 8601 format)       : 2023-12-11T10:46:02&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Advanced SMART Status                : FAILING&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Overall Health Rating                : N/A 100%&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Overall Performance Rating           : GOOD 100%&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Issues found                         : 2&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Serial Number                        : ZAD266H1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;WWN Id                               : 5 000c50 0a43e9761&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Volumes                              : Edit&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Device Path                          : /dev/disk14&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Total Capacity                       : 12.0 TB (12.002.350.243.840 Bytes)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Model Family                         : LaCie (Seagate-based) HDDs&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Model                                : LaCie   2big Dock Thunderbolt 3  Raid 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Form Factor                          : 3.5 inches&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Firmware Version                     : EN02&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Drive Type                           : HDD 7200 rpm&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Power On Time                        : 23.885 hours (33 months 5 days 5 hours)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Power Cycles Count                   : 8.097&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Current Power Cycle Time             : 2.6 hours&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;=== DEVICE CAPABILITIES ===&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;S.M.A.R.T. support enabled           : yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;DriveDx Active Diagnostic Config     : LaCie (Seagate-based) HDDs config [hdd.seagate.lacie]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Sector Logical Size                  : 512&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Sector Physical Size                 : 4096&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Physical Interconnect                : SATA&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Logical Protocol                     : SATA&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Removable                            : no&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Ejectable                            : no&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ATA Version                          : ACS-3 T13/2161-D revision 5&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SATA Version                         : SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;I/O Path                             : IOService:/AppleARMPE/arm-io/AppleT600xIO/apciec1@30000000/AppleT6000PCIeC/pcic1-bridge@0/IOPP/pci-bridge@0/IOPP/pci-bridge@1/IOPP/pci1b21,625@0/AppleAHCI/PRT0@0/IOAHCIDevice@0/AppleAHCIDiskDriver/IOAHCIBlockStorageDevice&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Attributes Data Structure Revision   : 10&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SMART Command Transport (SCT) flags  : 0x50bd&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SCT Status supported                 : yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SCT Feature Control supported        : yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SCT Data Table supported             : yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Error logging capabilities           : 0x1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Self-tests supported                 : yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Offline Data Collection capabilities : 0x7b&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Offline Data Collection status       : 0x82&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Auto Offline Data Collection flags   : 0x1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;[Known device                       ]: yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;[Drive State Flags                  ]: 0x42000000&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;=== CURRENT POWER CYCLE STATISTICS ===&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Time since computer startup         : 2 hours&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Data Read                           : 12.982.272 bytes (13.0 MB)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Data Written                        : 0 bytes (0 B)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Data Read/hour                      : 5.0 MB/hour&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Data Write/hour                     : 0 B/hour&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Operations (Read)                   : 1.744&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Read IOPS                           : 849&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Operations (Write)                  : 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Write IOPS                          : 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Throughput per operation (Read)     : 7.4 KB/Op&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Latency Time (Read)                 : 0 ns&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Latency Time (Write)                : 0 ns&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Retries (Read)                      : 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Retries (Write)                     : 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Errors (Read)                       : 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Errors (Write)                      : 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;=== PROBLEMS SUMMARY ===&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Failed Indicators (life-span / pre-fail)  : 0 (0 / 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Failing Indicators (life-span / pre-fail) : 1 (0 / 1)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Warnings (life-span / pre-fail)           : 1 (1 / 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Recently failed Self-tests (Short / Full) : 0 (0 / 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;I/O Error Count                           : 0 (0 / 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Time in Under temperature                 : 0 minutes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Time in Over temperature                  : 0 minutes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;=== IMPORTANT HEALTH INDICATORS ===&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ID  NAME                                         RAW VALUE                  STATUS&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;5 Reallocated Sector Count                     16                         100% Failing&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;187 Reported Uncorrectable Errors                0                          100% OK&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;197 Current Pending Sector Count                 0                          100% OK&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;198 Offline Uncorrectable Sector Count           0                          100% OK&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;199 UDMA CRC Error Count                         0                          100% OK&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;241 Total LBAs Written                           481.141.734.088 (246.3 TB) 100% OK&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ftdgr", "is_robot_indexable": true, "report_reasons": null, "author": "Subfader", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ftdgr/ironwolf_pro_raid_failing_reallocated_sector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ftdgr/ironwolf_pro_raid_failing_reallocated_sector/", "subreddit_subscribers": 717892, "created_utc": 1702297466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi friends,\n\nA couple of questions for any Linux users in the crowd who are into using LTO for archival:\n\n1) Are there any drives that have guaranteed out of the box compatibility with Ubuntu Linux as a desktop OS?\n\n2) For writing onto LTO in Ubuntu ... what GUIs are out there? I currently archive using M-Disc so K3B and a USB drive is my go-to but I don't believe it supports attached tape drives.\n\nTIA!", "author_fullname": "t2_poc45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know of any LTO drives that are compatible with Ubuntu Linux?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ftbdb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702297261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends,&lt;/p&gt;\n\n&lt;p&gt;A couple of questions for any Linux users in the crowd who are into using LTO for archival:&lt;/p&gt;\n\n&lt;p&gt;1) Are there any drives that have guaranteed out of the box compatibility with Ubuntu Linux as a desktop OS?&lt;/p&gt;\n\n&lt;p&gt;2) For writing onto LTO in Ubuntu ... what GUIs are out there? I currently archive using M-Disc so K3B and a USB drive is my go-to but I don&amp;#39;t believe it supports attached tape drives.&lt;/p&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ftbdb", "is_robot_indexable": true, "report_reasons": null, "author": "danielrosehill", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ftbdb/anyone_know_of_any_lto_drives_that_are_compatible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ftbdb/anyone_know_of_any_lto_drives_that_are_compatible/", "subreddit_subscribers": 717892, "created_utc": 1702297261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, \n\nlong time lurker here, just created an account for asking about your opinion :)  \n\n\nI have lots of captures of livestreams from sites like chaturbate and the amount of data is getting a bit out of hand. Must be about 40 TB by now. \n\nI was thinking that I could re-encode the videos in order to save some space. Live Streaming sites have to optimize for latency and therefore they probably don't use the most space efficient encoding in the first place and leave some space saving on the table. E.g. if I compare this to videos downloaded from youtube or pornhub, I am often surprised how small they are given the resolution and graphical fidelity. Furthermore, for content like chaturbate, the background is often static, which should allow for great compression?\n\nLet's ignore the monumental amount of compute time it would need on my part to do the actual re-encoding. \n\nHow do you guys that have much more experience in this store video data/which encoding do you use? \n\nI already did some digging around and tried a bunch of parameters with ffmpeg, i.e. -`c:v libx265 -crf 28 -preset slower` but am not really sold on the quality/space saving tradeoff. Which parameters do you use for content like this? Someone has probably already figured out the perfect values :)\n\n  \n(Obviously, deleting is not an option. We're data hoarders and not data deleters :))\n\nThanks!", "author_fullname": "t2_du1xzmt69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Re-Encoding Livestream Captures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f6fy7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702223461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;long time lurker here, just created an account for asking about your opinion :)  &lt;/p&gt;\n\n&lt;p&gt;I have lots of captures of livestreams from sites like chaturbate and the amount of data is getting a bit out of hand. Must be about 40 TB by now. &lt;/p&gt;\n\n&lt;p&gt;I was thinking that I could re-encode the videos in order to save some space. Live Streaming sites have to optimize for latency and therefore they probably don&amp;#39;t use the most space efficient encoding in the first place and leave some space saving on the table. E.g. if I compare this to videos downloaded from youtube or pornhub, I am often surprised how small they are given the resolution and graphical fidelity. Furthermore, for content like chaturbate, the background is often static, which should allow for great compression?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s ignore the monumental amount of compute time it would need on my part to do the actual re-encoding. &lt;/p&gt;\n\n&lt;p&gt;How do you guys that have much more experience in this store video data/which encoding do you use? &lt;/p&gt;\n\n&lt;p&gt;I already did some digging around and tried a bunch of parameters with ffmpeg, i.e. -&lt;code&gt;c:v libx265 -crf 28 -preset slower&lt;/code&gt; but am not really sold on the quality/space saving tradeoff. Which parameters do you use for content like this? Someone has probably already figured out the perfect values :)&lt;/p&gt;\n\n&lt;p&gt;(Obviously, deleting is not an option. We&amp;#39;re data hoarders and not data deleters :))&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18f6fy7", "is_robot_indexable": true, "report_reasons": null, "author": "BunchLegitimate1125", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18f6fy7/reencoding_livestream_captures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18f6fy7/reencoding_livestream_captures/", "subreddit_subscribers": 717892, "created_utc": 1702223461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Supposedly they gave like 1000 tb which is an insanely high number for most non-business consumer use (talking general use case average users)\n\nI am not chinese nor do i read it, but i had an account and it seems my login is no longer valid did they change parent companies? ", "author_fullname": "t2_n2vs6wilo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whatever happened to Yunpan 360? Did anyone ever use it or still use it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fm9i3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702269020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Supposedly they gave like 1000 tb which is an insanely high number for most non-business consumer use (talking general use case average users)&lt;/p&gt;\n\n&lt;p&gt;I am not chinese nor do i read it, but i had an account and it seems my login is no longer valid did they change parent companies? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fm9i3", "is_robot_indexable": true, "report_reasons": null, "author": "DwayneCmoney", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fm9i3/whatever_happened_to_yunpan_360_did_anyone_ever/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fm9i3/whatever_happened_to_yunpan_360_did_anyone_ever/", "subreddit_subscribers": 717892, "created_utc": 1702269020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an Elitedesk 800 G3 Mini and want to put an HDD inside for Plex, but seems hard to find.\n\nSo i ended up considering an external hdd like the \"WD Black P10\". It is CMR, 2.5\" 5TB, but with a problem. Its connector is soldered so i can't remove its case and put inside my HP.\n\nDo you know an alternative for this, internal or external? I'm thinking of buying it second hand", "author_fullname": "t2_3oh1flfu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an internal 2.5\" HDD 4TB or more for Plex", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fi7nm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702255926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an Elitedesk 800 G3 Mini and want to put an HDD inside for Plex, but seems hard to find.&lt;/p&gt;\n\n&lt;p&gt;So i ended up considering an external hdd like the &amp;quot;WD Black P10&amp;quot;. It is CMR, 2.5&amp;quot; 5TB, but with a problem. Its connector is soldered so i can&amp;#39;t remove its case and put inside my HP.&lt;/p&gt;\n\n&lt;p&gt;Do you know an alternative for this, internal or external? I&amp;#39;m thinking of buying it second hand&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18fi7nm", "is_robot_indexable": true, "report_reasons": null, "author": "MrDubsstep", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18fi7nm/looking_for_an_internal_25_hdd_4tb_or_more_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18fi7nm/looking_for_an_internal_25_hdd_4tb_or_more_for/", "subreddit_subscribers": 717892, "created_utc": 1702255926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So as the title describes I've repurposed an old PC with a bunch of disks (from 3TB to 8TB, 21TB in total) as a simple NAS for media playback. It's all on BTRFS for the integrity check, no RAID or anything since I wanted to maximize storage and don't care about uptime all that much.\n\nI've got a 3.5\" dock and a bunch of assorted disks for cold storage and backups, but it's all rather hands-on when I refresh things. I'm looking to improve this, especially as I'll replace a bunch of my disks with something in the 16-20TB range. My question is, how can I automate a backup of that magnitude without an equally large backup solution? Something where I can pop mismatched disks in an out and at least guarantee that I have a second copy of all my media. I've read up on git-annex but it seems like overkill for what I need.\n\nI was thinking that maybe I could find matching disks and just duplicate with rsync (or btrfs-sync) on a disk-by-disk basis. For the new 16TB+ disk, maybe create 2-3 partitions and match those to disks of the right size(s). Thoughts? I have no trouble making a few bash scripts to improve handling this, perhaps some automation based on which backup disk is connected.", "author_fullname": "t2_3fqk5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running an old PC with Linux and a bunch of drives as JBOD NAS. Looking for a straightforward backup solution.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18facbw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702234212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as the title describes I&amp;#39;ve repurposed an old PC with a bunch of disks (from 3TB to 8TB, 21TB in total) as a simple NAS for media playback. It&amp;#39;s all on BTRFS for the integrity check, no RAID or anything since I wanted to maximize storage and don&amp;#39;t care about uptime all that much.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a 3.5&amp;quot; dock and a bunch of assorted disks for cold storage and backups, but it&amp;#39;s all rather hands-on when I refresh things. I&amp;#39;m looking to improve this, especially as I&amp;#39;ll replace a bunch of my disks with something in the 16-20TB range. My question is, how can I automate a backup of that magnitude without an equally large backup solution? Something where I can pop mismatched disks in an out and at least guarantee that I have a second copy of all my media. I&amp;#39;ve read up on git-annex but it seems like overkill for what I need.&lt;/p&gt;\n\n&lt;p&gt;I was thinking that maybe I could find matching disks and just duplicate with rsync (or btrfs-sync) on a disk-by-disk basis. For the new 16TB+ disk, maybe create 2-3 partitions and match those to disks of the right size(s). Thoughts? I have no trouble making a few bash scripts to improve handling this, perhaps some automation based on which backup disk is connected.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18facbw", "is_robot_indexable": true, "report_reasons": null, "author": "amorpheus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18facbw/running_an_old_pc_with_linux_and_a_bunch_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18facbw/running_an_old_pc_with_linux_and_a_bunch_of/", "subreddit_subscribers": 717892, "created_utc": 1702234212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So recently I after a format I tried to setup again my synthing so I can have backups from my phone. Because I dont know exactly how Syncthing works I want to play with a file that I didnt care losing its data, so I can learn how it works. I added the Whatsapp folder to the game to see how its going. At some point syncthing show me the error \"the path of this folder does not exist\". Now I can't delete, move or rename the folder. When I try to delete the folder it shows this error \"0x80070091\". I added a file(lets call it P folder) with syncthing to the host folder and now syncthing show the same problem to the P folder. I cant delete the host folder either.  I have tried mutliple things, cmd commands, admin settings, safe mode delete. Nothing works  \n\n\nPlease help I'm going nuts with this thing! ", "author_fullname": "t2_kd7kctn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "0x80070091, but driving me nuts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f9kjd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702232103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So recently I after a format I tried to setup again my synthing so I can have backups from my phone. Because I dont know exactly how Syncthing works I want to play with a file that I didnt care losing its data, so I can learn how it works. I added the Whatsapp folder to the game to see how its going. At some point syncthing show me the error &amp;quot;the path of this folder does not exist&amp;quot;. Now I can&amp;#39;t delete, move or rename the folder. When I try to delete the folder it shows this error &amp;quot;0x80070091&amp;quot;. I added a file(lets call it P folder) with syncthing to the host folder and now syncthing show the same problem to the P folder. I cant delete the host folder either.  I have tried mutliple things, cmd commands, admin settings, safe mode delete. Nothing works  &lt;/p&gt;\n\n&lt;p&gt;Please help I&amp;#39;m going nuts with this thing! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18f9kjd", "is_robot_indexable": true, "report_reasons": null, "author": "masterios", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18f9kjd/0x80070091_but_driving_me_nuts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18f9kjd/0x80070091_but_driving_me_nuts/", "subreddit_subscribers": 717892, "created_utc": 1702232103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dug out my old DS218j for the sole use of mass downloading, I have around 3000 files all contained in one big . DLC file, I know there's a docker extension to enable downloading but the DS318j no longer supports docker :( is there an alternative I can use?", "author_fullname": "t2_5tyww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology + jDownloader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f5bfa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702220174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dug out my old DS218j for the sole use of mass downloading, I have around 3000 files all contained in one big . DLC file, I know there&amp;#39;s a docker extension to enable downloading but the DS318j no longer supports docker :( is there an alternative I can use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "48TB DS920+", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18f5bfa", "is_robot_indexable": true, "report_reasons": null, "author": "spong_miester", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18f5bfa/synology_jdownloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18f5bfa/synology_jdownloader/", "subreddit_subscribers": 717892, "created_utc": 1702220174.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}