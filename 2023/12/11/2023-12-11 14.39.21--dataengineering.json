{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. \n\nData is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. \n\nOverall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. \n\nAnyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. ", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I Was Happier Being a Bartender Compared to Being a 6 Figure DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ffzmx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 219, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 219, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702249969.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702249347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. &lt;/p&gt;\n\n&lt;p&gt;Data is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. &lt;/p&gt;\n\n&lt;p&gt;Overall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. &lt;/p&gt;\n\n&lt;p&gt;Anyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18ffzmx", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 104, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/", "subreddit_subscribers": 145279, "created_utc": 1702249347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What's the best SQL environment you've had the pleasure of working in?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best SQL environment you have ever worked in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fii0v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702256799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What&amp;#39;s the best SQL environment you&amp;#39;ve had the pleasure of working in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fii0v", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fii0v/what_is_the_best_sql_environment_you_have_ever/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fii0v/what_is_the_best_sql_environment_you_have_ever/", "subreddit_subscribers": 145279, "created_utc": 1702256799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.\n\nEncrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage PII?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7m1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702226742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.&lt;/p&gt;\n\n&lt;p&gt;Encrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f7m1t", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "subreddit_subscribers": 145279, "created_utc": 1702226742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone\n\nRecently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?", "author_fullname": "t2_gci8d6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here that works as data engineer in Canada as an immigrant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f4upz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702218794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone&lt;/p&gt;\n\n&lt;p&gt;Recently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18f4upz", "is_robot_indexable": true, "report_reasons": null, "author": "Entropico_88", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f4upz/anyone_here_that_works_as_data_engineer_in_canada/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f4upz/anyone_here_that_works_as_data_engineer_in_canada/", "subreddit_subscribers": 145279, "created_utc": 1702218794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ibis ([https://ibis-project.org/](https://ibis-project.org/)) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.\n\nIt looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.\n\nHowever, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?\n\nIbis' documentation is relatively limited and I haven't found much information about what organizations back this project.", "author_fullname": "t2_7v7x1pqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Ibis reliably be used in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fcbf0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702239463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ibis (&lt;a href=\"https://ibis-project.org/\"&gt;https://ibis-project.org/&lt;/a&gt;) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.&lt;/p&gt;\n\n&lt;p&gt;It looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.&lt;/p&gt;\n\n&lt;p&gt;However, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?&lt;/p&gt;\n\n&lt;p&gt;Ibis&amp;#39; documentation is relatively limited and I haven&amp;#39;t found much information about what organizations back this project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fcbf0", "is_robot_indexable": true, "report_reasons": null, "author": "cantthinkofoneuse", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "subreddit_subscribers": 145279, "created_utc": 1702239463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings, fellow data engineers\n\nI have been observing this site for a few months in order to get insights, and in the allotted six months, I have learned these skills here. \n\nTWO certificates in Azure.\n\nSQL/Postgres Database\n\nPy Spark\u00a0\n\nData Warehouse and Data modelling\u00a0\n\nGIT\n\nPython \n\nData Bricks notebook and Delta Lake certification\u00a0\n\nIn addition to loading a file from the local system and uploading it to S3, I was also able to build a pipeline using Airflow  Dags to fetch files from Postgres and transfer them to S3! without any transformation (since I wanted to learn the E AND L part first) I assume the transformation can be done using spark once you have the file in your local system from source.\n\nI find it difficult to determine if I have grasped the ETL part, which is the foundation of any data engineering, or if I am missing anything. Any insight into an ETE\u00a0project would be helpful.\n\nDo let me know if there is any other tool that I need to learn or continue to practice with.\n\n&amp;#x200B;", "author_fullname": "t2_54vsm483", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with learning ETL and Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fpg01", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702281540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, fellow data engineers&lt;/p&gt;\n\n&lt;p&gt;I have been observing this site for a few months in order to get insights, and in the allotted six months, I have learned these skills here. &lt;/p&gt;\n\n&lt;p&gt;TWO certificates in Azure.&lt;/p&gt;\n\n&lt;p&gt;SQL/Postgres Database&lt;/p&gt;\n\n&lt;p&gt;Py Spark\u00a0&lt;/p&gt;\n\n&lt;p&gt;Data Warehouse and Data modelling\u00a0&lt;/p&gt;\n\n&lt;p&gt;GIT&lt;/p&gt;\n\n&lt;p&gt;Python &lt;/p&gt;\n\n&lt;p&gt;Data Bricks notebook and Delta Lake certification\u00a0&lt;/p&gt;\n\n&lt;p&gt;In addition to loading a file from the local system and uploading it to S3, I was also able to build a pipeline using Airflow  Dags to fetch files from Postgres and transfer them to S3! without any transformation (since I wanted to learn the E AND L part first) I assume the transformation can be done using spark once you have the file in your local system from source.&lt;/p&gt;\n\n&lt;p&gt;I find it difficult to determine if I have grasped the ETL part, which is the foundation of any data engineering, or if I am missing anything. Any insight into an ETE\u00a0project would be helpful.&lt;/p&gt;\n\n&lt;p&gt;Do let me know if there is any other tool that I need to learn or continue to practice with.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fpg01", "is_robot_indexable": true, "report_reasons": null, "author": "slugabed123", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fpg01/need_help_with_learning_etl_and_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fpg01/need_help_with_learning_etl_and_tools/", "subreddit_subscribers": 145279, "created_utc": 1702281540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI'm a data engineer at a startup and have been working on getting various operational data into Databricks. My next challenge is to present an overview of our current data assets to the management team (Some C-level stakeholders also like to know). They have also requested an internal dashboard with some key info, but they have no idea what I currently have in the Databricks warehouse, and I and not entirely sure which data they'll find most relevant...As a first step, I'd like to have a clear and simple way to show them what data assets we have.\n\nI'm considering the following:\n\n1. A big data dictionary page in confluence: This is how it was done in my previous company a page starts with a big table with columns: table name, column name, column description, source, logic...etc.  I feel it'll be a good table for data analysts to refer to, but not sure if it will be too detailed for those senior stakeholders.\n2. A dashboard in Databricks: Chatgpt brought up this method to me, by using information tables in databricks, I will be able to create a databricks dashboard for data inventory. the pros will be it will automatically handle the new coming tables and cols which saves a bit of overhead. But I am not sure if it will be easy to add and put notes on the tables/columns or formatting the table in a more insightful way(grouping by topic for example)\n3. A mindmap: Came to my mind that a mindmap is usually easy to follow and helps in capturing the big picture and overall structure.  Perhaps a mind map that branches out based on topics, subtopics, and table names?\n\n4. Do you have any other suggestions for presenting data assets?\n\nThanks in advance!", "author_fullname": "t2_hf67tpfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presenting Data Inventory to the Management team in the startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fnity", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702273576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data engineer at a startup and have been working on getting various operational data into Databricks. My next challenge is to present an overview of our current data assets to the management team (Some C-level stakeholders also like to know). They have also requested an internal dashboard with some key info, but they have no idea what I currently have in the Databricks warehouse, and I and not entirely sure which data they&amp;#39;ll find most relevant...As a first step, I&amp;#39;d like to have a clear and simple way to show them what data assets we have.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A big data dictionary page in confluence: This is how it was done in my previous company a page starts with a big table with columns: table name, column name, column description, source, logic...etc.  I feel it&amp;#39;ll be a good table for data analysts to refer to, but not sure if it will be too detailed for those senior stakeholders.&lt;/li&gt;\n&lt;li&gt;A dashboard in Databricks: Chatgpt brought up this method to me, by using information tables in databricks, I will be able to create a databricks dashboard for data inventory. the pros will be it will automatically handle the new coming tables and cols which saves a bit of overhead. But I am not sure if it will be easy to add and put notes on the tables/columns or formatting the table in a more insightful way(grouping by topic for example)&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A mindmap: Came to my mind that a mindmap is usually easy to follow and helps in capturing the big picture and overall structure.  Perhaps a mind map that branches out based on topics, subtopics, and table names?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Do you have any other suggestions for presenting data assets?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fnity", "is_robot_indexable": true, "report_reasons": null, "author": "East-Garage2337", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fnity/presenting_data_inventory_to_the_management_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fnity/presenting_data_inventory_to_the_management_team/", "subreddit_subscribers": 145279, "created_utc": 1702273576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently unemployed for last 6 months. I am having 8 years of experience in ETL, data warehouse building, snowflake, spark and tableau. \n\nI also have bit of hands on experience in low code tools like fivetran, data modelling tools like dbt.\n\nDue to the market conditions I am not receiving any job offers, so planning to upskill in the time being. I already have certifications in aws solution architect associate, snowflake snowpro core and Apache spark developer by Databricks. \n\nMy question is, what new technology can I learn and get certified in to get an edge when the market is up? Should I try my hands on any NoSQL databases? Will learning kafka will open up my job chances? \n\nPS: my priority right now is landing up a job immediately once the market is up. I am continuously applying jobs, but not getting any calls. Seeing that there are not much calls, I am panicked and wants to learn something related to my field but can land me a job.", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What new to learn in DE now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18fusjx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702302069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently unemployed for last 6 months. I am having 8 years of experience in ETL, data warehouse building, snowflake, spark and tableau. &lt;/p&gt;\n\n&lt;p&gt;I also have bit of hands on experience in low code tools like fivetran, data modelling tools like dbt.&lt;/p&gt;\n\n&lt;p&gt;Due to the market conditions I am not receiving any job offers, so planning to upskill in the time being. I already have certifications in aws solution architect associate, snowflake snowpro core and Apache spark developer by Databricks. &lt;/p&gt;\n\n&lt;p&gt;My question is, what new technology can I learn and get certified in to get an edge when the market is up? Should I try my hands on any NoSQL databases? Will learning kafka will open up my job chances? &lt;/p&gt;\n\n&lt;p&gt;PS: my priority right now is landing up a job immediately once the market is up. I am continuously applying jobs, but not getting any calls. Seeing that there are not much calls, I am panicked and wants to learn something related to my field but can land me a job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18fusjx", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fusjx/what_new_to_learn_in_de_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fusjx/what_new_to_learn_in_de_now/", "subreddit_subscribers": 145279, "created_utc": 1702302069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if anyone has war stories, success or failures, of using DuckDB in their production pipelines. \n\nWhat is the ecosystem look like? What tools are you using? What drove success or failure in your case? \n\nI\u2019m an MSSQL shop, so hard sell to my team, but I\u2019m also the sole dev maintaining our pyspark environment, and think there\u2019s opportunities for duck, especially in some containerized use cases I\u2019m prototyping. Figured I\u2019d ask what others have done \ud83d\udcaa\ud83c\udffb", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Production DuckDB Setups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fig7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702256645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if anyone has war stories, success or failures, of using DuckDB in their production pipelines. &lt;/p&gt;\n\n&lt;p&gt;What is the ecosystem look like? What tools are you using? What drove success or failure in your case? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m an MSSQL shop, so hard sell to my team, but I\u2019m also the sole dev maintaining our pyspark environment, and think there\u2019s opportunities for duck, especially in some containerized use cases I\u2019m prototyping. Figured I\u2019d ask what others have done \ud83d\udcaa\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fig7l", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fig7l/production_duckdb_setups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fig7l/production_duckdb_setups/", "subreddit_subscribers": 145279, "created_utc": 1702256645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used Facebook API before?\n\nI\u2019ve been off Facebook for months, because the ads and Watch were getting ridiculous.\n\n\nI\u2019m wondering what Facebook API\u2019s limitations are, because it seems like it\u2019d be really simple to build- I\u2019d like to basically browse without any ads or timeline \u201calgorithm\u201d- most recent friends posts sorted by date. Also add Events near me and filter by date once a week maybe. In theory if filtering out posts with less than x likes, it\u2019d take 5 minutes tops per week to get everything needed and none of the garbage. \n\nHas anyone used Facebook API before? I don\u2019t want to ask there because most people monetize and aren\u2019t going to be keen on someone trying to get around their ads.", "author_fullname": "t2_vgxtzjvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Facebook API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ft5sq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702296708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used Facebook API before?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been off Facebook for months, because the ads and Watch were getting ridiculous.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering what Facebook API\u2019s limitations are, because it seems like it\u2019d be really simple to build- I\u2019d like to basically browse without any ads or timeline \u201calgorithm\u201d- most recent friends posts sorted by date. Also add Events near me and filter by date once a week maybe. In theory if filtering out posts with less than x likes, it\u2019d take 5 minutes tops per week to get everything needed and none of the garbage. &lt;/p&gt;\n\n&lt;p&gt;Has anyone used Facebook API before? I don\u2019t want to ask there because most people monetize and aren\u2019t going to be keen on someone trying to get around their ads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ft5sq", "is_robot_indexable": true, "report_reasons": null, "author": "BestTomatillo6197", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ft5sq/facebook_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ft5sq/facebook_api/", "subreddit_subscribers": 145279, "created_utc": 1702296708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, this is my first DE project.    [Baitur5/reddit\\_api\\_elt (github.com)](https://github.com/Baitur5/reddit_api_elt) . It is basically about   \na data pipeline that extracts Reddit data for a Google Data Studio report, focusing on a specific subreddit   \nCan you guys check it out , and give some advice &amp; tips on how to improve it or the next things I should add.  \n\n\nP.S. I followed steps from this repository but made some adjustments: [ABZ-Aaron/Reddit-API-Pipeline (github.com)](https://github.com/ABZ-Aaron/Reddit-API-Pipeline) ", "author_fullname": "t2_kq543w8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit ELT Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18froaz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702291003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, this is my first DE project.    &lt;a href=\"https://github.com/Baitur5/reddit_api_elt\"&gt;Baitur5/reddit_api_elt (github.com)&lt;/a&gt; . It is basically about&lt;br/&gt;\na data pipeline that extracts Reddit data for a Google Data Studio report, focusing on a specific subreddit&lt;br/&gt;\nCan you guys check it out , and give some advice &amp;amp; tips on how to improve it or the next things I should add.  &lt;/p&gt;\n\n&lt;p&gt;P.S. I followed steps from this repository but made some adjustments: &lt;a href=\"https://github.com/ABZ-Aaron/Reddit-API-Pipeline\"&gt;ABZ-Aaron/Reddit-API-Pipeline (github.com)&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18froaz", "is_robot_indexable": true, "report_reasons": null, "author": "ulukbekovbr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18froaz/reddit_elt_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18froaz/reddit_elt_pipeline/", "subreddit_subscribers": 145279, "created_utc": 1702291003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company just announced a restructuring of our central data platform org and my team was effected.\n\nPreviously we were basically a data engineering team for a specific data domain developing and owning pipelines, data lakes/warehouse, and visualizations. However, post-restructuring we\u2019re now \u201ctechnically\u201d part of the platform team, but from what I understand we\u2019re  basically end-users for the core platform team, and they\u2019ll be building tools for us to ingest, store, transform etc\u2026 we\u2019re just focused on ingestion.\n\nUltimately I\u2019m worried about the long term staying power of a team that ONLY does ingestion using simplified internal tools. What makes matters worse is that the platform team is looking to build these tools for general use by business data engineers. So in a future where other groups can use the same tools to ingest their own data, what\u2019s the value of a central data team?\n\nI\u2019m planning to talk with my manager about this, but I want to come with some idea of a better direction in mind before I have that conversation, so curious if anyone here has advice?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do central data platforms usually have dedicated central ingestion teams?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18frbal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702289515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company just announced a restructuring of our central data platform org and my team was effected.&lt;/p&gt;\n\n&lt;p&gt;Previously we were basically a data engineering team for a specific data domain developing and owning pipelines, data lakes/warehouse, and visualizations. However, post-restructuring we\u2019re now \u201ctechnically\u201d part of the platform team, but from what I understand we\u2019re  basically end-users for the core platform team, and they\u2019ll be building tools for us to ingest, store, transform etc\u2026 we\u2019re just focused on ingestion.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I\u2019m worried about the long term staying power of a team that ONLY does ingestion using simplified internal tools. What makes matters worse is that the platform team is looking to build these tools for general use by business data engineers. So in a future where other groups can use the same tools to ingest their own data, what\u2019s the value of a central data team?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m planning to talk with my manager about this, but I want to come with some idea of a better direction in mind before I have that conversation, so curious if anyone here has advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18frbal", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18frbal/do_central_data_platforms_usually_have_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18frbal/do_central_data_platforms_usually_have_dedicated/", "subreddit_subscribers": 145279, "created_utc": 1702289515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you perform updates and deletes in a data warehouse? I've been told that temporality in a data warehouse should be handled through either temporal surrogation or data vault methods. Neither maintains an \"expired\" attribute.  \n ", "author_fullname": "t2_jg3w8gbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Updates and Deletes in a Data Warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18fubwe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702300627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you perform updates and deletes in a data warehouse? I&amp;#39;ve been told that temporality in a data warehouse should be handled through either temporal surrogation or data vault methods. Neither maintains an &amp;quot;expired&amp;quot; attribute.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fubwe", "is_robot_indexable": true, "report_reasons": null, "author": "tamargal91", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fubwe/updates_and_deletes_in_a_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fubwe/updates_and_deletes_in_a_data_warehouse/", "subreddit_subscribers": 145279, "created_utc": 1702300627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, \n\nAs in the title, I am trying to run dbt jobs via Cloud Composer 2. From this video: [https://www.youtube.com/watch?v=Vl2KC7-BfuM&amp;t=670s&amp;ab\\_channel=PracticalGCP](https://www.youtube.com/watch?v=Vl2KC7-BfuM&amp;t=670s&amp;ab_channel=PracticalGCP), I understood that the way to do so is as follows: \n\n\\- Create a Cloud Composer 2 environment\n\n\\- Push DAGs to the Cloud Composer's \"default\" dag folder in Cloud Storage\n\n\\- Create a docker image with Dbt, push it to Artifact Registry\n\n\\- Create a kubernetes service account which impersonates an IAM service account (using workload identity)\n\n\\- In the DAG file, call the KubernetesPodOperator and retrieve the Dbt image from Cloud Storage \n\n  \nThis is not working for me. I get this error. does anyone have any idea what this error mean? for starters, I don't recognize the composer-2-5-2-airflow-2-6-3-4edb9f24:default service account.   \n\n\nThank you for your time if you read this.   \n\n\n    *** Reading remote log from gs://europe-west9-ga4k8podv4-4edb9f24-bucket/logs/dag_id=airflow_k8_dbt_demo/run_id=manual__2023-12-11T12:23:49.136775+00:00/task_id=run_dbt_job_on_k8_demo/attempt=1.log.\n    [2023-12-11, 12:23:54 UTC] {taskinstance.py:1104} INFO - Dependencies all met for dep_context=non-requeueable deps ti=&lt;TaskInstance: airflow_k8_dbt_demo.run_dbt_job_on_k8_demo manual__2023-12-11T12:23:49.136775+00:00 [queued]&gt;\n    [2023-12-11, 12:23:54 UTC] {taskinstance.py:1104} INFO - Dependencies all met for dep_context=requeueable deps ti=&lt;TaskInstance: airflow_k8_dbt_demo.run_dbt_job_on_k8_demo manual__2023-12-11T12:23:49.136775+00:00 [queued]&gt;\n    [2023-12-11, 12:23:54 UTC] {taskinstance.py:1309} INFO - Starting attempt 1 of 1\n    [2023-12-11, 12:23:54 UTC] {taskinstance.py:1328} INFO - Executing &lt;Task(KubernetesPodOperator): run_dbt_job_on_k8_demo&gt; on 2023-12-11 12:23:49.136775+00:00\n    [2023-12-11, 12:23:55 UTC] {standard_task_runner.py:57} INFO - Started process 164181 to run task\n    [2023-12-11, 12:23:55 UTC] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'airflow_k8_dbt_demo', 'run_dbt_job_on_k8_demo', 'manual__2023-12-11T12:23:49.136775+00:00', '--job-id', '840', '--raw', '--subdir', 'DAGS_FOLDER/airflow_k8_dbt_demo.py', '--cfg-path', '/tmp/tmpmibczwcn']\n    [2023-12-11, 12:23:55 UTC] {standard_task_runner.py:85} INFO - Job 840: Subtask run_dbt_job_on_k8_demo\n    [2023-12-11, 12:23:55 UTC] {task_command.py:414} INFO - Running &lt;TaskInstance: airflow_k8_dbt_demo.run_dbt_job_on_k8_demo manual__2023-12-11T12:23:49.136775+00:00 [running]&gt; on host airflow-worker-r8zh5\n    [2023-12-11, 12:23:56 UTC] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='airflow@example.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='airflow_k8_dbt_demo' AIRFLOW_CTX_TASK_ID='run_dbt_job_on_k8_demo' AIRFLOW_CTX_EXECUTION_DATE='2023-12-11T12:23:49.136775+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-12-11T12:23:49.136775+00:00'\n    [2023-12-11, 12:23:56 UTC] {pod.py:973} INFO - Building pod dbt-run-k8-f1mch4u9 with labels: {'dag_id': 'airflow_k8_dbt_demo', 'task_id': 'run_dbt_job_on_k8_demo', 'run_id': 'manual__2023-12-11T122349.1367750000-90891fe48', 'kubernetes_pod_operator': 'True', 'try_number': '1'}\n    [2023-12-11, 12:23:56 UTC] {pod.py:1027} ERROR - 'NoneType' object has no attribute 'metadata'\n    Traceback (most recent call last):\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 598, in execute_sync\n        self.pod = self.get_or_create_pod(  # must set `self.pod` for `on_kill`\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 555, in get_or_create_pod\n        pod = self.find_pod(self.namespace or pod_request_obj.metadata.namespace, context=context)\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 537, in find_pod\n        pod_list = self.client.list_namespaced_pod(\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py\", line 15697, in list_namespaced_pod\n        return self.list_namespaced_pod_with_http_info(namespace, **kwargs)  # noqa: E501\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py\", line 15812, in list_namespaced_pod_with_http_info\n        return self.api_client.call_api(\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py\", line 348, in call_api\n        return self.__call_api(resource_path, method,\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py\", line 180, in __call_api\n        response_data = self.request(\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py\", line 373, in request\n        return self.rest_client.GET(url,\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/rest.py\", line 240, in GET\n        return self.request(\"GET\", url,\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/rest.py\", line 234, in request\n        raise ApiException(http_resp=r)\n    kubernetes.client.exceptions.ApiException: (403)\n    Reason: Forbidden\n    HTTP response headers: HTTPHeaderDict({'Audit-Id': 'e17a5730-8c00-4075-90c9-c5234afe9c77', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'X-Kubernetes-Pf-Flowschema-Uid': '9a22eccf-11b5-4f8b-beaf-b3d3b2018a57', 'X-Kubernetes-Pf-Prioritylevel-Uid': '0bf9b385-6eb9-48a8-bb6b-5b6586ee65d1', 'Date': 'Mon, 11 Dec 2023 12:23:56 GMT', 'Content-Length': '317'})\n    HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"pods is forbidden: User \\\"system:serviceaccount:composer-2-5-2-airflow-2-6-3-4edb9f24:default\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"k8-executor2\\\"\",\"reason\":\"Forbidden\",\"details\":{\"kind\":\"pods\"},\"code\":403}\n    \n    \n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 865, in patch_already_checked\n        name=pod.metadata.name,\n    AttributeError: 'NoneType' object has no attribute 'metadata'\n    [2023-12-11, 12:23:57 UTC] {taskinstance.py:1826} ERROR - Task failed with exception\n    Traceback (most recent call last):\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 598, in execute_sync\n        self.pod = self.get_or_create_pod(  # must set `self.pod` for `on_kill`\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 555, in get_or_create_pod\n        pod = self.find_pod(self.namespace or pod_request_obj.metadata.namespace, context=context)\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 537, in find_pod\n        pod_list = self.client.list_namespaced_pod(\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py\", line 15697, in list_namespaced_pod\n        return self.list_namespaced_pod_with_http_info(namespace, **kwargs)  # noqa: E501\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py\", line 15812, in list_namespaced_pod_with_http_info\n        return self.api_client.call_api(\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py\", line 348, in call_api\n        return self.__call_api(resource_path, method,\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py\", line 180, in __call_api\n        response_data = self.request(\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py\", line 373, in request\n        return self.rest_client.GET(url,\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/rest.py\", line 240, in GET\n        return self.request(\"GET\", url,\n      File \"/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/rest.py\", line 234, in request\n        raise ApiException(http_resp=r)\n    kubernetes.client.exceptions.ApiException: (403)\n    Reason: Forbidden\n    HTTP response headers: HTTPHeaderDict({'Audit-Id': 'e17a5730-8c00-4075-90c9-c5234afe9c77', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'X-Kubernetes-Pf-Flowschema-Uid': '9a22eccf-11b5-4f8b-beaf-b3d3b2018a57', 'X-Kubernetes-Pf-Prioritylevel-Uid': '0bf9b385-6eb9-48a8-bb6b-5b6586ee65d1', 'Date': 'Mon, 11 Dec 2023 12:23:56 GMT', 'Content-Length': '317'})\n    HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"pods is forbidden: User \\\"system:serviceaccount:composer-2-5-2-airflow-2-6-3-4edb9f24:default\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"k8-executor2\\\"\",\"reason\":\"Forbidden\",\"details\":{\"kind\":\"pods\"},\"code\":403}\n    \n    \n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 592, in execute\n        return self.execute_sync(context)\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 632, in execute_sync\n        self.cleanup(\n      File \"/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py\", line 765, in cleanup\n        raise AirflowException(\n    airflow.exceptions.AirflowException: Pod dbt-run-k8-f1mch4u9 returned a failure.\n    remote_pod: None\n    [2023-12-11, 12:23:57 UTC] {taskinstance.py:1346} INFO - Marking task as FAILED. dag_id=airflow_k8_dbt_demo, task_id=run_dbt_job_on_k8_demo, execution_date=20231211T122349, start_date=20231211T122354, end_date=20231211T122357\n    [2023-12-11, 12:23:57 UTC] {standard_task_runner.py:104} ERROR - Failed to execute job 840 for task run_dbt_job_on_k8_demo (Pod dbt-run-k8-f1mch4u9 returned a failure.\n    remote_pod: None; 164181)\n    [2023-12-11, 12:23:57 UTC] {local_task_job_runner.py:225} INFO - Task exited with return code 1\n    [2023-12-11, 12:23:57 UTC] {taskinstance.py:2656} INFO - 0 downstream tasks scheduled from follow-on schedule check\n    ", "author_fullname": "t2_1jh436du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issue running Dbt via KubernetesPodOperator in Google Cloud Composer 2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ftgaa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702297739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;As in the title, I am trying to run dbt jobs via Cloud Composer 2. From this video: &lt;a href=\"https://www.youtube.com/watch?v=Vl2KC7-BfuM&amp;amp;t=670s&amp;amp;ab_channel=PracticalGCP\"&gt;https://www.youtube.com/watch?v=Vl2KC7-BfuM&amp;amp;t=670s&amp;amp;ab_channel=PracticalGCP&lt;/a&gt;, I understood that the way to do so is as follows: &lt;/p&gt;\n\n&lt;p&gt;- Create a Cloud Composer 2 environment&lt;/p&gt;\n\n&lt;p&gt;- Push DAGs to the Cloud Composer&amp;#39;s &amp;quot;default&amp;quot; dag folder in Cloud Storage&lt;/p&gt;\n\n&lt;p&gt;- Create a docker image with Dbt, push it to Artifact Registry&lt;/p&gt;\n\n&lt;p&gt;- Create a kubernetes service account which impersonates an IAM service account (using workload identity)&lt;/p&gt;\n\n&lt;p&gt;- In the DAG file, call the KubernetesPodOperator and retrieve the Dbt image from Cloud Storage &lt;/p&gt;\n\n&lt;p&gt;This is not working for me. I get this error. does anyone have any idea what this error mean? for starters, I don&amp;#39;t recognize the composer-2-5-2-airflow-2-6-3-4edb9f24:default service account.   &lt;/p&gt;\n\n&lt;p&gt;Thank you for your time if you read this.   &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;*** Reading remote log from gs://europe-west9-ga4k8podv4-4edb9f24-bucket/logs/dag_id=airflow_k8_dbt_demo/run_id=manual__2023-12-11T12:23:49.136775+00:00/task_id=run_dbt_job_on_k8_demo/attempt=1.log.\n[2023-12-11, 12:23:54 UTC] {taskinstance.py:1104} INFO - Dependencies all met for dep_context=non-requeueable deps ti=&amp;lt;TaskInstance: airflow_k8_dbt_demo.run_dbt_job_on_k8_demo manual__2023-12-11T12:23:49.136775+00:00 [queued]&amp;gt;\n[2023-12-11, 12:23:54 UTC] {taskinstance.py:1104} INFO - Dependencies all met for dep_context=requeueable deps ti=&amp;lt;TaskInstance: airflow_k8_dbt_demo.run_dbt_job_on_k8_demo manual__2023-12-11T12:23:49.136775+00:00 [queued]&amp;gt;\n[2023-12-11, 12:23:54 UTC] {taskinstance.py:1309} INFO - Starting attempt 1 of 1\n[2023-12-11, 12:23:54 UTC] {taskinstance.py:1328} INFO - Executing &amp;lt;Task(KubernetesPodOperator): run_dbt_job_on_k8_demo&amp;gt; on 2023-12-11 12:23:49.136775+00:00\n[2023-12-11, 12:23:55 UTC] {standard_task_runner.py:57} INFO - Started process 164181 to run task\n[2023-12-11, 12:23:55 UTC] {standard_task_runner.py:84} INFO - Running: [&amp;#39;airflow&amp;#39;, &amp;#39;tasks&amp;#39;, &amp;#39;run&amp;#39;, &amp;#39;airflow_k8_dbt_demo&amp;#39;, &amp;#39;run_dbt_job_on_k8_demo&amp;#39;, &amp;#39;manual__2023-12-11T12:23:49.136775+00:00&amp;#39;, &amp;#39;--job-id&amp;#39;, &amp;#39;840&amp;#39;, &amp;#39;--raw&amp;#39;, &amp;#39;--subdir&amp;#39;, &amp;#39;DAGS_FOLDER/airflow_k8_dbt_demo.py&amp;#39;, &amp;#39;--cfg-path&amp;#39;, &amp;#39;/tmp/tmpmibczwcn&amp;#39;]\n[2023-12-11, 12:23:55 UTC] {standard_task_runner.py:85} INFO - Job 840: Subtask run_dbt_job_on_k8_demo\n[2023-12-11, 12:23:55 UTC] {task_command.py:414} INFO - Running &amp;lt;TaskInstance: airflow_k8_dbt_demo.run_dbt_job_on_k8_demo manual__2023-12-11T12:23:49.136775+00:00 [running]&amp;gt; on host airflow-worker-r8zh5\n[2023-12-11, 12:23:56 UTC] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL=&amp;#39;airflow@example.com&amp;#39; AIRFLOW_CTX_DAG_OWNER=&amp;#39;airflow&amp;#39; AIRFLOW_CTX_DAG_ID=&amp;#39;airflow_k8_dbt_demo&amp;#39; AIRFLOW_CTX_TASK_ID=&amp;#39;run_dbt_job_on_k8_demo&amp;#39; AIRFLOW_CTX_EXECUTION_DATE=&amp;#39;2023-12-11T12:23:49.136775+00:00&amp;#39; AIRFLOW_CTX_TRY_NUMBER=&amp;#39;1&amp;#39; AIRFLOW_CTX_DAG_RUN_ID=&amp;#39;manual__2023-12-11T12:23:49.136775+00:00&amp;#39;\n[2023-12-11, 12:23:56 UTC] {pod.py:973} INFO - Building pod dbt-run-k8-f1mch4u9 with labels: {&amp;#39;dag_id&amp;#39;: &amp;#39;airflow_k8_dbt_demo&amp;#39;, &amp;#39;task_id&amp;#39;: &amp;#39;run_dbt_job_on_k8_demo&amp;#39;, &amp;#39;run_id&amp;#39;: &amp;#39;manual__2023-12-11T122349.1367750000-90891fe48&amp;#39;, &amp;#39;kubernetes_pod_operator&amp;#39;: &amp;#39;True&amp;#39;, &amp;#39;try_number&amp;#39;: &amp;#39;1&amp;#39;}\n[2023-12-11, 12:23:56 UTC] {pod.py:1027} ERROR - &amp;#39;NoneType&amp;#39; object has no attribute &amp;#39;metadata&amp;#39;\nTraceback (most recent call last):\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 598, in execute_sync\n    self.pod = self.get_or_create_pod(  # must set `self.pod` for `on_kill`\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 555, in get_or_create_pod\n    pod = self.find_pod(self.namespace or pod_request_obj.metadata.namespace, context=context)\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 537, in find_pod\n    pod_list = self.client.list_namespaced_pod(\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py&amp;quot;, line 15697, in list_namespaced_pod\n    return self.list_namespaced_pod_with_http_info(namespace, **kwargs)  # noqa: E501\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py&amp;quot;, line 15812, in list_namespaced_pod_with_http_info\n    return self.api_client.call_api(\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py&amp;quot;, line 348, in call_api\n    return self.__call_api(resource_path, method,\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py&amp;quot;, line 180, in __call_api\n    response_data = self.request(\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py&amp;quot;, line 373, in request\n    return self.rest_client.GET(url,\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/rest.py&amp;quot;, line 240, in GET\n    return self.request(&amp;quot;GET&amp;quot;, url,\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/rest.py&amp;quot;, line 234, in request\n    raise ApiException(http_resp=r)\nkubernetes.client.exceptions.ApiException: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({&amp;#39;Audit-Id&amp;#39;: &amp;#39;e17a5730-8c00-4075-90c9-c5234afe9c77&amp;#39;, &amp;#39;Cache-Control&amp;#39;: &amp;#39;no-cache, private&amp;#39;, &amp;#39;Content-Type&amp;#39;: &amp;#39;application/json&amp;#39;, &amp;#39;X-Content-Type-Options&amp;#39;: &amp;#39;nosniff&amp;#39;, &amp;#39;X-Kubernetes-Pf-Flowschema-Uid&amp;#39;: &amp;#39;9a22eccf-11b5-4f8b-beaf-b3d3b2018a57&amp;#39;, &amp;#39;X-Kubernetes-Pf-Prioritylevel-Uid&amp;#39;: &amp;#39;0bf9b385-6eb9-48a8-bb6b-5b6586ee65d1&amp;#39;, &amp;#39;Date&amp;#39;: &amp;#39;Mon, 11 Dec 2023 12:23:56 GMT&amp;#39;, &amp;#39;Content-Length&amp;#39;: &amp;#39;317&amp;#39;})\nHTTP response body: {&amp;quot;kind&amp;quot;:&amp;quot;Status&amp;quot;,&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;metadata&amp;quot;:{},&amp;quot;status&amp;quot;:&amp;quot;Failure&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;pods is forbidden: User \\&amp;quot;system:serviceaccount:composer-2-5-2-airflow-2-6-3-4edb9f24:default\\&amp;quot; cannot list resource \\&amp;quot;pods\\&amp;quot; in API group \\&amp;quot;\\&amp;quot; in the namespace \\&amp;quot;k8-executor2\\&amp;quot;&amp;quot;,&amp;quot;reason&amp;quot;:&amp;quot;Forbidden&amp;quot;,&amp;quot;details&amp;quot;:{&amp;quot;kind&amp;quot;:&amp;quot;pods&amp;quot;},&amp;quot;code&amp;quot;:403}\n\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 865, in patch_already_checked\n    name=pod.metadata.name,\nAttributeError: &amp;#39;NoneType&amp;#39; object has no attribute &amp;#39;metadata&amp;#39;\n[2023-12-11, 12:23:57 UTC] {taskinstance.py:1826} ERROR - Task failed with exception\nTraceback (most recent call last):\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 598, in execute_sync\n    self.pod = self.get_or_create_pod(  # must set `self.pod` for `on_kill`\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 555, in get_or_create_pod\n    pod = self.find_pod(self.namespace or pod_request_obj.metadata.namespace, context=context)\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 537, in find_pod\n    pod_list = self.client.list_namespaced_pod(\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py&amp;quot;, line 15697, in list_namespaced_pod\n    return self.list_namespaced_pod_with_http_info(namespace, **kwargs)  # noqa: E501\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py&amp;quot;, line 15812, in list_namespaced_pod_with_http_info\n    return self.api_client.call_api(\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py&amp;quot;, line 348, in call_api\n    return self.__call_api(resource_path, method,\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py&amp;quot;, line 180, in __call_api\n    response_data = self.request(\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/api_client.py&amp;quot;, line 373, in request\n    return self.rest_client.GET(url,\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/rest.py&amp;quot;, line 240, in GET\n    return self.request(&amp;quot;GET&amp;quot;, url,\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/kubernetes/client/rest.py&amp;quot;, line 234, in request\n    raise ApiException(http_resp=r)\nkubernetes.client.exceptions.ApiException: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({&amp;#39;Audit-Id&amp;#39;: &amp;#39;e17a5730-8c00-4075-90c9-c5234afe9c77&amp;#39;, &amp;#39;Cache-Control&amp;#39;: &amp;#39;no-cache, private&amp;#39;, &amp;#39;Content-Type&amp;#39;: &amp;#39;application/json&amp;#39;, &amp;#39;X-Content-Type-Options&amp;#39;: &amp;#39;nosniff&amp;#39;, &amp;#39;X-Kubernetes-Pf-Flowschema-Uid&amp;#39;: &amp;#39;9a22eccf-11b5-4f8b-beaf-b3d3b2018a57&amp;#39;, &amp;#39;X-Kubernetes-Pf-Prioritylevel-Uid&amp;#39;: &amp;#39;0bf9b385-6eb9-48a8-bb6b-5b6586ee65d1&amp;#39;, &amp;#39;Date&amp;#39;: &amp;#39;Mon, 11 Dec 2023 12:23:56 GMT&amp;#39;, &amp;#39;Content-Length&amp;#39;: &amp;#39;317&amp;#39;})\nHTTP response body: {&amp;quot;kind&amp;quot;:&amp;quot;Status&amp;quot;,&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;metadata&amp;quot;:{},&amp;quot;status&amp;quot;:&amp;quot;Failure&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;pods is forbidden: User \\&amp;quot;system:serviceaccount:composer-2-5-2-airflow-2-6-3-4edb9f24:default\\&amp;quot; cannot list resource \\&amp;quot;pods\\&amp;quot; in API group \\&amp;quot;\\&amp;quot; in the namespace \\&amp;quot;k8-executor2\\&amp;quot;&amp;quot;,&amp;quot;reason&amp;quot;:&amp;quot;Forbidden&amp;quot;,&amp;quot;details&amp;quot;:{&amp;quot;kind&amp;quot;:&amp;quot;pods&amp;quot;},&amp;quot;code&amp;quot;:403}\n\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 592, in execute\n    return self.execute_sync(context)\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 632, in execute_sync\n    self.cleanup(\n  File &amp;quot;/opt/python3.8/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py&amp;quot;, line 765, in cleanup\n    raise AirflowException(\nairflow.exceptions.AirflowException: Pod dbt-run-k8-f1mch4u9 returned a failure.\nremote_pod: None\n[2023-12-11, 12:23:57 UTC] {taskinstance.py:1346} INFO - Marking task as FAILED. dag_id=airflow_k8_dbt_demo, task_id=run_dbt_job_on_k8_demo, execution_date=20231211T122349, start_date=20231211T122354, end_date=20231211T122357\n[2023-12-11, 12:23:57 UTC] {standard_task_runner.py:104} ERROR - Failed to execute job 840 for task run_dbt_job_on_k8_demo (Pod dbt-run-k8-f1mch4u9 returned a failure.\nremote_pod: None; 164181)\n[2023-12-11, 12:23:57 UTC] {local_task_job_runner.py:225} INFO - Task exited with return code 1\n[2023-12-11, 12:23:57 UTC] {taskinstance.py:2656} INFO - 0 downstream tasks scheduled from follow-on schedule check\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FbM0kpxpV6rKs-zDAz8WfJcWDronehRVz2ZJEH8-QQ0.jpg?auto=webp&amp;s=49a9ac2e7f8909887f3a923455524be606b558aa", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/FbM0kpxpV6rKs-zDAz8WfJcWDronehRVz2ZJEH8-QQ0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c62cb165870af6f1875600c5c18c5ce608e00fc", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/FbM0kpxpV6rKs-zDAz8WfJcWDronehRVz2ZJEH8-QQ0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47c0aedc5efa1e896df913a06cf516572cf82d30", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/FbM0kpxpV6rKs-zDAz8WfJcWDronehRVz2ZJEH8-QQ0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e489abc426ffc469b614bf31ec566f6d2ff76a10", "width": 320, "height": 240}], "variants": {}, "id": "QaCH8_RWIIfwLhl75XOdnJzWga9QLFpLeUIP560k6ME"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ftgaa", "is_robot_indexable": true, "report_reasons": null, "author": "Jeannetton", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ftgaa/issue_running_dbt_via_kubernetespodoperator_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ftgaa/issue_running_dbt_via_kubernetespodoperator_in/", "subreddit_subscribers": 145279, "created_utc": 1702297739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been using Meltano to run an ELT job but I'm not sure I'm following the best practices. I was thinking:\n\n* Run a this job (dynamically) on schedule with the command **meltano el tap-mongodb target-&lt;snowflake&gt; --state-id tap\\_mongodb\\_&lt;dev&gt;**\n* I originally wanted to use GCP cloud run but it doesn't work because of the 60m timeout. This ELT process can take 5-8 hours on a single CPU, I want it to run daily. A couple more questions on this 2nd bullet-point:\n   * Are meltano **partitions** able to parallelize across CPUs / machines? If so then this job doesn't need to take anywhere near 5-8 hours (more like 5-10 minutes on 100 machines)?\n   * Should I be using Airflow to do this? Simple cronjob on a VM? GCP cloud composer / dataflow? SO MANY TOOLS!! I just want this to run on the cloud while being able to see the logs in gcp cloud logging, integrating CI/CD, and mounting a volume from GCP secret manager.\n\n&amp;#x200B;\n\nI am hesitant to spend time diving into Airflow, but curious to hear your thoughts. Here's the below script I've been using to run in production **host.py**:\n\n&amp;#x200B;\n\n    from flask import Flask, make_response, request, redirect, Blueprint, render_template\n    import os\n    import subprocess\n    from configparser import ConfigParser\n    from datetime import datetime\n    from waitress import serve\n    from apscheduler.schedulers.background import BackgroundScheduler\n    import logging\n    import json\n    \n    config = ConfigParser()\n    config.read('config.ini')\n    \n    app = Flask(__name__)\n    app.url_map.strict_slashes = False\n    \n    MELTANO_TARGET = config['TAP_MONGODB']['MELTANO_TARGET']\n    \n    assert isinstance(MELTANO_TARGET, str), 'could not determine target'\n    \n    \n    ###### routes ######\n    \n    @app.route('/')\n    def index():\n        return 'Server is running.'\n    \n    @app.route('/tap_mongodb_dev', methods=['GET'])\n    def tap_mongodb_dev():\n        with app.app_context():\n            project_dir = 'taps/tap-mongodb'\n            run_command = f'meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}'\n            subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n            return make_response(f'Last ran project tap-mongodb-dev target {MELTANO_TARGET} at {cur_timestamp()}.', 200)\n    \n    @app.route('/tap_mongodb_production', methods=['GET'])\n    def tap_mongodb_production():\n        with app.app_context():\n            project_dir = 'taps/tap-mongodb'\n            run_command = f'meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}'\n            subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n            return make_response(f'Last ran project tap-mongodb-production target {MELTANO_TARGET} at {cur_timestamp()}.', 200)\n    \n    \n    if __name__ == \"__main__\":\n        logging.info(f'\\n*** Running environment {ENVIRONMENT}. ***\\n')\n    \n        scheduler = BackgroundScheduler(job_defaults={'max_instances': 2})\n    \n        ###### tap-mongodb ######\n    \n        tap_mongodb_cron = json.loads(config['TAP_MONGODB'][f'{ENVIRONMENT}_CRON_PARAMS'])\n        scheduler.add_job(tap_mongodb_functions[ENVIRONMENT], trigger='cron', **tap_mongodb_cron, jitter=120)\n    \n        ###### host ######\n    \n        HOST = '0.0.0.0'\n        PORT = 5000\n        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n        logging.info(f'Server is listening on port {PORT}')\n        logging.info(f'Hosting environment {ENVIRONMENT}')\n    \n        scheduler.start()\n    \n        serve(app, host=HOST, port=PORT, threads=2)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8xq51rif", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you run a long ELT job in the cloud on schedule?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fhseu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702254625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Meltano to run an ELT job but I&amp;#39;m not sure I&amp;#39;m following the best practices. I was thinking:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run a this job (dynamically) on schedule with the command &lt;strong&gt;meltano el tap-mongodb target-&amp;lt;snowflake&amp;gt; --state-id tap_mongodb_&amp;lt;dev&amp;gt;&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I originally wanted to use GCP cloud run but it doesn&amp;#39;t work because of the 60m timeout. This ELT process can take 5-8 hours on a single CPU, I want it to run daily. A couple more questions on this 2nd bullet-point:\n\n&lt;ul&gt;\n&lt;li&gt;Are meltano &lt;strong&gt;partitions&lt;/strong&gt; able to parallelize across CPUs / machines? If so then this job doesn&amp;#39;t need to take anywhere near 5-8 hours (more like 5-10 minutes on 100 machines)?&lt;/li&gt;\n&lt;li&gt;Should I be using Airflow to do this? Simple cronjob on a VM? GCP cloud composer / dataflow? SO MANY TOOLS!! I just want this to run on the cloud while being able to see the logs in gcp cloud logging, integrating CI/CD, and mounting a volume from GCP secret manager.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am hesitant to spend time diving into Airflow, but curious to hear your thoughts. Here&amp;#39;s the below script I&amp;#39;ve been using to run in production &lt;strong&gt;host.py&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from flask import Flask, make_response, request, redirect, Blueprint, render_template\nimport os\nimport subprocess\nfrom configparser import ConfigParser\nfrom datetime import datetime\nfrom waitress import serve\nfrom apscheduler.schedulers.background import BackgroundScheduler\nimport logging\nimport json\n\nconfig = ConfigParser()\nconfig.read(&amp;#39;config.ini&amp;#39;)\n\napp = Flask(__name__)\napp.url_map.strict_slashes = False\n\nMELTANO_TARGET = config[&amp;#39;TAP_MONGODB&amp;#39;][&amp;#39;MELTANO_TARGET&amp;#39;]\n\nassert isinstance(MELTANO_TARGET, str), &amp;#39;could not determine target&amp;#39;\n\n\n###### routes ######\n\n@app.route(&amp;#39;/&amp;#39;)\ndef index():\n    return &amp;#39;Server is running.&amp;#39;\n\n@app.route(&amp;#39;/tap_mongodb_dev&amp;#39;, methods=[&amp;#39;GET&amp;#39;])\ndef tap_mongodb_dev():\n    with app.app_context():\n        project_dir = &amp;#39;taps/tap-mongodb&amp;#39;\n        run_command = f&amp;#39;meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}&amp;#39;\n        subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n        return make_response(f&amp;#39;Last ran project tap-mongodb-dev target {MELTANO_TARGET} at {cur_timestamp()}.&amp;#39;, 200)\n\n@app.route(&amp;#39;/tap_mongodb_production&amp;#39;, methods=[&amp;#39;GET&amp;#39;])\ndef tap_mongodb_production():\n    with app.app_context():\n        project_dir = &amp;#39;taps/tap-mongodb&amp;#39;\n        run_command = f&amp;#39;meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}&amp;#39;\n        subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n        return make_response(f&amp;#39;Last ran project tap-mongodb-production target {MELTANO_TARGET} at {cur_timestamp()}.&amp;#39;, 200)\n\n\nif __name__ == &amp;quot;__main__&amp;quot;:\n    logging.info(f&amp;#39;\\n*** Running environment {ENVIRONMENT}. ***\\n&amp;#39;)\n\n    scheduler = BackgroundScheduler(job_defaults={&amp;#39;max_instances&amp;#39;: 2})\n\n    ###### tap-mongodb ######\n\n    tap_mongodb_cron = json.loads(config[&amp;#39;TAP_MONGODB&amp;#39;][f&amp;#39;{ENVIRONMENT}_CRON_PARAMS&amp;#39;])\n    scheduler.add_job(tap_mongodb_functions[ENVIRONMENT], trigger=&amp;#39;cron&amp;#39;, **tap_mongodb_cron, jitter=120)\n\n    ###### host ######\n\n    HOST = &amp;#39;0.0.0.0&amp;#39;\n    PORT = 5000\n    logging.basicConfig(level=logging.INFO, format=&amp;#39;%(asctime)s - %(levelname)s - %(message)s&amp;#39;)\n    logging.info(f&amp;#39;Server is listening on port {PORT}&amp;#39;)\n    logging.info(f&amp;#39;Hosting environment {ENVIRONMENT}&amp;#39;)\n\n    scheduler.start()\n\n    serve(app, host=HOST, port=PORT, threads=2)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fhseu", "is_robot_indexable": true, "report_reasons": null, "author": "Training_Butterfly70", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fhseu/how_do_you_run_a_long_elt_job_in_the_cloud_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fhseu/how_do_you_run_a_long_elt_job_in_the_cloud_on/", "subreddit_subscribers": 145279, "created_utc": 1702254625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is using Azure Databricks and I'm an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a \"/FileStore/file\\_path/file\\_name.xlsx\" file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn't really helped me up to this point.\n\n`w = WorkspaceClient(host=server_hostname,token = api_key)`\n\nI can successfully run `w.dbfs.download('/FileStore/file_path/file_name.xlsx')`, but can't figure out how to save the file down to local storage as an Excel file.\n\n`w.dbutils.fs.cp('dbfs:/FileStore/file_path/file_name.xlsx', 'C:/Users/user_name/file_path/file_name.xlsx')` results in \"java.net.URISyntaxException: Relative path in absolute URI\" for the C drive file path. This looks like an absolute path to me. Am I missing something?\n\n`w.files.get_status('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"Get-status API is not enabled.\" How do I enable it? I don't see a toggle for it in Admin Settings.\n\n`w.files.download('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"The ':' character is not supported\", but when I remove it the file path is no longer valid.\n\nI feel like I'm close on one of these but just can't seem to get it across the finish line.", "author_fullname": "t2_yk6x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone moved files between Azure Databricks DBFS and local storage using the Databricks API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7vcb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702227447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is using Azure Databricks and I&amp;#39;m an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a &amp;quot;/FileStore/file_path/file_name.xlsx&amp;quot; file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn&amp;#39;t really helped me up to this point.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w = WorkspaceClient(host=server_hostname,token = api_key)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I can successfully run &lt;code&gt;w.dbfs.download(&amp;#39;/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt;, but can&amp;#39;t figure out how to save the file down to local storage as an Excel file.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.dbutils.fs.cp(&amp;#39;dbfs:/FileStore/file_path/file_name.xlsx&amp;#39;, &amp;#39;C:/Users/user_name/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;java.net.URISyntaxException: Relative path in absolute URI&amp;quot; for the C drive file path. This looks like an absolute path to me. Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.get_status(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;Get-status API is not enabled.&amp;quot; How do I enable it? I don&amp;#39;t see a toggle for it in Admin Settings.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.download(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;The &amp;#39;:&amp;#39; character is not supported&amp;quot;, but when I remove it the file path is no longer valid.&lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m close on one of these but just can&amp;#39;t seem to get it across the finish line.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18f7vcb", "is_robot_indexable": true, "report_reasons": null, "author": "CurlyW15", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "subreddit_subscribers": 145279, "created_utc": 1702227447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The key here is the ETL aspect. I need sample databases in postgres, oracle (or any sql dialect really) that have both data and ETL .sql scripts that I don't have to reinvent. Does anyone know one that exists that have both?", "author_fullname": "t2_rk16bnik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for sample postgres and oracle databases that have ETL scripts preloaded into them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ffap8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702247443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The key here is the ETL aspect. I need sample databases in postgres, oracle (or any sql dialect really) that have both data and ETL .sql scripts that I don&amp;#39;t have to reinvent. Does anyone know one that exists that have both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ffap8", "is_robot_indexable": true, "report_reasons": null, "author": "notstoppinguntil30", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ffap8/looking_for_sample_postgres_and_oracle_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ffap8/looking_for_sample_postgres_and_oracle_databases/", "subreddit_subscribers": 145279, "created_utc": 1702247443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, \n\nI am a employee in an MNC having 2.5 years of experience and currently working on a support based project with the tech stack as webmethods which is a integration tool based out of Java and it is being to connect different systems / businesses. I'm bored and frustrated at the same time because the work is same and almost repetitive with no signs of growth and learning. That's when I decided to switch domain but\n\nI was very confused while choosing between Data Engineering and Java. But I choose Data Engineering and started preparing for it. I currently have experience with python easy to medium, SQL mid to advance level and achieved two cloud certifications AZ-900 &amp; AZ-204 currently preparing for DP-203. I have basic knowledge and hands-on azure df, Synapse and storage. \n\nI tried looking for jobs matching my skills but whenever I started checking the job description I always lack in some or the other skills such as Big data and Spark. So if someone can guide me to tackel with this and land a job in data engineer domain it would be a great help. \n\nThanks in Advance :)", "author_fullname": "t2_ib9mu62z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Carrer switch to Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18frtzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702291672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, &lt;/p&gt;\n\n&lt;p&gt;I am a employee in an MNC having 2.5 years of experience and currently working on a support based project with the tech stack as webmethods which is a integration tool based out of Java and it is being to connect different systems / businesses. I&amp;#39;m bored and frustrated at the same time because the work is same and almost repetitive with no signs of growth and learning. That&amp;#39;s when I decided to switch domain but&lt;/p&gt;\n\n&lt;p&gt;I was very confused while choosing between Data Engineering and Java. But I choose Data Engineering and started preparing for it. I currently have experience with python easy to medium, SQL mid to advance level and achieved two cloud certifications AZ-900 &amp;amp; AZ-204 currently preparing for DP-203. I have basic knowledge and hands-on azure df, Synapse and storage. &lt;/p&gt;\n\n&lt;p&gt;I tried looking for jobs matching my skills but whenever I started checking the job description I always lack in some or the other skills such as Big data and Spark. So if someone can guide me to tackel with this and land a job in data engineer domain it would be a great help. &lt;/p&gt;\n\n&lt;p&gt;Thanks in Advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18frtzg", "is_robot_indexable": true, "report_reasons": null, "author": "iamDjsahu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18frtzg/carrer_switch_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18frtzg/carrer_switch_to_data_engineering/", "subreddit_subscribers": 145279, "created_utc": 1702291672.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}