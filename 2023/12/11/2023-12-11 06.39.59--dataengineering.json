{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. \n\nData is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. \n\nOverall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. \n\nAnyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. ", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I Was Happier Being a Bartender Compared to Being a 6 Figure DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ffzmx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 140, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 140, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702249969.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702249347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. &lt;/p&gt;\n\n&lt;p&gt;Data is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. &lt;/p&gt;\n\n&lt;p&gt;Overall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. &lt;/p&gt;\n\n&lt;p&gt;Anyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18ffzmx", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/", "subreddit_subscribers": 145197, "created_utc": 1702249347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry for the noob question if it sounds ignorant. Here is the situation. \n\nI joined a new company, and inherited a lot of legacy codes (ETL jobs, written in python) from another colleague who already left before I got to talk to him. Throughout his code, he made extensive use of duckdb just to query pandas dataframes, basically, something like this.\n\n```python3\nimport duckdb, pandas as pd\ndf:pd.DataFrame\n\ndf.loc[df['value']&gt;20] # Pandas form\n\nquery:str='SELECT * FROM df WHERE value&gt;20;'\nduckdb.sql(query=query).df() # SQL Form\n```\n\nSo it seems duckdb gives a SQL like interface to pandas, but is that all it does? Any trade off between just using pandas semantics as opposed to writing a raw SQL query in the code? The pandas semantic looks a lot cleaner to me, and introducing another dependency seems rather superfluous. \n\nSo the basic questions are \n* Why would a developer use duckdb? Does it provide better performance than pandas? Or is the SQL like interface the most important purpose here? \n* The examples are for some legacy codes that I am going through, but for myself, I use polars or dask for tabular data manipulation. Does duckdb work with those frameworks?", "author_fullname": "t2_kkymhi5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is DuckDB Used For?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f0tju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702204519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for the noob question if it sounds ignorant. Here is the situation. &lt;/p&gt;\n\n&lt;p&gt;I joined a new company, and inherited a lot of legacy codes (ETL jobs, written in python) from another colleague who already left before I got to talk to him. Throughout his code, he made extensive use of duckdb just to query pandas dataframes, basically, something like this.&lt;/p&gt;\n\n&lt;p&gt;```python3\nimport duckdb, pandas as pd\ndf:pd.DataFrame&lt;/p&gt;\n\n&lt;p&gt;df.loc[df[&amp;#39;value&amp;#39;]&amp;gt;20] # Pandas form&lt;/p&gt;\n\n&lt;p&gt;query:str=&amp;#39;SELECT * FROM df WHERE value&amp;gt;20;&amp;#39;\nduckdb.sql(query=query).df() # SQL Form\n```&lt;/p&gt;\n\n&lt;p&gt;So it seems duckdb gives a SQL like interface to pandas, but is that all it does? Any trade off between just using pandas semantics as opposed to writing a raw SQL query in the code? The pandas semantic looks a lot cleaner to me, and introducing another dependency seems rather superfluous. &lt;/p&gt;\n\n&lt;p&gt;So the basic questions are \n* Why would a developer use duckdb? Does it provide better performance than pandas? Or is the SQL like interface the most important purpose here? \n* The examples are for some legacy codes that I am going through, but for myself, I use polars or dask for tabular data manipulation. Does duckdb work with those frameworks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18f0tju", "is_robot_indexable": true, "report_reasons": null, "author": "SpiderMangauntlet", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f0tju/what_is_duckdb_used_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f0tju/what_is_duckdb_used_for/", "subreddit_subscribers": 145197, "created_utc": 1702204519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.\n\nEncrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage PII?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7m1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702226742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.&lt;/p&gt;\n\n&lt;p&gt;Encrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f7m1t", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "subreddit_subscribers": 145197, "created_utc": 1702226742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will soon start working with Delta lakes, and would like to know a bit about people's experiences with it.\n\nAny main reasons why you're using it? did it fulfill your data needs? was it worth the time investment? Please feel free to share everything :) \n\nThanks!", "author_fullname": "t2_m84na74fe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Delta Lake, pros, cons etc...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f3wo9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702215831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will soon start working with Delta lakes, and would like to know a bit about people&amp;#39;s experiences with it.&lt;/p&gt;\n\n&lt;p&gt;Any main reasons why you&amp;#39;re using it? did it fulfill your data needs? was it worth the time investment? Please feel free to share everything :) &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f3wo9", "is_robot_indexable": true, "report_reasons": null, "author": "Jazzlike-Change8493", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18f3wo9/thoughts_on_delta_lake_pros_cons_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f3wo9/thoughts_on_delta_lake_pros_cons_etc/", "subreddit_subscribers": 145197, "created_utc": 1702215831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What's the best SQL environment you've had the pleasure of working in?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best SQL environment you have ever worked in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fii0v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702256799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What&amp;#39;s the best SQL environment you&amp;#39;ve had the pleasure of working in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fii0v", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fii0v/what_is_the_best_sql_environment_you_have_ever/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fii0v/what_is_the_best_sql_environment_you_have_ever/", "subreddit_subscribers": 145197, "created_utc": 1702256799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone\n\nRecently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?", "author_fullname": "t2_gci8d6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here that works as data engineer in Canada as an immigrant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f4upz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702218794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone&lt;/p&gt;\n\n&lt;p&gt;Recently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18f4upz", "is_robot_indexable": true, "report_reasons": null, "author": "Entropico_88", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f4upz/anyone_here_that_works_as_data_engineer_in_canada/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f4upz/anyone_here_that_works_as_data_engineer_in_canada/", "subreddit_subscribers": 145197, "created_utc": 1702218794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ibis ([https://ibis-project.org/](https://ibis-project.org/)) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.\n\nIt looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.\n\nHowever, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?\n\nIbis' documentation is relatively limited and I haven't found much information about what organizations back this project.", "author_fullname": "t2_7v7x1pqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Ibis reliably be used in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fcbf0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702239463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ibis (&lt;a href=\"https://ibis-project.org/\"&gt;https://ibis-project.org/&lt;/a&gt;) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.&lt;/p&gt;\n\n&lt;p&gt;It looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.&lt;/p&gt;\n\n&lt;p&gt;However, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?&lt;/p&gt;\n\n&lt;p&gt;Ibis&amp;#39; documentation is relatively limited and I haven&amp;#39;t found much information about what organizations back this project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fcbf0", "is_robot_indexable": true, "report_reasons": null, "author": "cantthinkofoneuse", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "subreddit_subscribers": 145197, "created_utc": 1702239463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do the two compare w.r.t:\n1. Pay\n2. Career growth\n\nI have 9 years of experience. Currently enrolled in the EPGP (MBA) program at IIM Kozhikode.", "author_fullname": "t2_9815cpn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering vs Tech Consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f15jg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702205920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do the two compare w.r.t:\n1. Pay\n2. Career growth&lt;/p&gt;\n\n&lt;p&gt;I have 9 years of experience. Currently enrolled in the EPGP (MBA) program at IIM Kozhikode.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18f15jg", "is_robot_indexable": true, "report_reasons": null, "author": "brokeRichieRich", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f15jg/data_engineering_vs_tech_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f15jg/data_engineering_vs_tech_consulting/", "subreddit_subscribers": 145197, "created_utc": 1702205920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI'm a data engineer at a startup and have been working on getting various operational data into Databricks. My next challenge is to present an overview of our current data assets to the management team (Some C-level stakeholders also like to know). They have also requested an internal dashboard with some key info, but they have no idea what I currently have in the Databricks warehouse, and I and not entirely sure which data they'll find most relevant...As a first step, I'd like to have a clear and simple way to show them what data assets we have.\n\nI'm considering the following:\n\n1. A big data dictionary page in confluence: This is how it was done in my previous company a page starts with a big table with columns: table name, column name, column description, source, logic...etc.  I feel it'll be a good table for data analysts to refer to, but not sure if it will be too detailed for those senior stakeholders.\n2. A dashboard in Databricks: Chatgpt brought up this method to me, by using information tables in databricks, I will be able to create a databricks dashboard for data inventory. the pros will be it will automatically handle the new coming tables and cols which saves a bit of overhead. But I am not sure if it will be easy to add and put notes on the tables/columns or formatting the table in a more insightful way(grouping by topic for example)\n3. A mindmap: Came to my mind that a mindmap is usually easy to follow and helps in capturing the big picture and overall structure.  Perhaps a mind map that branches out based on topics, subtopics, and table names?\n\n4. Do you have any other suggestions for presenting data assets?\n\nThanks in advance!", "author_fullname": "t2_hf67tpfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presenting Data Inventory to the Management team in the startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18fnity", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702273576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data engineer at a startup and have been working on getting various operational data into Databricks. My next challenge is to present an overview of our current data assets to the management team (Some C-level stakeholders also like to know). They have also requested an internal dashboard with some key info, but they have no idea what I currently have in the Databricks warehouse, and I and not entirely sure which data they&amp;#39;ll find most relevant...As a first step, I&amp;#39;d like to have a clear and simple way to show them what data assets we have.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A big data dictionary page in confluence: This is how it was done in my previous company a page starts with a big table with columns: table name, column name, column description, source, logic...etc.  I feel it&amp;#39;ll be a good table for data analysts to refer to, but not sure if it will be too detailed for those senior stakeholders.&lt;/li&gt;\n&lt;li&gt;A dashboard in Databricks: Chatgpt brought up this method to me, by using information tables in databricks, I will be able to create a databricks dashboard for data inventory. the pros will be it will automatically handle the new coming tables and cols which saves a bit of overhead. But I am not sure if it will be easy to add and put notes on the tables/columns or formatting the table in a more insightful way(grouping by topic for example)&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A mindmap: Came to my mind that a mindmap is usually easy to follow and helps in capturing the big picture and overall structure.  Perhaps a mind map that branches out based on topics, subtopics, and table names?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Do you have any other suggestions for presenting data assets?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fnity", "is_robot_indexable": true, "report_reasons": null, "author": "East-Garage2337", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fnity/presenting_data_inventory_to_the_management_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fnity/presenting_data_inventory_to_the_management_team/", "subreddit_subscribers": 145197, "created_utc": 1702273576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if anyone has war stories, success or failures, of using DuckDB in their production pipelines. \n\nWhat is the ecosystem look like? What tools are you using? What drove success or failure in your case? \n\nI\u2019m an MSSQL shop, so hard sell to my team, but I\u2019m also the sole dev maintaining our pyspark environment, and think there\u2019s opportunities for duck, especially in some containerized use cases I\u2019m prototyping. Figured I\u2019d ask what others have done \ud83d\udcaa\ud83c\udffb", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Production DuckDB Setups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fig7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702256645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if anyone has war stories, success or failures, of using DuckDB in their production pipelines. &lt;/p&gt;\n\n&lt;p&gt;What is the ecosystem look like? What tools are you using? What drove success or failure in your case? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m an MSSQL shop, so hard sell to my team, but I\u2019m also the sole dev maintaining our pyspark environment, and think there\u2019s opportunities for duck, especially in some containerized use cases I\u2019m prototyping. Figured I\u2019d ask what others have done \ud83d\udcaa\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fig7l", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fig7l/production_duckdb_setups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fig7l/production_duckdb_setups/", "subreddit_subscribers": 145197, "created_utc": 1702256645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been using Meltano to run an ELT job but I'm not sure I'm following the best practices. I was thinking:\n\n* Run a this job (dynamically) on schedule with the command **meltano el tap-mongodb target-&lt;snowflake&gt; --state-id tap\\_mongodb\\_&lt;dev&gt;**\n* I originally wanted to use GCP cloud run but it doesn't work because of the 60m timeout. This ELT process can take 5-8 hours on a single CPU, I want it to run daily. A couple more questions on this 2nd bullet-point:\n   * Are meltano **partitions** able to parallelize across CPUs / machines? If so then this job doesn't need to take anywhere near 5-8 hours (more like 5-10 minutes on 100 machines)?\n   * Should I be using Airflow to do this? Simple cronjob on a VM? GCP cloud composer / dataflow? SO MANY TOOLS!! I just want this to run on the cloud while being able to see the logs in gcp cloud logging, integrating CI/CD, and mounting a volume from GCP secret manager.\n\n&amp;#x200B;\n\nI am hesitant to spend time diving into Airflow, but curious to hear your thoughts. Here's the below script I've been using to run in production **host.py**:\n\n&amp;#x200B;\n\n    from flask import Flask, make_response, request, redirect, Blueprint, render_template\n    import os\n    import subprocess\n    from configparser import ConfigParser\n    from datetime import datetime\n    from waitress import serve\n    from apscheduler.schedulers.background import BackgroundScheduler\n    import logging\n    import json\n    \n    config = ConfigParser()\n    config.read('config.ini')\n    \n    app = Flask(__name__)\n    app.url_map.strict_slashes = False\n    \n    MELTANO_TARGET = config['TAP_MONGODB']['MELTANO_TARGET']\n    \n    assert isinstance(MELTANO_TARGET, str), 'could not determine target'\n    \n    \n    ###### routes ######\n    \n    @app.route('/')\n    def index():\n        return 'Server is running.'\n    \n    @app.route('/tap_mongodb_dev', methods=['GET'])\n    def tap_mongodb_dev():\n        with app.app_context():\n            project_dir = 'taps/tap-mongodb'\n            run_command = f'meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}'\n            subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n            return make_response(f'Last ran project tap-mongodb-dev target {MELTANO_TARGET} at {cur_timestamp()}.', 200)\n    \n    @app.route('/tap_mongodb_production', methods=['GET'])\n    def tap_mongodb_production():\n        with app.app_context():\n            project_dir = 'taps/tap-mongodb'\n            run_command = f'meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}'\n            subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n            return make_response(f'Last ran project tap-mongodb-production target {MELTANO_TARGET} at {cur_timestamp()}.', 200)\n    \n    \n    if __name__ == \"__main__\":\n        logging.info(f'\\n*** Running environment {ENVIRONMENT}. ***\\n')\n    \n        scheduler = BackgroundScheduler(job_defaults={'max_instances': 2})\n    \n        ###### tap-mongodb ######\n    \n        tap_mongodb_cron = json.loads(config['TAP_MONGODB'][f'{ENVIRONMENT}_CRON_PARAMS'])\n        scheduler.add_job(tap_mongodb_functions[ENVIRONMENT], trigger='cron', **tap_mongodb_cron, jitter=120)\n    \n        ###### host ######\n    \n        HOST = '0.0.0.0'\n        PORT = 5000\n        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n        logging.info(f'Server is listening on port {PORT}')\n        logging.info(f'Hosting environment {ENVIRONMENT}')\n    \n        scheduler.start()\n    \n        serve(app, host=HOST, port=PORT, threads=2)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8xq51rif", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you run a long ELT job in the cloud on schedule?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fhseu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702254625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Meltano to run an ELT job but I&amp;#39;m not sure I&amp;#39;m following the best practices. I was thinking:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run a this job (dynamically) on schedule with the command &lt;strong&gt;meltano el tap-mongodb target-&amp;lt;snowflake&amp;gt; --state-id tap_mongodb_&amp;lt;dev&amp;gt;&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I originally wanted to use GCP cloud run but it doesn&amp;#39;t work because of the 60m timeout. This ELT process can take 5-8 hours on a single CPU, I want it to run daily. A couple more questions on this 2nd bullet-point:\n\n&lt;ul&gt;\n&lt;li&gt;Are meltano &lt;strong&gt;partitions&lt;/strong&gt; able to parallelize across CPUs / machines? If so then this job doesn&amp;#39;t need to take anywhere near 5-8 hours (more like 5-10 minutes on 100 machines)?&lt;/li&gt;\n&lt;li&gt;Should I be using Airflow to do this? Simple cronjob on a VM? GCP cloud composer / dataflow? SO MANY TOOLS!! I just want this to run on the cloud while being able to see the logs in gcp cloud logging, integrating CI/CD, and mounting a volume from GCP secret manager.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am hesitant to spend time diving into Airflow, but curious to hear your thoughts. Here&amp;#39;s the below script I&amp;#39;ve been using to run in production &lt;strong&gt;host.py&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from flask import Flask, make_response, request, redirect, Blueprint, render_template\nimport os\nimport subprocess\nfrom configparser import ConfigParser\nfrom datetime import datetime\nfrom waitress import serve\nfrom apscheduler.schedulers.background import BackgroundScheduler\nimport logging\nimport json\n\nconfig = ConfigParser()\nconfig.read(&amp;#39;config.ini&amp;#39;)\n\napp = Flask(__name__)\napp.url_map.strict_slashes = False\n\nMELTANO_TARGET = config[&amp;#39;TAP_MONGODB&amp;#39;][&amp;#39;MELTANO_TARGET&amp;#39;]\n\nassert isinstance(MELTANO_TARGET, str), &amp;#39;could not determine target&amp;#39;\n\n\n###### routes ######\n\n@app.route(&amp;#39;/&amp;#39;)\ndef index():\n    return &amp;#39;Server is running.&amp;#39;\n\n@app.route(&amp;#39;/tap_mongodb_dev&amp;#39;, methods=[&amp;#39;GET&amp;#39;])\ndef tap_mongodb_dev():\n    with app.app_context():\n        project_dir = &amp;#39;taps/tap-mongodb&amp;#39;\n        run_command = f&amp;#39;meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}&amp;#39;\n        subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n        return make_response(f&amp;#39;Last ran project tap-mongodb-dev target {MELTANO_TARGET} at {cur_timestamp()}.&amp;#39;, 200)\n\n@app.route(&amp;#39;/tap_mongodb_production&amp;#39;, methods=[&amp;#39;GET&amp;#39;])\ndef tap_mongodb_production():\n    with app.app_context():\n        project_dir = &amp;#39;taps/tap-mongodb&amp;#39;\n        run_command = f&amp;#39;meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}&amp;#39;\n        subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n        return make_response(f&amp;#39;Last ran project tap-mongodb-production target {MELTANO_TARGET} at {cur_timestamp()}.&amp;#39;, 200)\n\n\nif __name__ == &amp;quot;__main__&amp;quot;:\n    logging.info(f&amp;#39;\\n*** Running environment {ENVIRONMENT}. ***\\n&amp;#39;)\n\n    scheduler = BackgroundScheduler(job_defaults={&amp;#39;max_instances&amp;#39;: 2})\n\n    ###### tap-mongodb ######\n\n    tap_mongodb_cron = json.loads(config[&amp;#39;TAP_MONGODB&amp;#39;][f&amp;#39;{ENVIRONMENT}_CRON_PARAMS&amp;#39;])\n    scheduler.add_job(tap_mongodb_functions[ENVIRONMENT], trigger=&amp;#39;cron&amp;#39;, **tap_mongodb_cron, jitter=120)\n\n    ###### host ######\n\n    HOST = &amp;#39;0.0.0.0&amp;#39;\n    PORT = 5000\n    logging.basicConfig(level=logging.INFO, format=&amp;#39;%(asctime)s - %(levelname)s - %(message)s&amp;#39;)\n    logging.info(f&amp;#39;Server is listening on port {PORT}&amp;#39;)\n    logging.info(f&amp;#39;Hosting environment {ENVIRONMENT}&amp;#39;)\n\n    scheduler.start()\n\n    serve(app, host=HOST, port=PORT, threads=2)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fhseu", "is_robot_indexable": true, "report_reasons": null, "author": "Training_Butterfly70", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fhseu/how_do_you_run_a_long_elt_job_in_the_cloud_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fhseu/how_do_you_run_a_long_elt_job_in_the_cloud_on/", "subreddit_subscribers": 145197, "created_utc": 1702254625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is using Azure Databricks and I'm an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a \"/FileStore/file\\_path/file\\_name.xlsx\" file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn't really helped me up to this point.\n\n`w = WorkspaceClient(host=server_hostname,token = api_key)`\n\nI can successfully run `w.dbfs.download('/FileStore/file_path/file_name.xlsx')`, but can't figure out how to save the file down to local storage as an Excel file.\n\n`w.dbutils.fs.cp('dbfs:/FileStore/file_path/file_name.xlsx', 'C:/Users/user_name/file_path/file_name.xlsx')` results in \"java.net.URISyntaxException: Relative path in absolute URI\" for the C drive file path. This looks like an absolute path to me. Am I missing something?\n\n`w.files.get_status('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"Get-status API is not enabled.\" How do I enable it? I don't see a toggle for it in Admin Settings.\n\n`w.files.download('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"The ':' character is not supported\", but when I remove it the file path is no longer valid.\n\nI feel like I'm close on one of these but just can't seem to get it across the finish line.", "author_fullname": "t2_yk6x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone moved files between Azure Databricks DBFS and local storage using the Databricks API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7vcb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702227447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is using Azure Databricks and I&amp;#39;m an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a &amp;quot;/FileStore/file_path/file_name.xlsx&amp;quot; file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn&amp;#39;t really helped me up to this point.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w = WorkspaceClient(host=server_hostname,token = api_key)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I can successfully run &lt;code&gt;w.dbfs.download(&amp;#39;/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt;, but can&amp;#39;t figure out how to save the file down to local storage as an Excel file.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.dbutils.fs.cp(&amp;#39;dbfs:/FileStore/file_path/file_name.xlsx&amp;#39;, &amp;#39;C:/Users/user_name/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;java.net.URISyntaxException: Relative path in absolute URI&amp;quot; for the C drive file path. This looks like an absolute path to me. Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.get_status(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;Get-status API is not enabled.&amp;quot; How do I enable it? I don&amp;#39;t see a toggle for it in Admin Settings.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.download(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;The &amp;#39;:&amp;#39; character is not supported&amp;quot;, but when I remove it the file path is no longer valid.&lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m close on one of these but just can&amp;#39;t seem to get it across the finish line.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18f7vcb", "is_robot_indexable": true, "report_reasons": null, "author": "CurlyW15", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "subreddit_subscribers": 145197, "created_utc": 1702227447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The key here is the ETL aspect. I need sample databases in postgres, oracle (or any sql dialect really) that have both data and ETL .sql scripts that I don't have to reinvent. Does anyone know one that exists that have both?", "author_fullname": "t2_rk16bnik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for sample postgres and oracle databases that have ETL scripts preloaded into them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ffap8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702247443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The key here is the ETL aspect. I need sample databases in postgres, oracle (or any sql dialect really) that have both data and ETL .sql scripts that I don&amp;#39;t have to reinvent. Does anyone know one that exists that have both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ffap8", "is_robot_indexable": true, "report_reasons": null, "author": "notstoppinguntil30", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ffap8/looking_for_sample_postgres_and_oracle_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ffap8/looking_for_sample_postgres_and_oracle_databases/", "subreddit_subscribers": 145197, "created_utc": 1702247443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It is for building AI (into your) apps easily by integrating AI at the data's source.\n\nNot another database, but rather making your existing favorite database intelligent/super-duper (funny name for serious tech); think:\u00a0`db = superduper(your_database)`\n\nCurrently supported databases: MongoDB, Postgres, MySQL, S3, DuckDB, SQLite, Snowflake, BigQuery, ClickHouse and more.\n\nDefinitely check it out:\u00a0[https://github.com/SuperDuperDB/superduperdb](https://github.com/SuperDuperDB/superduperdb)", "author_fullname": "t2_sj4cy7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trending on GitHub top 10 for the 4th day in a row: Open-source framework for integrating AI with major databases, to elimitate the need to move data into complex pipelines and specialized vector dbs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fdaoz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702242870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702242089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It is for building AI (into your) apps easily by integrating AI at the data&amp;#39;s source.&lt;/p&gt;\n\n&lt;p&gt;Not another database, but rather making your existing favorite database intelligent/super-duper (funny name for serious tech); think:\u00a0&lt;code&gt;db = superduper(your_database)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently supported databases: MongoDB, Postgres, MySQL, S3, DuckDB, SQLite, Snowflake, BigQuery, ClickHouse and more.&lt;/p&gt;\n\n&lt;p&gt;Definitely check it out:\u00a0&lt;a href=\"https://github.com/SuperDuperDB/superduperdb\"&gt;https://github.com/SuperDuperDB/superduperdb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?auto=webp&amp;s=396409da2da4f40baf19284bab4053dcc28afa4e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe78d32a43569da51807866133ba782442e90225", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fd324f44ce51883fe61257857b78805f8cdee76", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=28fb518349163e869c86486384dffa06ea18c4b1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9256e4c6ce588d64e4a24b44d0d026205943ff9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=98c95689491edbc6246a40886aa2b9bb47a9de8c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bebf450f0241110b243b9f76ca84cea2174b9da2", "width": 1080, "height": 540}], "variants": {}, "id": "Dm5OzYBAS-qzFRjPkJ4vzFTkoenLemIsEb4OJX6z6FQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18fdaoz", "is_robot_indexable": true, "report_reasons": null, "author": "escalize", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fdaoz/trending_on_github_top_10_for_the_4th_day_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fdaoz/trending_on_github_top_10_for_the_4th_day_in_a/", "subreddit_subscribers": 145197, "created_utc": 1702242089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_p5gqem6h6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please Stop Using Google Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18f38wx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.32, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Please Stop Using Google Analytics", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "author_name": "Theo - t3\u2024gg", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/6xXSsu0YXWo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@t3dotgg"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18f38wx", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/MscxPo2ozHM1_dkAkbfqJDEUuZR-KsA4pgLKk8yVF38.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702213632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=6xXSsu0YXWo", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?auto=webp&amp;s=99f9cf88fbf8b204bdfb554c5686774edfb04ac9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c862112d73985f4a7ca6e8d7d7f8e854eb7a2ddf", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8852ce395422244d48cb3c7045dd899e496c484f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6073c63ec3325df2713fd5c40a4a8f12c62edc9", "width": 320, "height": 240}], "variants": {}, "id": "_Oc0viLfsdEgxq9iRXrD-Gc4J8e4EtHURDly-KXakWM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f38wx", "is_robot_indexable": true, "report_reasons": null, "author": "MySpermIs-Unvaxxd-01", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f38wx/please_stop_using_google_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=6xXSsu0YXWo", "subreddit_subscribers": 145197, "created_utc": 1702213632.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Please Stop Using Google Analytics", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "author_name": "Theo - t3\u2024gg", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/6xXSsu0YXWo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@t3dotgg"}}, "is_video": false}}], "before": null}}