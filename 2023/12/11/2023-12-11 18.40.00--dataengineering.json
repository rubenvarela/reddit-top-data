{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. \n\nData is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. \n\nOverall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. \n\nAnyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. ", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I Was Happier Being a Bartender Compared to Being a 6 Figure DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ffzmx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 271, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 271, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702249969.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702249347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. &lt;/p&gt;\n\n&lt;p&gt;Data is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. &lt;/p&gt;\n\n&lt;p&gt;Overall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. &lt;/p&gt;\n\n&lt;p&gt;Anyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18ffzmx", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 115, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/", "subreddit_subscribers": 145323, "created_utc": 1702249347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What's the best SQL environment you've had the pleasure of working in?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best SQL environment you have ever worked in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fii0v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702256799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What&amp;#39;s the best SQL environment you&amp;#39;ve had the pleasure of working in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fii0v", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fii0v/what_is_the_best_sql_environment_you_have_ever/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fii0v/what_is_the_best_sql_environment_you_have_ever/", "subreddit_subscribers": 145323, "created_utc": 1702256799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently unemployed for last 6 months. I am having 8 years of experience in ETL, data warehouse building, snowflake, spark and tableau. \n\nI also have bit of hands on experience in low code tools like fivetran, data modelling tools like dbt.\n\nDue to the market conditions I am not receiving any job offers, so planning to upskill in the time being. I already have certifications in aws solution architect associate, snowflake snowpro core and Apache spark developer by Databricks. \n\nMy question is, what new technology can I learn and get certified in to get an edge when the market is up? Should I try my hands on any NoSQL databases? Will learning kafka will open up my job chances? \n\nPS: my priority right now is landing up a job immediately once the market is up. I am continuously applying jobs, but not getting any calls. Seeing that there are not much calls, I am panicked and wants to learn something related to my field but can land me a job.", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What new to learn in DE now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fusjx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702302069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently unemployed for last 6 months. I am having 8 years of experience in ETL, data warehouse building, snowflake, spark and tableau. &lt;/p&gt;\n\n&lt;p&gt;I also have bit of hands on experience in low code tools like fivetran, data modelling tools like dbt.&lt;/p&gt;\n\n&lt;p&gt;Due to the market conditions I am not receiving any job offers, so planning to upskill in the time being. I already have certifications in aws solution architect associate, snowflake snowpro core and Apache spark developer by Databricks. &lt;/p&gt;\n\n&lt;p&gt;My question is, what new technology can I learn and get certified in to get an edge when the market is up? Should I try my hands on any NoSQL databases? Will learning kafka will open up my job chances? &lt;/p&gt;\n\n&lt;p&gt;PS: my priority right now is landing up a job immediately once the market is up. I am continuously applying jobs, but not getting any calls. Seeing that there are not much calls, I am panicked and wants to learn something related to my field but can land me a job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18fusjx", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fusjx/what_new_to_learn_in_de_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fusjx/what_new_to_learn_in_de_now/", "subreddit_subscribers": 145323, "created_utc": 1702302069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys! I work at [Taipy](https://github.com/Avaiga/taipy); we are an Open-Source Python library designed to create web applications using only Python. Some users had problems displaying charts based on big data, e.g., line charts with 100,000 points. We worked on a feature to reduce the number of displayed points while retaining the shape of the curve as much as possible and wanted to share how we did it. Feel free to take a look [here](https://www.taipy.io/posts/python-charting-taming-big-data-without-crashing): \n\n&amp;#x200B;\n\nhttps://preview.redd.it/ekligzx1qo5c1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=f6335259e1b9289d21593aa021b6568ab52be7e7", "author_fullname": "t2_4qttbe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plotting 1,000,000 points on a webpage using only Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 120, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ekligzx1qo5c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 92, "x": 108, "u": "https://preview.redd.it/ekligzx1qo5c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=03281d8d94d7592ca79bbd3ebf80c422d4f9d489"}, {"y": 185, "x": 216, "u": "https://preview.redd.it/ekligzx1qo5c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4f3656f5e88287a02339966edfea0d2a6a6cda4"}, {"y": 275, "x": 320, "u": "https://preview.redd.it/ekligzx1qo5c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ffdac175d1a8f0ef8fd9fc63f3716ce36507415"}, {"y": 550, "x": 640, "u": "https://preview.redd.it/ekligzx1qo5c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=30c966bc3838920c0ea79daff4f07d4e6f5f9ddc"}, {"y": 826, "x": 960, "u": "https://preview.redd.it/ekligzx1qo5c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=25fcfadc9242bc0fd11feb96319abf6bb8f4f385"}], "s": {"y": 910, "x": 1057, "u": "https://preview.redd.it/ekligzx1qo5c1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=f6335259e1b9289d21593aa021b6568ab52be7e7"}, "id": "ekligzx1qo5c1"}}, "name": "t3_18fx7o4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QxFAiEkUJcLtc2aqBETkrHMze1i9Z7HJAjB2xjBDtmM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702308842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys! I work at &lt;a href=\"https://github.com/Avaiga/taipy\"&gt;Taipy&lt;/a&gt;; we are an Open-Source Python library designed to create web applications using only Python. Some users had problems displaying charts based on big data, e.g., line charts with 100,000 points. We worked on a feature to reduce the number of displayed points while retaining the shape of the curve as much as possible and wanted to share how we did it. Feel free to take a look &lt;a href=\"https://www.taipy.io/posts/python-charting-taming-big-data-without-crashing\"&gt;here&lt;/a&gt;: &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ekligzx1qo5c1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6335259e1b9289d21593aa021b6568ab52be7e7\"&gt;https://preview.redd.it/ekligzx1qo5c1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6335259e1b9289d21593aa021b6568ab52be7e7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18fx7o4", "is_robot_indexable": true, "report_reasons": null, "author": "Alyx1337", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fx7o4/plotting_1000000_points_on_a_webpage_using_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fx7o4/plotting_1000000_points_on_a_webpage_using_only/", "subreddit_subscribers": 145323, "created_utc": 1702308842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ibis ([https://ibis-project.org/](https://ibis-project.org/)) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.\n\nIt looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.\n\nHowever, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?\n\nIbis' documentation is relatively limited and I haven't found much information about what organizations back this project.", "author_fullname": "t2_7v7x1pqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Ibis reliably be used in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fcbf0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702239463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ibis (&lt;a href=\"https://ibis-project.org/\"&gt;https://ibis-project.org/&lt;/a&gt;) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.&lt;/p&gt;\n\n&lt;p&gt;It looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.&lt;/p&gt;\n\n&lt;p&gt;However, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?&lt;/p&gt;\n\n&lt;p&gt;Ibis&amp;#39; documentation is relatively limited and I haven&amp;#39;t found much information about what organizations back this project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fcbf0", "is_robot_indexable": true, "report_reasons": null, "author": "cantthinkofoneuse", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "subreddit_subscribers": 145323, "created_utc": 1702239463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, this is my first DE project.    [Baitur5/reddit\\_api\\_elt (github.com)](https://github.com/Baitur5/reddit_api_elt) . It is basically about   \na data pipeline that extracts Reddit data for a Google Data Studio report, focusing on a specific subreddit   \nCan you guys check it out , and give some advice &amp; tips on how to improve it or the next things I should add.  \n\n\nP.S. I followed steps from this repository but made some adjustments: [ABZ-Aaron/Reddit-API-Pipeline (github.com)](https://github.com/ABZ-Aaron/Reddit-API-Pipeline) ", "author_fullname": "t2_kq543w8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit ELT Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18froaz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702291003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, this is my first DE project.    &lt;a href=\"https://github.com/Baitur5/reddit_api_elt\"&gt;Baitur5/reddit_api_elt (github.com)&lt;/a&gt; . It is basically about&lt;br/&gt;\na data pipeline that extracts Reddit data for a Google Data Studio report, focusing on a specific subreddit&lt;br/&gt;\nCan you guys check it out , and give some advice &amp;amp; tips on how to improve it or the next things I should add.  &lt;/p&gt;\n\n&lt;p&gt;P.S. I followed steps from this repository but made some adjustments: &lt;a href=\"https://github.com/ABZ-Aaron/Reddit-API-Pipeline\"&gt;ABZ-Aaron/Reddit-API-Pipeline (github.com)&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18froaz", "is_robot_indexable": true, "report_reasons": null, "author": "ulukbekovbr", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18froaz/reddit_elt_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18froaz/reddit_elt_pipeline/", "subreddit_subscribers": 145323, "created_utc": 1702291003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company just announced a restructuring of our central data platform org and my team was effected.\n\nPreviously we were basically a data engineering team for a specific data domain developing and owning pipelines, data lakes/warehouse, and visualizations. However, post-restructuring we\u2019re now \u201ctechnically\u201d part of the platform team, but from what I understand we\u2019re  basically end-users for the core platform team, and they\u2019ll be building tools for us to ingest, store, transform etc\u2026 we\u2019re just focused on ingestion.\n\nUltimately I\u2019m worried about the long term staying power of a team that ONLY does ingestion using simplified internal tools. What makes matters worse is that the platform team is looking to build these tools for general use by business data engineers. So in a future where other groups can use the same tools to ingest their own data, what\u2019s the value of a central data team?\n\nI\u2019m planning to talk with my manager about this, but I want to come with some idea of a better direction in mind before I have that conversation, so curious if anyone here has advice?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do central data platforms usually have dedicated central ingestion teams?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18frbal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702289515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company just announced a restructuring of our central data platform org and my team was effected.&lt;/p&gt;\n\n&lt;p&gt;Previously we were basically a data engineering team for a specific data domain developing and owning pipelines, data lakes/warehouse, and visualizations. However, post-restructuring we\u2019re now \u201ctechnically\u201d part of the platform team, but from what I understand we\u2019re  basically end-users for the core platform team, and they\u2019ll be building tools for us to ingest, store, transform etc\u2026 we\u2019re just focused on ingestion.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I\u2019m worried about the long term staying power of a team that ONLY does ingestion using simplified internal tools. What makes matters worse is that the platform team is looking to build these tools for general use by business data engineers. So in a future where other groups can use the same tools to ingest their own data, what\u2019s the value of a central data team?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m planning to talk with my manager about this, but I want to come with some idea of a better direction in mind before I have that conversation, so curious if anyone here has advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18frbal", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18frbal/do_central_data_platforms_usually_have_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18frbal/do_central_data_platforms_usually_have_dedicated/", "subreddit_subscribers": 145323, "created_utc": 1702289515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI'm a data engineer at a startup and have been working on getting various operational data into Databricks. My next challenge is to present an overview of our current data assets to the management team (Some C-level stakeholders also like to know). They have also requested an internal dashboard with some key info, but they have no idea what I currently have in the Databricks warehouse, and I and not entirely sure which data they'll find most relevant...As a first step, I'd like to have a clear and simple way to show them what data assets we have.\n\nI'm considering the following:\n\n1. A big data dictionary page in confluence: This is how it was done in my previous company a page starts with a big table with columns: table name, column name, column description, source, logic...etc.  I feel it'll be a good table for data analysts to refer to, but not sure if it will be too detailed for those senior stakeholders.\n2. A dashboard in Databricks: Chatgpt brought up this method to me, by using information tables in databricks, I will be able to create a databricks dashboard for data inventory. the pros will be it will automatically handle the new coming tables and cols which saves a bit of overhead. But I am not sure if it will be easy to add and put notes on the tables/columns or formatting the table in a more insightful way(grouping by topic for example)\n3. A mindmap: Came to my mind that a mindmap is usually easy to follow and helps in capturing the big picture and overall structure.  Perhaps a mind map that branches out based on topics, subtopics, and table names?\n\n4. Do you have any other suggestions for presenting data assets?\n\nThanks in advance!", "author_fullname": "t2_hf67tpfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presenting Data Inventory to the Management team in the startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fnity", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702273576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data engineer at a startup and have been working on getting various operational data into Databricks. My next challenge is to present an overview of our current data assets to the management team (Some C-level stakeholders also like to know). They have also requested an internal dashboard with some key info, but they have no idea what I currently have in the Databricks warehouse, and I and not entirely sure which data they&amp;#39;ll find most relevant...As a first step, I&amp;#39;d like to have a clear and simple way to show them what data assets we have.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A big data dictionary page in confluence: This is how it was done in my previous company a page starts with a big table with columns: table name, column name, column description, source, logic...etc.  I feel it&amp;#39;ll be a good table for data analysts to refer to, but not sure if it will be too detailed for those senior stakeholders.&lt;/li&gt;\n&lt;li&gt;A dashboard in Databricks: Chatgpt brought up this method to me, by using information tables in databricks, I will be able to create a databricks dashboard for data inventory. the pros will be it will automatically handle the new coming tables and cols which saves a bit of overhead. But I am not sure if it will be easy to add and put notes on the tables/columns or formatting the table in a more insightful way(grouping by topic for example)&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A mindmap: Came to my mind that a mindmap is usually easy to follow and helps in capturing the big picture and overall structure.  Perhaps a mind map that branches out based on topics, subtopics, and table names?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Do you have any other suggestions for presenting data assets?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fnity", "is_robot_indexable": true, "report_reasons": null, "author": "East-Garage2337", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fnity/presenting_data_inventory_to_the_management_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fnity/presenting_data_inventory_to_the_management_team/", "subreddit_subscribers": 145323, "created_utc": 1702273576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm part of a project that's implementing a new Data warehousing solution and we are just about to scale our team from about 10 people into around 30 divided into 5 teams or so. We are just about to implement DBT and has started building a Data Vault. The increase in team size and the phase we're currently in means that all of the teams will be working on modelling and implementing the Data Vault.\n\nWe are now thinking of how to set up our GIT repo(s) and DBT to accomodate this situation as good as possible. With Data Vault it feels like we need to build it in a mono repo, but with this many people and teams it feels like a recipe for disaster and it would be better to divide the Vault into multiple repos.\n\nHow have you guys handled this situation?", "author_fullname": "t2_o0nay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT and Data Vault with multiple teams, mono repo or not?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fveo7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702303860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m part of a project that&amp;#39;s implementing a new Data warehousing solution and we are just about to scale our team from about 10 people into around 30 divided into 5 teams or so. We are just about to implement DBT and has started building a Data Vault. The increase in team size and the phase we&amp;#39;re currently in means that all of the teams will be working on modelling and implementing the Data Vault.&lt;/p&gt;\n\n&lt;p&gt;We are now thinking of how to set up our GIT repo(s) and DBT to accomodate this situation as good as possible. With Data Vault it feels like we need to build it in a mono repo, but with this many people and teams it feels like a recipe for disaster and it would be better to divide the Vault into multiple repos.&lt;/p&gt;\n\n&lt;p&gt;How have you guys handled this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fveo7", "is_robot_indexable": true, "report_reasons": null, "author": "zirxo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fveo7/dbt_and_data_vault_with_multiple_teams_mono_repo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fveo7/dbt_and_data_vault_with_multiple_teams_mono_repo/", "subreddit_subscribers": 145323, "created_utc": 1702303860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if anyone has war stories, success or failures, of using DuckDB in their production pipelines. \n\nWhat is the ecosystem look like? What tools are you using? What drove success or failure in your case? \n\nI\u2019m an MSSQL shop, so hard sell to my team, but I\u2019m also the sole dev maintaining our pyspark environment, and think there\u2019s opportunities for duck, especially in some containerized use cases I\u2019m prototyping. Figured I\u2019d ask what others have done \ud83d\udcaa\ud83c\udffb", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Production DuckDB Setups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fig7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702256645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if anyone has war stories, success or failures, of using DuckDB in their production pipelines. &lt;/p&gt;\n\n&lt;p&gt;What is the ecosystem look like? What tools are you using? What drove success or failure in your case? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m an MSSQL shop, so hard sell to my team, but I\u2019m also the sole dev maintaining our pyspark environment, and think there\u2019s opportunities for duck, especially in some containerized use cases I\u2019m prototyping. Figured I\u2019d ask what others have done \ud83d\udcaa\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fig7l", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fig7l/production_duckdb_setups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fig7l/production_duckdb_setups/", "subreddit_subscribers": 145323, "created_utc": 1702256645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you perform updates and deletes in a data warehouse? I've been told that temporality in a data warehouse should be handled through either temporal surrogation or data vault methods. Neither maintains an \"expired\" attribute.  \n ", "author_fullname": "t2_jg3w8gbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Updates and Deletes in a Data Warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fubwe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702300627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you perform updates and deletes in a data warehouse? I&amp;#39;ve been told that temporality in a data warehouse should be handled through either temporal surrogation or data vault methods. Neither maintains an &amp;quot;expired&amp;quot; attribute.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fubwe", "is_robot_indexable": true, "report_reasons": null, "author": "tamargal91", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fubwe/updates_and_deletes_in_a_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fubwe/updates_and_deletes_in_a_data_warehouse/", "subreddit_subscribers": 145323, "created_utc": 1702300627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used Facebook API before?\n\nI\u2019ve been off Facebook for months, because the ads and Watch were getting ridiculous.\n\n\nI\u2019m wondering what Facebook API\u2019s limitations are, because it seems like it\u2019d be really simple to build- I\u2019d like to basically browse without any ads or timeline \u201calgorithm\u201d- most recent friends posts sorted by date. Also add Events near me and filter by date once a week maybe. In theory if filtering out posts with less than x likes, it\u2019d take 5 minutes tops per week to get everything needed and none of the garbage. \n\nHas anyone used Facebook API before? I don\u2019t want to ask there because most people monetize and aren\u2019t going to be keen on someone trying to get around their ads.", "author_fullname": "t2_vgxtzjvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Facebook API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ft5sq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702296708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used Facebook API before?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been off Facebook for months, because the ads and Watch were getting ridiculous.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering what Facebook API\u2019s limitations are, because it seems like it\u2019d be really simple to build- I\u2019d like to basically browse without any ads or timeline \u201calgorithm\u201d- most recent friends posts sorted by date. Also add Events near me and filter by date once a week maybe. In theory if filtering out posts with less than x likes, it\u2019d take 5 minutes tops per week to get everything needed and none of the garbage. &lt;/p&gt;\n\n&lt;p&gt;Has anyone used Facebook API before? I don\u2019t want to ask there because most people monetize and aren\u2019t going to be keen on someone trying to get around their ads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ft5sq", "is_robot_indexable": true, "report_reasons": null, "author": "BestTomatillo6197", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ft5sq/facebook_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ft5sq/facebook_api/", "subreddit_subscribers": 145323, "created_utc": 1702296708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been using Meltano to run an ELT job but I'm not sure I'm following the best practices. I was thinking:\n\n* Run a this job (dynamically) on schedule with the command **meltano el tap-mongodb target-&lt;snowflake&gt; --state-id tap\\_mongodb\\_&lt;dev&gt;**\n* I originally wanted to use GCP cloud run but it doesn't work because of the 60m timeout. This ELT process can take 5-8 hours on a single CPU, I want it to run daily. A couple more questions on this 2nd bullet-point:\n   * Are meltano **partitions** able to parallelize across CPUs / machines? If so then this job doesn't need to take anywhere near 5-8 hours (more like 5-10 minutes on 100 machines)?\n   * Should I be using Airflow to do this? Simple cronjob on a VM? GCP cloud composer / dataflow? SO MANY TOOLS!! I just want this to run on the cloud while being able to see the logs in gcp cloud logging, integrating CI/CD, and mounting a volume from GCP secret manager.\n\n&amp;#x200B;\n\nI am hesitant to spend time diving into Airflow, but curious to hear your thoughts. Here's the below script I've been using to run in production **host.py**:\n\n&amp;#x200B;\n\n    from flask import Flask, make_response, request, redirect, Blueprint, render_template\n    import os\n    import subprocess\n    from configparser import ConfigParser\n    from datetime import datetime\n    from waitress import serve\n    from apscheduler.schedulers.background import BackgroundScheduler\n    import logging\n    import json\n    \n    config = ConfigParser()\n    config.read('config.ini')\n    \n    app = Flask(__name__)\n    app.url_map.strict_slashes = False\n    \n    MELTANO_TARGET = config['TAP_MONGODB']['MELTANO_TARGET']\n    \n    assert isinstance(MELTANO_TARGET, str), 'could not determine target'\n    \n    \n    ###### routes ######\n    \n    @app.route('/')\n    def index():\n        return 'Server is running.'\n    \n    @app.route('/tap_mongodb_dev', methods=['GET'])\n    def tap_mongodb_dev():\n        with app.app_context():\n            project_dir = 'taps/tap-mongodb'\n            run_command = f'meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}'\n            subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n            return make_response(f'Last ran project tap-mongodb-dev target {MELTANO_TARGET} at {cur_timestamp()}.', 200)\n    \n    @app.route('/tap_mongodb_production', methods=['GET'])\n    def tap_mongodb_production():\n        with app.app_context():\n            project_dir = 'taps/tap-mongodb'\n            run_command = f'meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}'\n            subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n            return make_response(f'Last ran project tap-mongodb-production target {MELTANO_TARGET} at {cur_timestamp()}.', 200)\n    \n    \n    if __name__ == \"__main__\":\n        logging.info(f'\\n*** Running environment {ENVIRONMENT}. ***\\n')\n    \n        scheduler = BackgroundScheduler(job_defaults={'max_instances': 2})\n    \n        ###### tap-mongodb ######\n    \n        tap_mongodb_cron = json.loads(config['TAP_MONGODB'][f'{ENVIRONMENT}_CRON_PARAMS'])\n        scheduler.add_job(tap_mongodb_functions[ENVIRONMENT], trigger='cron', **tap_mongodb_cron, jitter=120)\n    \n        ###### host ######\n    \n        HOST = '0.0.0.0'\n        PORT = 5000\n        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n        logging.info(f'Server is listening on port {PORT}')\n        logging.info(f'Hosting environment {ENVIRONMENT}')\n    \n        scheduler.start()\n    \n        serve(app, host=HOST, port=PORT, threads=2)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8xq51rif", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you run a long ELT job in the cloud on schedule?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fhseu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702254625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Meltano to run an ELT job but I&amp;#39;m not sure I&amp;#39;m following the best practices. I was thinking:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run a this job (dynamically) on schedule with the command &lt;strong&gt;meltano el tap-mongodb target-&amp;lt;snowflake&amp;gt; --state-id tap_mongodb_&amp;lt;dev&amp;gt;&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I originally wanted to use GCP cloud run but it doesn&amp;#39;t work because of the 60m timeout. This ELT process can take 5-8 hours on a single CPU, I want it to run daily. A couple more questions on this 2nd bullet-point:\n\n&lt;ul&gt;\n&lt;li&gt;Are meltano &lt;strong&gt;partitions&lt;/strong&gt; able to parallelize across CPUs / machines? If so then this job doesn&amp;#39;t need to take anywhere near 5-8 hours (more like 5-10 minutes on 100 machines)?&lt;/li&gt;\n&lt;li&gt;Should I be using Airflow to do this? Simple cronjob on a VM? GCP cloud composer / dataflow? SO MANY TOOLS!! I just want this to run on the cloud while being able to see the logs in gcp cloud logging, integrating CI/CD, and mounting a volume from GCP secret manager.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am hesitant to spend time diving into Airflow, but curious to hear your thoughts. Here&amp;#39;s the below script I&amp;#39;ve been using to run in production &lt;strong&gt;host.py&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from flask import Flask, make_response, request, redirect, Blueprint, render_template\nimport os\nimport subprocess\nfrom configparser import ConfigParser\nfrom datetime import datetime\nfrom waitress import serve\nfrom apscheduler.schedulers.background import BackgroundScheduler\nimport logging\nimport json\n\nconfig = ConfigParser()\nconfig.read(&amp;#39;config.ini&amp;#39;)\n\napp = Flask(__name__)\napp.url_map.strict_slashes = False\n\nMELTANO_TARGET = config[&amp;#39;TAP_MONGODB&amp;#39;][&amp;#39;MELTANO_TARGET&amp;#39;]\n\nassert isinstance(MELTANO_TARGET, str), &amp;#39;could not determine target&amp;#39;\n\n\n###### routes ######\n\n@app.route(&amp;#39;/&amp;#39;)\ndef index():\n    return &amp;#39;Server is running.&amp;#39;\n\n@app.route(&amp;#39;/tap_mongodb_dev&amp;#39;, methods=[&amp;#39;GET&amp;#39;])\ndef tap_mongodb_dev():\n    with app.app_context():\n        project_dir = &amp;#39;taps/tap-mongodb&amp;#39;\n        run_command = f&amp;#39;meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}&amp;#39;\n        subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n        return make_response(f&amp;#39;Last ran project tap-mongodb-dev target {MELTANO_TARGET} at {cur_timestamp()}.&amp;#39;, 200)\n\n@app.route(&amp;#39;/tap_mongodb_production&amp;#39;, methods=[&amp;#39;GET&amp;#39;])\ndef tap_mongodb_production():\n    with app.app_context():\n        project_dir = &amp;#39;taps/tap-mongodb&amp;#39;\n        run_command = f&amp;#39;meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}&amp;#39;\n        subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n        return make_response(f&amp;#39;Last ran project tap-mongodb-production target {MELTANO_TARGET} at {cur_timestamp()}.&amp;#39;, 200)\n\n\nif __name__ == &amp;quot;__main__&amp;quot;:\n    logging.info(f&amp;#39;\\n*** Running environment {ENVIRONMENT}. ***\\n&amp;#39;)\n\n    scheduler = BackgroundScheduler(job_defaults={&amp;#39;max_instances&amp;#39;: 2})\n\n    ###### tap-mongodb ######\n\n    tap_mongodb_cron = json.loads(config[&amp;#39;TAP_MONGODB&amp;#39;][f&amp;#39;{ENVIRONMENT}_CRON_PARAMS&amp;#39;])\n    scheduler.add_job(tap_mongodb_functions[ENVIRONMENT], trigger=&amp;#39;cron&amp;#39;, **tap_mongodb_cron, jitter=120)\n\n    ###### host ######\n\n    HOST = &amp;#39;0.0.0.0&amp;#39;\n    PORT = 5000\n    logging.basicConfig(level=logging.INFO, format=&amp;#39;%(asctime)s - %(levelname)s - %(message)s&amp;#39;)\n    logging.info(f&amp;#39;Server is listening on port {PORT}&amp;#39;)\n    logging.info(f&amp;#39;Hosting environment {ENVIRONMENT}&amp;#39;)\n\n    scheduler.start()\n\n    serve(app, host=HOST, port=PORT, threads=2)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fhseu", "is_robot_indexable": true, "report_reasons": null, "author": "Training_Butterfly70", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fhseu/how_do_you_run_a_long_elt_job_in_the_cloud_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fhseu/how_do_you_run_a_long_elt_job_in_the_cloud_on/", "subreddit_subscribers": 145323, "created_utc": 1702254625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nI am no newbie to data migration using AWS tech and more. However, I would love to pick the brain of someone who is working with a reliable stack that serves a Business Intelligence team or Data Warehouse population.\n\nWould anyone care to meet on discord to discuss such topics? If you're company is using a combination of AWS and open-source tools to get the job done, and the job IS getting done, I really want to chat with you. The one constraint I would add is that I am not interested in any tooling that requires sending your data to a third-party network for treatment.\n\nTo be clear: I am not looking for work, I am not selling anything, and I am not a beginner. I am sincerely looking for advice and use cases to enrich my understanding in this field.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Request for mentor in AWS Land (for intermediate-advanced topics)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18g00ax", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702317252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I am no newbie to data migration using AWS tech and more. However, I would love to pick the brain of someone who is working with a reliable stack that serves a Business Intelligence team or Data Warehouse population.&lt;/p&gt;\n\n&lt;p&gt;Would anyone care to meet on discord to discuss such topics? If you&amp;#39;re company is using a combination of AWS and open-source tools to get the job done, and the job IS getting done, I really want to chat with you. The one constraint I would add is that I am not interested in any tooling that requires sending your data to a third-party network for treatment.&lt;/p&gt;\n\n&lt;p&gt;To be clear: I am not looking for work, I am not selling anything, and I am not a beginner. I am sincerely looking for advice and use cases to enrich my understanding in this field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18g00ax", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18g00ax/request_for_mentor_in_aws_land_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18g00ax/request_for_mentor_in_aws_land_for/", "subreddit_subscribers": 145323, "created_utc": 1702317252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been going through a few rounds of interviews for my first DE job, and while Im excited to break in to that 'role' from where I'm at now, I'm a bit concerned with the QoL the company seems to have. \n\nMost reviews Ive seen say its a grind, and that it's nonstop - but none of those kinds of reviews exist for the DE team I'd be on - A PS team serving clients data needs and projects. \n\nOn top of that, there are some red flags Ive seen, like the person who would be my manager telling me \"*Our first core tenant is that we don't hire assholes*\". They also 'live on the bleeding edge' and aren't afraid to pivot to new tech asap if old tech isn't working out. I asked how they every skill up and get used to the inner workings of tools they're using if theyre changing tech every 3 months, but he gave a real non-answer.\n\n\nIt has a weirdly 'bro-culture' vibe, and though the glassdoor reviews aren't overtly negative (~3.6), [and actually higher than my current company I work at and like] I still have a weird vibe about the place. I'm just on the fence about it because of the red flags.\n \n**I know DE is usually seen as an internal role to get data to different teams for reporting, so I'm curious if anyone else has worked in a similar client-facing DE role and could talk about it.** \n\nIt'd be like a ~40% raise to what Im making now (putting me at around 130,000) and they use the tools I want to learn that I can't use in my current day to day job (PySpark/Hadoop, PyTorch, and others) so I figure I can grind it out a year to get the skills then bounce - but Im someone who is very focused on maintaining a WLB and it sounds like they don't really have one.", "author_fullname": "t2_f6oir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone do DE on a 'professional services' client-facing team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fxx4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702310695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been going through a few rounds of interviews for my first DE job, and while Im excited to break in to that &amp;#39;role&amp;#39; from where I&amp;#39;m at now, I&amp;#39;m a bit concerned with the QoL the company seems to have. &lt;/p&gt;\n\n&lt;p&gt;Most reviews Ive seen say its a grind, and that it&amp;#39;s nonstop - but none of those kinds of reviews exist for the DE team I&amp;#39;d be on - A PS team serving clients data needs and projects. &lt;/p&gt;\n\n&lt;p&gt;On top of that, there are some red flags Ive seen, like the person who would be my manager telling me &amp;quot;&lt;em&gt;Our first core tenant is that we don&amp;#39;t hire assholes&lt;/em&gt;&amp;quot;. They also &amp;#39;live on the bleeding edge&amp;#39; and aren&amp;#39;t afraid to pivot to new tech asap if old tech isn&amp;#39;t working out. I asked how they every skill up and get used to the inner workings of tools they&amp;#39;re using if theyre changing tech every 3 months, but he gave a real non-answer.&lt;/p&gt;\n\n&lt;p&gt;It has a weirdly &amp;#39;bro-culture&amp;#39; vibe, and though the glassdoor reviews aren&amp;#39;t overtly negative (~3.6), [and actually higher than my current company I work at and like] I still have a weird vibe about the place. I&amp;#39;m just on the fence about it because of the red flags.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I know DE is usually seen as an internal role to get data to different teams for reporting, so I&amp;#39;m curious if anyone else has worked in a similar client-facing DE role and could talk about it.&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;d be like a ~40% raise to what Im making now (putting me at around 130,000) and they use the tools I want to learn that I can&amp;#39;t use in my current day to day job (PySpark/Hadoop, PyTorch, and others) so I figure I can grind it out a year to get the skills then bounce - but Im someone who is very focused on maintaining a WLB and it sounds like they don&amp;#39;t really have one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18fxx4a", "is_robot_indexable": true, "report_reasons": null, "author": "XxNerdAtHeartxX", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fxx4a/does_anyone_do_de_on_a_professional_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fxx4a/does_anyone_do_de_on_a_professional_services/", "subreddit_subscribers": 145323, "created_utc": 1702310695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6y0b4txf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why We Built a Streaming SQL Engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "name": "t3_18fxkru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2J0x5cInVFc4DsbYiW63I9eVgLbm79HBvsP1VKu7kbA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702309813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "epsio.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.epsio.io/blog/why-we-built-a-streaming-sql-engine", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?auto=webp&amp;s=bde1bb439f8042c2066e539de311e97a9b34f68c", "width": 1376, "height": 714}, "resolutions": [{"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eac5d4e9cb92dce7b0b63f164195fd8d9934b59c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41f34043110161b56e0b27712a353a975d56bf20", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b52001b9486b04f0742e761902c5cfcd8cbbf0dc", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a259d069e4ab50668673eb345d4d655ffd6c5cb5", "width": 640, "height": 332}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4fa737b762cbbbf12c1acfcccbaca8eb8785678f", "width": 960, "height": 498}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=213bb55bc131cf226f4518542ac26d7f7de4a0fa", "width": 1080, "height": 560}], "variants": {}, "id": "L6vsGJtlTpCBQYIHfzVB4U_eHzCDAFcguQ6gS8SW1ns"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18fxkru", "is_robot_indexable": true, "report_reasons": null, "author": "Giladkl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fxkru/why_we_built_a_streaming_sql_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.epsio.io/blog/why-we-built-a-streaming-sql-engine", "subreddit_subscribers": 145323, "created_utc": 1702309813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI am having some serious issues at work with trying to make user-facing dashboards work with the setup that was mostly made by the previous data engineer.\n\n# Context\n\nThe company makes software where people can book things (for example, meeting rooms). What I've been trying to build is a full-stack application where our users can create and edit dashboards consisting of widgets with KPIs, such as booking rates, most booked resources, locations... You can view it as some sort of homemade Looker, Tableau or Power BI application that is tailored for our use cases. \n\n# The current setup \n\nData is pulled from the transactional databases into a Postgres data warehouse via an ETL made with Luigi, homemade python and cronjobs, once a day. This was objectively enough for the previous use case (Power BI). However, since a Python API that I built is now serving the data for the frontend instead of the data layer of Power BI, the requirements have changed. Response times for some queries is awful (we're talking minutes here, absolutely unacceptable for a user-facing application).\n\n# An example\n\nQueries are sometimes quite complex : I'll walk you through the specs of one of the painful ones. Let's say you are part of \"Company\". Company has meeting rooms that have :  \n\n- an id\n- a location (declined in several columns : country, city, and so on)\n- a type\n- a name\n- a capacity\n- a creation time\n\nPeople have made bookings on these rooms. Bookings are defined as : \n\n- an id\n- a start_time (timestamp with timezone, stored as utc)\n- an end_time (same)\n- a room_id\n\nI've kept a lot of columns out of the definitions so you can better understand this specific use case, just know these are not the only columns in these tables.\n\nNow, we want, for a specific location, and for a given timeframe (can be anything from 1 month to 10 years) the average occupancy rate per hour of the day, however it is part of the specs for the user to be able to filter the time slots he wants. This could be 8AM to 6PM but it might as well be 8-9AM + 10-11AM + 1-2PM... and so on. \n\nSince the bookings are defined as start and end time, and users can filter out hours from the day, I made an incremental DBT model that calculates the occupancy rate of every hour for each room since its creation. That way, this data is only calculated once and can be queried way more easily. This however is *really* long for the first run since our clients may have tens of thousands of rooms and some have been with us for the better part of 10 years, which means they have millions of bookings on these rooms. X years times the amount of hours in a year, for every room... the resulting model is easily hundreds of millions of rows of calculated data.\n\nThis table looks like this :\n\n- timeslot (timestamp with timezone, stored as UTC)\n- client_id\n- room_id\n- occupancy_rate\n\n\nThis DBT model has sped up the queries pretty much 10x. This is not enough however, and even with the biggest RDS instance we've monitored the CPU of the instance to go to 100% for requests handling many rows. It feels like we've hit the limitations of postgres and might need a database more suited to our use cases.\n\n# More requirements and constraints\n\nWe have on-prem clients, and both the infrastructure team and data team are really small (I'm the only software engineer of the data team, and the infra team is made up of 3 people).\nThat means no cloud data warehouse, and avoiding high-maintenance databases.\nAlso, our clients can be anywhere in the world so the database needs to have timezone-related features.\n\n# My ideas for now\n\nAfter doing some benchmarks on a few MPP databases, I feel like Clickhouse may be a good database for this. I've also explored Apache Doris, Starrocks (which unfortunately does not have timezone features), and Apache Pinot.\n\nClickhouse however needs denormalized data to be performant, and changing the DBT model to include the rooms data means we need to be able to update millions of rows when a room's data changes... Which might be quite costly. Still better than the application not working, though.\n\nIf anyone has an idea on how to make this whole situation better, I'm all ears. I feel like I'm in way over my head sometimes. If you need additional information let me know.", "author_fullname": "t2_72swh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current data setup cannot handle user-facing dashboards", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fxgna", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702309509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am having some serious issues at work with trying to make user-facing dashboards work with the setup that was mostly made by the previous data engineer.&lt;/p&gt;\n\n&lt;h1&gt;Context&lt;/h1&gt;\n\n&lt;p&gt;The company makes software where people can book things (for example, meeting rooms). What I&amp;#39;ve been trying to build is a full-stack application where our users can create and edit dashboards consisting of widgets with KPIs, such as booking rates, most booked resources, locations... You can view it as some sort of homemade Looker, Tableau or Power BI application that is tailored for our use cases. &lt;/p&gt;\n\n&lt;h1&gt;The current setup&lt;/h1&gt;\n\n&lt;p&gt;Data is pulled from the transactional databases into a Postgres data warehouse via an ETL made with Luigi, homemade python and cronjobs, once a day. This was objectively enough for the previous use case (Power BI). However, since a Python API that I built is now serving the data for the frontend instead of the data layer of Power BI, the requirements have changed. Response times for some queries is awful (we&amp;#39;re talking minutes here, absolutely unacceptable for a user-facing application).&lt;/p&gt;\n\n&lt;h1&gt;An example&lt;/h1&gt;\n\n&lt;p&gt;Queries are sometimes quite complex : I&amp;#39;ll walk you through the specs of one of the painful ones. Let&amp;#39;s say you are part of &amp;quot;Company&amp;quot;. Company has meeting rooms that have :  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;an id&lt;/li&gt;\n&lt;li&gt;a location (declined in several columns : country, city, and so on)&lt;/li&gt;\n&lt;li&gt;a type&lt;/li&gt;\n&lt;li&gt;a name&lt;/li&gt;\n&lt;li&gt;a capacity&lt;/li&gt;\n&lt;li&gt;a creation time&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;People have made bookings on these rooms. Bookings are defined as : &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;an id&lt;/li&gt;\n&lt;li&gt;a start_time (timestamp with timezone, stored as utc)&lt;/li&gt;\n&lt;li&gt;an end_time (same)&lt;/li&gt;\n&lt;li&gt;a room_id&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve kept a lot of columns out of the definitions so you can better understand this specific use case, just know these are not the only columns in these tables.&lt;/p&gt;\n\n&lt;p&gt;Now, we want, for a specific location, and for a given timeframe (can be anything from 1 month to 10 years) the average occupancy rate per hour of the day, however it is part of the specs for the user to be able to filter the time slots he wants. This could be 8AM to 6PM but it might as well be 8-9AM + 10-11AM + 1-2PM... and so on. &lt;/p&gt;\n\n&lt;p&gt;Since the bookings are defined as start and end time, and users can filter out hours from the day, I made an incremental DBT model that calculates the occupancy rate of every hour for each room since its creation. That way, this data is only calculated once and can be queried way more easily. This however is &lt;em&gt;really&lt;/em&gt; long for the first run since our clients may have tens of thousands of rooms and some have been with us for the better part of 10 years, which means they have millions of bookings on these rooms. X years times the amount of hours in a year, for every room... the resulting model is easily hundreds of millions of rows of calculated data.&lt;/p&gt;\n\n&lt;p&gt;This table looks like this :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;timeslot (timestamp with timezone, stored as UTC)&lt;/li&gt;\n&lt;li&gt;client_id&lt;/li&gt;\n&lt;li&gt;room_id&lt;/li&gt;\n&lt;li&gt;occupancy_rate&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This DBT model has sped up the queries pretty much 10x. This is not enough however, and even with the biggest RDS instance we&amp;#39;ve monitored the CPU of the instance to go to 100% for requests handling many rows. It feels like we&amp;#39;ve hit the limitations of postgres and might need a database more suited to our use cases.&lt;/p&gt;\n\n&lt;h1&gt;More requirements and constraints&lt;/h1&gt;\n\n&lt;p&gt;We have on-prem clients, and both the infrastructure team and data team are really small (I&amp;#39;m the only software engineer of the data team, and the infra team is made up of 3 people).\nThat means no cloud data warehouse, and avoiding high-maintenance databases.\nAlso, our clients can be anywhere in the world so the database needs to have timezone-related features.&lt;/p&gt;\n\n&lt;h1&gt;My ideas for now&lt;/h1&gt;\n\n&lt;p&gt;After doing some benchmarks on a few MPP databases, I feel like Clickhouse may be a good database for this. I&amp;#39;ve also explored Apache Doris, Starrocks (which unfortunately does not have timezone features), and Apache Pinot.&lt;/p&gt;\n\n&lt;p&gt;Clickhouse however needs denormalized data to be performant, and changing the DBT model to include the rooms data means we need to be able to update millions of rows when a room&amp;#39;s data changes... Which might be quite costly. Still better than the application not working, though.&lt;/p&gt;\n\n&lt;p&gt;If anyone has an idea on how to make this whole situation better, I&amp;#39;m all ears. I feel like I&amp;#39;m in way over my head sometimes. If you need additional information let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fxgna", "is_robot_indexable": true, "report_reasons": null, "author": "Altarim", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fxgna/current_data_setup_cannot_handle_userfacing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fxgna/current_data_setup_cannot_handle_userfacing/", "subreddit_subscribers": 145323, "created_utc": 1702309509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hullo Data Engineers,\n\nLooking for some guidance. I have a company approach me as an independant contractor, and want me to set up a full on DE infrastructure. This would be my first official contracting job and my plan is to implement something simple according to their needs, along these are the parts that I want to implement. In AWS (new instance):\n\n* PostgreSQL db to start with as a \"Warehouse\".\n* Metabase for visualization on top of the above\n* PG Admin for administration\n* S3 as a Storage/data lake solution\n* MWAA low cost to start with, they have several systems they need data from\n* CI/CD (For the pipelines mwaa to s3 with testing, etc)\n* IaC setup from the start. \n* IAM policies and whatnot\n\nAll in all very exciting, however, I am at a loss as to how much to charge for a full project like this. Any advise? I don't want to over shoot and/or undershoot as much as I can\n\nMany thanks in advance\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_5wyo5ojc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost advise for a project implementation as a consultant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fwab6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702306351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hullo Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;Looking for some guidance. I have a company approach me as an independant contractor, and want me to set up a full on DE infrastructure. This would be my first official contracting job and my plan is to implement something simple according to their needs, along these are the parts that I want to implement. In AWS (new instance):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;PostgreSQL db to start with as a &amp;quot;Warehouse&amp;quot;.&lt;/li&gt;\n&lt;li&gt;Metabase for visualization on top of the above&lt;/li&gt;\n&lt;li&gt;PG Admin for administration&lt;/li&gt;\n&lt;li&gt;S3 as a Storage/data lake solution&lt;/li&gt;\n&lt;li&gt;MWAA low cost to start with, they have several systems they need data from&lt;/li&gt;\n&lt;li&gt;CI/CD (For the pipelines mwaa to s3 with testing, etc)&lt;/li&gt;\n&lt;li&gt;IaC setup from the start. &lt;/li&gt;\n&lt;li&gt;IAM policies and whatnot&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All in all very exciting, however, I am at a loss as to how much to charge for a full project like this. Any advise? I don&amp;#39;t want to over shoot and/or undershoot as much as I can&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fwab6", "is_robot_indexable": true, "report_reasons": null, "author": "alfredosuac", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18fwab6/cost_advise_for_a_project_implementation_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fwab6/cost_advise_for_a_project_implementation_as_a/", "subreddit_subscribers": 145323, "created_utc": 1702306351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi \n\nwhat is the difference between running the query in bigquery or running it using one of the editor such as datagrip or dbvisualizer? \n\nis it cheaper? faster?\n\nwhy people don\u2019t just use bigquery?", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery Computing Cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fpiih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702281816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi &lt;/p&gt;\n\n&lt;p&gt;what is the difference between running the query in bigquery or running it using one of the editor such as datagrip or dbvisualizer? &lt;/p&gt;\n\n&lt;p&gt;is it cheaper? faster?&lt;/p&gt;\n\n&lt;p&gt;why people don\u2019t just use bigquery?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fpiih", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fpiih/bigquery_computing_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fpiih/bigquery_computing_cost/", "subreddit_subscribers": 145323, "created_utc": 1702281816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The key here is the ETL aspect. I need sample databases in postgres, oracle (or any sql dialect really) that have both data and ETL .sql scripts that I don't have to reinvent. Does anyone know one that exists that have both?", "author_fullname": "t2_rk16bnik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for sample postgres and oracle databases that have ETL scripts preloaded into them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ffap8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702247443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The key here is the ETL aspect. I need sample databases in postgres, oracle (or any sql dialect really) that have both data and ETL .sql scripts that I don&amp;#39;t have to reinvent. Does anyone know one that exists that have both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ffap8", "is_robot_indexable": true, "report_reasons": null, "author": "notstoppinguntil30", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ffap8/looking_for_sample_postgres_and_oracle_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ffap8/looking_for_sample_postgres_and_oracle_databases/", "subreddit_subscribers": 145323, "created_utc": 1702247443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, \n\nI am a employee in an MNC having 2.5 years of experience and currently working on a support based project with the tech stack as webmethods which is a integration tool based out of Java and it is being to connect different systems / businesses. I'm bored and frustrated at the same time because the work is same and almost repetitive with no signs of growth and learning. That's when I decided to switch domain but\n\nI was very confused while choosing between Data Engineering and Java. But I choose Data Engineering and started preparing for it. I currently have experience with python easy to medium, SQL mid to advance level and achieved two cloud certifications AZ-900 &amp; AZ-204 currently preparing for DP-203. I have basic knowledge and hands-on azure df, Synapse and storage. \n\nI tried looking for jobs matching my skills but whenever I started checking the job description I always lack in some or the other skills such as Big data and Spark. So if someone can guide me to tackel with this and land a job in data engineer domain it would be a great help. \n\nThanks in Advance :)", "author_fullname": "t2_ib9mu62z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Carrer switch to Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18frtzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.27, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702291672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, &lt;/p&gt;\n\n&lt;p&gt;I am a employee in an MNC having 2.5 years of experience and currently working on a support based project with the tech stack as webmethods which is a integration tool based out of Java and it is being to connect different systems / businesses. I&amp;#39;m bored and frustrated at the same time because the work is same and almost repetitive with no signs of growth and learning. That&amp;#39;s when I decided to switch domain but&lt;/p&gt;\n\n&lt;p&gt;I was very confused while choosing between Data Engineering and Java. But I choose Data Engineering and started preparing for it. I currently have experience with python easy to medium, SQL mid to advance level and achieved two cloud certifications AZ-900 &amp;amp; AZ-204 currently preparing for DP-203. I have basic knowledge and hands-on azure df, Synapse and storage. &lt;/p&gt;\n\n&lt;p&gt;I tried looking for jobs matching my skills but whenever I started checking the job description I always lack in some or the other skills such as Big data and Spark. So if someone can guide me to tackel with this and land a job in data engineer domain it would be a great help. &lt;/p&gt;\n\n&lt;p&gt;Thanks in Advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18frtzg", "is_robot_indexable": true, "report_reasons": null, "author": "iamDjsahu", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18frtzg/carrer_switch_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18frtzg/carrer_switch_to_data_engineering/", "subreddit_subscribers": 145323, "created_utc": 1702291672.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}