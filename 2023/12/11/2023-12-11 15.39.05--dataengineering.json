{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. \n\nData is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. \n\nOverall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. \n\nAnyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. ", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I Was Happier Being a Bartender Compared to Being a 6 Figure DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ffzmx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 239, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 239, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702249969.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702249347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I\u2019ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. &lt;/p&gt;\n\n&lt;p&gt;Data is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. &lt;/p&gt;\n\n&lt;p&gt;Overall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. &lt;/p&gt;\n\n&lt;p&gt;Anyone else go into data and realize the grass wasn\u2019t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18ffzmx", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 107, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/", "subreddit_subscribers": 145293, "created_utc": 1702249347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What's the best SQL environment you've had the pleasure of working in?", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best SQL environment you have ever worked in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fii0v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702256799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What&amp;#39;s the best SQL environment you&amp;#39;ve had the pleasure of working in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fii0v", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fii0v/what_is_the_best_sql_environment_you_have_ever/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fii0v/what_is_the_best_sql_environment_you_have_ever/", "subreddit_subscribers": 145293, "created_utc": 1702256799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.\n\nEncrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage PII?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7m1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702226742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.&lt;/p&gt;\n\n&lt;p&gt;Encrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f7m1t", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "subreddit_subscribers": 145293, "created_utc": 1702226742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently unemployed for last 6 months. I am having 8 years of experience in ETL, data warehouse building, snowflake, spark and tableau. \n\nI also have bit of hands on experience in low code tools like fivetran, data modelling tools like dbt.\n\nDue to the market conditions I am not receiving any job offers, so planning to upskill in the time being. I already have certifications in aws solution architect associate, snowflake snowpro core and Apache spark developer by Databricks. \n\nMy question is, what new technology can I learn and get certified in to get an edge when the market is up? Should I try my hands on any NoSQL databases? Will learning kafka will open up my job chances? \n\nPS: my priority right now is landing up a job immediately once the market is up. I am continuously applying jobs, but not getting any calls. Seeing that there are not much calls, I am panicked and wants to learn something related to my field but can land me a job.", "author_fullname": "t2_o51po378", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What new to learn in DE now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18fusjx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702302069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently unemployed for last 6 months. I am having 8 years of experience in ETL, data warehouse building, snowflake, spark and tableau. &lt;/p&gt;\n\n&lt;p&gt;I also have bit of hands on experience in low code tools like fivetran, data modelling tools like dbt.&lt;/p&gt;\n\n&lt;p&gt;Due to the market conditions I am not receiving any job offers, so planning to upskill in the time being. I already have certifications in aws solution architect associate, snowflake snowpro core and Apache spark developer by Databricks. &lt;/p&gt;\n\n&lt;p&gt;My question is, what new technology can I learn and get certified in to get an edge when the market is up? Should I try my hands on any NoSQL databases? Will learning kafka will open up my job chances? &lt;/p&gt;\n\n&lt;p&gt;PS: my priority right now is landing up a job immediately once the market is up. I am continuously applying jobs, but not getting any calls. Seeing that there are not much calls, I am panicked and wants to learn something related to my field but can land me a job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18fusjx", "is_robot_indexable": true, "report_reasons": null, "author": "NeighborhoodCold5339", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fusjx/what_new_to_learn_in_de_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fusjx/what_new_to_learn_in_de_now/", "subreddit_subscribers": 145293, "created_utc": 1702302069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ibis ([https://ibis-project.org/](https://ibis-project.org/)) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.\n\nIt looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.\n\nHowever, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?\n\nIbis' documentation is relatively limited and I haven't found much information about what organizations back this project.", "author_fullname": "t2_7v7x1pqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Ibis reliably be used in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fcbf0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702239463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ibis (&lt;a href=\"https://ibis-project.org/\"&gt;https://ibis-project.org/&lt;/a&gt;) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.&lt;/p&gt;\n\n&lt;p&gt;It looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.&lt;/p&gt;\n\n&lt;p&gt;However, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?&lt;/p&gt;\n\n&lt;p&gt;Ibis&amp;#39; documentation is relatively limited and I haven&amp;#39;t found much information about what organizations back this project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fcbf0", "is_robot_indexable": true, "report_reasons": null, "author": "cantthinkofoneuse", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "subreddit_subscribers": 145293, "created_utc": 1702239463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company just announced a restructuring of our central data platform org and my team was effected.\n\nPreviously we were basically a data engineering team for a specific data domain developing and owning pipelines, data lakes/warehouse, and visualizations. However, post-restructuring we\u2019re now \u201ctechnically\u201d part of the platform team, but from what I understand we\u2019re  basically end-users for the core platform team, and they\u2019ll be building tools for us to ingest, store, transform etc\u2026 we\u2019re just focused on ingestion.\n\nUltimately I\u2019m worried about the long term staying power of a team that ONLY does ingestion using simplified internal tools. What makes matters worse is that the platform team is looking to build these tools for general use by business data engineers. So in a future where other groups can use the same tools to ingest their own data, what\u2019s the value of a central data team?\n\nI\u2019m planning to talk with my manager about this, but I want to come with some idea of a better direction in mind before I have that conversation, so curious if anyone here has advice?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do central data platforms usually have dedicated central ingestion teams?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18frbal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702289515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company just announced a restructuring of our central data platform org and my team was effected.&lt;/p&gt;\n\n&lt;p&gt;Previously we were basically a data engineering team for a specific data domain developing and owning pipelines, data lakes/warehouse, and visualizations. However, post-restructuring we\u2019re now \u201ctechnically\u201d part of the platform team, but from what I understand we\u2019re  basically end-users for the core platform team, and they\u2019ll be building tools for us to ingest, store, transform etc\u2026 we\u2019re just focused on ingestion.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I\u2019m worried about the long term staying power of a team that ONLY does ingestion using simplified internal tools. What makes matters worse is that the platform team is looking to build these tools for general use by business data engineers. So in a future where other groups can use the same tools to ingest their own data, what\u2019s the value of a central data team?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m planning to talk with my manager about this, but I want to come with some idea of a better direction in mind before I have that conversation, so curious if anyone here has advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18frbal", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18frbal/do_central_data_platforms_usually_have_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18frbal/do_central_data_platforms_usually_have_dedicated/", "subreddit_subscribers": 145293, "created_utc": 1702289515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI'm a data engineer at a startup and have been working on getting various operational data into Databricks. My next challenge is to present an overview of our current data assets to the management team (Some C-level stakeholders also like to know). They have also requested an internal dashboard with some key info, but they have no idea what I currently have in the Databricks warehouse, and I and not entirely sure which data they'll find most relevant...As a first step, I'd like to have a clear and simple way to show them what data assets we have.\n\nI'm considering the following:\n\n1. A big data dictionary page in confluence: This is how it was done in my previous company a page starts with a big table with columns: table name, column name, column description, source, logic...etc.  I feel it'll be a good table for data analysts to refer to, but not sure if it will be too detailed for those senior stakeholders.\n2. A dashboard in Databricks: Chatgpt brought up this method to me, by using information tables in databricks, I will be able to create a databricks dashboard for data inventory. the pros will be it will automatically handle the new coming tables and cols which saves a bit of overhead. But I am not sure if it will be easy to add and put notes on the tables/columns or formatting the table in a more insightful way(grouping by topic for example)\n3. A mindmap: Came to my mind that a mindmap is usually easy to follow and helps in capturing the big picture and overall structure.  Perhaps a mind map that branches out based on topics, subtopics, and table names?\n\n4. Do you have any other suggestions for presenting data assets?\n\nThanks in advance!", "author_fullname": "t2_hf67tpfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presenting Data Inventory to the Management team in the startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fnity", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702273576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data engineer at a startup and have been working on getting various operational data into Databricks. My next challenge is to present an overview of our current data assets to the management team (Some C-level stakeholders also like to know). They have also requested an internal dashboard with some key info, but they have no idea what I currently have in the Databricks warehouse, and I and not entirely sure which data they&amp;#39;ll find most relevant...As a first step, I&amp;#39;d like to have a clear and simple way to show them what data assets we have.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A big data dictionary page in confluence: This is how it was done in my previous company a page starts with a big table with columns: table name, column name, column description, source, logic...etc.  I feel it&amp;#39;ll be a good table for data analysts to refer to, but not sure if it will be too detailed for those senior stakeholders.&lt;/li&gt;\n&lt;li&gt;A dashboard in Databricks: Chatgpt brought up this method to me, by using information tables in databricks, I will be able to create a databricks dashboard for data inventory. the pros will be it will automatically handle the new coming tables and cols which saves a bit of overhead. But I am not sure if it will be easy to add and put notes on the tables/columns or formatting the table in a more insightful way(grouping by topic for example)&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A mindmap: Came to my mind that a mindmap is usually easy to follow and helps in capturing the big picture and overall structure.  Perhaps a mind map that branches out based on topics, subtopics, and table names?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Do you have any other suggestions for presenting data assets?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fnity", "is_robot_indexable": true, "report_reasons": null, "author": "East-Garage2337", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fnity/presenting_data_inventory_to_the_management_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fnity/presenting_data_inventory_to_the_management_team/", "subreddit_subscribers": 145293, "created_utc": 1702273576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, this is my first DE project.    [Baitur5/reddit\\_api\\_elt (github.com)](https://github.com/Baitur5/reddit_api_elt) . It is basically about   \na data pipeline that extracts Reddit data for a Google Data Studio report, focusing on a specific subreddit   \nCan you guys check it out , and give some advice &amp; tips on how to improve it or the next things I should add.  \n\n\nP.S. I followed steps from this repository but made some adjustments: [ABZ-Aaron/Reddit-API-Pipeline (github.com)](https://github.com/ABZ-Aaron/Reddit-API-Pipeline) ", "author_fullname": "t2_kq543w8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit ELT Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18froaz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702291003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, this is my first DE project.    &lt;a href=\"https://github.com/Baitur5/reddit_api_elt\"&gt;Baitur5/reddit_api_elt (github.com)&lt;/a&gt; . It is basically about&lt;br/&gt;\na data pipeline that extracts Reddit data for a Google Data Studio report, focusing on a specific subreddit&lt;br/&gt;\nCan you guys check it out , and give some advice &amp;amp; tips on how to improve it or the next things I should add.  &lt;/p&gt;\n\n&lt;p&gt;P.S. I followed steps from this repository but made some adjustments: &lt;a href=\"https://github.com/ABZ-Aaron/Reddit-API-Pipeline\"&gt;ABZ-Aaron/Reddit-API-Pipeline (github.com)&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18froaz", "is_robot_indexable": true, "report_reasons": null, "author": "ulukbekovbr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18froaz/reddit_elt_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18froaz/reddit_elt_pipeline/", "subreddit_subscribers": 145293, "created_utc": 1702291003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if anyone has war stories, success or failures, of using DuckDB in their production pipelines. \n\nWhat is the ecosystem look like? What tools are you using? What drove success or failure in your case? \n\nI\u2019m an MSSQL shop, so hard sell to my team, but I\u2019m also the sole dev maintaining our pyspark environment, and think there\u2019s opportunities for duck, especially in some containerized use cases I\u2019m prototyping. Figured I\u2019d ask what others have done \ud83d\udcaa\ud83c\udffb", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Production DuckDB Setups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fig7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702256645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if anyone has war stories, success or failures, of using DuckDB in their production pipelines. &lt;/p&gt;\n\n&lt;p&gt;What is the ecosystem look like? What tools are you using? What drove success or failure in your case? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m an MSSQL shop, so hard sell to my team, but I\u2019m also the sole dev maintaining our pyspark environment, and think there\u2019s opportunities for duck, especially in some containerized use cases I\u2019m prototyping. Figured I\u2019d ask what others have done \ud83d\udcaa\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fig7l", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fig7l/production_duckdb_setups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fig7l/production_duckdb_setups/", "subreddit_subscribers": 145293, "created_utc": 1702256645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm part of a project that's implementing a new Data warehousing solution and we are just about to scale our team from about 10 people into around 30 divided into 5 teams or so. We are just about to implement DBT and has started building a Data Vault. The increase in team size and the phase we're currently in means that all of the teams will be working on modelling and implementing the Data Vault.\n\nWe are now thinking of how to set up our GIT repo(s) and DBT to accomodate this situation as good as possible. With Data Vault it feels like we need to build it in a mono repo, but with this many people and teams it feels like a recipe for disaster and it would be better to divide the Vault into multiple repos.\n\nHow have you guys handled this situation?", "author_fullname": "t2_o0nay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT and Data Vault with multiple teams, mono repo or not?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18fveo7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702303860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m part of a project that&amp;#39;s implementing a new Data warehousing solution and we are just about to scale our team from about 10 people into around 30 divided into 5 teams or so. We are just about to implement DBT and has started building a Data Vault. The increase in team size and the phase we&amp;#39;re currently in means that all of the teams will be working on modelling and implementing the Data Vault.&lt;/p&gt;\n\n&lt;p&gt;We are now thinking of how to set up our GIT repo(s) and DBT to accomodate this situation as good as possible. With Data Vault it feels like we need to build it in a mono repo, but with this many people and teams it feels like a recipe for disaster and it would be better to divide the Vault into multiple repos.&lt;/p&gt;\n\n&lt;p&gt;How have you guys handled this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fveo7", "is_robot_indexable": true, "report_reasons": null, "author": "zirxo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fveo7/dbt_and_data_vault_with_multiple_teams_mono_repo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fveo7/dbt_and_data_vault_with_multiple_teams_mono_repo/", "subreddit_subscribers": 145293, "created_utc": 1702303860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you perform updates and deletes in a data warehouse? I've been told that temporality in a data warehouse should be handled through either temporal surrogation or data vault methods. Neither maintains an \"expired\" attribute.  \n ", "author_fullname": "t2_jg3w8gbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Updates and Deletes in a Data Warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fubwe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702300627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you perform updates and deletes in a data warehouse? I&amp;#39;ve been told that temporality in a data warehouse should be handled through either temporal surrogation or data vault methods. Neither maintains an &amp;quot;expired&amp;quot; attribute.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fubwe", "is_robot_indexable": true, "report_reasons": null, "author": "tamargal91", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fubwe/updates_and_deletes_in_a_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fubwe/updates_and_deletes_in_a_data_warehouse/", "subreddit_subscribers": 145293, "created_utc": 1702300627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used Facebook API before?\n\nI\u2019ve been off Facebook for months, because the ads and Watch were getting ridiculous.\n\n\nI\u2019m wondering what Facebook API\u2019s limitations are, because it seems like it\u2019d be really simple to build- I\u2019d like to basically browse without any ads or timeline \u201calgorithm\u201d- most recent friends posts sorted by date. Also add Events near me and filter by date once a week maybe. In theory if filtering out posts with less than x likes, it\u2019d take 5 minutes tops per week to get everything needed and none of the garbage. \n\nHas anyone used Facebook API before? I don\u2019t want to ask there because most people monetize and aren\u2019t going to be keen on someone trying to get around their ads.", "author_fullname": "t2_vgxtzjvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Facebook API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ft5sq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702296708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used Facebook API before?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been off Facebook for months, because the ads and Watch were getting ridiculous.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering what Facebook API\u2019s limitations are, because it seems like it\u2019d be really simple to build- I\u2019d like to basically browse without any ads or timeline \u201calgorithm\u201d- most recent friends posts sorted by date. Also add Events near me and filter by date once a week maybe. In theory if filtering out posts with less than x likes, it\u2019d take 5 minutes tops per week to get everything needed and none of the garbage. &lt;/p&gt;\n\n&lt;p&gt;Has anyone used Facebook API before? I don\u2019t want to ask there because most people monetize and aren\u2019t going to be keen on someone trying to get around their ads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ft5sq", "is_robot_indexable": true, "report_reasons": null, "author": "BestTomatillo6197", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ft5sq/facebook_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ft5sq/facebook_api/", "subreddit_subscribers": 145293, "created_utc": 1702296708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hullo Data Engineers,\n\nLooking for some guidance. I have a company approach me as an independant contractor, and want me to set up a full on DE infrastructure. This would be my first official contracting job and my plan is to implement something simple according to their needs, along these are the parts that I want to implement. In AWS (new instance):\n\n* PostgreSQL db to start with as a \"Warehouse\".\n* Metabase for visualization on top of the above\n* PG Admin for administration\n* S3 as a Storage/data lake solution\n* MWAA low cost to start with, they have several systems they need data from\n* CI/CD (For the pipelines mwaa to s3 with testing, etc)\n* IaC setup from the start. \n* IAM policies and whatnot\n\nAll in all very exciting, however, I am at a loss as to how much to charge for a full project like this. Any advise? I don't want to over shoot and/or undershoot as much as I can\n\nMany thanks in advance\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_5wyo5ojc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost advise for a project implementation as a consultant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18fwab6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702306351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hullo Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;Looking for some guidance. I have a company approach me as an independant contractor, and want me to set up a full on DE infrastructure. This would be my first official contracting job and my plan is to implement something simple according to their needs, along these are the parts that I want to implement. In AWS (new instance):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;PostgreSQL db to start with as a &amp;quot;Warehouse&amp;quot;.&lt;/li&gt;\n&lt;li&gt;Metabase for visualization on top of the above&lt;/li&gt;\n&lt;li&gt;PG Admin for administration&lt;/li&gt;\n&lt;li&gt;S3 as a Storage/data lake solution&lt;/li&gt;\n&lt;li&gt;MWAA low cost to start with, they have several systems they need data from&lt;/li&gt;\n&lt;li&gt;CI/CD (For the pipelines mwaa to s3 with testing, etc)&lt;/li&gt;\n&lt;li&gt;IaC setup from the start. &lt;/li&gt;\n&lt;li&gt;IAM policies and whatnot&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All in all very exciting, however, I am at a loss as to how much to charge for a full project like this. Any advise? I don&amp;#39;t want to over shoot and/or undershoot as much as I can&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fwab6", "is_robot_indexable": true, "report_reasons": null, "author": "alfredosuac", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18fwab6/cost_advise_for_a_project_implementation_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fwab6/cost_advise_for_a_project_implementation_as_a/", "subreddit_subscribers": 145293, "created_utc": 1702306351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi \n\nwhat is the difference between running the query in bigquery or running it using one of the editor such as datagrip or dbvisualizer? \n\nis it cheaper? faster?\n\nwhy people don\u2019t just use bigquery?", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery Computing Cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fpiih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702281816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi &lt;/p&gt;\n\n&lt;p&gt;what is the difference between running the query in bigquery or running it using one of the editor such as datagrip or dbvisualizer? &lt;/p&gt;\n\n&lt;p&gt;is it cheaper? faster?&lt;/p&gt;\n\n&lt;p&gt;why people don\u2019t just use bigquery?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18fpiih", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fpiih/bigquery_computing_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fpiih/bigquery_computing_cost/", "subreddit_subscribers": 145293, "created_utc": 1702281816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been using Meltano to run an ELT job but I'm not sure I'm following the best practices. I was thinking:\n\n* Run a this job (dynamically) on schedule with the command **meltano el tap-mongodb target-&lt;snowflake&gt; --state-id tap\\_mongodb\\_&lt;dev&gt;**\n* I originally wanted to use GCP cloud run but it doesn't work because of the 60m timeout. This ELT process can take 5-8 hours on a single CPU, I want it to run daily. A couple more questions on this 2nd bullet-point:\n   * Are meltano **partitions** able to parallelize across CPUs / machines? If so then this job doesn't need to take anywhere near 5-8 hours (more like 5-10 minutes on 100 machines)?\n   * Should I be using Airflow to do this? Simple cronjob on a VM? GCP cloud composer / dataflow? SO MANY TOOLS!! I just want this to run on the cloud while being able to see the logs in gcp cloud logging, integrating CI/CD, and mounting a volume from GCP secret manager.\n\n&amp;#x200B;\n\nI am hesitant to spend time diving into Airflow, but curious to hear your thoughts. Here's the below script I've been using to run in production **host.py**:\n\n&amp;#x200B;\n\n    from flask import Flask, make_response, request, redirect, Blueprint, render_template\n    import os\n    import subprocess\n    from configparser import ConfigParser\n    from datetime import datetime\n    from waitress import serve\n    from apscheduler.schedulers.background import BackgroundScheduler\n    import logging\n    import json\n    \n    config = ConfigParser()\n    config.read('config.ini')\n    \n    app = Flask(__name__)\n    app.url_map.strict_slashes = False\n    \n    MELTANO_TARGET = config['TAP_MONGODB']['MELTANO_TARGET']\n    \n    assert isinstance(MELTANO_TARGET, str), 'could not determine target'\n    \n    \n    ###### routes ######\n    \n    @app.route('/')\n    def index():\n        return 'Server is running.'\n    \n    @app.route('/tap_mongodb_dev', methods=['GET'])\n    def tap_mongodb_dev():\n        with app.app_context():\n            project_dir = 'taps/tap-mongodb'\n            run_command = f'meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}'\n            subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n            return make_response(f'Last ran project tap-mongodb-dev target {MELTANO_TARGET} at {cur_timestamp()}.', 200)\n    \n    @app.route('/tap_mongodb_production', methods=['GET'])\n    def tap_mongodb_production():\n        with app.app_context():\n            project_dir = 'taps/tap-mongodb'\n            run_command = f'meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}'\n            subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n            return make_response(f'Last ran project tap-mongodb-production target {MELTANO_TARGET} at {cur_timestamp()}.', 200)\n    \n    \n    if __name__ == \"__main__\":\n        logging.info(f'\\n*** Running environment {ENVIRONMENT}. ***\\n')\n    \n        scheduler = BackgroundScheduler(job_defaults={'max_instances': 2})\n    \n        ###### tap-mongodb ######\n    \n        tap_mongodb_cron = json.loads(config['TAP_MONGODB'][f'{ENVIRONMENT}_CRON_PARAMS'])\n        scheduler.add_job(tap_mongodb_functions[ENVIRONMENT], trigger='cron', **tap_mongodb_cron, jitter=120)\n    \n        ###### host ######\n    \n        HOST = '0.0.0.0'\n        PORT = 5000\n        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n        logging.info(f'Server is listening on port {PORT}')\n        logging.info(f'Hosting environment {ENVIRONMENT}')\n    \n        scheduler.start()\n    \n        serve(app, host=HOST, port=PORT, threads=2)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8xq51rif", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you run a long ELT job in the cloud on schedule?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fhseu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702254625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Meltano to run an ELT job but I&amp;#39;m not sure I&amp;#39;m following the best practices. I was thinking:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run a this job (dynamically) on schedule with the command &lt;strong&gt;meltano el tap-mongodb target-&amp;lt;snowflake&amp;gt; --state-id tap_mongodb_&amp;lt;dev&amp;gt;&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I originally wanted to use GCP cloud run but it doesn&amp;#39;t work because of the 60m timeout. This ELT process can take 5-8 hours on a single CPU, I want it to run daily. A couple more questions on this 2nd bullet-point:\n\n&lt;ul&gt;\n&lt;li&gt;Are meltano &lt;strong&gt;partitions&lt;/strong&gt; able to parallelize across CPUs / machines? If so then this job doesn&amp;#39;t need to take anywhere near 5-8 hours (more like 5-10 minutes on 100 machines)?&lt;/li&gt;\n&lt;li&gt;Should I be using Airflow to do this? Simple cronjob on a VM? GCP cloud composer / dataflow? SO MANY TOOLS!! I just want this to run on the cloud while being able to see the logs in gcp cloud logging, integrating CI/CD, and mounting a volume from GCP secret manager.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am hesitant to spend time diving into Airflow, but curious to hear your thoughts. Here&amp;#39;s the below script I&amp;#39;ve been using to run in production &lt;strong&gt;host.py&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from flask import Flask, make_response, request, redirect, Blueprint, render_template\nimport os\nimport subprocess\nfrom configparser import ConfigParser\nfrom datetime import datetime\nfrom waitress import serve\nfrom apscheduler.schedulers.background import BackgroundScheduler\nimport logging\nimport json\n\nconfig = ConfigParser()\nconfig.read(&amp;#39;config.ini&amp;#39;)\n\napp = Flask(__name__)\napp.url_map.strict_slashes = False\n\nMELTANO_TARGET = config[&amp;#39;TAP_MONGODB&amp;#39;][&amp;#39;MELTANO_TARGET&amp;#39;]\n\nassert isinstance(MELTANO_TARGET, str), &amp;#39;could not determine target&amp;#39;\n\n\n###### routes ######\n\n@app.route(&amp;#39;/&amp;#39;)\ndef index():\n    return &amp;#39;Server is running.&amp;#39;\n\n@app.route(&amp;#39;/tap_mongodb_dev&amp;#39;, methods=[&amp;#39;GET&amp;#39;])\ndef tap_mongodb_dev():\n    with app.app_context():\n        project_dir = &amp;#39;taps/tap-mongodb&amp;#39;\n        run_command = f&amp;#39;meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}&amp;#39;\n        subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n        return make_response(f&amp;#39;Last ran project tap-mongodb-dev target {MELTANO_TARGET} at {cur_timestamp()}.&amp;#39;, 200)\n\n@app.route(&amp;#39;/tap_mongodb_production&amp;#39;, methods=[&amp;#39;GET&amp;#39;])\ndef tap_mongodb_production():\n    with app.app_context():\n        project_dir = &amp;#39;taps/tap-mongodb&amp;#39;\n        run_command = f&amp;#39;meltano --environment={ENVIRONMENT} el tap-mongodb target-{MELTANO_TARGET} --state-id MELTANO_{ENVIRONMENT}_{MELTANO_TARGET}&amp;#39;\n        subprocess.Popen(run_command, shell=True, cwd=os.path.join(app.root_path, project_dir))\n        return make_response(f&amp;#39;Last ran project tap-mongodb-production target {MELTANO_TARGET} at {cur_timestamp()}.&amp;#39;, 200)\n\n\nif __name__ == &amp;quot;__main__&amp;quot;:\n    logging.info(f&amp;#39;\\n*** Running environment {ENVIRONMENT}. ***\\n&amp;#39;)\n\n    scheduler = BackgroundScheduler(job_defaults={&amp;#39;max_instances&amp;#39;: 2})\n\n    ###### tap-mongodb ######\n\n    tap_mongodb_cron = json.loads(config[&amp;#39;TAP_MONGODB&amp;#39;][f&amp;#39;{ENVIRONMENT}_CRON_PARAMS&amp;#39;])\n    scheduler.add_job(tap_mongodb_functions[ENVIRONMENT], trigger=&amp;#39;cron&amp;#39;, **tap_mongodb_cron, jitter=120)\n\n    ###### host ######\n\n    HOST = &amp;#39;0.0.0.0&amp;#39;\n    PORT = 5000\n    logging.basicConfig(level=logging.INFO, format=&amp;#39;%(asctime)s - %(levelname)s - %(message)s&amp;#39;)\n    logging.info(f&amp;#39;Server is listening on port {PORT}&amp;#39;)\n    logging.info(f&amp;#39;Hosting environment {ENVIRONMENT}&amp;#39;)\n\n    scheduler.start()\n\n    serve(app, host=HOST, port=PORT, threads=2)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fhseu", "is_robot_indexable": true, "report_reasons": null, "author": "Training_Butterfly70", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fhseu/how_do_you_run_a_long_elt_job_in_the_cloud_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fhseu/how_do_you_run_a_long_elt_job_in_the_cloud_on/", "subreddit_subscribers": 145293, "created_utc": 1702254625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is using Azure Databricks and I'm an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a \"/FileStore/file\\_path/file\\_name.xlsx\" file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn't really helped me up to this point.\n\n`w = WorkspaceClient(host=server_hostname,token = api_key)`\n\nI can successfully run `w.dbfs.download('/FileStore/file_path/file_name.xlsx')`, but can't figure out how to save the file down to local storage as an Excel file.\n\n`w.dbutils.fs.cp('dbfs:/FileStore/file_path/file_name.xlsx', 'C:/Users/user_name/file_path/file_name.xlsx')` results in \"java.net.URISyntaxException: Relative path in absolute URI\" for the C drive file path. This looks like an absolute path to me. Am I missing something?\n\n`w.files.get_status('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"Get-status API is not enabled.\" How do I enable it? I don't see a toggle for it in Admin Settings.\n\n`w.files.download('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"The ':' character is not supported\", but when I remove it the file path is no longer valid.\n\nI feel like I'm close on one of these but just can't seem to get it across the finish line.", "author_fullname": "t2_yk6x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone moved files between Azure Databricks DBFS and local storage using the Databricks API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7vcb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702227447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is using Azure Databricks and I&amp;#39;m an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a &amp;quot;/FileStore/file_path/file_name.xlsx&amp;quot; file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn&amp;#39;t really helped me up to this point.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w = WorkspaceClient(host=server_hostname,token = api_key)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I can successfully run &lt;code&gt;w.dbfs.download(&amp;#39;/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt;, but can&amp;#39;t figure out how to save the file down to local storage as an Excel file.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.dbutils.fs.cp(&amp;#39;dbfs:/FileStore/file_path/file_name.xlsx&amp;#39;, &amp;#39;C:/Users/user_name/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;java.net.URISyntaxException: Relative path in absolute URI&amp;quot; for the C drive file path. This looks like an absolute path to me. Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.get_status(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;Get-status API is not enabled.&amp;quot; How do I enable it? I don&amp;#39;t see a toggle for it in Admin Settings.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.download(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;The &amp;#39;:&amp;#39; character is not supported&amp;quot;, but when I remove it the file path is no longer valid.&lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m close on one of these but just can&amp;#39;t seem to get it across the finish line.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18f7vcb", "is_robot_indexable": true, "report_reasons": null, "author": "CurlyW15", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "subreddit_subscribers": 145293, "created_utc": 1702227447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The key here is the ETL aspect. I need sample databases in postgres, oracle (or any sql dialect really) that have both data and ETL .sql scripts that I don't have to reinvent. Does anyone know one that exists that have both?", "author_fullname": "t2_rk16bnik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for sample postgres and oracle databases that have ETL scripts preloaded into them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ffap8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702247443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The key here is the ETL aspect. I need sample databases in postgres, oracle (or any sql dialect really) that have both data and ETL .sql scripts that I don&amp;#39;t have to reinvent. Does anyone know one that exists that have both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ffap8", "is_robot_indexable": true, "report_reasons": null, "author": "notstoppinguntil30", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ffap8/looking_for_sample_postgres_and_oracle_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ffap8/looking_for_sample_postgres_and_oracle_databases/", "subreddit_subscribers": 145293, "created_utc": 1702247443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, \n\nI am a employee in an MNC having 2.5 years of experience and currently working on a support based project with the tech stack as webmethods which is a integration tool based out of Java and it is being to connect different systems / businesses. I'm bored and frustrated at the same time because the work is same and almost repetitive with no signs of growth and learning. That's when I decided to switch domain but\n\nI was very confused while choosing between Data Engineering and Java. But I choose Data Engineering and started preparing for it. I currently have experience with python easy to medium, SQL mid to advance level and achieved two cloud certifications AZ-900 &amp; AZ-204 currently preparing for DP-203. I have basic knowledge and hands-on azure df, Synapse and storage. \n\nI tried looking for jobs matching my skills but whenever I started checking the job description I always lack in some or the other skills such as Big data and Spark. So if someone can guide me to tackel with this and land a job in data engineer domain it would be a great help. \n\nThanks in Advance :)", "author_fullname": "t2_ib9mu62z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Carrer switch to Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18frtzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702291672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, &lt;/p&gt;\n\n&lt;p&gt;I am a employee in an MNC having 2.5 years of experience and currently working on a support based project with the tech stack as webmethods which is a integration tool based out of Java and it is being to connect different systems / businesses. I&amp;#39;m bored and frustrated at the same time because the work is same and almost repetitive with no signs of growth and learning. That&amp;#39;s when I decided to switch domain but&lt;/p&gt;\n\n&lt;p&gt;I was very confused while choosing between Data Engineering and Java. But I choose Data Engineering and started preparing for it. I currently have experience with python easy to medium, SQL mid to advance level and achieved two cloud certifications AZ-900 &amp;amp; AZ-204 currently preparing for DP-203. I have basic knowledge and hands-on azure df, Synapse and storage. &lt;/p&gt;\n\n&lt;p&gt;I tried looking for jobs matching my skills but whenever I started checking the job description I always lack in some or the other skills such as Big data and Spark. So if someone can guide me to tackel with this and land a job in data engineer domain it would be a great help. &lt;/p&gt;\n\n&lt;p&gt;Thanks in Advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18frtzg", "is_robot_indexable": true, "report_reasons": null, "author": "iamDjsahu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18frtzg/carrer_switch_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18frtzg/carrer_switch_to_data_engineering/", "subreddit_subscribers": 145293, "created_utc": 1702291672.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}