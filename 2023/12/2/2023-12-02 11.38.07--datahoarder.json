{"kind": "Listing", "data": {"after": "t3_188mmsz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_nkjjxlogw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OpenZFS 2.2.2 &amp; OpenZFS 2.1.14 Released To Fix Data Corruption Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 32, "top_awarded_type": null, "hide_score": false, "name": "t3_188iisi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/C3J-PPf9ruFnUoztYnowHiM3OVWlomFR89cSbjBi5tU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701454518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phoronix.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phoronix.com/news/OpenZFS-2.2.2-Released", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YeZU0zqbcxzAbRmKiKTOuqYCfwgo3Q9NQCN3-9UmBNk.jpg?auto=webp&amp;s=290e66bc1d62aca154cfc1434c4cbf45ed665922", "width": 480, "height": 112}, "resolutions": [{"url": "https://external-preview.redd.it/YeZU0zqbcxzAbRmKiKTOuqYCfwgo3Q9NQCN3-9UmBNk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff05640a43fd26d907342eefe5902eceb68b9a77", "width": 108, "height": 25}, {"url": "https://external-preview.redd.it/YeZU0zqbcxzAbRmKiKTOuqYCfwgo3Q9NQCN3-9UmBNk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c96224bf738fa5ee7d86fc7533e7db4ca0a14efc", "width": 216, "height": 50}, {"url": "https://external-preview.redd.it/YeZU0zqbcxzAbRmKiKTOuqYCfwgo3Q9NQCN3-9UmBNk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d2f2e2fa08bc3b3ce49dee56e4f0a850621881d", "width": 320, "height": 74}], "variants": {}, "id": "47OJV16hWJg-Xd_JV5FZb5odNpLb-dDOzhUibc1PqG0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188iisi", "is_robot_indexable": true, "report_reasons": null, "author": "TorrentplsZOMG", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188iisi/openzfs_222_openzfs_2114_released_to_fix_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phoronix.com/news/OpenZFS-2.2.2-Released", "subreddit_subscribers": 715700, "created_utc": 1701454518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hard drive prices could go up significantly after the rumored 'collapse' of key player \u2014 paving the way for growing SSD domination in the storage market | TechRadar", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_188w1p9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_hrK4Hsue8gXRPD1H7qBoHaftP0dIqbOZfx_4iTSI2E.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701492969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techradar.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techradar.com/pro/hard-drive-prices-could-go-up-significantly-after-the-rumored-collapse-of-key-player-paving-the-way-for-growing-ssd-domination-in-the-storage-market", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UmFMhrcUeRqH2fsZIAZ0dFU-Nq4JIYv2wGcJT7PFSzA.jpg?auto=webp&amp;s=a2f171ce9782e69968f9565e1228a1f0636da5d6", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/UmFMhrcUeRqH2fsZIAZ0dFU-Nq4JIYv2wGcJT7PFSzA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc9f2dbcc2f25a6add685f11f716965a21948f26", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UmFMhrcUeRqH2fsZIAZ0dFU-Nq4JIYv2wGcJT7PFSzA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a3dcc532ea0e5d94141fd3ce87c847a4ff2d672", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/UmFMhrcUeRqH2fsZIAZ0dFU-Nq4JIYv2wGcJT7PFSzA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=615aeb8a2c91c9aa566240ae83c392872797356b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/UmFMhrcUeRqH2fsZIAZ0dFU-Nq4JIYv2wGcJT7PFSzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9f3b8886d7c857bfff963d9fc9769a849af9998", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/UmFMhrcUeRqH2fsZIAZ0dFU-Nq4JIYv2wGcJT7PFSzA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e98858479a99358c9767aee7dae10e7ea75ac532", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/UmFMhrcUeRqH2fsZIAZ0dFU-Nq4JIYv2wGcJT7PFSzA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=144f54e8efe51a66052d23a4715f49d24d71fa16", "width": 1080, "height": 607}], "variants": {}, "id": "vyIzqgZ6Ve_TcdDDgQo7Yi6HuPbm7Tnf0T7F_RtOE34"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188w1p9", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/188w1p9/hard_drive_prices_could_go_up_significantly_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techradar.com/pro/hard-drive-prices-could-go-up-significantly-after-the-rumored-collapse-of-key-player-paving-the-way-for-growing-ssd-domination-in-the-storage-market", "subreddit_subscribers": 715700, "created_utc": 1701492969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hey! \n\nI'm part of a non-profit that refurbished older PCs and distributes them to people who needs them. since what we get is donations, everything is driveless. \n\nI'm looking for recommendations for cheap, low capacity (120-240gb) 2.5 SSDs with high reliability, low failure rate, and as low a cost as possible. bulk purchasing options are definitely a plus. \n\nthanks!", "author_fullname": "t2_942r1vjh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap, low capacity, reliable SSDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188i4sa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701453543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey! &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m part of a non-profit that refurbished older PCs and distributes them to people who needs them. since what we get is donations, everything is driveless. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for recommendations for cheap, low capacity (120-240gb) 2.5 SSDs with high reliability, low failure rate, and as low a cost as possible. bulk purchasing options are definitely a plus. &lt;/p&gt;\n\n&lt;p&gt;thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188i4sa", "is_robot_indexable": true, "report_reasons": null, "author": "GorbigliontheStrong", "discussion_type": null, "num_comments": 69, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188i4sa/cheap_low_capacity_reliable_ssds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188i4sa/cheap_low_capacity_reliable_ssds/", "subreddit_subscribers": 715700, "created_utc": 1701453543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! Not sure if this is the best place to ask\u2026\n\nI\u2019ve been tasked with combing through a large database of photos and organizing them by keywording in a way that makes it easy to search for images containing certain things (example: coworker wants to find image of dog in park, they search in explorer \u201ctags: (dog and park)\u201d.\n\nConsidering the functionality of iPhotos to recognize animals and human faces,\nI wonder if there\u2019s a program that can be used to search this data base using A.I? So that without keywords, the program could search for contents in an image and show results similar to what was searched (think Pinterest)\n\nThere\u2019s also a potential solution which is a program that has auto keywording\u2026 does this exist? \n\nAny ideas would be helpful thank you!", "author_fullname": "t2_e5ixk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo Detection app / software?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188dnl8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701442067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! Not sure if this is the best place to ask\u2026&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been tasked with combing through a large database of photos and organizing them by keywording in a way that makes it easy to search for images containing certain things (example: coworker wants to find image of dog in park, they search in explorer \u201ctags: (dog and park)\u201d.&lt;/p&gt;\n\n&lt;p&gt;Considering the functionality of iPhotos to recognize animals and human faces,\nI wonder if there\u2019s a program that can be used to search this data base using A.I? So that without keywords, the program could search for contents in an image and show results similar to what was searched (think Pinterest)&lt;/p&gt;\n\n&lt;p&gt;There\u2019s also a potential solution which is a program that has auto keywording\u2026 does this exist? &lt;/p&gt;\n\n&lt;p&gt;Any ideas would be helpful thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188dnl8", "is_robot_indexable": true, "report_reasons": null, "author": "_Yoke", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188dnl8/photo_detection_app_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188dnl8/photo_detection_app_software/", "subreddit_subscribers": 715700, "created_utc": 1701442067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have T7 shield - 2TB. When I used it for the first time few months ago, it was fast. And I didn't use it for almost like 7 to 8 months and then I have been using to for a month now just to transfer some video files from my Samsung phone to T7 and it is super slow. It takes around 5 to 10 mins to transfer 20GB of data. What could be the reason behind it?", "author_fullname": "t2_c4xzzhla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "T7 Shield Samsung help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188gb1p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701448876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have T7 shield - 2TB. When I used it for the first time few months ago, it was fast. And I didn&amp;#39;t use it for almost like 7 to 8 months and then I have been using to for a month now just to transfer some video files from my Samsung phone to T7 and it is super slow. It takes around 5 to 10 mins to transfer 20GB of data. What could be the reason behind it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "188gb1p", "is_robot_indexable": true, "report_reasons": null, "author": "davidbrown8796", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188gb1p/t7_shield_samsung_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188gb1p/t7_shield_samsung_help/", "subreddit_subscribers": 715700, "created_utc": 1701448876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry, just venting a bit and also seeing if anybody can offer any insight.\n\nI made a post, asking the subreddit if it were normal that my Seagate Exos drives were sold/packaged/shipped they way there were [here](https://www.reddit.com/r/DataHoarder/comments/1878c2k/how_are_brand_new_exos_drives_soldpackagedshipped/?utm_source=share&amp;utm_medium=web2x&amp;context=3).\n\nA few people in the comments mentioned that I should make sure that my warranty on them is actually five years and not four, like many had experienced. Sure enough when I checked my serial numbers on the warranty checker page on Seagate, that was the case for me. The same people said that I just needed to contact Seagate with proof of purchase and that they would fix it for me.\n\nI spent the last 45 mins chatting and sending pictures of my drives and proof of purchase to the rep from Seagate, only to be told that these were \"OEM drives\" that Newegg shouldn't be selling to me and that they cannot extend, nor honor the warranty. They said that I would have to go to Newegg and figure it out with them. \n\nIs my only recourse to return these hard drives to newegg? Others have mentioned that when you go to newegg to talk about warranty, they'll refer you to Seagate.\n\nnewegg canada is based on the west coast, so i have to wait another hour before i can call them and as of now, I just escalated this with Seagate's product support escalation team.\n\nAnybody have similar experiences?", "author_fullname": "t2_j3gnkmjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warranty issues with Seagate Exos Hard Drives from Newegg", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188ejvd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701444344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry, just venting a bit and also seeing if anybody can offer any insight.&lt;/p&gt;\n\n&lt;p&gt;I made a post, asking the subreddit if it were normal that my Seagate Exos drives were sold/packaged/shipped they way there were &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/1878c2k/how_are_brand_new_exos_drives_soldpackagedshipped/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;A few people in the comments mentioned that I should make sure that my warranty on them is actually five years and not four, like many had experienced. Sure enough when I checked my serial numbers on the warranty checker page on Seagate, that was the case for me. The same people said that I just needed to contact Seagate with proof of purchase and that they would fix it for me.&lt;/p&gt;\n\n&lt;p&gt;I spent the last 45 mins chatting and sending pictures of my drives and proof of purchase to the rep from Seagate, only to be told that these were &amp;quot;OEM drives&amp;quot; that Newegg shouldn&amp;#39;t be selling to me and that they cannot extend, nor honor the warranty. They said that I would have to go to Newegg and figure it out with them. &lt;/p&gt;\n\n&lt;p&gt;Is my only recourse to return these hard drives to newegg? Others have mentioned that when you go to newegg to talk about warranty, they&amp;#39;ll refer you to Seagate.&lt;/p&gt;\n\n&lt;p&gt;newegg canada is based on the west coast, so i have to wait another hour before i can call them and as of now, I just escalated this with Seagate&amp;#39;s product support escalation team.&lt;/p&gt;\n\n&lt;p&gt;Anybody have similar experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188ejvd", "is_robot_indexable": true, "report_reasons": null, "author": "fatboycraig", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188ejvd/warranty_issues_with_seagate_exos_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188ejvd/warranty_issues_with_seagate_exos_hard_drives/", "subreddit_subscribers": 715700, "created_utc": 1701444344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_nek10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spotted in the wild. 10Pb of drives on a pallet...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": true, "name": "t3_1891b15", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WYgpUcLE-kXASiYTJZv0N78diWYIZKJVDn7woel7Qlo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701514472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/h2txqr2t3v3c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?auto=webp&amp;s=e4bf2e7a20d85d109863376c5bd31a088a3f75e8", "width": 1163, "height": 709}, "resolutions": [{"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5f7862b205c8e9042a311325725f5db955859fc", "width": 108, "height": 65}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=07b7385112bc8d58604896bf047ecb8dd2544cad", "width": 216, "height": 131}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=355f0933643bc712d31a493e7cb288eb37a95134", "width": 320, "height": 195}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56ab70a07f307b5de0777145de061d6d63cf141b", "width": 640, "height": 390}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1f1711a31c642a61b07a2a5603dd1dac8348419", "width": 960, "height": 585}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8d60a130edb7c8baa9495f4cedacc30d8e1d4b75", "width": 1080, "height": 658}], "variants": {}, "id": "drLeK54cjXTqXHZs18pduA0gjgwHsj5NFC_PNPGiabQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1891b15", "is_robot_indexable": true, "report_reasons": null, "author": "Switchblade88", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1891b15/spotted_in_the_wild_10pb_of_drives_on_a_pallet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/h2txqr2t3v3c1.png", "subreddit_subscribers": 715700, "created_utc": 1701514472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Whenever I'm shutting down anything like:\n\n* A Linux server\n* A Windows desktop (my own, family members, clients)\n* A cloud storage account (OneDrive/GDrive/Dropbox etc)\n* Individual websites\n* Other large things like big database dumps taken immediately before some major change, that I want to retain forever\n\nI generally take a final archive of it (even if it was being backed up by more \"backup\" focused software during its running lifetime).  \n\nAnd I really want these to be in a mountable format, so that I can browse them + view files etc without having to extract copies.\n\nSo far I've mostly used squashfs for this, which is mountable on Linux, but not easily Windows as far as I can tell.\n\n---\n\n##### My overly optimistic + unrealistic wish list:\n\n* **Mountable:** both on Windows + Linux\n  * ...if it can be mountable even over remote object storage, that would be awesome (e.g. like restic + kopia are)\n* **Compressed**\n* **Deduped** as much as possible\nlong-term consistent/stable format is what I'm after\n* Encryption: maybe not a hard requirement, but always handy to have the option, especially if storing on cloud/object storage\n* **Splittable:** e.g. if I want to break up into 100mb or 1gb chunks\n* **Editable:** If the format also had the option to be editable while mounted that would awesome too... as it means I could just immediately create these archives, then worry about culling all the crap in them that wasn't needed later on... having to do this pre-emptively before creating the archive chews up a lot of time cause I'm kinda neurotic about this.\n* I guess backup software like restic + kopia are what will give most of these features\n  * But maybe not the right tools for this \"once-off final state\" long term archiving \n  * They're also kinda complex to setup in the future for restore/mounting, which may be further complicated by their changing formats across versions, given they're not really designed for these long-term archiving use case.\n\nI'm open to hearing about all sorts of options here.  I've considered things like virtual machine image formats too (for the \"editable\" feature above), even though I'm not actually ever actually backing-up/archiving whole-drive images.\n\nAnd I'm guessing that nothing is going to give me ***everything*** above.  But keen to hear thougts from anyone on stuff you've used for these kinds of uses cases.  Even if it ended up being a mistake in the end, always some learning in there!", "author_fullname": "t2_43cuz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What mountable archive/image formats do you use for large backups/archive the \"final state\" of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188wvlb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701495851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whenever I&amp;#39;m shutting down anything like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A Linux server&lt;/li&gt;\n&lt;li&gt;A Windows desktop (my own, family members, clients)&lt;/li&gt;\n&lt;li&gt;A cloud storage account (OneDrive/GDrive/Dropbox etc)&lt;/li&gt;\n&lt;li&gt;Individual websites&lt;/li&gt;\n&lt;li&gt;Other large things like big database dumps taken immediately before some major change, that I want to retain forever&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I generally take a final archive of it (even if it was being backed up by more &amp;quot;backup&amp;quot; focused software during its running lifetime).  &lt;/p&gt;\n\n&lt;p&gt;And I really want these to be in a mountable format, so that I can browse them + view files etc without having to extract copies.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve mostly used squashfs for this, which is mountable on Linux, but not easily Windows as far as I can tell.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h5&gt;My overly optimistic + unrealistic wish list:&lt;/h5&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Mountable:&lt;/strong&gt; both on Windows + Linux\n\n&lt;ul&gt;\n&lt;li&gt;...if it can be mountable even over remote object storage, that would be awesome (e.g. like restic + kopia are)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Compressed&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Deduped&lt;/strong&gt; as much as possible\nlong-term consistent/stable format is what I&amp;#39;m after&lt;/li&gt;\n&lt;li&gt;Encryption: maybe not a hard requirement, but always handy to have the option, especially if storing on cloud/object storage&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Splittable:&lt;/strong&gt; e.g. if I want to break up into 100mb or 1gb chunks&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Editable:&lt;/strong&gt; If the format also had the option to be editable while mounted that would awesome too... as it means I could just immediately create these archives, then worry about culling all the crap in them that wasn&amp;#39;t needed later on... having to do this pre-emptively before creating the archive chews up a lot of time cause I&amp;#39;m kinda neurotic about this.&lt;/li&gt;\n&lt;li&gt;I guess backup software like restic + kopia are what will give most of these features\n\n&lt;ul&gt;\n&lt;li&gt;But maybe not the right tools for this &amp;quot;once-off final state&amp;quot; long term archiving &lt;/li&gt;\n&lt;li&gt;They&amp;#39;re also kinda complex to setup in the future for restore/mounting, which may be further complicated by their changing formats across versions, given they&amp;#39;re not really designed for these long-term archiving use case.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m open to hearing about all sorts of options here.  I&amp;#39;ve considered things like virtual machine image formats too (for the &amp;quot;editable&amp;quot; feature above), even though I&amp;#39;m not actually ever actually backing-up/archiving whole-drive images.&lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;m guessing that nothing is going to give me &lt;strong&gt;&lt;em&gt;everything&lt;/em&gt;&lt;/strong&gt; above.  But keen to hear thougts from anyone on stuff you&amp;#39;ve used for these kinds of uses cases.  Even if it ended up being a mistake in the end, always some learning in there!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188wvlb", "is_robot_indexable": true, "report_reasons": null, "author": "r0ck0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188wvlb/what_mountable_archiveimage_formats_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188wvlb/what_mountable_archiveimage_formats_do_you_use/", "subreddit_subscribers": 715700, "created_utc": 1701495851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been on planning on doing some major external drive consolidation and organization and have pretty much settled on going the route of a simple external RAID enclosure housing a couple large drives in a RAID 1 configuration. For some reason it seems impossible to find a DAS enclosure with the following:\n\n* Adequate cooling\n* Not annoying fan noise\n* USB 3.2 Gen 2 (10Gbps versus the slower 5Gbps)\n* Hardware RAID\n\nI was really liking what I was reading about the QNAP TR-002 or TR-004, but it appears they're limited to USB 3.1 Gen 1 speeds, annoyingly.\n\nI was also liking what I was seeing about the Sabrent DS-2BCR 2-Bay enclosure, but apparently the fan is very annoying. And the Sabrent 4 Bay enclosure doesn't do hardware RAID. But maybe Mac OS' software RAID would be fine?\n\nWhy does it feel impossible to find a good, simple RAID enclosure? Is there a product I'm missing that would tick all of these boxes?", "author_fullname": "t2_7zaca5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does a simple, good RAID enclosure exist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188kgip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701459576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been on planning on doing some major external drive consolidation and organization and have pretty much settled on going the route of a simple external RAID enclosure housing a couple large drives in a RAID 1 configuration. For some reason it seems impossible to find a DAS enclosure with the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Adequate cooling&lt;/li&gt;\n&lt;li&gt;Not annoying fan noise&lt;/li&gt;\n&lt;li&gt;USB 3.2 Gen 2 (10Gbps versus the slower 5Gbps)&lt;/li&gt;\n&lt;li&gt;Hardware RAID&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I was really liking what I was reading about the QNAP TR-002 or TR-004, but it appears they&amp;#39;re limited to USB 3.1 Gen 1 speeds, annoyingly.&lt;/p&gt;\n\n&lt;p&gt;I was also liking what I was seeing about the Sabrent DS-2BCR 2-Bay enclosure, but apparently the fan is very annoying. And the Sabrent 4 Bay enclosure doesn&amp;#39;t do hardware RAID. But maybe Mac OS&amp;#39; software RAID would be fine?&lt;/p&gt;\n\n&lt;p&gt;Why does it feel impossible to find a good, simple RAID enclosure? Is there a product I&amp;#39;m missing that would tick all of these boxes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188kgip", "is_robot_indexable": true, "report_reasons": null, "author": "Eighty4s", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188kgip/does_a_simple_good_raid_enclosure_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188kgip/does_a_simple_good_raid_enclosure_exist/", "subreddit_subscribers": 715700, "created_utc": 1701459576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a old device running pfsense as my firewall/router and am looking to upgrade (because I can).  It's a Supermicro Atom D525 platform and I'd like to reuse the chassis.  Any recommendations on an affordable platform that I can upgrade to that will be fairly quiet?  \n\nI have shitty DSL with a 30/3 up/down connection  so that will always be my bottleneck.  I would like to work on suricata to keep myself secure but if I can't I'll manage.", "author_fullname": "t2_b5ztvbyu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Firewall Upgrade Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188k3pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701458620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a old device running pfsense as my firewall/router and am looking to upgrade (because I can).  It&amp;#39;s a Supermicro Atom D525 platform and I&amp;#39;d like to reuse the chassis.  Any recommendations on an affordable platform that I can upgrade to that will be fairly quiet?  &lt;/p&gt;\n\n&lt;p&gt;I have shitty DSL with a 30/3 up/down connection  so that will always be my bottleneck.  I would like to work on suricata to keep myself secure but if I can&amp;#39;t I&amp;#39;ll manage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188k3pe", "is_robot_indexable": true, "report_reasons": null, "author": "DiscracedSith", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188k3pe/firewall_upgrade_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188k3pe/firewall_upgrade_advice/", "subreddit_subscribers": 715700, "created_utc": 1701458620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\n\n  Have an old home server running WHS2012 R2, running out of room so I wanted to clone my OS ssd drive (250gig) to a partition on a 4TB hdd, use that as my new OS drive instead of the 250gig ssd then add the rest to the pool.  The problem is, will drivepool allow the remaining 3.7TB to be added to the pool when all other drives on the pool are GPT?", "author_fullname": "t2_egkhg7bt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I pool an MBR drive using stablebit drivepool with a group of GPT drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188zclc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701505947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have an old home server running WHS2012 R2, running out of room so I wanted to clone my OS ssd drive (250gig) to a partition on a 4TB hdd, use that as my new OS drive instead of the 250gig ssd then add the rest to the pool.  The problem is, will drivepool allow the remaining 3.7TB to be added to the pool when all other drives on the pool are GPT?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188zclc", "is_robot_indexable": true, "report_reasons": null, "author": "VladX0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188zclc/can_i_pool_an_mbr_drive_using_stablebit_drivepool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188zclc/can_i_pool_an_mbr_drive_using_stablebit_drivepool/", "subreddit_subscribers": 715700, "created_utc": 1701505947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey,\n\nI\u2019m extremely worried. I was hospitalized this year and because of this I haven\u2019t been able to access my free MEGA account for a while, I d say 4 to 5 months. Yesterday I access it and I see that there\u2019s no more data left. Everything is GONE! There were 14 years full of photos, memories, artistic feautures, videos, so much important stuff which is not backed up somewhere else. I feel stupid and at the same time I feel like half of me has died\u2026. I wrote them if they could be able to recover my datas, even if it\u2019s gonna cost me thousands of dollars, I don\u2019t care at this point\u2026\nWhat do you alll think?\n\nAhhhhhhhhh I\u2019m never been so desperate \ud83d\ude30\ud83d\ude30\ud83d\ude30\ud83d\ude30\ud83d\ude30", "author_fullname": "t2_so70si0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mega Cloud deleted all my data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188xv6s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.52, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701499671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m extremely worried. I was hospitalized this year and because of this I haven\u2019t been able to access my free MEGA account for a while, I d say 4 to 5 months. Yesterday I access it and I see that there\u2019s no more data left. Everything is GONE! There were 14 years full of photos, memories, artistic feautures, videos, so much important stuff which is not backed up somewhere else. I feel stupid and at the same time I feel like half of me has died\u2026. I wrote them if they could be able to recover my datas, even if it\u2019s gonna cost me thousands of dollars, I don\u2019t care at this point\u2026\nWhat do you alll think?&lt;/p&gt;\n\n&lt;p&gt;Ahhhhhhhhh I\u2019m never been so desperate \ud83d\ude30\ud83d\ude30\ud83d\ude30\ud83d\ude30\ud83d\ude30&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188xv6s", "is_robot_indexable": true, "report_reasons": null, "author": "phersper", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188xv6s/mega_cloud_deleted_all_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188xv6s/mega_cloud_deleted_all_my_data/", "subreddit_subscribers": 715700, "created_utc": 1701499671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I\u2019m looking to buy a used Fujitsu Eternus LT20 with an installed LTO4 tape drive. Is it possible to replace the LTO4 drive with a LTO7 drive?\n\nAre there any special things I need to take note of?", "author_fullname": "t2_o7ic76n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacing a LTO 4 with LTO 7 drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188xhti", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701498202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019m looking to buy a used Fujitsu Eternus LT20 with an installed LTO4 tape drive. Is it possible to replace the LTO4 drive with a LTO7 drive?&lt;/p&gt;\n\n&lt;p&gt;Are there any special things I need to take note of?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188xhti", "is_robot_indexable": true, "report_reasons": null, "author": "kahn2k", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188xhti/replacing_a_lto_4_with_lto_7_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188xhti/replacing_a_lto_4_with_lto_7_drive/", "subreddit_subscribers": 715700, "created_utc": 1701498202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI've been checking posts on this sub, but was unable to find anything even close to what I'm trying to implement. All what I could find was about the data ALREADY hashed (either at source, or during the copy process) and being verified at target AFTER such copy.\n\nMy question is: how one would verify the integrity (for the lack of a better word) of the data which has NOT been hashed BEFORE it appeared on a drive? I apologize for a possible confusion, so to clinarify what I mean, imagine that there is a drive (an external HDD or SSD \u2013 not important which type) with some data on it. This data was at some point in time placed there, but no means of its verification were used at the time (say, while copying to that drive). Now, when I have this drive, how can I make sure that the files are doing fine (they're not corrupt, they can be read, copied, etc.)? Is the only way to ensure the integrity of these files (or readability maybe?) is to check the physical characteristics of that drive (with something like HDDScan or the like)? To make sure that sectors are accessible (therefore (theoretically) the data is intact and readable). Of course, running a hash-creating app (and saving the results on that drive) afterwards will ensure to some extent that when I check that drive IN THE FUTURE, I can determine whether the data is still the same. No issue here. But what about that initial attempt to verify the data?\n\nI'd appreciate all your input and advice (should you have some).\n\nThank you.", "author_fullname": "t2_kqwsgjqcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data integrity check for data NOT previously hashed (while copying, for example) \u2013 please advise..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188sd4m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701481034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been checking posts on this sub, but was unable to find anything even close to what I&amp;#39;m trying to implement. All what I could find was about the data ALREADY hashed (either at source, or during the copy process) and being verified at target AFTER such copy.&lt;/p&gt;\n\n&lt;p&gt;My question is: how one would verify the integrity (for the lack of a better word) of the data which has NOT been hashed BEFORE it appeared on a drive? I apologize for a possible confusion, so to clinarify what I mean, imagine that there is a drive (an external HDD or SSD \u2013 not important which type) with some data on it. This data was at some point in time placed there, but no means of its verification were used at the time (say, while copying to that drive). Now, when I have this drive, how can I make sure that the files are doing fine (they&amp;#39;re not corrupt, they can be read, copied, etc.)? Is the only way to ensure the integrity of these files (or readability maybe?) is to check the physical characteristics of that drive (with something like HDDScan or the like)? To make sure that sectors are accessible (therefore (theoretically) the data is intact and readable). Of course, running a hash-creating app (and saving the results on that drive) afterwards will ensure to some extent that when I check that drive IN THE FUTURE, I can determine whether the data is still the same. No issue here. But what about that initial attempt to verify the data?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate all your input and advice (should you have some).&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188sd4m", "is_robot_indexable": true, "report_reasons": null, "author": "Future-Cod-7565", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188sd4m/data_integrity_check_for_data_not_previously/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188sd4m/data_integrity_check_for_data_not_previously/", "subreddit_subscribers": 715700, "created_utc": 1701481034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have a bunch of HDDs with my stuff. Some drives are used as a backup and I keep a couple of them in a drawer, I update the backups on them about twice a year. I'm not an extreme data hoarder with hundreds of TBs worth of storage, I just want my photos, music, game mods, etc. backed up in case a drive dies.\n\nUntil now I've been using FreeFileSync and simply doing a synchronisation between the relevant folders to keep the backups up to date.\n\nMy question is, is this good enough for someone like me? I've been reading around and see that HDDs can degrade after some years, that bit rot is a thing, and using checksums or par is recommended, etc.\n\nAll I want to do is to have all the data on these drives as usable in 10-20 years time as it is today. The photos viewable, music listenable, game mods installable. Is the simple copy-paste operation I'm doing sufficient for that purpose, or should I be worrying about the aforementioned things?\n\nI'm on Windows 10, don't know anything about Linux.", "author_fullname": "t2_jmjvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for someone who uses a basic local backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188s3ho", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701480259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a bunch of HDDs with my stuff. Some drives are used as a backup and I keep a couple of them in a drawer, I update the backups on them about twice a year. I&amp;#39;m not an extreme data hoarder with hundreds of TBs worth of storage, I just want my photos, music, game mods, etc. backed up in case a drive dies.&lt;/p&gt;\n\n&lt;p&gt;Until now I&amp;#39;ve been using FreeFileSync and simply doing a synchronisation between the relevant folders to keep the backups up to date.&lt;/p&gt;\n\n&lt;p&gt;My question is, is this good enough for someone like me? I&amp;#39;ve been reading around and see that HDDs can degrade after some years, that bit rot is a thing, and using checksums or par is recommended, etc.&lt;/p&gt;\n\n&lt;p&gt;All I want to do is to have all the data on these drives as usable in 10-20 years time as it is today. The photos viewable, music listenable, game mods installable. Is the simple copy-paste operation I&amp;#39;m doing sufficient for that purpose, or should I be worrying about the aforementioned things?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on Windows 10, don&amp;#39;t know anything about Linux.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188s3ho", "is_robot_indexable": true, "report_reasons": null, "author": "Filipi_7", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188s3ho/advice_for_someone_who_uses_a_basic_local_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188s3ho/advice_for_someone_who_uses_a_basic_local_backup/", "subreddit_subscribers": 715700, "created_utc": 1701480259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\n\nI have been using an external USB-powered hard drive attached to a Raspberry Pi 4 for about a year.  I use it mostly as a Plex server, but also for Home Assistant and some household NAS stuff.  I have filled up the connected drive, so I'd like to upgrade to using multiple drives at once, for more storage and eventually to be backing up more responsibly.  I have narrowed my choices for a solution down--I think--to these two options:\n\n* [Sabrent](https://www.amazon.com/Sabrent-4-Bay-Docking-Station-DS-U3B4/dp/B07H11KXCL/ref=sr_1_9?crid=U6I3TFAZYKZG&amp;keywords=4+bay+hard+drive+enclosure&amp;qid=1701475340&amp;refinements=p_76%3A1249137011%2Cp_72%3A1248879011&amp;rnid=1248877011&amp;rps=1&amp;s=electronics&amp;sprefix=4+bay+hard+drive+enclosure%2Celectronics%2C147&amp;sr=1-9&amp;ufe=app_do%3Aamzn1.fos.18630bbb-fcbb-42f8-9767-857e17e03685)\n* [Mediasonic](https://www.amazon.com/Mediasonic-PROBOX-SATA-Drive-Enclosure/dp/B09WPPJHSS?th=1)\n\nI would love to hear any other suggestions that I might not have thought of, or to be corrected if I've made some false assumptions about my needs. My only thoughts during my search were that it needs 4+ bays, and that it be powered, since I understand that powering more than one hard drive off of the Pi itself causes issues.  Thanks for any help you can provide.", "author_fullname": "t2_55d4n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please help me choose one of these 4-drive bays for use with a Raspberry Pi Plex server/NAS :)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188qwf9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701476793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I have been using an external USB-powered hard drive attached to a Raspberry Pi 4 for about a year.  I use it mostly as a Plex server, but also for Home Assistant and some household NAS stuff.  I have filled up the connected drive, so I&amp;#39;d like to upgrade to using multiple drives at once, for more storage and eventually to be backing up more responsibly.  I have narrowed my choices for a solution down--I think--to these two options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.com/Sabrent-4-Bay-Docking-Station-DS-U3B4/dp/B07H11KXCL/ref=sr_1_9?crid=U6I3TFAZYKZG&amp;amp;keywords=4+bay+hard+drive+enclosure&amp;amp;qid=1701475340&amp;amp;refinements=p_76%3A1249137011%2Cp_72%3A1248879011&amp;amp;rnid=1248877011&amp;amp;rps=1&amp;amp;s=electronics&amp;amp;sprefix=4+bay+hard+drive+enclosure%2Celectronics%2C147&amp;amp;sr=1-9&amp;amp;ufe=app_do%3Aamzn1.fos.18630bbb-fcbb-42f8-9767-857e17e03685\"&gt;Sabrent&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.com/Mediasonic-PROBOX-SATA-Drive-Enclosure/dp/B09WPPJHSS?th=1\"&gt;Mediasonic&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would love to hear any other suggestions that I might not have thought of, or to be corrected if I&amp;#39;ve made some false assumptions about my needs. My only thoughts during my search were that it needs 4+ bays, and that it be powered, since I understand that powering more than one hard drive off of the Pi itself causes issues.  Thanks for any help you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188qwf9", "is_robot_indexable": true, "report_reasons": null, "author": "Mike8813", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188qwf9/please_help_me_choose_one_of_these_4drive_bays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188qwf9/please_help_me_choose_one_of_these_4drive_bays/", "subreddit_subscribers": 715700, "created_utc": 1701476793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there fellow data hoarders. I have 2 failing external drives (a 4TB Seagate Backup Plus, newer, but not newest model - around 2 yrs old; and a very old 750GB Seagate Backup Plus drive - around 8+ yrs old). Opened CrystalDiskInfo with them attached one day and found the 4TB drive has C05 and C06 errors, and the 750GB drive only has a C05 error. But I want to pull the data from them while I'm still able to - before they fail completely. I haven't done much with them since finding they were throwing caution errors. I did transfer some data from the 4TB drive, but haven't written to the drives (as far as I know) since then.\n\nI recently snagged a 2TB WD Black XboxOne Gamedrive and a 6TB Seagate Expansion drive (externally powered). Planning to use the 6TB to back up the data from the aforementioned drives. I have heard that making an image of failing hdd's, then running recovery tools against the image is the best way to go about things. But, since storage is expensive, I was hoping someone might have some input on how to pull the data directly to the new 6TB drive. Rather than creating a backup image.\n\nWould my best bet be to enable write protection in Windows Policy Management (to prevent writing being done to the drives allegedly going bad), then using some tool to copy over the data? I will also most likely be copying data over incrementally since copying 4TB via USB will take AGES and I'd rather not leave the house with my PC on and churning away without me being present...\n\nAnyways, transfer speed isn't really an issue as I'll be fine just picking at it until everything I want to save is copied over. I'm also fine with dismantling the drives and connecting them as internal SATA drives if that might increase speed and reliability. Other than the caution errors and only one file on the 4TB drive that I know of causing stalling and failing when attempting to copy data from it, they seem fine otherwise. Once the data is secure I plan to try and fix the bad sectors being reported if possible.\n\n&amp;#x200B;\n\nJust looking for suggestions on the best ways to transfer data from failing to good storage, any apps I could use or methods to reduce further damage to the drives, and any ways to fix the bad sectors and use as disposable storage later - if possible. Thanks!", "author_fullname": "t2_2kxyufad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with failing drives - Options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188p4qh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701471872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there fellow data hoarders. I have 2 failing external drives (a 4TB Seagate Backup Plus, newer, but not newest model - around 2 yrs old; and a very old 750GB Seagate Backup Plus drive - around 8+ yrs old). Opened CrystalDiskInfo with them attached one day and found the 4TB drive has C05 and C06 errors, and the 750GB drive only has a C05 error. But I want to pull the data from them while I&amp;#39;m still able to - before they fail completely. I haven&amp;#39;t done much with them since finding they were throwing caution errors. I did transfer some data from the 4TB drive, but haven&amp;#39;t written to the drives (as far as I know) since then.&lt;/p&gt;\n\n&lt;p&gt;I recently snagged a 2TB WD Black XboxOne Gamedrive and a 6TB Seagate Expansion drive (externally powered). Planning to use the 6TB to back up the data from the aforementioned drives. I have heard that making an image of failing hdd&amp;#39;s, then running recovery tools against the image is the best way to go about things. But, since storage is expensive, I was hoping someone might have some input on how to pull the data directly to the new 6TB drive. Rather than creating a backup image.&lt;/p&gt;\n\n&lt;p&gt;Would my best bet be to enable write protection in Windows Policy Management (to prevent writing being done to the drives allegedly going bad), then using some tool to copy over the data? I will also most likely be copying data over incrementally since copying 4TB via USB will take AGES and I&amp;#39;d rather not leave the house with my PC on and churning away without me being present...&lt;/p&gt;\n\n&lt;p&gt;Anyways, transfer speed isn&amp;#39;t really an issue as I&amp;#39;ll be fine just picking at it until everything I want to save is copied over. I&amp;#39;m also fine with dismantling the drives and connecting them as internal SATA drives if that might increase speed and reliability. Other than the caution errors and only one file on the 4TB drive that I know of causing stalling and failing when attempting to copy data from it, they seem fine otherwise. Once the data is secure I plan to try and fix the bad sectors being reported if possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Just looking for suggestions on the best ways to transfer data from failing to good storage, any apps I could use or methods to reduce further damage to the drives, and any ways to fix the bad sectors and use as disposable storage later - if possible. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188p4qh", "is_robot_indexable": true, "report_reasons": null, "author": "Terrapin2190", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188p4qh/help_with_failing_drives_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188p4qh/help_with_failing_drives_options/", "subreddit_subscribers": 715700, "created_utc": 1701471872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi. So I just bought a WD 18tb Elements drive and shucked it. The drive inside is the WD180EDGZ. Installed it in my desktop. But I couldn't get it to show up in BIOS or in Windows. However, it works fine on the USB 3.0 external dock.\n\nSo using the dock, I used command prompt, cleaned the drive, converted it to GPT, created a primary partition and formatted it to NTFS. Put the drive back into the desktop and connected it to the same SATA data and power connectors that the 8TB WD drive (that this 18TB is replacing) was using . So I know those connections are good. Still nothing.\n\nIt's my understanding that this drive doesn't require the 3.3v mod. So, before I start putzing around with Kapton tape and such I thought I'd ask this community what might be going on here.\n\nPlease any help would be appreciated. Thank you.", "author_fullname": "t2_kmyx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 18TB Elements (shucked) not showing in BIOS or in Windows Disk Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188m08p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701463642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. So I just bought a WD 18tb Elements drive and shucked it. The drive inside is the WD180EDGZ. Installed it in my desktop. But I couldn&amp;#39;t get it to show up in BIOS or in Windows. However, it works fine on the USB 3.0 external dock.&lt;/p&gt;\n\n&lt;p&gt;So using the dock, I used command prompt, cleaned the drive, converted it to GPT, created a primary partition and formatted it to NTFS. Put the drive back into the desktop and connected it to the same SATA data and power connectors that the 8TB WD drive (that this 18TB is replacing) was using . So I know those connections are good. Still nothing.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s my understanding that this drive doesn&amp;#39;t require the 3.3v mod. So, before I start putzing around with Kapton tape and such I thought I&amp;#39;d ask this community what might be going on here.&lt;/p&gt;\n\n&lt;p&gt;Please any help would be appreciated. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188m08p", "is_robot_indexable": true, "report_reasons": null, "author": "shetum", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188m08p/wd_18tb_elements_shucked_not_showing_in_bios_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188m08p/wd_18tb_elements_shucked_not_showing_in_bios_or/", "subreddit_subscribers": 715700, "created_utc": 1701463642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What's the best source of refurbished or recertified HDDs, similar to https://serverpartdeals.com/ but operating in the EU?", "author_fullname": "t2_z5c6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refurbished/recertified HDDs in the EU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188i872", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701453762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best source of refurbished or recertified HDDs, similar to &lt;a href=\"https://serverpartdeals.com/\"&gt;https://serverpartdeals.com/&lt;/a&gt; but operating in the EU?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?auto=webp&amp;s=5d1b92aacede02a2cd533065be2e80c8e499f1c0", "width": 2000, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ea6cf72bbc76c5e0dca433670a432a46687ed60", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47e1acbaf47daeac1c2c691978a22ae59bac6606", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=700981f9d49424bce29b2b1957ba8183cc63569c", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfd4a90a6191b9fcefd7fdd84b600dcbe1654d51", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33e6363d54f10b581d3f1e26f43ed03595ac6c3d", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98c5524ec5cb53e7808cd6a2f023c4349ec44464", "width": 1080, "height": 1080}], "variants": {}, "id": "K0YRiTXu10JV568UhWpDWrTiVgxQeLs5_Dz-kQiG_XQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "188i872", "is_robot_indexable": true, "report_reasons": null, "author": "econopl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188i872/refurbishedrecertified_hdds_in_the_eu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188i872/refurbishedrecertified_hdds_in_the_eu/", "subreddit_subscribers": 715700, "created_utc": 1701453762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking at the:\n\n\\- Seagate Expansion 4TB  \n\\- WD Elements 4TB  \n\\- Toshiba Canvio Basic 4TB\n\nAre one of those more reliable\\\\durable than the others?  I want to buy a couple and perhaps the 5tb versions instead, so I don't want to pick the wrong brand.\n\nThanks!", "author_fullname": "t2_8y2c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What external HDD brand makes the most reliable\\durable drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18912lj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701513472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at the:&lt;/p&gt;\n\n&lt;p&gt;- Seagate Expansion 4TB&lt;br/&gt;\n- WD Elements 4TB&lt;br/&gt;\n- Toshiba Canvio Basic 4TB&lt;/p&gt;\n\n&lt;p&gt;Are one of those more reliable\\durable than the others?  I want to buy a couple and perhaps the 5tb versions instead, so I don&amp;#39;t want to pick the wrong brand.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18912lj", "is_robot_indexable": true, "report_reasons": null, "author": "x0y0z0", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18912lj/what_external_hdd_brand_makes_the_most/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18912lj/what_external_hdd_brand_makes_the_most/", "subreddit_subscribers": 715700, "created_utc": 1701513472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got an old 4TB My Cloud Home that I'd like to use as a backup for some data. Haven't used it in a while since I upgraded to a Synology for my Plex files and such. All the MY Cloud would be used for is purely storage, but since there aren't any security updates being pushed in the future I'm wondering if it's safe to use now.", "author_fullname": "t2_3sipc6qf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is It Safe To Use A My Cloud Home Now That WD Has Discontinued The Line?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188w3no", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701493170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got an old 4TB My Cloud Home that I&amp;#39;d like to use as a backup for some data. Haven&amp;#39;t used it in a while since I upgraded to a Synology for my Plex files and such. All the MY Cloud would be used for is purely storage, but since there aren&amp;#39;t any security updates being pushed in the future I&amp;#39;m wondering if it&amp;#39;s safe to use now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188w3no", "is_robot_indexable": true, "report_reasons": null, "author": "WxaithBrynger", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188w3no/is_it_safe_to_use_a_my_cloud_home_now_that_wd_has/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188w3no/is_it_safe_to_use_a_my_cloud_home_now_that_wd_has/", "subreddit_subscribers": 715700, "created_utc": 1701493170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So Im using downthemall to retrieve photos for inspiration.\n\nThe website Im downloading from is behance. When I go to a link like [here](https://www.behance.net/gallery/159059361/Sofia-in-Saint-Petersburg) It pulls up many many download links but only 7 are the actual files. many of the links are just duplicates at different resolutions. But not all photos are available at the highest resolution.\n\n1. is there any way to get the addon to simply just download the higher resolution versions (some not being as high a resolution as the others)\n\n2. is there any way to have the download folder reflect the name that shows in the title bar when youre at the link. In this case \"Sofia in Saint-Petersburg on behance) without having to type the name in manually every time I make a new download from a different page? \n\n\nor is there a simpler way to accomplish what Im looking to do with any other options available. Im using firefox and the latest macOS. I just want to download all images present on a page like the one linked above, in the highest reoslution available.", "author_fullname": "t2_y7xmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downthemall firefox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188tcl7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701484055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So Im using downthemall to retrieve photos for inspiration.&lt;/p&gt;\n\n&lt;p&gt;The website Im downloading from is behance. When I go to a link like &lt;a href=\"https://www.behance.net/gallery/159059361/Sofia-in-Saint-Petersburg\"&gt;here&lt;/a&gt; It pulls up many many download links but only 7 are the actual files. many of the links are just duplicates at different resolutions. But not all photos are available at the highest resolution.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;is there any way to get the addon to simply just download the higher resolution versions (some not being as high a resolution as the others)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;is there any way to have the download folder reflect the name that shows in the title bar when youre at the link. In this case &amp;quot;Sofia in Saint-Petersburg on behance) without having to type the name in manually every time I make a new download from a different page? &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;or is there a simpler way to accomplish what Im looking to do with any other options available. Im using firefox and the latest macOS. I just want to download all images present on a page like the one linked above, in the highest reoslution available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/62hyFph6x1hNkw6-5Q4JLbPpiMzzNVp6JgQRIKJXuO0.jpg?auto=webp&amp;s=54132329e37331a57cefbb0a97a2a6baaa588fc4", "width": 808, "height": 632}, "resolutions": [{"url": "https://external-preview.redd.it/62hyFph6x1hNkw6-5Q4JLbPpiMzzNVp6JgQRIKJXuO0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8233e51d423077a7ba9c06a90c29b8114c45667", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/62hyFph6x1hNkw6-5Q4JLbPpiMzzNVp6JgQRIKJXuO0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e932b16e69b8c4f5206c893448286a119b8ece9", "width": 216, "height": 168}, {"url": "https://external-preview.redd.it/62hyFph6x1hNkw6-5Q4JLbPpiMzzNVp6JgQRIKJXuO0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6bc4da41abcc1c572fad5a3c57fdd13bcfe3a74", "width": 320, "height": 250}, {"url": "https://external-preview.redd.it/62hyFph6x1hNkw6-5Q4JLbPpiMzzNVp6JgQRIKJXuO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2575649f776b8d818079657ac9343c7825259bde", "width": 640, "height": 500}], "variants": {}, "id": "LDV9u6510gTUNAsh843EScnouKj_f6pYfvarkzjxMdQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188tcl7", "is_robot_indexable": true, "report_reasons": null, "author": "AllAboutGadgets", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188tcl7/downthemall_firefox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188tcl7/downthemall_firefox/", "subreddit_subscribers": 715700, "created_utc": 1701484055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On windows, you can macrium or aomeibackupper and plenty of other ways to create a full image of the disk to back it up, restore, or even mount to view the files.\n\nIS there a way to do this on my SAmsung s8?  MY s8 is unrooted, but i had there was a way to do this using usb debugger mode and command functions with ADB?", "author_fullname": "t2_1xu3xsg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup/fully image my SAmsung s8 like you can with a Windows PC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188rnr5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701478981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On windows, you can macrium or aomeibackupper and plenty of other ways to create a full image of the disk to back it up, restore, or even mount to view the files.&lt;/p&gt;\n\n&lt;p&gt;IS there a way to do this on my SAmsung s8?  MY s8 is unrooted, but i had there was a way to do this using usb debugger mode and command functions with ADB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188rnr5", "is_robot_indexable": true, "report_reasons": null, "author": "NoobNup", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188rnr5/how_to_backupfully_image_my_samsung_s8_like_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188rnr5/how_to_backupfully_image_my_samsung_s8_like_you/", "subreddit_subscribers": 715700, "created_utc": 1701478981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey friends, I got started previously with two Exos X20 20TB drives, now looking to purchase 2-3 more 20TB drives from SPD. It looks like they recently added the X22 20TB drives for the **exact same price as the X20 20TB drives**, so cost isn't a factor; I just need to decide which to get. I've read some conflicting discussion on the subreddit:\n\n&amp;#x200B;\n\nIn favor of X22:\n\n\\- they offer slightly increased max transfer speeds\n\n\\- obviously are newer with less hours on them\n\n\\- [X22 has a cache of 512 MB as opposed to 256 MB for the X20](https://beta.reddit.com/r/DataHoarder/comments/171pn20/difference_between_seagate_x22_and_x20/k4089e0/)\n\n&amp;#x200B;\n\nIn favor of X20:\n\n\\- [it might be more reliable to buy X20 vs X22 when it comes to the 20TB size](https://www.reddit.com/r/DataHoarder/comments/171pn20/comment/k3togzw/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\n\\- I already have two X20 drives\n\n&amp;#x200B;\n\nGiven that I already have two X20 drives, is cache + max transfer speeds capped at the X20 specs anyway? Which should I go with?", "author_fullname": "t2_u4f7a0x5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When increasing capacity, should I buy newer gen drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188njai", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701467603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey friends, I got started previously with two Exos X20 20TB drives, now looking to purchase 2-3 more 20TB drives from SPD. It looks like they recently added the X22 20TB drives for the &lt;strong&gt;exact same price as the X20 20TB drives&lt;/strong&gt;, so cost isn&amp;#39;t a factor; I just need to decide which to get. I&amp;#39;ve read some conflicting discussion on the subreddit:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In favor of X22:&lt;/p&gt;\n\n&lt;p&gt;- they offer slightly increased max transfer speeds&lt;/p&gt;\n\n&lt;p&gt;- obviously are newer with less hours on them&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://beta.reddit.com/r/DataHoarder/comments/171pn20/difference_between_seagate_x22_and_x20/k4089e0/\"&gt;X22 has a cache of 512 MB as opposed to 256 MB for the X20&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In favor of X20:&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/171pn20/comment/k3togzw/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;it might be more reliable to buy X20 vs X22 when it comes to the 20TB size&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- I already have two X20 drives&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Given that I already have two X20 drives, is cache + max transfer speeds capped at the X20 specs anyway? Which should I go with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188njai", "is_robot_indexable": true, "report_reasons": null, "author": "GeneticStroke", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188njai/when_increasing_capacity_should_i_buy_newer_gen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188njai/when_increasing_capacity_should_i_buy_newer_gen/", "subreddit_subscribers": 715700, "created_utc": 1701467603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Folks, I have a question. I'm looking to search through data because I have a list of topics or companies, and I want to gather all the relevant news or information about them from online sources. Additionally, I want to enable people to easily search through this collected information. How should I construct a news ingestion or automatic service, similar to Google Alerts, that saves everything in one place?", "author_fullname": "t2_7zup0hfs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A data scrapping question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188mmsz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701465248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Folks, I have a question. I&amp;#39;m looking to search through data because I have a list of topics or companies, and I want to gather all the relevant news or information about them from online sources. Additionally, I want to enable people to easily search through this collected information. How should I construct a news ingestion or automatic service, similar to Google Alerts, that saves everything in one place?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188mmsz", "is_robot_indexable": true, "report_reasons": null, "author": "digital-bolkonsky", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188mmsz/a_data_scrapping_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188mmsz/a_data_scrapping_question/", "subreddit_subscribers": 715700, "created_utc": 1701465248.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}