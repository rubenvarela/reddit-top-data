{"kind": "Listing", "data": {"after": "t3_1893mm0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_nek10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spotted in the wild. 10Pb of drives on a pallet...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "name": "t3_1891b15", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 219, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 219, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WYgpUcLE-kXASiYTJZv0N78diWYIZKJVDn7woel7Qlo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701514472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/h2txqr2t3v3c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?auto=webp&amp;s=e4bf2e7a20d85d109863376c5bd31a088a3f75e8", "width": 1163, "height": 709}, "resolutions": [{"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5f7862b205c8e9042a311325725f5db955859fc", "width": 108, "height": 65}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=07b7385112bc8d58604896bf047ecb8dd2544cad", "width": 216, "height": 131}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=355f0933643bc712d31a493e7cb288eb37a95134", "width": 320, "height": 195}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56ab70a07f307b5de0777145de061d6d63cf141b", "width": 640, "height": 390}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1f1711a31c642a61b07a2a5603dd1dac8348419", "width": 960, "height": 585}, {"url": "https://preview.redd.it/h2txqr2t3v3c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8d60a130edb7c8baa9495f4cedacc30d8e1d4b75", "width": 1080, "height": 658}], "variants": {}, "id": "drLeK54cjXTqXHZs18pduA0gjgwHsj5NFC_PNPGiabQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1891b15", "is_robot_indexable": true, "report_reasons": null, "author": "Switchblade88", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1891b15/spotted_in_the_wild_10pb_of_drives_on_a_pallet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/h2txqr2t3v3c1.png", "subreddit_subscribers": 715743, "created_utc": 1701514472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_nkjjxlogw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OpenZFS 2.2.2 &amp; OpenZFS 2.1.14 Released To Fix Data Corruption Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 32, "top_awarded_type": null, "hide_score": false, "name": "t3_188iisi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/C3J-PPf9ruFnUoztYnowHiM3OVWlomFR89cSbjBi5tU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701454518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "phoronix.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.phoronix.com/news/OpenZFS-2.2.2-Released", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YeZU0zqbcxzAbRmKiKTOuqYCfwgo3Q9NQCN3-9UmBNk.jpg?auto=webp&amp;s=290e66bc1d62aca154cfc1434c4cbf45ed665922", "width": 480, "height": 112}, "resolutions": [{"url": "https://external-preview.redd.it/YeZU0zqbcxzAbRmKiKTOuqYCfwgo3Q9NQCN3-9UmBNk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff05640a43fd26d907342eefe5902eceb68b9a77", "width": 108, "height": 25}, {"url": "https://external-preview.redd.it/YeZU0zqbcxzAbRmKiKTOuqYCfwgo3Q9NQCN3-9UmBNk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c96224bf738fa5ee7d86fc7533e7db4ca0a14efc", "width": 216, "height": 50}, {"url": "https://external-preview.redd.it/YeZU0zqbcxzAbRmKiKTOuqYCfwgo3Q9NQCN3-9UmBNk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d2f2e2fa08bc3b3ce49dee56e4f0a850621881d", "width": 320, "height": 74}], "variants": {}, "id": "47OJV16hWJg-Xd_JV5FZb5odNpLb-dDOzhUibc1PqG0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188iisi", "is_robot_indexable": true, "report_reasons": null, "author": "TorrentplsZOMG", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188iisi/openzfs_222_openzfs_2114_released_to_fix_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.phoronix.com/news/OpenZFS-2.2.2-Released", "subreddit_subscribers": 715743, "created_utc": 1701454518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hey! \n\nI'm part of a non-profit that refurbished older PCs and distributes them to people who needs them. since what we get is donations, everything is driveless. \n\nI'm looking for recommendations for cheap, low capacity (120-240gb) 2.5 SSDs with high reliability, low failure rate, and as low a cost as possible. bulk purchasing options are definitely a plus. \n\nthanks!", "author_fullname": "t2_942r1vjh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap, low capacity, reliable SSDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188i4sa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701453543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey! &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m part of a non-profit that refurbished older PCs and distributes them to people who needs them. since what we get is donations, everything is driveless. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for recommendations for cheap, low capacity (120-240gb) 2.5 SSDs with high reliability, low failure rate, and as low a cost as possible. bulk purchasing options are definitely a plus. &lt;/p&gt;\n\n&lt;p&gt;thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188i4sa", "is_robot_indexable": true, "report_reasons": null, "author": "GorbigliontheStrong", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188i4sa/cheap_low_capacity_reliable_ssds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188i4sa/cheap_low_capacity_reliable_ssds/", "subreddit_subscribers": 715743, "created_utc": 1701453543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey,\n\nI\u2019m extremely worried. I was hospitalized this year and because of this I haven\u2019t been able to access my free MEGA account for a while, I d say 4 to 5 months. Yesterday I access it and I see that there\u2019s no more data left. Everything is GONE! There were 14 years full of photos, memories, artistic feautures, videos, so much important stuff which is not backed up somewhere else. I feel stupid and at the same time I feel like half of me has died\u2026. I wrote them if they could be able to recover my datas, even if it\u2019s gonna cost me thousands of dollars, I don\u2019t care at this point\u2026\nWhat do you alll think?\n\nAhhhhhhhhh I\u2019m never been so desperate \ud83d\ude30\ud83d\ude30\ud83d\ude30\ud83d\ude30\ud83d\ude30", "author_fullname": "t2_so70si0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mega Cloud deleted all my data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188xv6s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701499671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m extremely worried. I was hospitalized this year and because of this I haven\u2019t been able to access my free MEGA account for a while, I d say 4 to 5 months. Yesterday I access it and I see that there\u2019s no more data left. Everything is GONE! There were 14 years full of photos, memories, artistic feautures, videos, so much important stuff which is not backed up somewhere else. I feel stupid and at the same time I feel like half of me has died\u2026. I wrote them if they could be able to recover my datas, even if it\u2019s gonna cost me thousands of dollars, I don\u2019t care at this point\u2026\nWhat do you alll think?&lt;/p&gt;\n\n&lt;p&gt;Ahhhhhhhhh I\u2019m never been so desperate \ud83d\ude30\ud83d\ude30\ud83d\ude30\ud83d\ude30\ud83d\ude30&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188xv6s", "is_robot_indexable": true, "report_reasons": null, "author": "phersper", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188xv6s/mega_cloud_deleted_all_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188xv6s/mega_cloud_deleted_all_my_data/", "subreddit_subscribers": 715743, "created_utc": 1701499671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Put simply, everytime pclouds desktop client starts, it launches a promo window with ads for discounts on their deals. Started a while ago with exclusive country-deals, has gotten unbearable with black friday, getting a [popup every few hours](https://i.imgur.com/NiLIgxt.png) for their deals.   \nMy first reaction was just using anti-popup software that usually removes these windows everytime they appear - but on my first attempts, it didn't work. The reason for that is because the window title looks like \"pCloud Promo\", but they deliberately made [the second \"o\" of \"promo\" the cyrillic unicode letter U+043E](https://i.imgur.com/5EgDsEH.png) to avoid anti-adware software to detect the window. This is completely over the line for me. I paid hundreds of euros for this and in return i get popups with shitty ads that they try to keep you from closing.", "author_fullname": "t2_1keudfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: pCloud feels like it's slowly turning into adware, including some very scummy design practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1893r1e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701523760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Put simply, everytime pclouds desktop client starts, it launches a promo window with ads for discounts on their deals. Started a while ago with exclusive country-deals, has gotten unbearable with black friday, getting a &lt;a href=\"https://i.imgur.com/NiLIgxt.png\"&gt;popup every few hours&lt;/a&gt; for their deals.&lt;br/&gt;\nMy first reaction was just using anti-popup software that usually removes these windows everytime they appear - but on my first attempts, it didn&amp;#39;t work. The reason for that is because the window title looks like &amp;quot;pCloud Promo&amp;quot;, but they deliberately made &lt;a href=\"https://i.imgur.com/5EgDsEH.png\"&gt;the second &amp;quot;o&amp;quot; of &amp;quot;promo&amp;quot; the cyrillic unicode letter U+043E&lt;/a&gt; to avoid anti-adware software to detect the window. This is completely over the line for me. I paid hundreds of euros for this and in return i get popups with shitty ads that they try to keep you from closing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/56TvikG82vkVV60IvYXsiESlXV7mzxCWj5uUulG8v0c.png?auto=webp&amp;s=d025fc68c8b3f7fe7f800493d11dcd521b06ea31", "width": 802, "height": 632}, "resolutions": [{"url": "https://external-preview.redd.it/56TvikG82vkVV60IvYXsiESlXV7mzxCWj5uUulG8v0c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=25b53b077af7b2137a93ab52923e75911ef07494", "width": 108, "height": 85}, {"url": "https://external-preview.redd.it/56TvikG82vkVV60IvYXsiESlXV7mzxCWj5uUulG8v0c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6993b519ac0939cfa97d210e65f2d54dcabb40ee", "width": 216, "height": 170}, {"url": "https://external-preview.redd.it/56TvikG82vkVV60IvYXsiESlXV7mzxCWj5uUulG8v0c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4e9fa56cea41afd1b7d6ff33faf4cc200f4aa2f", "width": 320, "height": 252}, {"url": "https://external-preview.redd.it/56TvikG82vkVV60IvYXsiESlXV7mzxCWj5uUulG8v0c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cea04ab406320909ee18fa502c52d97fd5e319e", "width": 640, "height": 504}], "variants": {}, "id": "KuqXXG1ISo0-62w48IzpB_L9hxpelxk07wFtRF8YBpo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1893r1e", "is_robot_indexable": true, "report_reasons": null, "author": "Cobracrystal", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1893r1e/psa_pcloud_feels_like_its_slowly_turning_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1893r1e/psa_pcloud_feels_like_its_slowly_turning_into/", "subreddit_subscribers": 715743, "created_utc": 1701523760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been on planning on doing some major external drive consolidation and organization and have pretty much settled on going the route of a simple external RAID enclosure housing a couple large drives in a RAID 1 configuration. For some reason it seems impossible to find a DAS enclosure with the following:\n\n* Adequate cooling\n* Not annoying fan noise\n* USB 3.2 Gen 2 (10Gbps versus the slower 5Gbps)\n* Hardware RAID\n\nI was really liking what I was reading about the QNAP TR-002 or TR-004, but it appears they're limited to USB 3.1 Gen 1 speeds, annoyingly.\n\nI was also liking what I was seeing about the Sabrent DS-2BCR 2-Bay enclosure, but apparently the fan is very annoying. And the Sabrent 4 Bay enclosure doesn't do hardware RAID. But maybe Mac OS' software RAID would be fine?\n\nWhy does it feel impossible to find a good, simple RAID enclosure? Is there a product I'm missing that would tick all of these boxes?", "author_fullname": "t2_7zaca5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does a simple, good RAID enclosure exist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188kgip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701459576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been on planning on doing some major external drive consolidation and organization and have pretty much settled on going the route of a simple external RAID enclosure housing a couple large drives in a RAID 1 configuration. For some reason it seems impossible to find a DAS enclosure with the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Adequate cooling&lt;/li&gt;\n&lt;li&gt;Not annoying fan noise&lt;/li&gt;\n&lt;li&gt;USB 3.2 Gen 2 (10Gbps versus the slower 5Gbps)&lt;/li&gt;\n&lt;li&gt;Hardware RAID&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I was really liking what I was reading about the QNAP TR-002 or TR-004, but it appears they&amp;#39;re limited to USB 3.1 Gen 1 speeds, annoyingly.&lt;/p&gt;\n\n&lt;p&gt;I was also liking what I was seeing about the Sabrent DS-2BCR 2-Bay enclosure, but apparently the fan is very annoying. And the Sabrent 4 Bay enclosure doesn&amp;#39;t do hardware RAID. But maybe Mac OS&amp;#39; software RAID would be fine?&lt;/p&gt;\n\n&lt;p&gt;Why does it feel impossible to find a good, simple RAID enclosure? Is there a product I&amp;#39;m missing that would tick all of these boxes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188kgip", "is_robot_indexable": true, "report_reasons": null, "author": "Eighty4s", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188kgip/does_a_simple_good_raid_enclosure_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188kgip/does_a_simple_good_raid_enclosure_exist/", "subreddit_subscribers": 715743, "created_utc": 1701459576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\n[Does anyone know why it takes so long to save pages Or why the site is overloaded?](https://preview.redd.it/y9j56jcx5v3c1.jpg?width=1228&amp;format=pjpg&amp;auto=webp&amp;s=5c1d25c87b5605b46aa639cc26691cff711bfd9a)", "author_fullname": "t2_8h7flhqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current State of archive.org", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 21, "top_awarded_type": null, "hide_score": false, "media_metadata": {"y9j56jcx5v3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 16, "x": 108, "u": "https://preview.redd.it/y9j56jcx5v3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19af536eef48770edfdd0ac227112e639afb9beb"}, {"y": 33, "x": 216, "u": "https://preview.redd.it/y9j56jcx5v3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e028e0679a347a483abb620db10e52b899a3eedb"}, {"y": 49, "x": 320, "u": "https://preview.redd.it/y9j56jcx5v3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72334957c82019c40aedc69e72153b9006fc3308"}, {"y": 99, "x": 640, "u": "https://preview.redd.it/y9j56jcx5v3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bc2e4605aa3ef805e1764030bb901a3bf22cd8d"}, {"y": 148, "x": 960, "u": "https://preview.redd.it/y9j56jcx5v3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=df48238687d400d4c5abf1f0d753d1860b5cf470"}, {"y": 167, "x": 1080, "u": "https://preview.redd.it/y9j56jcx5v3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a27ada28758f5897cd6ea6aeef3c1d4d0f27a39f"}], "s": {"y": 190, "x": 1228, "u": "https://preview.redd.it/y9j56jcx5v3c1.jpg?width=1228&amp;format=pjpg&amp;auto=webp&amp;s=5c1d25c87b5605b46aa639cc26691cff711bfd9a"}, "id": "y9j56jcx5v3c1"}}, "name": "t3_1891h0x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WsecGtdJvZRuKAHImf1T8ljhTFxC0-gyVXBIeRwaZBs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701515124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y9j56jcx5v3c1.jpg?width=1228&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5c1d25c87b5605b46aa639cc26691cff711bfd9a\"&gt;Does anyone know why it takes so long to save pages Or why the site is overloaded?&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1891h0x", "is_robot_indexable": true, "report_reasons": null, "author": "Slipshower", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1891h0x/current_state_of_archiveorg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1891h0x/current_state_of_archiveorg/", "subreddit_subscribers": 715743, "created_utc": 1701515124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Whenever I'm shutting down anything like:\n\n* A Linux server\n* A Windows desktop (my own, family members, clients)\n* A cloud storage account (OneDrive/GDrive/Dropbox etc)\n* Individual websites\n* Other large things like big database dumps taken immediately before some major change, that I want to retain forever\n\nI generally take a final archive of it (even if it was being backed up by more \"backup\" focused software during its running lifetime).  \n\nAnd I really want these to be in a mountable format, so that I can browse them + view files etc without having to extract copies.\n\nSo far I've mostly used squashfs for this, which is mountable on Linux, but not easily Windows as far as I can tell.\n\n---\n\n##### My overly optimistic + unrealistic wish list:\n\n* **Mountable:** both on Windows + Linux\n  * ...if it can be mountable even over remote object storage, that would be awesome (e.g. like restic + kopia are)\n* **Compressed**\n* **Deduped** as much as possible\nlong-term consistent/stable format is what I'm after\n* Encryption: maybe not a hard requirement, but always handy to have the option, especially if storing on cloud/object storage\n* **Splittable:** e.g. if I want to break up into 100mb or 1gb chunks\n* **Editable:** If the format also had the option to be editable while mounted that would awesome too... as it means I could just immediately create these archives, then worry about culling all the crap in them that wasn't needed later on... having to do this pre-emptively before creating the archive chews up a lot of time cause I'm kinda neurotic about this.\n* I guess backup software like restic + kopia are what will give most of these features\n  * But maybe not the right tools for this \"once-off final state\" long term archiving \n  * They're also kinda complex to setup in the future for restore/mounting, which may be further complicated by their changing formats across versions, given they're not really designed for these long-term archiving use case.\n\nI'm open to hearing about all sorts of options here.  I've considered things like virtual machine image formats too (for the \"editable\" feature above), even though I'm not actually ever actually backing-up/archiving whole-drive images.\n\nAnd I'm guessing that nothing is going to give me ***everything*** above.  But keen to hear thougts from anyone on stuff you've used for these kinds of uses cases.  Even if it ended up being a mistake in the end, always some learning in there!", "author_fullname": "t2_43cuz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What mountable archive/image formats do you use for large backups/archive the \"final state\" of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188wvlb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701495851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whenever I&amp;#39;m shutting down anything like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A Linux server&lt;/li&gt;\n&lt;li&gt;A Windows desktop (my own, family members, clients)&lt;/li&gt;\n&lt;li&gt;A cloud storage account (OneDrive/GDrive/Dropbox etc)&lt;/li&gt;\n&lt;li&gt;Individual websites&lt;/li&gt;\n&lt;li&gt;Other large things like big database dumps taken immediately before some major change, that I want to retain forever&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I generally take a final archive of it (even if it was being backed up by more &amp;quot;backup&amp;quot; focused software during its running lifetime).  &lt;/p&gt;\n\n&lt;p&gt;And I really want these to be in a mountable format, so that I can browse them + view files etc without having to extract copies.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve mostly used squashfs for this, which is mountable on Linux, but not easily Windows as far as I can tell.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h5&gt;My overly optimistic + unrealistic wish list:&lt;/h5&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Mountable:&lt;/strong&gt; both on Windows + Linux\n\n&lt;ul&gt;\n&lt;li&gt;...if it can be mountable even over remote object storage, that would be awesome (e.g. like restic + kopia are)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Compressed&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Deduped&lt;/strong&gt; as much as possible\nlong-term consistent/stable format is what I&amp;#39;m after&lt;/li&gt;\n&lt;li&gt;Encryption: maybe not a hard requirement, but always handy to have the option, especially if storing on cloud/object storage&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Splittable:&lt;/strong&gt; e.g. if I want to break up into 100mb or 1gb chunks&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Editable:&lt;/strong&gt; If the format also had the option to be editable while mounted that would awesome too... as it means I could just immediately create these archives, then worry about culling all the crap in them that wasn&amp;#39;t needed later on... having to do this pre-emptively before creating the archive chews up a lot of time cause I&amp;#39;m kinda neurotic about this.&lt;/li&gt;\n&lt;li&gt;I guess backup software like restic + kopia are what will give most of these features\n\n&lt;ul&gt;\n&lt;li&gt;But maybe not the right tools for this &amp;quot;once-off final state&amp;quot; long term archiving &lt;/li&gt;\n&lt;li&gt;They&amp;#39;re also kinda complex to setup in the future for restore/mounting, which may be further complicated by their changing formats across versions, given they&amp;#39;re not really designed for these long-term archiving use case.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m open to hearing about all sorts of options here.  I&amp;#39;ve considered things like virtual machine image formats too (for the &amp;quot;editable&amp;quot; feature above), even though I&amp;#39;m not actually ever actually backing-up/archiving whole-drive images.&lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;m guessing that nothing is going to give me &lt;strong&gt;&lt;em&gt;everything&lt;/em&gt;&lt;/strong&gt; above.  But keen to hear thougts from anyone on stuff you&amp;#39;ve used for these kinds of uses cases.  Even if it ended up being a mistake in the end, always some learning in there!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188wvlb", "is_robot_indexable": true, "report_reasons": null, "author": "r0ck0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188wvlb/what_mountable_archiveimage_formats_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188wvlb/what_mountable_archiveimage_formats_do_you_use/", "subreddit_subscribers": 715743, "created_utc": 1701495851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a old device running pfsense as my firewall/router and am looking to upgrade (because I can).  It's a Supermicro Atom D525 platform and I'd like to reuse the chassis.  Any recommendations on an affordable platform that I can upgrade to that will be fairly quiet?  \n\nI have shitty DSL with a 30/3 up/down connection  so that will always be my bottleneck.  I would like to work on suricata to keep myself secure but if I can't I'll manage.", "author_fullname": "t2_b5ztvbyu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Firewall Upgrade Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188k3pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701458620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a old device running pfsense as my firewall/router and am looking to upgrade (because I can).  It&amp;#39;s a Supermicro Atom D525 platform and I&amp;#39;d like to reuse the chassis.  Any recommendations on an affordable platform that I can upgrade to that will be fairly quiet?  &lt;/p&gt;\n\n&lt;p&gt;I have shitty DSL with a 30/3 up/down connection  so that will always be my bottleneck.  I would like to work on suricata to keep myself secure but if I can&amp;#39;t I&amp;#39;ll manage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188k3pe", "is_robot_indexable": true, "report_reasons": null, "author": "DiscracedSith", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188k3pe/firewall_upgrade_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188k3pe/firewall_upgrade_advice/", "subreddit_subscribers": 715743, "created_utc": 1701458620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've bought a Sandisk Ultra USB stick with big speed specs and it tanked in performance. Newegg let me keep it. I buy a TeamGroup Hish Speed C212 3.2 256GB and used it a couple times and its fast. But after a few uses now it doesn't enumerate. Any suggestions for a fast high capacity USB 3.2 gen 2 stick?", "author_fullname": "t2_jz3109vj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quality large capacity thumb drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1897r3q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701535860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve bought a Sandisk Ultra USB stick with big speed specs and it tanked in performance. Newegg let me keep it. I buy a TeamGroup Hish Speed C212 3.2 256GB and used it a couple times and its fast. But after a few uses now it doesn&amp;#39;t enumerate. Any suggestions for a fast high capacity USB 3.2 gen 2 stick?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1897r3q", "is_robot_indexable": true, "report_reasons": null, "author": "outdoorszy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1897r3q/quality_large_capacity_thumb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1897r3q/quality_large_capacity_thumb_drives/", "subreddit_subscribers": 715743, "created_utc": 1701535860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 8TB 2-bay WD MyCloud EX2 in RAID 0 and Disk 1 started displaying a SMART error. I copied all my data onto a USB drive, and I want to upgrade both hard drives. I bougth 2 14TB WD RED NAS drives, but I'm pretty sure I can't just pop both in and still be able to log into the device like if it was in RAID1 since both drives will be completely empty. \n\nWhats the best way to install these two drives and then install the OS?", "author_fullname": "t2_4dbue7un", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for some help with WD MyCloud EX2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1897jmv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701535250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 8TB 2-bay WD MyCloud EX2 in RAID 0 and Disk 1 started displaying a SMART error. I copied all my data onto a USB drive, and I want to upgrade both hard drives. I bougth 2 14TB WD RED NAS drives, but I&amp;#39;m pretty sure I can&amp;#39;t just pop both in and still be able to log into the device like if it was in RAID1 since both drives will be completely empty. &lt;/p&gt;\n\n&lt;p&gt;Whats the best way to install these two drives and then install the OS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1897jmv", "is_robot_indexable": true, "report_reasons": null, "author": "Particular-Steak-832", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1897jmv/looking_for_some_help_with_wd_mycloud_ex2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1897jmv/looking_for_some_help_with_wd_mycloud_ex2/", "subreddit_subscribers": 715743, "created_utc": 1701535250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I totally get how crucial the 3-2-1 Backup rule is, and I'm totally on it. \n\nHere's something I'm curious about: How do you make sure your cold storage backups are solid without going through the entire process of reading and comparing all your data sets? Hashing is the only crude way that I know.\n\nHashing those big files or folders can take ages. Also, when you've got bit rot or sneaky bad sectors creeping up during those long cold storage periods, wouldn't the file's hash be affected when you read it? I mean, does the system let you know about any read errors, or does it quietly give you the wrong data? ", "author_fullname": "t2_6hu06u4y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you verify your backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1896cpd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701531741.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I totally get how crucial the 3-2-1 Backup rule is, and I&amp;#39;m totally on it. &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s something I&amp;#39;m curious about: How do you make sure your cold storage backups are solid without going through the entire process of reading and comparing all your data sets? Hashing is the only crude way that I know.&lt;/p&gt;\n\n&lt;p&gt;Hashing those big files or folders can take ages. Also, when you&amp;#39;ve got bit rot or sneaky bad sectors creeping up during those long cold storage periods, wouldn&amp;#39;t the file&amp;#39;s hash be affected when you read it? I mean, does the system let you know about any read errors, or does it quietly give you the wrong data? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "25TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1896cpd", "is_robot_indexable": true, "report_reasons": null, "author": "SuperElephantX", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1896cpd/how_do_you_verify_your_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1896cpd/how_do_you_verify_your_backups/", "subreddit_subscribers": 715743, "created_utc": 1701531741.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently upgraded my network to 10Gbe and am regretting encrypting my TS-873 QNAP nas as the encryption is now a bottleneck. I\u2019ve read that the only way to remove the encryption is to wipe the data pool and restart, but obviously this presents the problem of where to temporarily store the 45tb of data currently on the NAS. \n\nDo you have any suggestions for a service that might have a reasonable price for doing this? I only have a 1gbe fiber connection so I a cloud service would take a loooong time.", "author_fullname": "t2_2hfy1nyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Temp storage to unencrypt nas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1895bsv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701528703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently upgraded my network to 10Gbe and am regretting encrypting my TS-873 QNAP nas as the encryption is now a bottleneck. I\u2019ve read that the only way to remove the encryption is to wipe the data pool and restart, but obviously this presents the problem of where to temporarily store the 45tb of data currently on the NAS. &lt;/p&gt;\n\n&lt;p&gt;Do you have any suggestions for a service that might have a reasonable price for doing this? I only have a 1gbe fiber connection so I a cloud service would take a loooong time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1895bsv", "is_robot_indexable": true, "report_reasons": null, "author": "BigLittleLeeg", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1895bsv/temp_storage_to_unencrypt_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1895bsv/temp_storage_to_unencrypt_nas/", "subreddit_subscribers": 715743, "created_utc": 1701528703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Basically, I'm looking for Tellico but have it working on the Mac.\n\nI already tried MacPorts and unfortunately it didn't work. So now I'm looking for a native alternative. Something which will let me catalog more than just movies. It doesn't have to play anything. It doesn't have to scrape data from anywhere. I does have to be free though. And I would prefer it if it didn't send everything I do straight to Apple. So far the closest thing I have found is Collections Database ([https://collectionsdb.com](https://collectionsdb.com)), which I tried and really loved, but the fact it uploads everything to iCloud is a big no.\n\nOne thing I love about Tellico is you can make your own entry fields. So you are not limited to any particular kind of media. Something like that would be amazing!\n\nAnyone have any recommendations?\n\n&amp;#x200B;", "author_fullname": "t2_54ni9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tellico alternative? Looking for a Collection database/management applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18931dn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701521301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically, I&amp;#39;m looking for Tellico but have it working on the Mac.&lt;/p&gt;\n\n&lt;p&gt;I already tried MacPorts and unfortunately it didn&amp;#39;t work. So now I&amp;#39;m looking for a native alternative. Something which will let me catalog more than just movies. It doesn&amp;#39;t have to play anything. It doesn&amp;#39;t have to scrape data from anywhere. I does have to be free though. And I would prefer it if it didn&amp;#39;t send everything I do straight to Apple. So far the closest thing I have found is Collections Database (&lt;a href=\"https://collectionsdb.com\"&gt;https://collectionsdb.com&lt;/a&gt;), which I tried and really loved, but the fact it uploads everything to iCloud is a big no.&lt;/p&gt;\n\n&lt;p&gt;One thing I love about Tellico is you can make your own entry fields. So you are not limited to any particular kind of media. Something like that would be amazing!&lt;/p&gt;\n\n&lt;p&gt;Anyone have any recommendations?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18931dn", "is_robot_indexable": true, "report_reasons": null, "author": "Chie_Satonaka", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18931dn/tellico_alternative_looking_for_a_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18931dn/tellico_alternative_looking_for_a_collection/", "subreddit_subscribers": 715743, "created_utc": 1701521301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am currently looking for cloud cold storage and am deciding between AWS' S3 Deep Archive and Microsoft's Azure Archive tier storage. The pricing seems comparable with no more than \u20ac0,1 difference in my use case.\n\n&amp;#x200B;\n\nI want to ask about your personal recommendations and experiences in choosing which service to use. Please note that I am a newbie in cloud storage and used to store everything on random USB drives, but those are too unreliable compared to cloud, I think. Feel free to prove me wrong.", "author_fullname": "t2_6m008lir", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "S3 Deep Archive vs Azure Archive Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1892edj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701518843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently looking for cloud cold storage and am deciding between AWS&amp;#39; S3 Deep Archive and Microsoft&amp;#39;s Azure Archive tier storage. The pricing seems comparable with no more than \u20ac0,1 difference in my use case.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I want to ask about your personal recommendations and experiences in choosing which service to use. Please note that I am a newbie in cloud storage and used to store everything on random USB drives, but those are too unreliable compared to cloud, I think. Feel free to prove me wrong.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "May floppies always rest in our hearts", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1892edj", "is_robot_indexable": true, "report_reasons": null, "author": "creeper6530", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1892edj/s3_deep_archive_vs_azure_archive_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1892edj/s3_deep_archive_vs_azure_archive_storage/", "subreddit_subscribers": 715743, "created_utc": 1701518843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\n\n  Have an old home server running WHS2012 R2, running out of room so I wanted to clone my OS ssd drive (250gig) to a partition on a 4TB hdd, use that as my new OS drive instead of the 250gig ssd then add the rest to the pool.  The problem is, will drivepool allow the remaining 3.7TB to be added to the pool when all other drives on the pool are GPT?", "author_fullname": "t2_egkhg7bt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I pool an MBR drive using stablebit drivepool with a group of GPT drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188zclc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701505947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have an old home server running WHS2012 R2, running out of room so I wanted to clone my OS ssd drive (250gig) to a partition on a 4TB hdd, use that as my new OS drive instead of the 250gig ssd then add the rest to the pool.  The problem is, will drivepool allow the remaining 3.7TB to be added to the pool when all other drives on the pool are GPT?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188zclc", "is_robot_indexable": true, "report_reasons": null, "author": "VladX0", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188zclc/can_i_pool_an_mbr_drive_using_stablebit_drivepool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188zclc/can_i_pool_an_mbr_drive_using_stablebit_drivepool/", "subreddit_subscribers": 715743, "created_utc": 1701505947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I\u2019m looking to buy a used Fujitsu Eternus LT20 with an installed LTO4 tape drive. Is it possible to replace the LTO4 drive with a LTO7 drive?\n\nAre there any special things I need to take note of?", "author_fullname": "t2_o7ic76n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacing a LTO 4 with LTO 7 drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188xhti", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701498202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019m looking to buy a used Fujitsu Eternus LT20 with an installed LTO4 tape drive. Is it possible to replace the LTO4 drive with a LTO7 drive?&lt;/p&gt;\n\n&lt;p&gt;Are there any special things I need to take note of?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188xhti", "is_robot_indexable": true, "report_reasons": null, "author": "kahn2k", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188xhti/replacing_a_lto_4_with_lto_7_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188xhti/replacing_a_lto_4_with_lto_7_drive/", "subreddit_subscribers": 715743, "created_utc": 1701498202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got an old 4TB My Cloud Home that I'd like to use as a backup for some data. Haven't used it in a while since I upgraded to a Synology for my Plex files and such. All the MY Cloud would be used for is purely storage, but since there aren't any security updates being pushed in the future I'm wondering if it's safe to use now.", "author_fullname": "t2_3sipc6qf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is It Safe To Use A My Cloud Home Now That WD Has Discontinued The Line?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188w3no", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701493170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got an old 4TB My Cloud Home that I&amp;#39;d like to use as a backup for some data. Haven&amp;#39;t used it in a while since I upgraded to a Synology for my Plex files and such. All the MY Cloud would be used for is purely storage, but since there aren&amp;#39;t any security updates being pushed in the future I&amp;#39;m wondering if it&amp;#39;s safe to use now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188w3no", "is_robot_indexable": true, "report_reasons": null, "author": "WxaithBrynger", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188w3no/is_it_safe_to_use_a_my_cloud_home_now_that_wd_has/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188w3no/is_it_safe_to_use_a_my_cloud_home_now_that_wd_has/", "subreddit_subscribers": 715743, "created_utc": 1701493170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI've been checking posts on this sub, but was unable to find anything even close to what I'm trying to implement. All what I could find was about the data ALREADY hashed (either at source, or during the copy process) and being verified at target AFTER such copy.\n\nMy question is: how one would verify the integrity (for the lack of a better word) of the data which has NOT been hashed BEFORE it appeared on a drive? I apologize for a possible confusion, so to clinarify what I mean, imagine that there is a drive (an external HDD or SSD \u2013 not important which type) with some data on it. This data was at some point in time placed there, but no means of its verification were used at the time (say, while copying to that drive). Now, when I have this drive, how can I make sure that the files are doing fine (they're not corrupt, they can be read, copied, etc.)? Is the only way to ensure the integrity of these files (or readability maybe?) is to check the physical characteristics of that drive (with something like HDDScan or the like)? To make sure that sectors are accessible (therefore (theoretically) the data is intact and readable). Of course, running a hash-creating app (and saving the results on that drive) afterwards will ensure to some extent that when I check that drive IN THE FUTURE, I can determine whether the data is still the same. No issue here. But what about that initial attempt to verify the data?\n\nI'd appreciate all your input and advice (should you have some).\n\nThank you.", "author_fullname": "t2_kqwsgjqcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data integrity check for data NOT previously hashed (while copying, for example) \u2013 please advise..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188sd4m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701481034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been checking posts on this sub, but was unable to find anything even close to what I&amp;#39;m trying to implement. All what I could find was about the data ALREADY hashed (either at source, or during the copy process) and being verified at target AFTER such copy.&lt;/p&gt;\n\n&lt;p&gt;My question is: how one would verify the integrity (for the lack of a better word) of the data which has NOT been hashed BEFORE it appeared on a drive? I apologize for a possible confusion, so to clinarify what I mean, imagine that there is a drive (an external HDD or SSD \u2013 not important which type) with some data on it. This data was at some point in time placed there, but no means of its verification were used at the time (say, while copying to that drive). Now, when I have this drive, how can I make sure that the files are doing fine (they&amp;#39;re not corrupt, they can be read, copied, etc.)? Is the only way to ensure the integrity of these files (or readability maybe?) is to check the physical characteristics of that drive (with something like HDDScan or the like)? To make sure that sectors are accessible (therefore (theoretically) the data is intact and readable). Of course, running a hash-creating app (and saving the results on that drive) afterwards will ensure to some extent that when I check that drive IN THE FUTURE, I can determine whether the data is still the same. No issue here. But what about that initial attempt to verify the data?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate all your input and advice (should you have some).&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188sd4m", "is_robot_indexable": true, "report_reasons": null, "author": "Future-Cod-7565", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188sd4m/data_integrity_check_for_data_not_previously/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188sd4m/data_integrity_check_for_data_not_previously/", "subreddit_subscribers": 715743, "created_utc": 1701481034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have a bunch of HDDs with my stuff. Some drives are used as a backup and I keep a couple of them in a drawer, I update the backups on them about twice a year. I'm not an extreme data hoarder with hundreds of TBs worth of storage, I just want my photos, music, game mods, etc. backed up in case a drive dies.\n\nUntil now I've been using FreeFileSync and simply doing a synchronisation between the relevant folders to keep the backups up to date.\n\nMy question is, is this good enough for someone like me? I've been reading around and see that HDDs can degrade after some years, that bit rot is a thing, and using checksums or par is recommended, etc.\n\nAll I want to do is to have all the data on these drives as usable in 10-20 years time as it is today. The photos viewable, music listenable, game mods installable. Is the simple copy-paste operation I'm doing sufficient for that purpose, or should I be worrying about the aforementioned things?\n\nI'm on Windows 10, don't know anything about Linux.", "author_fullname": "t2_jmjvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for someone who uses a basic local backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188s3ho", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701480259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a bunch of HDDs with my stuff. Some drives are used as a backup and I keep a couple of them in a drawer, I update the backups on them about twice a year. I&amp;#39;m not an extreme data hoarder with hundreds of TBs worth of storage, I just want my photos, music, game mods, etc. backed up in case a drive dies.&lt;/p&gt;\n\n&lt;p&gt;Until now I&amp;#39;ve been using FreeFileSync and simply doing a synchronisation between the relevant folders to keep the backups up to date.&lt;/p&gt;\n\n&lt;p&gt;My question is, is this good enough for someone like me? I&amp;#39;ve been reading around and see that HDDs can degrade after some years, that bit rot is a thing, and using checksums or par is recommended, etc.&lt;/p&gt;\n\n&lt;p&gt;All I want to do is to have all the data on these drives as usable in 10-20 years time as it is today. The photos viewable, music listenable, game mods installable. Is the simple copy-paste operation I&amp;#39;m doing sufficient for that purpose, or should I be worrying about the aforementioned things?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on Windows 10, don&amp;#39;t know anything about Linux.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188s3ho", "is_robot_indexable": true, "report_reasons": null, "author": "Filipi_7", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188s3ho/advice_for_someone_who_uses_a_basic_local_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188s3ho/advice_for_someone_who_uses_a_basic_local_backup/", "subreddit_subscribers": 715743, "created_utc": 1701480259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\n\nI have been using an external USB-powered hard drive attached to a Raspberry Pi 4 for about a year.  I use it mostly as a Plex server, but also for Home Assistant and some household NAS stuff.  I have filled up the connected drive, so I'd like to upgrade to using multiple drives at once, for more storage and eventually to be backing up more responsibly.  I have narrowed my choices for a solution down--I think--to these two options:\n\n* [Sabrent](https://www.amazon.com/Sabrent-4-Bay-Docking-Station-DS-U3B4/dp/B07H11KXCL/ref=sr_1_9?crid=U6I3TFAZYKZG&amp;keywords=4+bay+hard+drive+enclosure&amp;qid=1701475340&amp;refinements=p_76%3A1249137011%2Cp_72%3A1248879011&amp;rnid=1248877011&amp;rps=1&amp;s=electronics&amp;sprefix=4+bay+hard+drive+enclosure%2Celectronics%2C147&amp;sr=1-9&amp;ufe=app_do%3Aamzn1.fos.18630bbb-fcbb-42f8-9767-857e17e03685)\n* [Mediasonic](https://www.amazon.com/Mediasonic-PROBOX-SATA-Drive-Enclosure/dp/B09WPPJHSS?th=1)\n\nI would love to hear any other suggestions that I might not have thought of, or to be corrected if I've made some false assumptions about my needs. My only thoughts during my search were that it needs 4+ bays, and that it be powered, since I understand that powering more than one hard drive off of the Pi itself causes issues.  Thanks for any help you can provide.", "author_fullname": "t2_55d4n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please help me choose one of these 4-drive bays for use with a Raspberry Pi Plex server/NAS :)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188qwf9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701476793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I have been using an external USB-powered hard drive attached to a Raspberry Pi 4 for about a year.  I use it mostly as a Plex server, but also for Home Assistant and some household NAS stuff.  I have filled up the connected drive, so I&amp;#39;d like to upgrade to using multiple drives at once, for more storage and eventually to be backing up more responsibly.  I have narrowed my choices for a solution down--I think--to these two options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.com/Sabrent-4-Bay-Docking-Station-DS-U3B4/dp/B07H11KXCL/ref=sr_1_9?crid=U6I3TFAZYKZG&amp;amp;keywords=4+bay+hard+drive+enclosure&amp;amp;qid=1701475340&amp;amp;refinements=p_76%3A1249137011%2Cp_72%3A1248879011&amp;amp;rnid=1248877011&amp;amp;rps=1&amp;amp;s=electronics&amp;amp;sprefix=4+bay+hard+drive+enclosure%2Celectronics%2C147&amp;amp;sr=1-9&amp;amp;ufe=app_do%3Aamzn1.fos.18630bbb-fcbb-42f8-9767-857e17e03685\"&gt;Sabrent&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.com/Mediasonic-PROBOX-SATA-Drive-Enclosure/dp/B09WPPJHSS?th=1\"&gt;Mediasonic&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would love to hear any other suggestions that I might not have thought of, or to be corrected if I&amp;#39;ve made some false assumptions about my needs. My only thoughts during my search were that it needs 4+ bays, and that it be powered, since I understand that powering more than one hard drive off of the Pi itself causes issues.  Thanks for any help you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188qwf9", "is_robot_indexable": true, "report_reasons": null, "author": "Mike8813", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188qwf9/please_help_me_choose_one_of_these_4drive_bays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188qwf9/please_help_me_choose_one_of_these_4drive_bays/", "subreddit_subscribers": 715743, "created_utc": 1701476793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there fellow data hoarders. I have 2 failing external drives (a 4TB Seagate Backup Plus, newer, but not newest model - around 2 yrs old; and a very old 750GB Seagate Backup Plus drive - around 8+ yrs old). Opened CrystalDiskInfo with them attached one day and found the 4TB drive has C05 and C06 errors, and the 750GB drive only has a C05 error. But I want to pull the data from them while I'm still able to - before they fail completely. I haven't done much with them since finding they were throwing caution errors. I did transfer some data from the 4TB drive, but haven't written to the drives (as far as I know) since then.\n\nI recently snagged a 2TB WD Black XboxOne Gamedrive and a 6TB Seagate Expansion drive (externally powered). Planning to use the 6TB to back up the data from the aforementioned drives. I have heard that making an image of failing hdd's, then running recovery tools against the image is the best way to go about things. But, since storage is expensive, I was hoping someone might have some input on how to pull the data directly to the new 6TB drive. Rather than creating a backup image.\n\nWould my best bet be to enable write protection in Windows Policy Management (to prevent writing being done to the drives allegedly going bad), then using some tool to copy over the data? I will also most likely be copying data over incrementally since copying 4TB via USB will take AGES and I'd rather not leave the house with my PC on and churning away without me being present...\n\nAnyways, transfer speed isn't really an issue as I'll be fine just picking at it until everything I want to save is copied over. I'm also fine with dismantling the drives and connecting them as internal SATA drives if that might increase speed and reliability. Other than the caution errors and only one file on the 4TB drive that I know of causing stalling and failing when attempting to copy data from it, they seem fine otherwise. Once the data is secure I plan to try and fix the bad sectors being reported if possible.\n\n&amp;#x200B;\n\nJust looking for suggestions on the best ways to transfer data from failing to good storage, any apps I could use or methods to reduce further damage to the drives, and any ways to fix the bad sectors and use as disposable storage later - if possible. Thanks!", "author_fullname": "t2_2kxyufad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with failing drives - Options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188p4qh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701471872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there fellow data hoarders. I have 2 failing external drives (a 4TB Seagate Backup Plus, newer, but not newest model - around 2 yrs old; and a very old 750GB Seagate Backup Plus drive - around 8+ yrs old). Opened CrystalDiskInfo with them attached one day and found the 4TB drive has C05 and C06 errors, and the 750GB drive only has a C05 error. But I want to pull the data from them while I&amp;#39;m still able to - before they fail completely. I haven&amp;#39;t done much with them since finding they were throwing caution errors. I did transfer some data from the 4TB drive, but haven&amp;#39;t written to the drives (as far as I know) since then.&lt;/p&gt;\n\n&lt;p&gt;I recently snagged a 2TB WD Black XboxOne Gamedrive and a 6TB Seagate Expansion drive (externally powered). Planning to use the 6TB to back up the data from the aforementioned drives. I have heard that making an image of failing hdd&amp;#39;s, then running recovery tools against the image is the best way to go about things. But, since storage is expensive, I was hoping someone might have some input on how to pull the data directly to the new 6TB drive. Rather than creating a backup image.&lt;/p&gt;\n\n&lt;p&gt;Would my best bet be to enable write protection in Windows Policy Management (to prevent writing being done to the drives allegedly going bad), then using some tool to copy over the data? I will also most likely be copying data over incrementally since copying 4TB via USB will take AGES and I&amp;#39;d rather not leave the house with my PC on and churning away without me being present...&lt;/p&gt;\n\n&lt;p&gt;Anyways, transfer speed isn&amp;#39;t really an issue as I&amp;#39;ll be fine just picking at it until everything I want to save is copied over. I&amp;#39;m also fine with dismantling the drives and connecting them as internal SATA drives if that might increase speed and reliability. Other than the caution errors and only one file on the 4TB drive that I know of causing stalling and failing when attempting to copy data from it, they seem fine otherwise. Once the data is secure I plan to try and fix the bad sectors being reported if possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Just looking for suggestions on the best ways to transfer data from failing to good storage, any apps I could use or methods to reduce further damage to the drives, and any ways to fix the bad sectors and use as disposable storage later - if possible. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188p4qh", "is_robot_indexable": true, "report_reasons": null, "author": "Terrapin2190", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188p4qh/help_with_failing_drives_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188p4qh/help_with_failing_drives_options/", "subreddit_subscribers": 715743, "created_utc": 1701471872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi. So I just bought a WD 18tb Elements drive and shucked it. The drive inside is the WD180EDGZ. Installed it in my desktop. But I couldn't get it to show up in BIOS or in Windows. However, it works fine on the USB 3.0 external dock.\n\nSo using the dock, I used command prompt, cleaned the drive, converted it to GPT, created a primary partition and formatted it to NTFS. Put the drive back into the desktop and connected it to the same SATA data and power connectors that the 8TB WD drive (that this 18TB is replacing) was using . So I know those connections are good. Still nothing.\n\nIt's my understanding that this drive doesn't require the 3.3v mod. So, before I start putzing around with Kapton tape and such I thought I'd ask this community what might be going on here.\n\nPlease any help would be appreciated. Thank you.\n\n\n**UPDATE:** *It's all good now guys. I used the electrical tape fix mentioned by /u/ionhowto below and it worked like magic. It was easier than using Kapton (which is a bear to work with because it's so thin), or some of the other, more invasive, suggestions such as cutting or removing the 3.3v line or buying special connectors. It was simple and very easy to do. Now I have a fully-functioning 18TB drive in my desktop. Thank you all for your help and suggestions. This was a good learning experience for me. Hope this post will help some other folks in the same dilemma as I was in.*", "author_fullname": "t2_kmyx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 18TB Elements (shucked) not showing in BIOS or in Windows Disk Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188m08p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701537006.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701463642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. So I just bought a WD 18tb Elements drive and shucked it. The drive inside is the WD180EDGZ. Installed it in my desktop. But I couldn&amp;#39;t get it to show up in BIOS or in Windows. However, it works fine on the USB 3.0 external dock.&lt;/p&gt;\n\n&lt;p&gt;So using the dock, I used command prompt, cleaned the drive, converted it to GPT, created a primary partition and formatted it to NTFS. Put the drive back into the desktop and connected it to the same SATA data and power connectors that the 8TB WD drive (that this 18TB is replacing) was using . So I know those connections are good. Still nothing.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s my understanding that this drive doesn&amp;#39;t require the 3.3v mod. So, before I start putzing around with Kapton tape and such I thought I&amp;#39;d ask this community what might be going on here.&lt;/p&gt;\n\n&lt;p&gt;Please any help would be appreciated. Thank you.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; &lt;em&gt;It&amp;#39;s all good now guys. I used the electrical tape fix mentioned by &lt;a href=\"/u/ionhowto\"&gt;/u/ionhowto&lt;/a&gt; below and it worked like magic. It was easier than using Kapton (which is a bear to work with because it&amp;#39;s so thin), or some of the other, more invasive, suggestions such as cutting or removing the 3.3v line or buying special connectors. It was simple and very easy to do. Now I have a fully-functioning 18TB drive in my desktop. Thank you all for your help and suggestions. This was a good learning experience for me. Hope this post will help some other folks in the same dilemma as I was in.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "188m08p", "is_robot_indexable": true, "report_reasons": null, "author": "shetum", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188m08p/wd_18tb_elements_shucked_not_showing_in_bios_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188m08p/wd_18tb_elements_shucked_not_showing_in_bios_or/", "subreddit_subscribers": 715743, "created_utc": 1701463642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What's the best source of refurbished or recertified HDDs, similar to https://serverpartdeals.com/ but operating in the EU?", "author_fullname": "t2_z5c6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refurbished/recertified HDDs in the EU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_188i872", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701453762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best source of refurbished or recertified HDDs, similar to &lt;a href=\"https://serverpartdeals.com/\"&gt;https://serverpartdeals.com/&lt;/a&gt; but operating in the EU?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?auto=webp&amp;s=5d1b92aacede02a2cd533065be2e80c8e499f1c0", "width": 2000, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ea6cf72bbc76c5e0dca433670a432a46687ed60", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47e1acbaf47daeac1c2c691978a22ae59bac6606", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=700981f9d49424bce29b2b1957ba8183cc63569c", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfd4a90a6191b9fcefd7fdd84b600dcbe1654d51", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33e6363d54f10b581d3f1e26f43ed03595ac6c3d", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/LGkaIZoLDPH21g_97bH6VokGocsmBANtB9j84_C_I04.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98c5524ec5cb53e7808cd6a2f023c4349ec44464", "width": 1080, "height": 1080}], "variants": {}, "id": "K0YRiTXu10JV568UhWpDWrTiVgxQeLs5_Dz-kQiG_XQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "188i872", "is_robot_indexable": true, "report_reasons": null, "author": "econopl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/188i872/refurbishedrecertified_hdds_in_the_eu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/188i872/refurbishedrecertified_hdds_in_the_eu/", "subreddit_subscribers": 715743, "created_utc": 1701453762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm attempting to understand the following statement in the Snapraid FAQ and not sure if they are instructing to remove the highest number parity from the config file (i.e parity 2 in a 2 parity array) or to remove the parity reference physically highest in the Snapraid.config file? (I e. Parity 1)\n\n\n\"If you wish to remove a parity, you can simply remove the highest \"N-parity\" option from the configuration and then delete the parity file\"", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding/removing parity from existing array? Snapraid", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1893mm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701523347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m attempting to understand the following statement in the Snapraid FAQ and not sure if they are instructing to remove the highest number parity from the config file (i.e parity 2 in a 2 parity array) or to remove the parity reference physically highest in the Snapraid.config file? (I e. Parity 1)&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;If you wish to remove a parity, you can simply remove the highest &amp;quot;N-parity&amp;quot; option from the configuration and then delete the parity file&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1893mm0", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1893mm0/addingremoving_parity_from_existing_array_snapraid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1893mm0/addingremoving_parity_from_existing_array_snapraid/", "subreddit_subscribers": 715743, "created_utc": 1701523347.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}