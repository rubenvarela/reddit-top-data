{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My dad is 65 and was laid off about 6 months ago. He has been working in data engineering for over 40 years. He has about 5-15 interviews a week but cannot land a job. Are his skills just outdated in this field? He doesn\u2019t know AWS or cloud and he cannot develop software, he moves data. He says he is an expert in Azure. I\u2019m at a loss on how to help him. His age is working against him and the longer he goes without a job the more hopeless I feel.\n\nETA: Do these types of jobs care about misdemeanors? He has one from 2019 that may also be working against him", "author_fullname": "t2_qhrlh8cu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is AWS/cloud super necessary nowadays?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18d288d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701986952.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701974393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My dad is 65 and was laid off about 6 months ago. He has been working in data engineering for over 40 years. He has about 5-15 interviews a week but cannot land a job. Are his skills just outdated in this field? He doesn\u2019t know AWS or cloud and he cannot develop software, he moves data. He says he is an expert in Azure. I\u2019m at a loss on how to help him. His age is working against him and the longer he goes without a job the more hopeless I feel.&lt;/p&gt;\n\n&lt;p&gt;ETA: Do these types of jobs care about misdemeanors? He has one from 2019 that may also be working against him&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18d288d", "is_robot_indexable": true, "report_reasons": null, "author": "butyoutolerateit5", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18d288d/is_awscloud_super_necessary_nowadays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18d288d/is_awscloud_super_necessary_nowadays/", "subreddit_subscribers": 144572, "created_utc": 1701974393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there\n\nI've embarked on a unique trajectory to write a book in the open. The book lives on [dedp.online](https://www.dedp.online/), you can also find it on [dataengineeringdesignpatterns.com](https://www.dataengineeringdesignpatterns.com), but I like shorter :).\n\nTo be clear, this is just the beginning. But as I'd like to make it a great book, I'd like to strive for feedback early on, so I decided to share it with you. The latest changes are documented in the changelog.\n\nThe online book is entirely free to read.\n\n## What does the book contain as of now?\n\nYou might wonder if it's worth delving into the book in its current, unfinished form. Well, it depends.There's already a wealth of content. However, the much-anticipated design patterns are still in the works. Suppose that's what you're most excited about. In that case, you'll find the beginnings of exploring the patterns of `Caching` and `Ad-hoc Querying` in the first Convergent Evolution chapter, covering \"BI vs. Semantic Layer vs. Modern OLAP vs. Data Virtualization\".\n\n## The Content\n\nAs of now, the book offers:\n\n* The outline of the book: What it is all about (to keep you excited \ud83d\ude09)\n* Introduction to the Field of Data Engineering with the history and state of DE and challenges along the data engineering lifecycle.\n* Introduction to Data Engineering Design Patterns (DEDP) with the critical starting point with Convergent Evolution and what it is.\n* An overview of some of the patterns and design patterns that forked from the Convergent Evolutions\n* The first is an analysis of four terms that form a Convergent Evolution and their patterns.\n\n## Bonus\n\nIn addition, I've enriched my second brain with sixty new terms, creating a valuable resource for anyone in data engineering. Explore this at [Second Brain](https://brain.ssp.sh/).\n\n## Seeking your early feedback\n\nYour critiques, suggestions, and questions are very welcome. This book has just started, but it may spark some thoughts, ideas, and terms you heard repeatedly. This feedback I'd super appreciate featuring it in my book.\n\nI am looking forward to your honest and constructive feedback. Things will change and hopefully improve.\n\nThanks.", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Living Book on Data Engineering Design Patterns - Feedback Welcome", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cxamc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701960835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve embarked on a unique trajectory to write a book in the open. The book lives on &lt;a href=\"https://www.dedp.online/\"&gt;dedp.online&lt;/a&gt;, you can also find it on &lt;a href=\"https://www.dataengineeringdesignpatterns.com\"&gt;dataengineeringdesignpatterns.com&lt;/a&gt;, but I like shorter :).&lt;/p&gt;\n\n&lt;p&gt;To be clear, this is just the beginning. But as I&amp;#39;d like to make it a great book, I&amp;#39;d like to strive for feedback early on, so I decided to share it with you. The latest changes are documented in the changelog.&lt;/p&gt;\n\n&lt;p&gt;The online book is entirely free to read.&lt;/p&gt;\n\n&lt;h2&gt;What does the book contain as of now?&lt;/h2&gt;\n\n&lt;p&gt;You might wonder if it&amp;#39;s worth delving into the book in its current, unfinished form. Well, it depends.There&amp;#39;s already a wealth of content. However, the much-anticipated design patterns are still in the works. Suppose that&amp;#39;s what you&amp;#39;re most excited about. In that case, you&amp;#39;ll find the beginnings of exploring the patterns of &lt;code&gt;Caching&lt;/code&gt; and &lt;code&gt;Ad-hoc Querying&lt;/code&gt; in the first Convergent Evolution chapter, covering &amp;quot;BI vs. Semantic Layer vs. Modern OLAP vs. Data Virtualization&amp;quot;.&lt;/p&gt;\n\n&lt;h2&gt;The Content&lt;/h2&gt;\n\n&lt;p&gt;As of now, the book offers:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The outline of the book: What it is all about (to keep you excited \ud83d\ude09)&lt;/li&gt;\n&lt;li&gt;Introduction to the Field of Data Engineering with the history and state of DE and challenges along the data engineering lifecycle.&lt;/li&gt;\n&lt;li&gt;Introduction to Data Engineering Design Patterns (DEDP) with the critical starting point with Convergent Evolution and what it is.&lt;/li&gt;\n&lt;li&gt;An overview of some of the patterns and design patterns that forked from the Convergent Evolutions&lt;/li&gt;\n&lt;li&gt;The first is an analysis of four terms that form a Convergent Evolution and their patterns.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Bonus&lt;/h2&gt;\n\n&lt;p&gt;In addition, I&amp;#39;ve enriched my second brain with sixty new terms, creating a valuable resource for anyone in data engineering. Explore this at &lt;a href=\"https://brain.ssp.sh/\"&gt;Second Brain&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h2&gt;Seeking your early feedback&lt;/h2&gt;\n\n&lt;p&gt;Your critiques, suggestions, and questions are very welcome. This book has just started, but it may spark some thoughts, ideas, and terms you heard repeatedly. This feedback I&amp;#39;d super appreciate featuring it in my book.&lt;/p&gt;\n\n&lt;p&gt;I am looking forward to your honest and constructive feedback. Things will change and hopefully improve.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nX11obwAu8YGj6KvhTJsdq9QhuxPp929ZK3jmqLqaUo.jpg?auto=webp&amp;s=8999b2b201ee737220ac1d188d5c323e6f76a3dc", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/nX11obwAu8YGj6KvhTJsdq9QhuxPp929ZK3jmqLqaUo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=96ed6244af0fdd5c5e9cd44bf4f7f472614a3379", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/nX11obwAu8YGj6KvhTJsdq9QhuxPp929ZK3jmqLqaUo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e699f7eee5a3caa3f37e3d341b901c0bd8886390", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/nX11obwAu8YGj6KvhTJsdq9QhuxPp929ZK3jmqLqaUo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebc5b7e6692bd5cce07aa3c4564c3438b4ddc45a", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/nX11obwAu8YGj6KvhTJsdq9QhuxPp929ZK3jmqLqaUo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4160381e4dc35539638f45b8c74163695d37770", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/nX11obwAu8YGj6KvhTJsdq9QhuxPp929ZK3jmqLqaUo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=984840a34173c717d34e4f3860408391f0502dc2", "width": 960, "height": 960}], "variants": {}, "id": "ZxGWGXDif7U8OZj2xEZ9uinreNiBy6Z4EtOatirwLwk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18cxamc", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18cxamc/living_book_on_data_engineering_design_patterns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cxamc/living_book_on_data_engineering_design_patterns/", "subreddit_subscribers": 144572, "created_utc": 1701960835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've spent a decent amount of time indexing and formatting a lot of machine learning datasets that include images, audio, video, and natural language processing datasets.\n\nI know what's worked well for me in the past in the context of deep learning / machine learning projects, so wanted to propose a simple format that might help us standardize a format for the data with a little more structure. Wouldn't say it is ground breaking, but I feel like could be a good practice.\n\nCurious what this sub thinks or people are actually doing.\n\n[https://blog.oxen.ai/suds-a-guide-to-structuring-unstructured-data/](https://blog.oxen.ai/suds-a-guide-to-structuring-unstructured-data/)\n\nFeedback welcome.", "author_fullname": "t2_90isk4fs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does everyone store &amp; format unstructured data? ie: images, video, audio, text?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18da6yz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701995750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve spent a decent amount of time indexing and formatting a lot of machine learning datasets that include images, audio, video, and natural language processing datasets.&lt;/p&gt;\n\n&lt;p&gt;I know what&amp;#39;s worked well for me in the past in the context of deep learning / machine learning projects, so wanted to propose a simple format that might help us standardize a format for the data with a little more structure. Wouldn&amp;#39;t say it is ground breaking, but I feel like could be a good practice.&lt;/p&gt;\n\n&lt;p&gt;Curious what this sub thinks or people are actually doing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.oxen.ai/suds-a-guide-to-structuring-unstructured-data/\"&gt;https://blog.oxen.ai/suds-a-guide-to-structuring-unstructured-data/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feedback welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XeTZ8ji1IEoEe95mwGST5K-LSA_tnd3_AoH9utpbSy4.jpg?auto=webp&amp;s=c943497c778ac2f5e79b9c113170761fe4d2c50c", "width": 1200, "height": 491}, "resolutions": [{"url": "https://external-preview.redd.it/XeTZ8ji1IEoEe95mwGST5K-LSA_tnd3_AoH9utpbSy4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01a48fc0b679ce69a2336e21564659d2342292a2", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/XeTZ8ji1IEoEe95mwGST5K-LSA_tnd3_AoH9utpbSy4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8bb1f1a19a7a6046805ac1c70d92865e08c2d70", "width": 216, "height": 88}, {"url": "https://external-preview.redd.it/XeTZ8ji1IEoEe95mwGST5K-LSA_tnd3_AoH9utpbSy4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a38f8a031a480face37093a7bef69986f8571af", "width": 320, "height": 130}, {"url": "https://external-preview.redd.it/XeTZ8ji1IEoEe95mwGST5K-LSA_tnd3_AoH9utpbSy4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=309cb9ee732c11b7cc9fb6dea3fd14bd36c887ab", "width": 640, "height": 261}, {"url": "https://external-preview.redd.it/XeTZ8ji1IEoEe95mwGST5K-LSA_tnd3_AoH9utpbSy4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f56f0f1ae55753b60764b2c6182e052bdadb841", "width": 960, "height": 392}, {"url": "https://external-preview.redd.it/XeTZ8ji1IEoEe95mwGST5K-LSA_tnd3_AoH9utpbSy4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4207420c0f1788b55a039463dc01d525bd68ac6f", "width": 1080, "height": 441}], "variants": {}, "id": "_gIuBWYP_0TWrDiVxl5DOnSU1XwCl5oooHzEojxCCKA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18da6yz", "is_robot_indexable": true, "report_reasons": null, "author": "FallMindless3563", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18da6yz/how_does_everyone_store_format_unstructured_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18da6yz/how_does_everyone_store_format_unstructured_data/", "subreddit_subscribers": 144572, "created_utc": 1701995750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently joined a new company, and the data engineering team I'm working with exclusively does their development in AWS EC2 terminal instances. They complain about the inefficiency this creates, and indeed I've found their delivery rather slow, but our team's senior director has said that pipeline development in the terminal is the only way to truly ensure data security (we deal with a lot of HIPAA-covered PII). How true is this? To me, it doesn't make much sense that an IDE automatically would introduce data security risks, but admittedly I don't have much information security knowledge. Are there best practices for balancing security and ease of development that I can cite in an argument to leadership?  ", "author_fullname": "t2_16x914", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this a normal data security measure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18d7lsk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701988388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently joined a new company, and the data engineering team I&amp;#39;m working with exclusively does their development in AWS EC2 terminal instances. They complain about the inefficiency this creates, and indeed I&amp;#39;ve found their delivery rather slow, but our team&amp;#39;s senior director has said that pipeline development in the terminal is the only way to truly ensure data security (we deal with a lot of HIPAA-covered PII). How true is this? To me, it doesn&amp;#39;t make much sense that an IDE automatically would introduce data security risks, but admittedly I don&amp;#39;t have much information security knowledge. Are there best practices for balancing security and ease of development that I can cite in an argument to leadership?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18d7lsk", "is_robot_indexable": true, "report_reasons": null, "author": "RosmarysBabyBjorn", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18d7lsk/is_this_a_normal_data_security_measure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18d7lsk/is_this_a_normal_data_security_measure/", "subreddit_subscribers": 144572, "created_utc": 1701988388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI've been setting up DAGs for my company for a while now using mostly Dagster, Pandas, BigQuery and DBT. It's been working OK but I keep feeling like the by far most common bug/timesink I encounter are debugging pandas type conversions before it enters into BQ. Great fun.\n\nThe problem I have right now is that I'm working with integer data that can be null in the API, but when I start working with pandas it automatically casts this to floats and things start getting a bit funky. Integer ID's suddenly have an extra .0 after it, or rows that where empty suddenly contain zeros or something. \n\nWhat am I missing? What do you guys like working with? Or is pandas a good solution and there are ways to improve the workflow? Thanks!", "author_fullname": "t2_t1xeu37", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I use instead of Pandas for data ingestion/cleaning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cxnsk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701961848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been setting up DAGs for my company for a while now using mostly Dagster, Pandas, BigQuery and DBT. It&amp;#39;s been working OK but I keep feeling like the by far most common bug/timesink I encounter are debugging pandas type conversions before it enters into BQ. Great fun.&lt;/p&gt;\n\n&lt;p&gt;The problem I have right now is that I&amp;#39;m working with integer data that can be null in the API, but when I start working with pandas it automatically casts this to floats and things start getting a bit funky. Integer ID&amp;#39;s suddenly have an extra .0 after it, or rows that where empty suddenly contain zeros or something. &lt;/p&gt;\n\n&lt;p&gt;What am I missing? What do you guys like working with? Or is pandas a good solution and there are ways to improve the workflow? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18cxnsk", "is_robot_indexable": true, "report_reasons": null, "author": "Fox_News_Shill", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cxnsk/what_should_i_use_instead_of_pandas_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cxnsk/what_should_i_use_instead_of_pandas_for_data/", "subreddit_subscribers": 144572, "created_utc": 1701961848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bxjjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting Started With PyFlink on Kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18cz7r2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bhX9PYCJCqpsJj2lh-PZgF2Gq5bdY8Oz5muM3t0qN-A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701966114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "decodable.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.decodable.co/blog/getting-started-with-pyflink-on-kubernetes", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pU_0dlJ8oH0FWaQsTgztQ6T7uOccZw-K1RDvyoDqfGE.jpg?auto=webp&amp;s=831389fa48a24495f60e6f68edd63d48ad8f517f", "width": 1024, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/pU_0dlJ8oH0FWaQsTgztQ6T7uOccZw-K1RDvyoDqfGE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=80e15bb4ef364f9bcd5852161edb07ba9e6ba065", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/pU_0dlJ8oH0FWaQsTgztQ6T7uOccZw-K1RDvyoDqfGE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=32c209dab3bd3e2f97d047e2652683792b4059d2", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/pU_0dlJ8oH0FWaQsTgztQ6T7uOccZw-K1RDvyoDqfGE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b0dd9e2160c381e9ba497f84e8f5ecfe64caf56", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/pU_0dlJ8oH0FWaQsTgztQ6T7uOccZw-K1RDvyoDqfGE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4ee0cdf0c6b83418250a39d1a01e1c1ed77851b", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/pU_0dlJ8oH0FWaQsTgztQ6T7uOccZw-K1RDvyoDqfGE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d151a97f053c94523341036babb61028053e444", "width": 960, "height": 960}], "variants": {}, "id": "1OkrA8L99fPhROo50RLnWcieIM4ToyQ19H7HTlAu8s8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18cz7r2", "is_robot_indexable": true, "report_reasons": null, "author": "gunnarmorling", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cz7r2/getting_started_with_pyflink_on_kubernetes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.decodable.co/blog/getting-started-with-pyflink-on-kubernetes", "subreddit_subscribers": 144572, "created_utc": 1701966114.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m teaching a data warehousing course at a state university this next year as an adjunct professor. I work full time as a data engineer. \n\nWe are currently having students build a relational database on an on prem SQL Server that the university hosts as a quick refresher of what they learned in the previous class. For the rest of the class, they use Snowflake and build a kimball style data warehouse with a different dataset. I would love to use the same dataset and somehow ETL the relational database into Snowflake. With limited time in the course and already covering lots of material, i don\u2019t want to spend a ton of time moving the data in a complicated way. Anybody have any ideas for a free ETL tool that can do this? I just want to extract and load and it would be a one time load.\n\nThoughts I\u2019ve already had: Stitch (14 day free trial), talend (outdated but free).", "author_fullname": "t2_bdrvjr4t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehousing Class ETL Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dg713", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702015131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m teaching a data warehousing course at a state university this next year as an adjunct professor. I work full time as a data engineer. &lt;/p&gt;\n\n&lt;p&gt;We are currently having students build a relational database on an on prem SQL Server that the university hosts as a quick refresher of what they learned in the previous class. For the rest of the class, they use Snowflake and build a kimball style data warehouse with a different dataset. I would love to use the same dataset and somehow ETL the relational database into Snowflake. With limited time in the course and already covering lots of material, i don\u2019t want to spend a ton of time moving the data in a complicated way. Anybody have any ideas for a free ETL tool that can do this? I just want to extract and load and it would be a one time load.&lt;/p&gt;\n\n&lt;p&gt;Thoughts I\u2019ve already had: Stitch (14 day free trial), talend (outdated but free).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18dg713", "is_robot_indexable": true, "report_reasons": null, "author": "WarthogSwimming8862", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dg713/data_warehousing_class_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dg713/data_warehousing_class_etl_tool/", "subreddit_subscribers": 144572, "created_utc": 1702015131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m tasked with modernizing an existing on-prem sql server dw (tabular cube) to cloud dw. Primary drivers are the typical ones including aging hardware and performance issues.\n\nThe data sources are mainly on premise SQL servers, but a number of sources are brought in via API.\n\nThe data is simple as is the end user requirements. Think typical sales and labour reporting. The foot print is small too\u2026 500gb in total (which includes all history going back 20 years). The dw is currently built using Kimball approach.\n\nThere is some desire to eventually use ai and ml to do more advanced analytics, and I think there\u2019s a ton of value going this direction.\n\nI\u2019ve built a POC using fivetran to load raw data into a snowflake db instance and I\u2019ve built views that act similarly to the cubes facts and dimensions using a kimball style approach with clustered indexes on the fields I join. I\u2019ve got users testing out sigma, tableau and PowerBI.\n\nSome questions:\n\n1. Am I oversimplifying this?\n2. Should I be using materialized tables vs views?\n3. What is best practice for dw design in terms of the relational data?\n4. Anything else I\u2019m missing?", "author_fullname": "t2_nuco2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving to a cloud dw", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dc5i2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702001773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m tasked with modernizing an existing on-prem sql server dw (tabular cube) to cloud dw. Primary drivers are the typical ones including aging hardware and performance issues.&lt;/p&gt;\n\n&lt;p&gt;The data sources are mainly on premise SQL servers, but a number of sources are brought in via API.&lt;/p&gt;\n\n&lt;p&gt;The data is simple as is the end user requirements. Think typical sales and labour reporting. The foot print is small too\u2026 500gb in total (which includes all history going back 20 years). The dw is currently built using Kimball approach.&lt;/p&gt;\n\n&lt;p&gt;There is some desire to eventually use ai and ml to do more advanced analytics, and I think there\u2019s a ton of value going this direction.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve built a POC using fivetran to load raw data into a snowflake db instance and I\u2019ve built views that act similarly to the cubes facts and dimensions using a kimball style approach with clustered indexes on the fields I join. I\u2019ve got users testing out sigma, tableau and PowerBI.&lt;/p&gt;\n\n&lt;p&gt;Some questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Am I oversimplifying this?&lt;/li&gt;\n&lt;li&gt;Should I be using materialized tables vs views?&lt;/li&gt;\n&lt;li&gt;What is best practice for dw design in terms of the relational data?&lt;/li&gt;\n&lt;li&gt;Anything else I\u2019m missing?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18dc5i2", "is_robot_indexable": true, "report_reasons": null, "author": "2000gt", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dc5i2/moving_to_a_cloud_dw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dc5i2/moving_to_a_cloud_dw/", "subreddit_subscribers": 144572, "created_utc": 1702001773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey, I am in outsourcing company and I was deployed in the Data Engineering Devision in a big tech. Since I didn't have CS degree and this is my first engineering job. I am assigned to a Data Operation Unit. My main task is to transfer CSV files to other devices or tableau. Sometimes I change format excel files to CSV and make sure everything transferred correctly. I barely use Tableau and I rarely write any codes. My team member is writing codes with Spark and write some codes on Jupyter Notebook.  My other task is to check if there are some errors in Airflow and if there are some, my responsibility is to mention and tell that in slack and notify errors to my team.  \n\nI am curious if this is normal for anyone to experience like this. Maybe in the future, they said I might able to touch codes in airflows but I don't exactly know when that happens. \n\nThese being said, I am learning quite lot. I am learning AWS (KES, S3 and EFS), shell commands, crontab, GoogleSpreadsheet, airflows and basic Tableau navigation.. I am really interested in what you guys think and do you think this is a good deal and what kinda career can I purse after this?  Thanks", "author_fullname": "t2_4xluce6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need your guys opinion...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18d93dl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701994387.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701992510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey, I am in outsourcing company and I was deployed in the Data Engineering Devision in a big tech. Since I didn&amp;#39;t have CS degree and this is my first engineering job. I am assigned to a Data Operation Unit. My main task is to transfer CSV files to other devices or tableau. Sometimes I change format excel files to CSV and make sure everything transferred correctly. I barely use Tableau and I rarely write any codes. My team member is writing codes with Spark and write some codes on Jupyter Notebook.  My other task is to check if there are some errors in Airflow and if there are some, my responsibility is to mention and tell that in slack and notify errors to my team.  &lt;/p&gt;\n\n&lt;p&gt;I am curious if this is normal for anyone to experience like this. Maybe in the future, they said I might able to touch codes in airflows but I don&amp;#39;t exactly know when that happens. &lt;/p&gt;\n\n&lt;p&gt;These being said, I am learning quite lot. I am learning AWS (KES, S3 and EFS), shell commands, crontab, GoogleSpreadsheet, airflows and basic Tableau navigation.. I am really interested in what you guys think and do you think this is a good deal and what kinda career can I purse after this?  Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18d93dl", "is_robot_indexable": true, "report_reasons": null, "author": "lastsamurai0414", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18d93dl/i_need_your_guys_opinion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18d93dl/i_need_your_guys_opinion/", "subreddit_subscribers": 144572, "created_utc": 1701992510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI work as a mix of being a data engineer/data analyst and I recently got more into API integration. I was just wondering, if there are any best practices on how.to access data from an API and integrate it into a dwh.\n\nFrom what I found so far on the web, it seems to be either:\nAuthentifcate, Store the json object raw, flatten in the dwh VS. Authentifcate, flatten the json and then store it.\nOn this part I am quite clear and I think it depends on the usecase? However, I am am unsure about the actual way of implementing it, for example in python for a given endpoint:\n\nPy Request (or similar), filter json, write to df, df to SQL/CSV, repeat for pagination.\nAlso where would on put unit tests? During the actual part of accessing the data?\n\nThank you in advance, any tips are welcome.", "author_fullname": "t2_yobj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice for API data integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dichi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702023901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I work as a mix of being a data engineer/data analyst and I recently got more into API integration. I was just wondering, if there are any best practices on how.to access data from an API and integrate it into a dwh.&lt;/p&gt;\n\n&lt;p&gt;From what I found so far on the web, it seems to be either:\nAuthentifcate, Store the json object raw, flatten in the dwh VS. Authentifcate, flatten the json and then store it.\nOn this part I am quite clear and I think it depends on the usecase? However, I am am unsure about the actual way of implementing it, for example in python for a given endpoint:&lt;/p&gt;\n\n&lt;p&gt;Py Request (or similar), filter json, write to df, df to SQL/CSV, repeat for pagination.\nAlso where would on put unit tests? During the actual part of accessing the data?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance, any tips are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18dichi", "is_robot_indexable": true, "report_reasons": null, "author": "lschozar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dichi/best_practice_for_api_data_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dichi/best_practice_for_api_data_integration/", "subreddit_subscribers": 144572, "created_utc": 1702023901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, a brief blurb about my background:\n\nAmerican born, but built my career in data across East Asia, getting a late start after switching out of academia/education (I was just working to travel essentially, in my early and mid twenties). I began in the data field about 7 years ago, as an intern, then a data scientist, then a data engineer, then senior, then manager of a team. I gained a ton of knowledge and skills, but my background is highly heterogenous, and I'm a slow thinker and get distracted if someone is staring at me as I try to think through a problem, and so tend to fail live coding sessions (though my on-the-job work more than speaks for itself, with complex systems built, end to end pipelines inclusive of CICD, governance for data sets and resource consumption, strong spark, server, network, and cloud engineering knowledge, but no certifications to \"prove\" it). \n\nI applied for two sure fit positions but got declined for both, the first due to the live coding session (I was sleep deprived due to taking it at like 5am due to time zone differences, that didn't help), then second due perhaps being a manager currently, and getting interviewed by someone at my current organizational level who thought maybe I'm too senior for the kind of grunt work role.\n\nIs it entirely pointless to apply if I can't live code for the life of me, or if I don't have an in in a company, even if I have years of experience at this point and pretty vast knowledge? Or, is applying for IC or senior engineer roles a serious res flag for someone managing a small department of engineers (~20 people plus vendors)?", "author_fullname": "t2_ejt24ok7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice in job hunting in the USA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18d67f5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701984687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, a brief blurb about my background:&lt;/p&gt;\n\n&lt;p&gt;American born, but built my career in data across East Asia, getting a late start after switching out of academia/education (I was just working to travel essentially, in my early and mid twenties). I began in the data field about 7 years ago, as an intern, then a data scientist, then a data engineer, then senior, then manager of a team. I gained a ton of knowledge and skills, but my background is highly heterogenous, and I&amp;#39;m a slow thinker and get distracted if someone is staring at me as I try to think through a problem, and so tend to fail live coding sessions (though my on-the-job work more than speaks for itself, with complex systems built, end to end pipelines inclusive of CICD, governance for data sets and resource consumption, strong spark, server, network, and cloud engineering knowledge, but no certifications to &amp;quot;prove&amp;quot; it). &lt;/p&gt;\n\n&lt;p&gt;I applied for two sure fit positions but got declined for both, the first due to the live coding session (I was sleep deprived due to taking it at like 5am due to time zone differences, that didn&amp;#39;t help), then second due perhaps being a manager currently, and getting interviewed by someone at my current organizational level who thought maybe I&amp;#39;m too senior for the kind of grunt work role.&lt;/p&gt;\n\n&lt;p&gt;Is it entirely pointless to apply if I can&amp;#39;t live code for the life of me, or if I don&amp;#39;t have an in in a company, even if I have years of experience at this point and pretty vast knowledge? Or, is applying for IC or senior engineer roles a serious res flag for someone managing a small department of engineers (~20 people plus vendors)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18d67f5", "is_robot_indexable": true, "report_reasons": null, "author": "suterebaiiiii", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18d67f5/advice_in_job_hunting_in_the_usa/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18d67f5/advice_in_job_hunting_in_the_usa/", "subreddit_subscribers": 144572, "created_utc": 1701984687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Background**\n\nMy enterprise is undergoing a massive modernization where we are moving our 14+ databases from 2 On Prem servers and SAS to Azure Synapse pipelines and delta+serverless /dedicated pools for ( bronze/silver/gold data layers).\n\nEarly on, it was decided that all of the 20/30 databases would be orchestrated by 1 synapse workspace, 1 storage account and 1 dedicated pool.\n\nWe built 3 environments ( dev, uat, prod) and 3 CI/CD pipelines ( workspace, serverless, dedicated SQL) using YAML ( and PowerShell for serverless) to build and release to the respective environments using AzureDevOps.\n\n**Problem**\n\nThe architecture has had great success so far. However, now that the lift and shift is really picking up momentum, we have 12+ devs working collaboratively on different projects, with the codes going to the same code base. Our release cycle changed from once a week to on demand due to the high number of requests.\n\nNow the Development managers are asking that we allow partial deployments of the code, as UAT cycles dont perfectly overlap and they dont want code for certain projects getting stuck in UAT for longer than it needs to, because one of the UAT projects is not ready to go to prod. They are asking for partial deployments.\n\n**My perspective**\n\nI have strongly opposed this, and explained that this isnt possible without manual script executions and live env changes (in technical terms). \n\n**Questions**\n\n* Has anyone gone through this kind of request?\n*  Is there actually a way to run partial deployments that I'm not aware of?\n*  How do we explain this in business terms? \n* What alternatives can be provided to help the dev teams deliver without breaking the CI/CD validation benefits ?", "author_fullname": "t2_f8hvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Partial Deployments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18d13h9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701971227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My enterprise is undergoing a massive modernization where we are moving our 14+ databases from 2 On Prem servers and SAS to Azure Synapse pipelines and delta+serverless /dedicated pools for ( bronze/silver/gold data layers).&lt;/p&gt;\n\n&lt;p&gt;Early on, it was decided that all of the 20/30 databases would be orchestrated by 1 synapse workspace, 1 storage account and 1 dedicated pool.&lt;/p&gt;\n\n&lt;p&gt;We built 3 environments ( dev, uat, prod) and 3 CI/CD pipelines ( workspace, serverless, dedicated SQL) using YAML ( and PowerShell for serverless) to build and release to the respective environments using AzureDevOps.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The architecture has had great success so far. However, now that the lift and shift is really picking up momentum, we have 12+ devs working collaboratively on different projects, with the codes going to the same code base. Our release cycle changed from once a week to on demand due to the high number of requests.&lt;/p&gt;\n\n&lt;p&gt;Now the Development managers are asking that we allow partial deployments of the code, as UAT cycles dont perfectly overlap and they dont want code for certain projects getting stuck in UAT for longer than it needs to, because one of the UAT projects is not ready to go to prod. They are asking for partial deployments.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My perspective&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have strongly opposed this, and explained that this isnt possible without manual script executions and live env changes (in technical terms). &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Has anyone gone through this kind of request?&lt;/li&gt;\n&lt;li&gt; Is there actually a way to run partial deployments that I&amp;#39;m not aware of?&lt;/li&gt;\n&lt;li&gt; How do we explain this in business terms? &lt;/li&gt;\n&lt;li&gt;What alternatives can be provided to help the dev teams deliver without breaking the CI/CD validation benefits ?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18d13h9", "is_robot_indexable": true, "report_reasons": null, "author": "Mefsha5", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18d13h9/partial_deployments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18d13h9/partial_deployments/", "subreddit_subscribers": 144572, "created_utc": 1701971227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been messing around with Meltano recently and have been quite impressed. The yaml file structure sells it for me and baking in DBT directly into the same repo is pretty slick as well.\n\nOne thing that is bugging the heck out of me is the lack of a clean way to load assets from Meltano to be orchestrated by Dagster. The online help I have found has been outdated or bug ridden. I know Meltano suggests using Airflow but I want to avoid using Airflow as much as possible.\n\nAnyone have recommendations for a repo they have found similar to the above?", "author_fullname": "t2_d56f4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demo Meltano/DBT/Dagster Repo or Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cyfc8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701963959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been messing around with Meltano recently and have been quite impressed. The yaml file structure sells it for me and baking in DBT directly into the same repo is pretty slick as well.&lt;/p&gt;\n\n&lt;p&gt;One thing that is bugging the heck out of me is the lack of a clean way to load assets from Meltano to be orchestrated by Dagster. The online help I have found has been outdated or bug ridden. I know Meltano suggests using Airflow but I want to avoid using Airflow as much as possible.&lt;/p&gt;\n\n&lt;p&gt;Anyone have recommendations for a repo they have found similar to the above?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18cyfc8", "is_robot_indexable": true, "report_reasons": null, "author": "mowmail", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cyfc8/demo_meltanodbtdagster_repo_or_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cyfc8/demo_meltanodbtdagster_repo_or_project/", "subreddit_subscribers": 144572, "created_utc": 1701963959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I have created some data catalog tables in Glue using Terraform but I am now looking for advice on how to maintain them ? Is it a good practice to manage Glue tables using IaC ? If not, what would you advice ? ", "author_fullname": "t2_i9ozc53a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to maintain Glue catalog from source code ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dibu7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702023820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I have created some data catalog tables in Glue using Terraform but I am now looking for advice on how to maintain them ? Is it a good practice to manage Glue tables using IaC ? If not, what would you advice ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18dibu7", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Evidence2331", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dibu7/how_to_maintain_glue_catalog_from_source_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dibu7/how_to_maintain_glue_catalog_from_source_code/", "subreddit_subscribers": 144572, "created_utc": 1702023820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey , wondering who has used Snowflake\u2019s Snowpipe feature? Can you vouch for it, or have you had issues?\n\nWe have an extensive glue job and step functions setup for simply copying CSV\u2019s to S3 to Snowflake, but it causes grief, and unnecessary complexity in the debugging process when source CSV\u2019s change etc, causing IT incidents. \n\nWondering if Snowpipe can help by getting the CSV\u2019s directly and how simple it is to implement?\n\nWould love to hear your thoughts on the tool and if it\u2019s been helpful for you at all.", "author_fullname": "t2_jkvzr8r3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with Snowpipe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18dgo8q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702016943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey , wondering who has used Snowflake\u2019s Snowpipe feature? Can you vouch for it, or have you had issues?&lt;/p&gt;\n\n&lt;p&gt;We have an extensive glue job and step functions setup for simply copying CSV\u2019s to S3 to Snowflake, but it causes grief, and unnecessary complexity in the debugging process when source CSV\u2019s change etc, causing IT incidents. &lt;/p&gt;\n\n&lt;p&gt;Wondering if Snowpipe can help by getting the CSV\u2019s directly and how simple it is to implement?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts on the tool and if it\u2019s been helpful for you at all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18dgo8q", "is_robot_indexable": true, "report_reasons": null, "author": "pbower2049", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18dgo8q/experience_with_snowpipe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18dgo8q/experience_with_snowpipe/", "subreddit_subscribers": 144572, "created_utc": 1702016943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have been working with data for 2.5 years, including 6 months as a Data Engineer trainee and 2 years as a Junior Data Engineer. Although I haven't completed my college degree yet (with another 2 years to go), I am actively seeking local/global remote opportunities in this field. I am based in Brazil but possess advanced English language skills.\n\nI'm curious to know whether it's too early for me to explore such opportunities or if there is a chance for me to secure a global remote position. Any tips or advice for my future career would be greatly appreciated.\n\nCurrently, my work revolves around Python, Pyspark, AWS, SQL, DataStudio(Looker), and Power BI. While my portfolio may not be strong, I'm struggling to come up with project ideas to enhance it. Any suggestions on potential projects to kickstart this improvement process would be valuable.", "author_fullname": "t2_8fsha4lla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help with exploring Data Jobs, Seeking Tips and Project Ideas!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18d2vf4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701976004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been working with data for 2.5 years, including 6 months as a Data Engineer trainee and 2 years as a Junior Data Engineer. Although I haven&amp;#39;t completed my college degree yet (with another 2 years to go), I am actively seeking local/global remote opportunities in this field. I am based in Brazil but possess advanced English language skills.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to know whether it&amp;#39;s too early for me to explore such opportunities or if there is a chance for me to secure a global remote position. Any tips or advice for my future career would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Currently, my work revolves around Python, Pyspark, AWS, SQL, DataStudio(Looker), and Power BI. While my portfolio may not be strong, I&amp;#39;m struggling to come up with project ideas to enhance it. Any suggestions on potential projects to kickstart this improvement process would be valuable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18d2vf4", "is_robot_indexable": true, "report_reasons": null, "author": "Dry-Resident-4200", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18d2vf4/i_need_help_with_exploring_data_jobs_seeking_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18d2vf4/i_need_help_with_exploring_data_jobs_seeking_tips/", "subreddit_subscribers": 144572, "created_utc": 1701976004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to be able to cross-reference events from different audit subsystems with alerts from different security solutions (e.g. endpoint protection controls like EDR or network protection controls like IPS), and am wondering if DuckDB + dbt would be a good combination of tools to use for this purpose.\n\nI'd like to be able to perform event correlation either online or offline, and if possible, don't want to spin up a database server so test results can be broadly shared through whatever channels make sense to the end user (e.g. a local directory, SMB, S3, OneDrive, etc.)\n\nOriginally, I was thinking of doing this in Redis, or using Trino, but, then I discovered DuckDB, and, I like it.\n\nIf I were to use DuckDB + dbt to perform event correlation, how could I actually trigger the data pipelines?\n\nI'm thinking of using something like inotify locally, and something like AWS Lambda, S3, and SNS in AWS, but, maybe there's a better option?\n\nI like the idea of using SeaweedFS as an intermediate layer with object write [notifications](https://github.com/seaweedfs/seaweedfs/tree/master/weed/notification) going to SQS, RabbitMQ, or a local file, which could also allow me to observe the changes to different files through a metric collection layer like Prometheus and Grafana.\n\nWhen it comes to orchestration, Dagster seems cool, but, I'm not sure if it'd work locally, in an airgapped environment, etc. (I'm working with ransomware, worms, etc.).\n\nThe pipeline is basically:\n\n* Stream events from different audit sources into a sensor written in Go\n* Normalize the events within the sensor (e.g. translate events from Windows Event Log into process start/stop events)\n* Write events to a directory\n* Stream alerts from different alert sources using serverless functions written in Python\n* Write alerts to a directory\n* Create a materialized view that contains a list of all processes forming a given process tree\n* Cross-reference alerts with any process in the process tree to determine which processes produced alerts\n\nIt's necessary to reconstruct the process tree so I can identify alerts related to the descendants of a given child process (e.g. a command that detonates ransomware).", "author_fullname": "t2_lwr1wvz8q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB + dbt for a serverless event correlation pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cymev", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701965532.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701964515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to be able to cross-reference events from different audit subsystems with alerts from different security solutions (e.g. endpoint protection controls like EDR or network protection controls like IPS), and am wondering if DuckDB + dbt would be a good combination of tools to use for this purpose.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to be able to perform event correlation either online or offline, and if possible, don&amp;#39;t want to spin up a database server so test results can be broadly shared through whatever channels make sense to the end user (e.g. a local directory, SMB, S3, OneDrive, etc.)&lt;/p&gt;\n\n&lt;p&gt;Originally, I was thinking of doing this in Redis, or using Trino, but, then I discovered DuckDB, and, I like it.&lt;/p&gt;\n\n&lt;p&gt;If I were to use DuckDB + dbt to perform event correlation, how could I actually trigger the data pipelines?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of using something like inotify locally, and something like AWS Lambda, S3, and SNS in AWS, but, maybe there&amp;#39;s a better option?&lt;/p&gt;\n\n&lt;p&gt;I like the idea of using SeaweedFS as an intermediate layer with object write &lt;a href=\"https://github.com/seaweedfs/seaweedfs/tree/master/weed/notification\"&gt;notifications&lt;/a&gt; going to SQS, RabbitMQ, or a local file, which could also allow me to observe the changes to different files through a metric collection layer like Prometheus and Grafana.&lt;/p&gt;\n\n&lt;p&gt;When it comes to orchestration, Dagster seems cool, but, I&amp;#39;m not sure if it&amp;#39;d work locally, in an airgapped environment, etc. (I&amp;#39;m working with ransomware, worms, etc.).&lt;/p&gt;\n\n&lt;p&gt;The pipeline is basically:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Stream events from different audit sources into a sensor written in Go&lt;/li&gt;\n&lt;li&gt;Normalize the events within the sensor (e.g. translate events from Windows Event Log into process start/stop events)&lt;/li&gt;\n&lt;li&gt;Write events to a directory&lt;/li&gt;\n&lt;li&gt;Stream alerts from different alert sources using serverless functions written in Python&lt;/li&gt;\n&lt;li&gt;Write alerts to a directory&lt;/li&gt;\n&lt;li&gt;Create a materialized view that contains a list of all processes forming a given process tree&lt;/li&gt;\n&lt;li&gt;Cross-reference alerts with any process in the process tree to determine which processes produced alerts&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s necessary to reconstruct the process tree so I can identify alerts related to the descendants of a given child process (e.g. a command that detonates ransomware).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Lbat6Ay_WjUPuejj9f72QN-qLCNXvIl2Om2nGp4wC8I.jpg?auto=webp&amp;s=406f3edc1cce3e62958688e31cec3561632ea19a", "width": 437, "height": 437}, "resolutions": [{"url": "https://external-preview.redd.it/Lbat6Ay_WjUPuejj9f72QN-qLCNXvIl2Om2nGp4wC8I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=239fcbfe6466fb3698553bdbb13e1103ac7f3f74", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Lbat6Ay_WjUPuejj9f72QN-qLCNXvIl2Om2nGp4wC8I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=436cab45737d55008f173999f33f928c46311c19", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Lbat6Ay_WjUPuejj9f72QN-qLCNXvIl2Om2nGp4wC8I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7698b035832dcb22b485c2a0dc32213e0680efa5", "width": 320, "height": 320}], "variants": {}, "id": "FQeVW-uG_jVwW31yLJzth5utdtMXvjEul4hfAofUbgw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18cymev", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Importance-1605", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cymev/duckdb_dbt_for_a_serverless_event_correlation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cymev/duckdb_dbt_for_a_serverless_event_correlation/", "subreddit_subscribers": 144572, "created_utc": 1701964515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context \nIn the entirety of my experience I've always found that organisation of data in warehouse using schema(dataset in BQ) was done using schema name same as a business usecase, or team ownership, or the most generic name for schema that became ambiguous immediately the next day. The commonality across all the schemas were that they did not age well with time; Business usecase grew bigger or shaped up into something new, teams folded or have multiple sub categories, etc. In all, the schema space seemed like an encyclopedia of historical decisions which have now turned into 'domain knowledge'. \n\nQuestion \nMy controversial view is when naming a schema we should only use vocabulary that can hold true for the lifetime of the tables, because it is impossible to keep updating the schema name as per relevance. Rather offload any meaningful grouping ex : team / business usecase to tags. I'm currently have this once in a life time opportunity to define the schema space in my org and I am tempted to keep it flat - single schema for all structured data. Looking for some feedback from the group on this controversial decision.  \n\n\nExtra Information \nI've put the guard rail that only production spark jobs and applications can write to the schema above. I intend to double down on good cataloging practices on these tables - make data that matters trust worthy. \n\nFor all adhoc analysis purposes I am providing a 'playground' with no rules on data organisation. We intend to control cost here by ensuring short retention on playground.\n\nUpdate: one obvious challenge might be in using same bucket for entire warehouse. I'm discounting that as a problem for the sake of the discussion. IMO this can be handled in my org as we have control over table creation centrally.", "author_fullname": "t2_ug6a7t7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use of schemas in modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cuxto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701954158.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701953521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context \nIn the entirety of my experience I&amp;#39;ve always found that organisation of data in warehouse using schema(dataset in BQ) was done using schema name same as a business usecase, or team ownership, or the most generic name for schema that became ambiguous immediately the next day. The commonality across all the schemas were that they did not age well with time; Business usecase grew bigger or shaped up into something new, teams folded or have multiple sub categories, etc. In all, the schema space seemed like an encyclopedia of historical decisions which have now turned into &amp;#39;domain knowledge&amp;#39;. &lt;/p&gt;\n\n&lt;p&gt;Question \nMy controversial view is when naming a schema we should only use vocabulary that can hold true for the lifetime of the tables, because it is impossible to keep updating the schema name as per relevance. Rather offload any meaningful grouping ex : team / business usecase to tags. I&amp;#39;m currently have this once in a life time opportunity to define the schema space in my org and I am tempted to keep it flat - single schema for all structured data. Looking for some feedback from the group on this controversial decision.  &lt;/p&gt;\n\n&lt;p&gt;Extra Information \nI&amp;#39;ve put the guard rail that only production spark jobs and applications can write to the schema above. I intend to double down on good cataloging practices on these tables - make data that matters trust worthy. &lt;/p&gt;\n\n&lt;p&gt;For all adhoc analysis purposes I am providing a &amp;#39;playground&amp;#39; with no rules on data organisation. We intend to control cost here by ensuring short retention on playground.&lt;/p&gt;\n\n&lt;p&gt;Update: one obvious challenge might be in using same bucket for entire warehouse. I&amp;#39;m discounting that as a problem for the sake of the discussion. IMO this can be handled in my org as we have control over table creation centrally.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18cuxto", "is_robot_indexable": true, "report_reasons": null, "author": "that-pipe-dream", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cuxto/use_of_schemas_in_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cuxto/use_of_schemas_in_modeling/", "subreddit_subscribers": 144572, "created_utc": 1701953521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I will give you a background. We are agency and we are integrating Customer Data Platforms like Bloomreach etc. It has its own API on fronend and backend, but when we send documentation and everything we need to integrate project, DEVs on our client side dont know much about it and often they made a solution which is bad - sending bad data to our platform, incorrect types etc and it is real-time, so we cannot change that later.\n\nI want to code tracking by myself, but to make a tracking solution - sending events like cart\\_update, view\\_item etc. Client is happy with this idea, but i dont know how to start and what to do. I think I need to make our new application that will send that, but I dont know how to because all the logic is in code maintained by their DEVS and i willl not be able to change their code.", "author_fullname": "t2_31exp9bt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New service for our clients - behavior tracking solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18d3ijh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701977713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I will give you a background. We are agency and we are integrating Customer Data Platforms like Bloomreach etc. It has its own API on fronend and backend, but when we send documentation and everything we need to integrate project, DEVs on our client side dont know much about it and often they made a solution which is bad - sending bad data to our platform, incorrect types etc and it is real-time, so we cannot change that later.&lt;/p&gt;\n\n&lt;p&gt;I want to code tracking by myself, but to make a tracking solution - sending events like cart_update, view_item etc. Client is happy with this idea, but i dont know how to start and what to do. I think I need to make our new application that will send that, but I dont know how to because all the logic is in code maintained by their DEVS and i willl not be able to change their code.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18d3ijh", "is_robot_indexable": true, "report_reasons": null, "author": "Sonny-Orkidea", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18d3ijh/new_service_for_our_clients_behavior_tracking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18d3ijh/new_service_for_our_clients_behavior_tracking/", "subreddit_subscribers": 144572, "created_utc": 1701977713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you all handle non standard features  in your pipelines? I am doing a mix of if else statements, config files and parameters stored in db. Still everything seems overwhelming.", "author_fullname": "t2_l5b7eg3m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non standard things in your pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18d0u5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701970519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you all handle non standard features  in your pipelines? I am doing a mix of if else statements, config files and parameters stored in db. Still everything seems overwhelming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18d0u5s", "is_robot_indexable": true, "report_reasons": null, "author": "BetResponsible4418", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18d0u5s/non_standard_things_in_your_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18d0u5s/non_standard_things_in_your_pipeline/", "subreddit_subscribers": 144572, "created_utc": 1701970519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So i noticed a lot of companies invent role names that have close requirements as data engineer", "author_fullname": "t2_hxue1umo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What job keywords or names that you saw on LinkedIn and they are basically data engineering related?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18cvv43", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701956585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i noticed a lot of companies invent role names that have close requirements as data engineer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18cvv43", "is_robot_indexable": true, "report_reasons": null, "author": "Single-Sound-1865", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18cvv43/what_job_keywords_or_names_that_you_saw_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18cvv43/what_job_keywords_or_names_that_you_saw_on/", "subreddit_subscribers": 144572, "created_utc": 1701956585.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}