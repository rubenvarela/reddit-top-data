{"kind": "Listing", "data": {"after": "t3_18jpeg9", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_cymqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tech-America has 61.44 TB Solidigm SSDs in stock for anyone who would like to pick up one (or more)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18jr7bi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JRDbdU5p-ih-TLRUkK1T_TfJH-PB6IdvyomTaYlbfCc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702733258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech-america.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tech-america.com/item/solidigm-ssd-sbfpf2bv614t001-d5-p5336-61-44tb-2-5-pcie4-0x4-3d5-qlc-retail/sbfpf2bv614t001", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S1JCBVuI14muOgLT5HGiENcuI9RdQ6PKFRra5I3Tmec.jpg?auto=webp&amp;s=ddab834e87c77f2a20110b14192274d36fa3f585", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/S1JCBVuI14muOgLT5HGiENcuI9RdQ6PKFRra5I3Tmec.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db4eed6a6ce54fd45798353a908a24e6b13f9c60", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/S1JCBVuI14muOgLT5HGiENcuI9RdQ6PKFRra5I3Tmec.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc6e307375e63311a623141b2a9b19e3108d1a77", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/S1JCBVuI14muOgLT5HGiENcuI9RdQ6PKFRra5I3Tmec.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24413b113bcef8db88734888ee9a6f0bf1b85ec8", "width": 320, "height": 320}], "variants": {}, "id": "DonADQrU4gV7WaMEXYTEwb2zitE-NnkvTPiB8yncAH4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "18jr7bi", "is_robot_indexable": true, "report_reasons": null, "author": "Torley_", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jr7bi/techamerica_has_6144_tb_solidigm_ssds_in_stock/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tech-america.com/item/solidigm-ssd-sbfpf2bv614t001-d5-p5336-61-44tb-2-5-pcie4-0x4-3d5-qlc-retail/sbfpf2bv614t001", "subreddit_subscribers": 718546, "created_utc": 1702733258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My drives get fanned 24/7, and since it's winter they get pretty chill. At what temp should I worry and do something? under 30 degrees C? Under 25? 20?\n\nMy drives are hovering around 25, some drives even lower at times, like 21/22.", "author_fullname": "t2_9ymyrd1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How cold can you keep the hard drives running at?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jmer8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702713380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My drives get fanned 24/7, and since it&amp;#39;s winter they get pretty chill. At what temp should I worry and do something? under 30 degrees C? Under 25? 20?&lt;/p&gt;\n\n&lt;p&gt;My drives are hovering around 25, some drives even lower at times, like 21/22.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "400TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jmer8", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Arugula-1592", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18jmer8/how_cold_can_you_keep_the_hard_drives_running_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jmer8/how_cold_can_you_keep_the_hard_drives_running_at/", "subreddit_subscribers": 718546, "created_utc": 1702713380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've recently acquired access to a huge number of smaller (2-3TB) HDDs. My question is, how do I connect such a massive number of HDDs (think \"we need a scalable solution that connects 100\\~ of these\") in an economically feasible way? (the main pro of the drives is they're free/extremely cheap)\n\nI've thought about USB -&gt; 8x sata breakouts, but it seems they don't really exist. Bandwidth isn't the limitation, because its very unlikely all 8 drives in a cluster would be active at once. # of ports is what I don't know how to get past, because even with 8x PCIE sata cards, it's infeasible to connect enough of them.\n\nIdeally there's USB in the link somewhere, because I don't need them all drawing power when idle. \n\nWhat's the best solution here? Is it feasible at all, or should I give up and relegate these to cold storage?", "author_fullname": "t2_aw8fvxxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feasible way to connect massive amounts of HDDs to one computer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jmli8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702714179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently acquired access to a huge number of smaller (2-3TB) HDDs. My question is, how do I connect such a massive number of HDDs (think &amp;quot;we need a scalable solution that connects 100~ of these&amp;quot;) in an economically feasible way? (the main pro of the drives is they&amp;#39;re free/extremely cheap)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve thought about USB -&amp;gt; 8x sata breakouts, but it seems they don&amp;#39;t really exist. Bandwidth isn&amp;#39;t the limitation, because its very unlikely all 8 drives in a cluster would be active at once. # of ports is what I don&amp;#39;t know how to get past, because even with 8x PCIE sata cards, it&amp;#39;s infeasible to connect enough of them.&lt;/p&gt;\n\n&lt;p&gt;Ideally there&amp;#39;s USB in the link somewhere, because I don&amp;#39;t need them all drawing power when idle. &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best solution here? Is it feasible at all, or should I give up and relegate these to cold storage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "48TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jmli8", "is_robot_indexable": true, "report_reasons": null, "author": "DrDrago-4", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18jmli8/feasible_way_to_connect_massive_amounts_of_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jmli8/feasible_way_to_connect_massive_amounts_of_hdds/", "subreddit_subscribers": 718546, "created_utc": 1702714179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I dont know a lot about servers Just had a few websites before. But can i rent a webserver without really a website on it. And just use it for me own little cloud storage i can access anywhere. Is this  valid use case or are there better ways? I dont really know the difference between servers like what a vps is etc. And it seems you always have to get a domein name with it even tho i dont use the website function just sftp.", "author_fullname": "t2_3c7nq2fk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can i use a webserver sftp as cloud storage or is that weird, dumb?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jny5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702720146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I dont know a lot about servers Just had a few websites before. But can i rent a webserver without really a website on it. And just use it for me own little cloud storage i can access anywhere. Is this  valid use case or are there better ways? I dont really know the difference between servers like what a vps is etc. And it seems you always have to get a domein name with it even tho i dont use the website function just sftp.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jny5f", "is_robot_indexable": true, "report_reasons": null, "author": "Anakhsunamon", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jny5f/can_i_use_a_webserver_sftp_as_cloud_storage_or_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jny5f/can_i_use_a_webserver_sftp_as_cloud_storage_or_is/", "subreddit_subscribers": 718546, "created_utc": 1702720146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All!\n\nIs there a (safe) way to mirror my Linux boot drive to another disk? I recently had a scare where I thought my boot drive died, so now I want to have an automatic backup of my data\n\nTo clarify, I'm not looking for the mirror to be bootable (and would prefer it's not, to avoid the issues that could lead to) - I just want to have a second place the same data exists in case my drive dies. My first intuition is to do this with a ZFS mirror, but I'm not sure that's safe to do on the same drive ZFS is running from, or if it can be done after-the-fact when I didn't install Ubuntu on ZFS, as I know it now supports\n\nThanks!", "author_fullname": "t2_4id0x6ry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring Boot Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jydso", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702754240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All!&lt;/p&gt;\n\n&lt;p&gt;Is there a (safe) way to mirror my Linux boot drive to another disk? I recently had a scare where I thought my boot drive died, so now I want to have an automatic backup of my data&lt;/p&gt;\n\n&lt;p&gt;To clarify, I&amp;#39;m not looking for the mirror to be bootable (and would prefer it&amp;#39;s not, to avoid the issues that could lead to) - I just want to have a second place the same data exists in case my drive dies. My first intuition is to do this with a ZFS mirror, but I&amp;#39;m not sure that&amp;#39;s safe to do on the same drive ZFS is running from, or if it can be done after-the-fact when I didn&amp;#39;t install Ubuntu on ZFS, as I know it now supports&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jydso", "is_robot_indexable": true, "report_reasons": null, "author": "ThatFireGuy0", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jydso/mirroring_boot_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jydso/mirroring_boot_drive/", "subreddit_subscribers": 718546, "created_utc": 1702754240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nJust curious what the consensus is on saving archives of email accounts, best formats for viewing them that is somewhat 'future proof'.  What do you think about importing them into Thunderbird?  I believe Thunderbird is multi-platform.\n\nI made the mistake of using Microsoft Entourage (Mac-only) back from about 1999-2005 or so.  If I have those drives, it will be an adventure getting that database converted.", "author_fullname": "t2_6pky9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving (and future-proofing) email accounts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jua51", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702742691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Just curious what the consensus is on saving archives of email accounts, best formats for viewing them that is somewhat &amp;#39;future proof&amp;#39;.  What do you think about importing them into Thunderbird?  I believe Thunderbird is multi-platform.&lt;/p&gt;\n\n&lt;p&gt;I made the mistake of using Microsoft Entourage (Mac-only) back from about 1999-2005 or so.  If I have those drives, it will be an adventure getting that database converted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jua51", "is_robot_indexable": true, "report_reasons": null, "author": "drycounty", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jua51/archiving_and_futureproofing_email_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jua51/archiving_and_futureproofing_email_accounts/", "subreddit_subscribers": 718546, "created_utc": 1702742691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "`httm` now directly supports Time Machine backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jgsg8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_hokp5z5a", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "zfs", "selftext": "You can have it all -- the convenience of Time Machine backups and the speed and flexibility of `httm`.\n\nOf course, one could always use `httm` to browse all files in a folder backed up via `rsync` to a remote ZFS or btrfs share, if that share has a hidden snapshot directory:\n\n```\n# mount the share\n\u279c open smb://&lt;your name&gt;@&lt;your remote share&gt;.local/Home\n# execute httm \n\u279c httm -b -R /Volumes/Home\n```\n\nOr one could use `httm` to browse all files in your local MacOS home directory, if you mapped that directory to a remote dataset. Note: The difference from above is, here, you're browsing files from a \"live\" directory:\n\n```\n# mount the share\n\u279c open smb://&lt;your name&gt;@&lt;your remote share&gt;.local/Home\n# execute httm\n\u279c httm -b -R --map-aliases /Users/&lt;your name&gt;:/Volumes/Home\n```\n\nNow, `httm` supports your Time Machine backups directly, [when those backups are mounted](https://github.com/kimono-koans/httm/blob/master/scripts/equine.bash):\n\n```\n\u279c httm .zshrc\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTue May 09 22:57:09 2023  6.7 KiB  \"/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-11-08-212757.backup/2023-11-08-212757.backup/Data/Users/kimono/.zshrc\"\nSun Nov 12 20:29:57 2023  6.7 KiB  \"/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-11-18-011056.backup/2023-11-18-011056.backup/Data/Users/kimono/.zshrc\"\nSun Nov 26 02:14:56 2023  6.7 KiB  \"/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-12-13-054342.backup/2023-12-13-054342.backup/Data/Users/kimono/.zshrc\"\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSun Nov 26 02:14:56 2023  6.7 KiB  \"/Users/kimono/.zshrc\"\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n\n[httm](https://github.com/kimono-koans/httm) prints the size, date and corresponding locations of available unique versions (deduplicated by modify time and size) of files residing on snapshots, but can also be used interactively to select and restore files, even snapshot mounts by file! httm might change the way you use snapshots (because ZFS/BTRFS/NILFS2 aren't designed for finding for unique file versions) or the Time Machine concept (because httm is very fast!).  \n\nFor more info, see the [README](https://github.com/kimono-koans/httm/blob/master/README.md).", "author_fullname": "t2_hokp5z5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "`httm` now directly supports Time Machine backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/zfs", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jgcv8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702693342.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702691673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.zfs", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can have it all -- the convenience of Time Machine backups and the speed and flexibility of &lt;code&gt;httm&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;Of course, one could always use &lt;code&gt;httm&lt;/code&gt; to browse all files in a folder backed up via &lt;code&gt;rsync&lt;/code&gt; to a remote ZFS or btrfs share, if that share has a hidden snapshot directory:&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h1&gt;mount the share&lt;/h1&gt;\n\n&lt;p&gt;\u279c open smb://&amp;lt;your name&amp;gt;@&amp;lt;your remote share&amp;gt;.local/Home&lt;/p&gt;\n\n&lt;h1&gt;execute httm&lt;/h1&gt;\n\n&lt;p&gt;\u279c httm -b -R /Volumes/Home\n```&lt;/p&gt;\n\n&lt;p&gt;Or one could use &lt;code&gt;httm&lt;/code&gt; to browse all files in your local MacOS home directory, if you mapped that directory to a remote dataset. Note: The difference from above is, here, you&amp;#39;re browsing files from a &amp;quot;live&amp;quot; directory:&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h1&gt;mount the share&lt;/h1&gt;\n\n&lt;p&gt;\u279c open smb://&amp;lt;your name&amp;gt;@&amp;lt;your remote share&amp;gt;.local/Home&lt;/p&gt;\n\n&lt;h1&gt;execute httm&lt;/h1&gt;\n\n&lt;p&gt;\u279c httm -b -R --map-aliases /Users/&amp;lt;your name&amp;gt;:/Volumes/Home\n```&lt;/p&gt;\n\n&lt;p&gt;Now, &lt;code&gt;httm&lt;/code&gt; supports your Time Machine backups directly, &lt;a href=\"https://github.com/kimono-koans/httm/blob/master/scripts/equine.bash\"&gt;when those backups are mounted&lt;/a&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n\u279c httm .zshrc\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTue May 09 22:57:09 2023  6.7 KiB  &amp;quot;/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-11-08-212757.backup/2023-11-08-212757.backup/Data/Users/kimono/.zshrc&amp;quot;\nSun Nov 12 20:29:57 2023  6.7 KiB  &amp;quot;/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-11-18-011056.backup/2023-11-18-011056.backup/Data/Users/kimono/.zshrc&amp;quot;\nSun Nov 26 02:14:56 2023  6.7 KiB  &amp;quot;/Volumes/.timemachine/842A693F-CB54-4C5A-9AB1-C73681D4DFCD/2023-12-13-054342.backup/2023-12-13-054342.backup/Data/Users/kimono/.zshrc&amp;quot;\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSun Nov 26 02:14:56 2023  6.7 KiB  &amp;quot;/Users/kimono/.zshrc&amp;quot;\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/kimono-koans/httm\"&gt;httm&lt;/a&gt; prints the size, date and corresponding locations of available unique versions (deduplicated by modify time and size) of files residing on snapshots, but can also be used interactively to select and restore files, even snapshot mounts by file! httm might change the way you use snapshots (because ZFS/BTRFS/NILFS2 aren&amp;#39;t designed for finding for unique file versions) or the Time Machine concept (because httm is very fast!).  &lt;/p&gt;\n\n&lt;p&gt;For more info, see the &lt;a href=\"https://github.com/kimono-koans/httm/blob/master/README.md\"&gt;README&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?auto=webp&amp;s=289ae7bd911636bd23ad7b0471e03d0338afc69f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=72d6e44d66b51c9708547e518ab2ecd08b74d294", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4bb531ce82a16032f4b57a0e091cb765ee005a1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f2232cce32ad3e34bd35cc0f1c936d05f321500", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a994e185edcff2ef789a0edaca551c4259242a4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ae3a0051e16d201e98f9716ff78e2906b12f3a5a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8382e573d51bb691efc81f59ac110ffc4d867671", "width": 1080, "height": 540}], "variants": {}, "id": "ls_jgHNP-aKIiMW8JRNP0at33ieMFcWHSWWlcWs9tVY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ruui", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "18jgcv8", "is_robot_indexable": true, "report_reasons": null, "author": "small_kimono", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/zfs/comments/18jgcv8/httm_now_directly_supports_time_machine_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/zfs/comments/18jgcv8/httm_now_directly_supports_time_machine_backups/", "subreddit_subscribers": 28944, "created_utc": 1702691673.0, "num_crossposts": 5, "media": null, "is_video": false}], "created": 1702693078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.zfs", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/zfs/comments/18jgcv8/httm_now_directly_supports_time_machine_backups/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?auto=webp&amp;s=289ae7bd911636bd23ad7b0471e03d0338afc69f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=72d6e44d66b51c9708547e518ab2ecd08b74d294", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4bb531ce82a16032f4b57a0e091cb765ee005a1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f2232cce32ad3e34bd35cc0f1c936d05f321500", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a994e185edcff2ef789a0edaca551c4259242a4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ae3a0051e16d201e98f9716ff78e2906b12f3a5a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/SiyCEwtw1dtimlOuLI-paFpLtnfU6pMxAwDK8QkvuiM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8382e573d51bb691efc81f59ac110ffc4d867671", "width": 1080, "height": 540}], "variants": {}, "id": "ls_jgHNP-aKIiMW8JRNP0at33ieMFcWHSWWlcWs9tVY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jgsg8", "is_robot_indexable": true, "report_reasons": null, "author": "small_kimono", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_18jgcv8", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jgsg8/httm_now_directly_supports_time_machine_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/zfs/comments/18jgcv8/httm_now_directly_supports_time_machine_backups/", "subreddit_subscribers": 718546, "created_utc": 1702693078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does Synology DS1821+ lock out  (blacklist) third party NICs (or whitelist only theirs), or is that very short compatibility list just the devices that they guarantee will work with the DS1821+ NAS?\n\n&amp;#x200B;\n\nI was considering the TP-Link TX201", "author_fullname": "t2_5a3zhit0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology DS1821+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18k69b0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702777465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does Synology DS1821+ lock out  (blacklist) third party NICs (or whitelist only theirs), or is that very short compatibility list just the devices that they guarantee will work with the DS1821+ NAS?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was considering the TP-Link TX201&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k69b0", "is_robot_indexable": true, "report_reasons": null, "author": "FalconSteve89", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k69b0/synology_ds1821/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k69b0/synology_ds1821/", "subreddit_subscribers": 718546, "created_utc": 1702777465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there any script or way to download all TikTok URLs from your downloaded data like videos and text files? I realize the URLs have to be clicked on first. And then you can get the actual URL for downloading those videos. Tiktok downloaders cannot download links from the text file provided by the downloaded Tiktok data. Is there a workaround to this? ", "author_fullname": "t2_2vmpw7em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "tiktok mass download from downloaded data liked videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k1fdf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702763038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any script or way to download all TikTok URLs from your downloaded data like videos and text files? I realize the URLs have to be clicked on first. And then you can get the actual URL for downloading those videos. Tiktok downloaders cannot download links from the text file provided by the downloaded Tiktok data. Is there a workaround to this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k1fdf", "is_robot_indexable": true, "report_reasons": null, "author": "UltraElixir", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k1fdf/tiktok_mass_download_from_downloaded_data_liked/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k1fdf/tiktok_mass_download_from_downloaded_data_liked/", "subreddit_subscribers": 718546, "created_utc": 1702763038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks, I've searched and there are several threads about UltraDMA CRC Error Count - I took the advice in those threads and swapped cables, but the error persists, and I think my value (91) seems higher than most people have.\n\nDrive is WD RED 6TB WD60EFAX (most of the other people I've seen with the issue were also using WD RED). It's out of warranty so I'm wondering if there's any other things worth trying before I have to replace it.\n\nHere's smartctl output:\n    \n    SMART Attributes Data Structure revision number: 16\n    Vendor Specific SMART Attributes with Thresholds:\n    ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n      1 Raw_Read_Error_Rate     0x002f   200   200   051    Pre-fail  Always       -       0\n      3 Spin_Up_Time            0x0027   224   224   021    Pre-fail  Always       -       3775\n      4 Start_Stop_Count        0x0032   096   096   000    Old_age   Always       -       4889\n      5 Reallocated_Sector_Ct   0x0033   200   200   140    Pre-fail  Always       -       0\n      7 Seek_Error_Rate         0x002e   200   200   000    Old_age   Always       -       0\n      9 Power_On_Hours          0x0032   088   088   000    Old_age   Always       -       9323\n     10 Spin_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0\n     11 Calibration_Retry_Count 0x0032   100   100   000    Old_age   Always       -       0\n     12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       107\n    192 Power-Off_Retract_Count 0x0032   200   200   000    Old_age   Always       -       48\n    193 Load_Cycle_Count        0x0032   199   199   000    Old_age   Always       -       4869\n    194 Temperature_Celsius     0x0022   120   102   000    Old_age   Always       -       30\n    196 Reallocated_Event_Count 0x0032   200   200   000    Old_age   Always       -       0\n    197 Current_Pending_Sector  0x0032   200   200   000    Old_age   Always       -       0\n    198 Offline_Uncorrectable   0x0030   100   253   000    Old_age   Offline      -       0\n    **199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       91**\n    200 Multi_Zone_Error_Rate   0x0008   100   253   000    Old_age   Offline      -       0\n    \nDoesn't seem like much of an issue in smartctl but scrutiny calls it a fail. It also runs a lot colder than my Ironwolf. Obviously I'm not that experienced with this hardware so this is a little open ended. Thanks in advance for any ideas.", "author_fullname": "t2_qb9m0ag7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UDMA_CRC_Error_Count persists after changing cables - anything else to try?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jxjo8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702751895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, I&amp;#39;ve searched and there are several threads about UltraDMA CRC Error Count - I took the advice in those threads and swapped cables, but the error persists, and I think my value (91) seems higher than most people have.&lt;/p&gt;\n\n&lt;p&gt;Drive is WD RED 6TB WD60EFAX (most of the other people I&amp;#39;ve seen with the issue were also using WD RED). It&amp;#39;s out of warranty so I&amp;#39;m wondering if there&amp;#39;s any other things worth trying before I have to replace it.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s smartctl output:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n  1 Raw_Read_Error_Rate     0x002f   200   200   051    Pre-fail  Always       -       0\n  3 Spin_Up_Time            0x0027   224   224   021    Pre-fail  Always       -       3775\n  4 Start_Stop_Count        0x0032   096   096   000    Old_age   Always       -       4889\n  5 Reallocated_Sector_Ct   0x0033   200   200   140    Pre-fail  Always       -       0\n  7 Seek_Error_Rate         0x002e   200   200   000    Old_age   Always       -       0\n  9 Power_On_Hours          0x0032   088   088   000    Old_age   Always       -       9323\n 10 Spin_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0\n 11 Calibration_Retry_Count 0x0032   100   100   000    Old_age   Always       -       0\n 12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       107\n192 Power-Off_Retract_Count 0x0032   200   200   000    Old_age   Always       -       48\n193 Load_Cycle_Count        0x0032   199   199   000    Old_age   Always       -       4869\n194 Temperature_Celsius     0x0022   120   102   000    Old_age   Always       -       30\n196 Reallocated_Event_Count 0x0032   200   200   000    Old_age   Always       -       0\n197 Current_Pending_Sector  0x0032   200   200   000    Old_age   Always       -       0\n198 Offline_Uncorrectable   0x0030   100   253   000    Old_age   Offline      -       0\n**199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       91**\n200 Multi_Zone_Error_Rate   0x0008   100   253   000    Old_age   Offline      -       0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Doesn&amp;#39;t seem like much of an issue in smartctl but scrutiny calls it a fail. It also runs a lot colder than my Ironwolf. Obviously I&amp;#39;m not that experienced with this hardware so this is a little open ended. Thanks in advance for any ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jxjo8", "is_robot_indexable": true, "report_reasons": null, "author": "ambiance6462", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jxjo8/udma_crc_error_count_persists_after_changing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jxjo8/udma_crc_error_count_persists_after_changing/", "subreddit_subscribers": 718546, "created_utc": 1702751895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nI'm not exactly a data hoarder... compared to others here at least, so I really hope this is the appropriate sub for this question. I have a fair amount of archival stuff for old computers such as software and HDD images, so not small quantities of data, plus several TB of personal data, mainly documents and build outputs (software dev).\n\nGot myself an LTO-5 drive (HP EH958A external SAS drive) and a box of 25 tapes, tapes are yet to arrive but drive is here. Was pretty dusty when it arrived both in and outside so I cracked the external enclosure case and cleaned it out thouroughly. Daren't open the actual drive itself for fear of destroying it.\n\nLooking through the front flap, I see a fair amount of dust. Using an electronics wipe, I gently reached in and got rid of most of the dust on the surface taking care not to touch anything past the front cavity, and not wipe away any lubricating greases.\n\nI can see the takeup reel at the back of the unit which appears to have a transparent plastic tape attached to it, which I guess is normal.\n\nThe reel itself looks pretty dusty, and knowing that's where the tape media will eventually be spooled has me concerned that its just going to rub all of that dust off, onto the side edge of the tape.\n\nAnything I can/should do about this dust?\n\nAny other first timer advice would be greatly appreciated too, as I am still seeking some open source software for forever-incremental backups to tapes for both personal and archival offsite backups (append changes to dataset to existing tape rather than rewrite entire tape every time, until tape gets full, then rewrite entire tape)\n\nThanks!", "author_fullname": "t2_1pmvzm8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO-5 Drive takeup reel dusty, cause for concern? + Forever-incremental software recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jxah2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702751158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not exactly a data hoarder... compared to others here at least, so I really hope this is the appropriate sub for this question. I have a fair amount of archival stuff for old computers such as software and HDD images, so not small quantities of data, plus several TB of personal data, mainly documents and build outputs (software dev).&lt;/p&gt;\n\n&lt;p&gt;Got myself an LTO-5 drive (HP EH958A external SAS drive) and a box of 25 tapes, tapes are yet to arrive but drive is here. Was pretty dusty when it arrived both in and outside so I cracked the external enclosure case and cleaned it out thouroughly. Daren&amp;#39;t open the actual drive itself for fear of destroying it.&lt;/p&gt;\n\n&lt;p&gt;Looking through the front flap, I see a fair amount of dust. Using an electronics wipe, I gently reached in and got rid of most of the dust on the surface taking care not to touch anything past the front cavity, and not wipe away any lubricating greases.&lt;/p&gt;\n\n&lt;p&gt;I can see the takeup reel at the back of the unit which appears to have a transparent plastic tape attached to it, which I guess is normal.&lt;/p&gt;\n\n&lt;p&gt;The reel itself looks pretty dusty, and knowing that&amp;#39;s where the tape media will eventually be spooled has me concerned that its just going to rub all of that dust off, onto the side edge of the tape.&lt;/p&gt;\n\n&lt;p&gt;Anything I can/should do about this dust?&lt;/p&gt;\n\n&lt;p&gt;Any other first timer advice would be greatly appreciated too, as I am still seeking some open source software for forever-incremental backups to tapes for both personal and archival offsite backups (append changes to dataset to existing tape rather than rewrite entire tape every time, until tape gets full, then rewrite entire tape)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jxah2", "is_robot_indexable": true, "report_reasons": null, "author": "DevelopedLogic", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jxah2/lto5_drive_takeup_reel_dusty_cause_for_concern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jxah2/lto5_drive_takeup_reel_dusty_cause_for_concern/", "subreddit_subscribers": 718546, "created_utc": 1702751158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just have to warn that I'm a little obsessive with my data. I have thousands of PDFs stored and I always intent to keep them safe and intact, like a large portable library. In addition to having external hard drives I use three cloud services.\n\n1) Two days ago I tried to enter Google Drive and it \u00ab\u00a0freezed\u00a0\u00bb: the page remained blank. I tried again and finally it worked. However, I had a bad feeling, I thought it may have disrupted or corrupted my PDF files. I decided to count them one by one, in each folder, to make sure that none are missing because in Google drive nothing indicates the number of files stored. Finally, nothing was missing. But who knows if some files have not been corrupted and can no longer be opened? \n\n2) Yesterday another problem happened to me on my other cloud: while I was consulting my files my internet connection was interrupted and the file lists became gray, indicating that it was no longer possible to access it. Fortunately the connection was reestablished as well as the access to the files. But maybe this internet interruption also corrupted the files? \n\n3) I know this sounds weird but I am obsessed with the integrity of my files. I know that nothing lasts forever, but do you think I'm worrying unnecessarily? Besides, what can really corrupt a pdf file in the cloud? Even external hard drives worry me because I've read that SSDs, if not plugged in regularly, can lose data. So what to do?", "author_fullname": "t2_870lxxpo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could my data in the cloud be corrupted?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jvvkr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702747223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just have to warn that I&amp;#39;m a little obsessive with my data. I have thousands of PDFs stored and I always intent to keep them safe and intact, like a large portable library. In addition to having external hard drives I use three cloud services.&lt;/p&gt;\n\n&lt;p&gt;1) Two days ago I tried to enter Google Drive and it \u00ab\u00a0freezed\u00a0\u00bb: the page remained blank. I tried again and finally it worked. However, I had a bad feeling, I thought it may have disrupted or corrupted my PDF files. I decided to count them one by one, in each folder, to make sure that none are missing because in Google drive nothing indicates the number of files stored. Finally, nothing was missing. But who knows if some files have not been corrupted and can no longer be opened? &lt;/p&gt;\n\n&lt;p&gt;2) Yesterday another problem happened to me on my other cloud: while I was consulting my files my internet connection was interrupted and the file lists became gray, indicating that it was no longer possible to access it. Fortunately the connection was reestablished as well as the access to the files. But maybe this internet interruption also corrupted the files? &lt;/p&gt;\n\n&lt;p&gt;3) I know this sounds weird but I am obsessed with the integrity of my files. I know that nothing lasts forever, but do you think I&amp;#39;m worrying unnecessarily? Besides, what can really corrupt a pdf file in the cloud? Even external hard drives worry me because I&amp;#39;ve read that SSDs, if not plugged in regularly, can lose data. So what to do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jvvkr", "is_robot_indexable": true, "report_reasons": null, "author": "Caranthir-Hondero", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jvvkr/could_my_data_in_the_cloud_be_corrupted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jvvkr/could_my_data_in_the_cloud_be_corrupted/", "subreddit_subscribers": 718546, "created_utc": 1702747223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use Epson V600 Photo and \"Restore colors\" (Epson ScanSmart). I get better results than in VueScan.\n\nIs it possible to reduce scanning passes for multiple crop areas in v600 (or any another scanner)?\n\nI want to scan multiple images (e.g. 3) at once (see picture).  \nI open Epson ScanSmart:\n\n* I can scan maximum area with all photos (without auto or manual cropping). It's 1 pass for scanner.\n* Or I can crop my 3 images and scanner will do 3 passes.\n\nOf course it's waste of time and scanner resources but this is not big deals for me.  \nMain difference for me is restored colors. Restored colors are different. Because of when I scan all photos at once scanner takes into account empty space (white cushioning bit or black sheet of paper).\n\nQuestion: Can I make 3 crops and make only 1 pass (not 3) with restoring colors?\n\nIf not I will need to make 1 pass without restoring, then crop images and restore colors in Photoshop or Lightroom.\n\nWith VueSmart I also make 3\\*1 passes.\n\n[process](https://preview.redd.it/v7e44naf5o6c1.jpg?width=840&amp;format=pjpg&amp;auto=webp&amp;s=7abb27d2193e0cfe559f6ba9769847bc04bdfb0f)", "author_fullname": "t2_7i0wwc92", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Epson Perfection V600 Photo Scanner] Reduce scanning passes for multiple crop areas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"v7e44naf5o6c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 110, "x": 108, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df0278a96552236d13de1a04ae8a59ebfdac27f7"}, {"y": 221, "x": 216, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=124de0050e21f4c427f11ff7262088dd9722194e"}, {"y": 327, "x": 320, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=96758b2a9505948fdcbc73c1bb51811293999142"}, {"y": 655, "x": 640, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a457d9d7535823920cc8eec3d3da05afbb9fd34"}], "s": {"y": 860, "x": 840, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=840&amp;format=pjpg&amp;auto=webp&amp;s=7abb27d2193e0cfe559f6ba9769847bc04bdfb0f"}, "id": "v7e44naf5o6c1"}}, "name": "t3_18jsna1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5FPSCWV7s3UcY5c2vPQxs3_0P2CEycKhlXoYNWdERig.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702737811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Epson V600 Photo and &amp;quot;Restore colors&amp;quot; (Epson ScanSmart). I get better results than in VueScan.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to reduce scanning passes for multiple crop areas in v600 (or any another scanner)?&lt;/p&gt;\n\n&lt;p&gt;I want to scan multiple images (e.g. 3) at once (see picture).&lt;br/&gt;\nI open Epson ScanSmart:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I can scan maximum area with all photos (without auto or manual cropping). It&amp;#39;s 1 pass for scanner.&lt;/li&gt;\n&lt;li&gt;Or I can crop my 3 images and scanner will do 3 passes.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Of course it&amp;#39;s waste of time and scanner resources but this is not big deals for me.&lt;br/&gt;\nMain difference for me is restored colors. Restored colors are different. Because of when I scan all photos at once scanner takes into account empty space (white cushioning bit or black sheet of paper).&lt;/p&gt;\n\n&lt;p&gt;Question: Can I make 3 crops and make only 1 pass (not 3) with restoring colors?&lt;/p&gt;\n\n&lt;p&gt;If not I will need to make 1 pass without restoring, then crop images and restore colors in Photoshop or Lightroom.&lt;/p&gt;\n\n&lt;p&gt;With VueSmart I also make 3*1 passes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v7e44naf5o6c1.jpg?width=840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7abb27d2193e0cfe559f6ba9769847bc04bdfb0f\"&gt;process&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jsna1", "is_robot_indexable": true, "report_reasons": null, "author": "michl-pfeiff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jsna1/epson_perfection_v600_photo_scanner_reduce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jsna1/epson_perfection_v600_photo_scanner_reduce/", "subreddit_subscribers": 718546, "created_utc": 1702737811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My accountant was asking me for help in backing up her data (I'm an IT guy).    She uses a cloud-based system called Tax Dome where she stores all of her files and she is worried she is putting all of her eggs in one basket with them.   It's only about 300GB of data but it would be a big deal if she lost it all.\n\nTax Dome mounts as a drive in Windows requiring occasional authentication.   Was thinking some sort of continuous cloud backup service (backing up the Tax Dome drive) would work for her as the machine is likely not running 24/7 and she is typically authenticated into Tax Dome during the work day, at least.\n\nAny recommendations for her that are simple to use, very reliable, has good support, and do a good job alerting if the backup is failing for any reason?   Was initially thinking IDrive or pCloud but the reviews seem quite mixed.", "author_fullname": "t2_ewor6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple and Reliable Continuous Backup Options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jrixz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702734318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My accountant was asking me for help in backing up her data (I&amp;#39;m an IT guy).    She uses a cloud-based system called Tax Dome where she stores all of her files and she is worried she is putting all of her eggs in one basket with them.   It&amp;#39;s only about 300GB of data but it would be a big deal if she lost it all.&lt;/p&gt;\n\n&lt;p&gt;Tax Dome mounts as a drive in Windows requiring occasional authentication.   Was thinking some sort of continuous cloud backup service (backing up the Tax Dome drive) would work for her as the machine is likely not running 24/7 and she is typically authenticated into Tax Dome during the work day, at least.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations for her that are simple to use, very reliable, has good support, and do a good job alerting if the backup is failing for any reason?   Was initially thinking IDrive or pCloud but the reviews seem quite mixed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jrixz", "is_robot_indexable": true, "report_reasons": null, "author": "christoman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jrixz/simple_and_reliable_continuous_backup_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jrixz/simple_and_reliable_continuous_backup_options/", "subreddit_subscribers": 718546, "created_utc": 1702734318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Should one download audio files from youtube into Flac? It seems like youtube supports decently high bitrates (a file I just downloaded has a 20khz cutoff). I don't know if there is a way to download their version without converting it into another lossy transcoded file format. It seems like a waste of space to download non lossless audio into lossless but I'm a little worried about file degradation. Let me know what you think!", "author_fullname": "t2_ld10r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice when converting youtube audio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jp8qx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702725730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Should one download audio files from youtube into Flac? It seems like youtube supports decently high bitrates (a file I just downloaded has a 20khz cutoff). I don&amp;#39;t know if there is a way to download their version without converting it into another lossy transcoded file format. It seems like a waste of space to download non lossless audio into lossless but I&amp;#39;m a little worried about file degradation. Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jp8qx", "is_robot_indexable": true, "report_reasons": null, "author": "MadMax2230", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jp8qx/best_practice_when_converting_youtube_audio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jp8qx/best_practice_when_converting_youtube_audio/", "subreddit_subscribers": 718546, "created_utc": 1702725730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello dear colleagues,  \nSince there were similar questions in the past with relatively unclear conditions, I have tried to make the subject line as clear as possible. I tried some of the previously suggested methods in older threads like Firefox add-on titled \"Reddit All Comments Viewer by Draxxx\", but it appears that one is not operational anymore - I have tried several experiments but I am not getting browser option \"Load All Comments\" with that add-on.   \n\n\nSo basically my conditions are the following:  \n\n\n\\- Tool that would allow the saving/exporting the entire Reddit thread/post - especially targeted for large threads containing over 500 comments.  \n\\- Manual options are not practical since for proper archiving the user has to manually open/expand hundreds of comments and replies to save the entire thread properly. (similar questions in the past by other users haven't explained that part well which lead to many redundant replies).  \n\\- So the tool should load all comments/replies automatically and then the entire thread could be saved properly to capture all for historical reference/offline browsing.\n\nDoes anyone know is there a working option for that currently in 2023/2024.   \n\n\nBest wishes.   \n\n\n  \n", "author_fullname": "t2_p695jcz7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best method in 2023/2024 to save/extract large Reddit threads with all comments expanded/opened for offline browsing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jo6e8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702721143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello dear colleagues,&lt;br/&gt;\nSince there were similar questions in the past with relatively unclear conditions, I have tried to make the subject line as clear as possible. I tried some of the previously suggested methods in older threads like Firefox add-on titled &amp;quot;Reddit All Comments Viewer by Draxxx&amp;quot;, but it appears that one is not operational anymore - I have tried several experiments but I am not getting browser option &amp;quot;Load All Comments&amp;quot; with that add-on.   &lt;/p&gt;\n\n&lt;p&gt;So basically my conditions are the following:  &lt;/p&gt;\n\n&lt;p&gt;- Tool that would allow the saving/exporting the entire Reddit thread/post - especially targeted for large threads containing over 500 comments.&lt;br/&gt;\n- Manual options are not practical since for proper archiving the user has to manually open/expand hundreds of comments and replies to save the entire thread properly. (similar questions in the past by other users haven&amp;#39;t explained that part well which lead to many redundant replies).&lt;br/&gt;\n- So the tool should load all comments/replies automatically and then the entire thread could be saved properly to capture all for historical reference/offline browsing.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know is there a working option for that currently in 2023/2024.   &lt;/p&gt;\n\n&lt;p&gt;Best wishes.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jo6e8", "is_robot_indexable": true, "report_reasons": null, "author": "ObservationDeck2001", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jo6e8/what_is_the_best_method_in_20232024_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jo6e8/what_is_the_best_method_in_20232024_to/", "subreddit_subscribers": 718546, "created_utc": 1702721143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I feel like I'm not asking for much.\n\n1. On the PC side: just some proper syncing technologies that sync from my PC to Cloud (i.e. for my past academic work, I just want to have it properly stored and stuff); for my current work, I want to be able to just have a proper 1 to 1 sync, i.e. if I delete something, cloud side should delete as well.\n2. On my phone side (android): same old proper syncing, maybe with an option for me to keep some stuff in the cloud that I don't need on my phone (i.e. certain older albums)\n\nI have been trying to drag and drop stuff to onedrive but when folder size is too big it just freeze entirely.\n\nOneDrive sync is total crap and it just doesn't work...\n\nAre there better cloud service provider out there? I am considering stuff like pCloud but am worried if they would be around few years from now.\n\nReally really appreciate any advice!!!", "author_fullname": "t2_frutaosj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Honestly, what is the best cloud storage provider that has proper syncing capabilities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jo5if", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702721039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like I&amp;#39;m not asking for much.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;On the PC side: just some proper syncing technologies that sync from my PC to Cloud (i.e. for my past academic work, I just want to have it properly stored and stuff); for my current work, I want to be able to just have a proper 1 to 1 sync, i.e. if I delete something, cloud side should delete as well.&lt;/li&gt;\n&lt;li&gt;On my phone side (android): same old proper syncing, maybe with an option for me to keep some stuff in the cloud that I don&amp;#39;t need on my phone (i.e. certain older albums)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have been trying to drag and drop stuff to onedrive but when folder size is too big it just freeze entirely.&lt;/p&gt;\n\n&lt;p&gt;OneDrive sync is total crap and it just doesn&amp;#39;t work...&lt;/p&gt;\n\n&lt;p&gt;Are there better cloud service provider out there? I am considering stuff like pCloud but am worried if they would be around few years from now.&lt;/p&gt;\n\n&lt;p&gt;Really really appreciate any advice!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jo5if", "is_robot_indexable": true, "report_reasons": null, "author": "warrior123456781", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jo5if/honestly_what_is_the_best_cloud_storage_provider/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jo5if/honestly_what_is_the_best_cloud_storage_provider/", "subreddit_subscribers": 718546, "created_utc": 1702721039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nLong story short - I've spent recent days to recover and move files from multiple HDDs from the past 10 years. Overall about 3 TB of data.\n\n- Now I have about 5 different partitions\n- Some of the files are duplicates\n- Some of the files are sadly corrupted\n-I'm dealing with many different files from images, office files, Time Machine backups and so on\n\nNow as a next step I would like to organise everything, separate them by file types, ideally remove duplicates and corrupted files and move them to a single location. \n\nIs there any (maybe even free) software that you would recommend to do such thing?", "author_fullname": "t2_6cmljsoq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any decent software to categorise and move files based on file type?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jnj91", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702718328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Long story short - I&amp;#39;ve spent recent days to recover and move files from multiple HDDs from the past 10 years. Overall about 3 TB of data.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Now I have about 5 different partitions&lt;/li&gt;\n&lt;li&gt;Some of the files are duplicates&lt;/li&gt;\n&lt;li&gt;Some of the files are sadly corrupted\n-I&amp;#39;m dealing with many different files from images, office files, Time Machine backups and so on&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now as a next step I would like to organise everything, separate them by file types, ideally remove duplicates and corrupted files and move them to a single location. &lt;/p&gt;\n\n&lt;p&gt;Is there any (maybe even free) software that you would recommend to do such thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jnj91", "is_robot_indexable": true, "report_reasons": null, "author": "Tuttle489", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jnj91/any_decent_software_to_categorise_and_move_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jnj91/any_decent_software_to_categorise_and_move_files/", "subreddit_subscribers": 718546, "created_utc": 1702718328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good day,\n\nLost in the forest of choice and contradicting review, time to ask the community..  \nI want to backup my NAS (or part of it as least) to HDD/SSD that i will store offsite.\n\nI'll have multiple copies (+Glacier Deep Archive as last resort), and was wondering which HDD/SSD I should (not) use.\n\nThe reviews are slightly confusing when it comes reliabilty'.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_lnyanlqi9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost efficiency reliable disks for backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jlj7e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702709713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day,&lt;/p&gt;\n\n&lt;p&gt;Lost in the forest of choice and contradicting review, time to ask the community..&lt;br/&gt;\nI want to backup my NAS (or part of it as least) to HDD/SSD that i will store offsite.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll have multiple copies (+Glacier Deep Archive as last resort), and was wondering which HDD/SSD I should (not) use.&lt;/p&gt;\n\n&lt;p&gt;The reviews are slightly confusing when it comes reliabilty&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jlj7e", "is_robot_indexable": true, "report_reasons": null, "author": "ChrisTheChti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jlj7e/cost_efficiency_reliable_disks_for_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jlj7e/cost_efficiency_reliable_disks_for_backup/", "subreddit_subscribers": 718546, "created_utc": 1702709713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I figured this was the place to rant since we all have a problem (not enough space). I pay for Gdrive every month, 202GB currently so just a couple bucks but with Android/Gmail/image sharing I use over 80% of it. Google being themselves, discontinued the app for Windows way back in the day, forcing you to go thru https. Downloading anything of a large size, waiting for it to compress it all, and praying it doesn't outright fail seems like an insult with how much they already compress what they can.\n\nFirefox, admittedly with a pile of security and privacy changes won't even work with downloading a folder, so I use a clean run of Edge but can still fail, stall, or take an insane amount of time.\n\nI use JDownloader 2 for most everything else, and I'm looking for fixes since either I'm doing something wrong, or GDrive is just bad at certain things. Any advice?", "author_fullname": "t2_fx6xk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A small rant about GDrive - is it just me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jkm0x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702706119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I figured this was the place to rant since we all have a problem (not enough space). I pay for Gdrive every month, 202GB currently so just a couple bucks but with Android/Gmail/image sharing I use over 80% of it. Google being themselves, discontinued the app for Windows way back in the day, forcing you to go thru https. Downloading anything of a large size, waiting for it to compress it all, and praying it doesn&amp;#39;t outright fail seems like an insult with how much they already compress what they can.&lt;/p&gt;\n\n&lt;p&gt;Firefox, admittedly with a pile of security and privacy changes won&amp;#39;t even work with downloading a folder, so I use a clean run of Edge but can still fail, stall, or take an insane amount of time.&lt;/p&gt;\n\n&lt;p&gt;I use JDownloader 2 for most everything else, and I&amp;#39;m looking for fixes since either I&amp;#39;m doing something wrong, or GDrive is just bad at certain things. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jkm0x", "is_robot_indexable": true, "report_reasons": null, "author": "super_starfox", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jkm0x/a_small_rant_about_gdrive_is_it_just_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jkm0x/a_small_rant_about_gdrive_is_it_just_me/", "subreddit_subscribers": 718546, "created_utc": 1702706119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1opf2inv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "How do I read this CrystalDiskInfo on my internal SSD? Thank you for the help in advance.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"7uoevp35br6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 129, "x": 108, "u": "https://preview.redd.it/7uoevp35br6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b3f7e2b94f2e3f4bfd11239269578161d8e9a0d"}, {"y": 259, "x": 216, "u": "https://preview.redd.it/7uoevp35br6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=717c99d2978f5fcc73693c9fb7f2674726396379"}, {"y": 384, "x": 320, "u": "https://preview.redd.it/7uoevp35br6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a351ec758ab637ebef41d3d36f1354e530f87068"}, {"y": 768, "x": 640, "u": "https://preview.redd.it/7uoevp35br6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3fdd9da2a42511ece2c0776ae0d113d45855dee8"}], "s": {"y": 1011, "x": 842, "u": "https://preview.redd.it/7uoevp35br6c1.png?width=842&amp;format=png&amp;auto=webp&amp;s=3aa94088927d536c3d0e54ec4a41d44eb8fb4d45"}, "id": "7uoevp35br6c1"}, "wnsk4aw5br6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 129, "x": 108, "u": "https://preview.redd.it/wnsk4aw5br6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=465dd58542486f3572fc6c42e12e9aee4df9d377"}, {"y": 259, "x": 216, "u": "https://preview.redd.it/wnsk4aw5br6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e713e20ffeaa2b79bef02f123cbf98d952a12bc"}, {"y": 384, "x": 320, "u": "https://preview.redd.it/wnsk4aw5br6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc37c1a316a13e11b9e25ff1158474521884df49"}, {"y": 768, "x": 640, "u": "https://preview.redd.it/wnsk4aw5br6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92f5b6d4571d9fadb58512884430e5df319c8e93"}], "s": {"y": 1011, "x": 842, "u": "https://preview.redd.it/wnsk4aw5br6c1.png?width=842&amp;format=png&amp;auto=webp&amp;s=ab5b7b7b3084fa0ad6adf96b1d116690243424b9"}, "id": "wnsk4aw5br6c1"}}, "name": "t3_18k5t5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "7uoevp35br6c1", "id": 374932806}, {"media_id": "wnsk4aw5br6c1", "id": 374932807}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zkMBESqeLmy0JMiMTKFlhXiDhLoJnOZE4ziwzXG-GrI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702776058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/18k5t5f", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k5t5f", "is_robot_indexable": true, "report_reasons": null, "author": "Sharp-Astronomer6866", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k5t5f/how_do_i_read_this_crystaldiskinfo_on_my_internal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/18k5t5f", "subreddit_subscribers": 718546, "created_utc": 1702776058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a very humble script that calls one json endpoint a day, I can get to it from my laptop, but it stopped working from github actions.\n\nAnyone in the same boat?", "author_fullname": "t2_4uxoi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone accessing reddit .json endpoints noticed getting blocked recently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k1qw3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702763993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a very humble script that calls one json endpoint a day, I can get to it from my laptop, but it stopped working from github actions.&lt;/p&gt;\n\n&lt;p&gt;Anyone in the same boat?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k1qw3", "is_robot_indexable": true, "report_reasons": null, "author": "Madd0g", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k1qw3/anyone_accessing_reddit_json_endpoints_noticed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k1qw3/anyone_accessing_reddit_json_endpoints_noticed/", "subreddit_subscribers": 718546, "created_utc": 1702763993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there,\n\nI know there are a bunch of posts about this topic, but I'm in the process of doing a NAS upgrade, and I'm likely going from 5 drives to 3. I currently have 4 red plus drives and 1 schucked drive. These are likely quieter than the Exos drives, but I'm going from 5 to 3. Would 3 Exos drives be a lot noisier than 5 of the drives I currently have?\n\nAlso, I do have a dedicated server closet, and I've put extra sound protection on the door. When I walk by, I still hear some fans, though, so I'm overall looking to reduce heat / noise in the closet. Curious to get your thoughts here. Thanks!", "author_fullname": "t2_bra1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exos drive noise considerations... a few questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jy8ci", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702753817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;I know there are a bunch of posts about this topic, but I&amp;#39;m in the process of doing a NAS upgrade, and I&amp;#39;m likely going from 5 drives to 3. I currently have 4 red plus drives and 1 schucked drive. These are likely quieter than the Exos drives, but I&amp;#39;m going from 5 to 3. Would 3 Exos drives be a lot noisier than 5 of the drives I currently have?&lt;/p&gt;\n\n&lt;p&gt;Also, I do have a dedicated server closet, and I&amp;#39;ve put extra sound protection on the door. When I walk by, I still hear some fans, though, so I&amp;#39;m overall looking to reduce heat / noise in the closet. Curious to get your thoughts here. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18jy8ci", "is_robot_indexable": true, "report_reasons": null, "author": "hungarianhc", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jy8ci/exos_drive_noise_considerations_a_few_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jy8ci/exos_drive_noise_considerations_a_few_questions/", "subreddit_subscribers": 718546, "created_utc": 1702753817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Reddit,\n\nI have 8 hard drives for media storage, which constitute my active drives my PC (Ryzen 7 5800x, GB x570 AORUS Elite). \n\nI use three redundant sets of backup hard drives (3x2=6 drives), which I refresh at regular intervals. All are Western Digital, either Gold or Red Pro. All are of significant sizes (16TB or larger), setup using BitLocker in identical configurations, and all drives are less than three years old.\n\nWhen re-connecting a drive or copying content to a drive for the first time, I copy contents to the drive, power down the machine, power up again to verify contents, physically disconnect the drive, and place it into storage.\n\nWhen it comes time to do a backup, I re-connect the drive, update the content, and disconnect the drive again, which I store at an alternate location until the next regularly scheduled backup. \n\nI transport that drives in padded cases inside anti-static bags when moving them.\n\nWhen I reconnect any drive, I use the same SATA cables and power cables i.e., changing out the drive to any one of the other 5 backup drives works.\n\nThe past three instances I have tried to do a backup, a Western Digital Red Pro 20TB has failed after the first use: I connected the new drive, setup BitLocker, and copied content to the drive, and then stored it. \n\nHowever, when I reconnect that drive, it does not detect: not visible in BIOS or Windows. The drive does not spin-up\u2014you cannot hear feel it begin spinning or hear it power up. The drive is simply dead. This has now happened x3 *IN A ROW*. \n\nI tried swapping SATA and power cables for the dead drive to either of the drives in the PC, and those cables work\u2014only the one drive was dead. I can also connect any other drive using same cables.\n\nThe first time this happened, I RMA\u2019d and sent it back to Western Digital, which provided a replacement, but then it happened again: I connected the new drive and copied content to it and then stored it. \n\nHowever, when I reconnected that drive, it does not detect: not visible in BIOS or Windows\u2014the drive is dead and won\u2019t detect, but every other drive will.\n\nSo I RMA\u2019d that drive and sent it back to Western Digital, received a replacement, and then it happened again\u2013I can copy content to the drive once, but after reconnecting the drive to update the content, the drive is dead.\n\nSo I RMA\u2019d that drive\u2014now I am on my fourth replacement, but it is statistically improbably that 3 drives would fail in a row. \n\nDoes Reddit have any ideas? Western Digital Support said it is \u201creally unusual\u201d. I work in a technical company, and my technical team doesn\u2019t have any ideas and agrees it is almost impossible that 3 drives would fail in a row but all of the other drives work fine using same cables.", "author_fullname": "t2_q2ju67pv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "x3 20TB Western Digital Red Pro failed... in a row!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jvj0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702746239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt;\n\n&lt;p&gt;I have 8 hard drives for media storage, which constitute my active drives my PC (Ryzen 7 5800x, GB x570 AORUS Elite). &lt;/p&gt;\n\n&lt;p&gt;I use three redundant sets of backup hard drives (3x2=6 drives), which I refresh at regular intervals. All are Western Digital, either Gold or Red Pro. All are of significant sizes (16TB or larger), setup using BitLocker in identical configurations, and all drives are less than three years old.&lt;/p&gt;\n\n&lt;p&gt;When re-connecting a drive or copying content to a drive for the first time, I copy contents to the drive, power down the machine, power up again to verify contents, physically disconnect the drive, and place it into storage.&lt;/p&gt;\n\n&lt;p&gt;When it comes time to do a backup, I re-connect the drive, update the content, and disconnect the drive again, which I store at an alternate location until the next regularly scheduled backup. &lt;/p&gt;\n\n&lt;p&gt;I transport that drives in padded cases inside anti-static bags when moving them.&lt;/p&gt;\n\n&lt;p&gt;When I reconnect any drive, I use the same SATA cables and power cables i.e., changing out the drive to any one of the other 5 backup drives works.&lt;/p&gt;\n\n&lt;p&gt;The past three instances I have tried to do a backup, a Western Digital Red Pro 20TB has failed after the first use: I connected the new drive, setup BitLocker, and copied content to the drive, and then stored it. &lt;/p&gt;\n\n&lt;p&gt;However, when I reconnect that drive, it does not detect: not visible in BIOS or Windows. The drive does not spin-up\u2014you cannot hear feel it begin spinning or hear it power up. The drive is simply dead. This has now happened x3 &lt;em&gt;IN A ROW&lt;/em&gt;. &lt;/p&gt;\n\n&lt;p&gt;I tried swapping SATA and power cables for the dead drive to either of the drives in the PC, and those cables work\u2014only the one drive was dead. I can also connect any other drive using same cables.&lt;/p&gt;\n\n&lt;p&gt;The first time this happened, I RMA\u2019d and sent it back to Western Digital, which provided a replacement, but then it happened again: I connected the new drive and copied content to it and then stored it. &lt;/p&gt;\n\n&lt;p&gt;However, when I reconnected that drive, it does not detect: not visible in BIOS or Windows\u2014the drive is dead and won\u2019t detect, but every other drive will.&lt;/p&gt;\n\n&lt;p&gt;So I RMA\u2019d that drive and sent it back to Western Digital, received a replacement, and then it happened again\u2013I can copy content to the drive once, but after reconnecting the drive to update the content, the drive is dead.&lt;/p&gt;\n\n&lt;p&gt;So I RMA\u2019d that drive\u2014now I am on my fourth replacement, but it is statistically improbably that 3 drives would fail in a row. &lt;/p&gt;\n\n&lt;p&gt;Does Reddit have any ideas? Western Digital Support said it is \u201creally unusual\u201d. I work in a technical company, and my technical team doesn\u2019t have any ideas and agrees it is almost impossible that 3 drives would fail in a row but all of the other drives work fine using same cables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jvj0v", "is_robot_indexable": true, "report_reasons": null, "author": "RecoveringDataHoard2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jvj0v/x3_20tb_western_digital_red_pro_failed_in_a_row/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jvj0v/x3_20tb_western_digital_red_pro_failed_in_a_row/", "subreddit_subscribers": 718546, "created_utc": 1702746239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all sorry if this has been asked to death, but I was wondering what the best suggestions for what type of hardware I should get to capture VHS with little/no data loss so I can keep my tapes as close to source as possible? I have used OBS to capture on the software side so I am covered on that, just curious since I came into a new MacBook what are the best converters/hardware I can get for one that has an M2 chip.", "author_fullname": "t2_16aefx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best hardware for \u201clossless\u201d VHS capture on M2 Mac", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jpeg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702726386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all sorry if this has been asked to death, but I was wondering what the best suggestions for what type of hardware I should get to capture VHS with little/no data loss so I can keep my tapes as close to source as possible? I have used OBS to capture on the software side so I am covered on that, just curious since I came into a new MacBook what are the best converters/hardware I can get for one that has an M2 chip.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jpeg9", "is_robot_indexable": true, "report_reasons": null, "author": "JFlood23", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jpeg9/best_hardware_for_lossless_vhs_capture_on_m2_mac/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jpeg9/best_hardware_for_lossless_vhs_capture_on_m2_mac/", "subreddit_subscribers": 718546, "created_utc": 1702726386.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}