{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all! I'm fresh out of college with an MSIT degree. I've been applying for DA/DE jobs in vain. If you have experience as a recruiter or have supported someone in recruiting a junior Data Analyst/Data Engineer/Analytics Engineer, what are a few must haves?  (I'm aware that they are intrinsically different, but I'm confident that I can do a great job at either of them.)  \nI've had experience with ETL (mostly OLAP data), SQL &amp; Python, Databricks and AWS amongst few other things, but with each recruiter having their own infrastructure, I'm finding it hard to make focus on one specific skill. A post from the other day ([How I Interview Data Engineers - by Yordan Ivanov (substack.com)](https://datagibberish.substack.com/p/how-i-interview-data-engineers?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web)) gave me one person's view on how to give an interview. I'm trying get to the interview, which is proving to be difficult. How do I land an interview? Any pointers will be helpful. Thanks in advance.", "author_fullname": "t2_hqiwxblm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you look for on a resume when hiring for a Data Engineer (Junior/fresh out of college) role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k208t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702764707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! I&amp;#39;m fresh out of college with an MSIT degree. I&amp;#39;ve been applying for DA/DE jobs in vain. If you have experience as a recruiter or have supported someone in recruiting a junior Data Analyst/Data Engineer/Analytics Engineer, what are a few must haves?  (I&amp;#39;m aware that they are intrinsically different, but I&amp;#39;m confident that I can do a great job at either of them.)&lt;br/&gt;\nI&amp;#39;ve had experience with ETL (mostly OLAP data), SQL &amp;amp; Python, Databricks and AWS amongst few other things, but with each recruiter having their own infrastructure, I&amp;#39;m finding it hard to make focus on one specific skill. A post from the other day (&lt;a href=\"https://datagibberish.substack.com/p/how-i-interview-data-engineers?r=odlo3&amp;amp;utm_campaign=post&amp;amp;utm_medium=web\"&gt;How I Interview Data Engineers - by Yordan Ivanov (substack.com)&lt;/a&gt;) gave me one person&amp;#39;s view on how to give an interview. I&amp;#39;m trying get to the interview, which is proving to be difficult. How do I land an interview? Any pointers will be helpful. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?auto=webp&amp;s=d62d1df611eb5fd3b6c201a2ca1198764c34576e", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d233877d035936e4cdd051c8308d2ed8687ea4b4", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=658a8bd49e51df3ec03cb2ee2e11353625e67188", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6fa1c75aa9279837a1bc754209d43a4ecc494bf", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e81e57fe8694136946f0e0b90cd174e447289020", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c2d765d29649bb8542956c04e415c72a8794e9e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b00fa2cb6c57cffff61869c14eb6b4e6cbb86d4", "width": 1080, "height": 720}], "variants": {}, "id": "5zTWOZ4g6_ZEJv5W5FoiMbIfg0hblAMLhiLqgJKuoFg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18k208t", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional_Reason59", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18k208t/what_do_you_look_for_on_a_resume_when_hiring_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18k208t/what_do_you_look_for_on_a_resume_when_hiring_for/", "subreddit_subscribers": 146596, "created_utc": 1702764707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is Databricks fundamentally/architecturally different from other hyper scalers when all them have launched their own Lakehouse replicas? For e.g. BigLake under GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kfn28", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702813427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18kfn28", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kfn28/how_is_databricks_fundamentallyarchitecturally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kfn28/how_is_databricks_fundamentallyarchitecturally/", "subreddit_subscribers": 146596, "created_utc": 1702813427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.instagram.com/reel/C0UMsuNrwX5/?igshid=MzRlODBiNWFlZA==\n\nI've seen it suggested on this sub a few times. Surely it's a waste of time and won't ever be directly applicable to your actual career skills beyond some network basics that you'd get tinkering with AWS anyway?\n\nOr am I missing something?", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would someone want to be building RPI clusters which have so few transferable skills when you could just be doing projects in AWS/cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18khf1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702819930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.instagram.com/reel/C0UMsuNrwX5/?igshid=MzRlODBiNWFlZA==\"&gt;https://www.instagram.com/reel/C0UMsuNrwX5/?igshid=MzRlODBiNWFlZA==&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen it suggested on this sub a few times. Surely it&amp;#39;s a waste of time and won&amp;#39;t ever be directly applicable to your actual career skills beyond some network basics that you&amp;#39;d get tinkering with AWS anyway?&lt;/p&gt;\n\n&lt;p&gt;Or am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-th6YlprDrdAa9jQoqT_f7yvdg8EEAfT44tHbx2vI1w.jpg?auto=webp&amp;s=870722f67c3bc0e2671bdff2eb5e54768215f381", "width": 540, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/-th6YlprDrdAa9jQoqT_f7yvdg8EEAfT44tHbx2vI1w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a56880ad097e4106d1bd35563580d925591b958", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/-th6YlprDrdAa9jQoqT_f7yvdg8EEAfT44tHbx2vI1w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4acc9953c0bd0089ad7e45894f33886def0be756", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/-th6YlprDrdAa9jQoqT_f7yvdg8EEAfT44tHbx2vI1w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8316399932d8a190da49675ca5749033abbadea0", "width": 320, "height": 320}], "variants": {}, "id": "AprF7A7Vjv6kqmuhxUJ-C9dMsGDdNHfawA9lPfuzvBE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18khf1r", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18khf1r/why_would_someone_want_to_be_building_rpi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18khf1r/why_would_someone_want_to_be_building_rpi/", "subreddit_subscribers": 146596, "created_utc": 1702819930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Goals:**\n\nGain cloud experience so that I am more marketable to employers. My secondary goal is to create a more usable [NPI dataset](https://npiregistry.cms.hhs.gov/search). Everything I do for work currently is on premises Oracle orchestrated by airflow.\n\n**Project description:**\n\nThe project is to pull in a \\~9GB .csv healthcare provider file from NPPES (CMS) monthly. I want python [to download the .csv](https://download.cms.gov/nppes/NPI_Files.html) and full replace the existing table as my starting point. I have other analytic, data cleaning goals, and reach API development goals down the road.\n\nI have a linux VM setup with an airflow install via docker set up and have begun to set up the SQL db.\n\n**.CSV landing spot problems:**\n\nI am stuck on what is the 'best' way to process this .csv. I first downloaded it to my local machine and tried to bulk insert via SSMS for a proof of concept but I had file permissions issues. Abandoning that form of insert, I tried to upload it to Azure storage container but it took forever to upload and I don't think I want to go that route when it is automated. Should I try downloading to the linux vm and bulk inserting from there?  Where should this .csv go before it is bulk inserted?\n\n**Bulk Insert and/or .CSV processing:**\n\nI would like to drop many of the columns from the .csv before uploading. I haven't figured out if a format file could drop columns for me during bulk insert. Is column mapping during bulk insert possible if the table has less columns than the .csv? I have read suggestions about doing a full bulk insert and then creating a view, or bulk inserting to a temp table and then inserting from there but I thought it would maybe be easier to pre-process the .csv first? I am not as sharp on T-SQL. Any thoughts would be appreciated.\n\nMy efficiency questions are wondering if any pre-processing of the .csv is possible. I would love to drop a bunch of the columns and even partition the .csv's into many smaller .csv's by state. My hunch is that the file will be too large to do this using pandas which is what I am most familiar with. Any thoughts or suggestions on pre-processing .csvs in comparison to being savvier with the bulk insert?", "author_fullname": "t2_f1q7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Cloud Project Help - NPI Healthcare Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k750l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702780351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Gain cloud experience so that I am more marketable to employers. My secondary goal is to create a more usable &lt;a href=\"https://npiregistry.cms.hhs.gov/search\"&gt;NPI dataset&lt;/a&gt;. Everything I do for work currently is on premises Oracle orchestrated by airflow.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Project description:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The project is to pull in a ~9GB .csv healthcare provider file from NPPES (CMS) monthly. I want python &lt;a href=\"https://download.cms.gov/nppes/NPI_Files.html\"&gt;to download the .csv&lt;/a&gt; and full replace the existing table as my starting point. I have other analytic, data cleaning goals, and reach API development goals down the road.&lt;/p&gt;\n\n&lt;p&gt;I have a linux VM setup with an airflow install via docker set up and have begun to set up the SQL db.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;.CSV landing spot problems:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am stuck on what is the &amp;#39;best&amp;#39; way to process this .csv. I first downloaded it to my local machine and tried to bulk insert via SSMS for a proof of concept but I had file permissions issues. Abandoning that form of insert, I tried to upload it to Azure storage container but it took forever to upload and I don&amp;#39;t think I want to go that route when it is automated. Should I try downloading to the linux vm and bulk inserting from there?  Where should this .csv go before it is bulk inserted?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bulk Insert and/or .CSV processing:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I would like to drop many of the columns from the .csv before uploading. I haven&amp;#39;t figured out if a format file could drop columns for me during bulk insert. Is column mapping during bulk insert possible if the table has less columns than the .csv? I have read suggestions about doing a full bulk insert and then creating a view, or bulk inserting to a temp table and then inserting from there but I thought it would maybe be easier to pre-process the .csv first? I am not as sharp on T-SQL. Any thoughts would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;My efficiency questions are wondering if any pre-processing of the .csv is possible. I would love to drop a bunch of the columns and even partition the .csv&amp;#39;s into many smaller .csv&amp;#39;s by state. My hunch is that the file will be too large to do this using pandas which is what I am most familiar with. Any thoughts or suggestions on pre-processing .csvs in comparison to being savvier with the bulk insert?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18k750l", "is_robot_indexable": true, "report_reasons": null, "author": "machinegunke11y", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18k750l/personal_cloud_project_help_npi_healthcare_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18k750l/personal_cloud_project_help_npi_healthcare_data/", "subreddit_subscribers": 146596, "created_utc": 1702780351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I inherited a DE team when I joined my new job as a team lead. For some reason (performance, on-premise datacenter) they are using MySql for everything, including multi-terabyte datawarehouses. This led to literally 50+ database servers with hundreds of user accounts. While I prepare a migration to a more appropriate toolchain, I still need to manage access to all of these servers. Any pointers to decent, ideally open source, solutions to manage users, roles and permissions across servers. My Google-fu is failing me.", "author_fullname": "t2_dxt8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Database user management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kj9gp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702825600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I inherited a DE team when I joined my new job as a team lead. For some reason (performance, on-premise datacenter) they are using MySql for everything, including multi-terabyte datawarehouses. This led to literally 50+ database servers with hundreds of user accounts. While I prepare a migration to a more appropriate toolchain, I still need to manage access to all of these servers. Any pointers to decent, ideally open source, solutions to manage users, roles and permissions across servers. My Google-fu is failing me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18kj9gp", "is_robot_indexable": true, "report_reasons": null, "author": "pokepip", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kj9gp/cross_database_user_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kj9gp/cross_database_user_management/", "subreddit_subscribers": 146596, "created_utc": 1702825600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\nI'm planning to launch an OpenSource project on interoperability modules between different data collection and storage sources (KoboToolBox, ODK, Monkey, DHIS2, etc.).\nToday, when you work on this different project, you have to create your own data extraction code.\nCould we work together on this project?\nThe languages to be used are R and Python.", "author_fullname": "t2_gnhzyiiu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Library for interoperability datasources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 69, "top_awarded_type": null, "hide_score": false, "name": "t3_18kg22e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-HgeX6Gn5tgepqRvzYfUYxKNoXyMi7zvKjMP4OXoREM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702815041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,\nI&amp;#39;m planning to launch an OpenSource project on interoperability modules between different data collection and storage sources (KoboToolBox, ODK, Monkey, DHIS2, etc.).\nToday, when you work on this different project, you have to create your own data extraction code.\nCould we work together on this project?\nThe languages to be used are R and Python.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/frdblj8dju6c1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?auto=webp&amp;s=a9726b7fc3a6a933fc2948f9a2dc9ce040d0c80d", "width": 784, "height": 391}, "resolutions": [{"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c65a96d42aa22cbd9e749eb01cf77be3df258fae", "width": 108, "height": 53}, {"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad480beed7fb891aa1e310f5bf88ec471fa6d680", "width": 216, "height": 107}, {"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c7d9156a2a5f220628386bea858918713c9005f", "width": 320, "height": 159}, {"url": "https://preview.redd.it/frdblj8dju6c1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c97bbc7692690e699b23f2c0b2510277676998cf", "width": 640, "height": 319}], "variants": {}, "id": "OOIGtN6jtekJhRzY3_ykgoXStsBRU36dxSvM21Xy0ho"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18kg22e", "is_robot_indexable": true, "report_reasons": null, "author": "hlama26", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kg22e/library_for_interoperability_datasources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/frdblj8dju6c1.jpeg", "subreddit_subscribers": 146596, "created_utc": 1702815041.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer using python, AWS and gitlab in my current project. I have been working for the past 1.7 years in my current role.\n\nI earn \u00a331k per year. \n\nI have a portfolio. I am looking to focus on a new portfolio project now. I have a professional profile picture. I have AWS certifications related to my work as well Python certs.\n\nHow do you choose viable roles to apply for. I am quite gun shy about applying. I have built myself up a lot in preparation for applications next year. I am trying to understand as much as I can. So I can develop more.", "author_fullname": "t2_bkou9use8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on natural career progression. Ask from a \u00a331k UK data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k8ncd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702785645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer using python, AWS and gitlab in my current project. I have been working for the past 1.7 years in my current role.&lt;/p&gt;\n\n&lt;p&gt;I earn \u00a331k per year. &lt;/p&gt;\n\n&lt;p&gt;I have a portfolio. I am looking to focus on a new portfolio project now. I have a professional profile picture. I have AWS certifications related to my work as well Python certs.&lt;/p&gt;\n\n&lt;p&gt;How do you choose viable roles to apply for. I am quite gun shy about applying. I have built myself up a lot in preparation for applications next year. I am trying to understand as much as I can. So I can develop more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18k8ncd", "is_robot_indexable": true, "report_reasons": null, "author": "RagingCharlotte", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18k8ncd/advice_on_natural_career_progression_ask_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18k8ncd/advice_on_natural_career_progression_ask_from_a/", "subreddit_subscribers": 146596, "created_utc": 1702785645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A little bit of my career background. I am a MS in CS, batch 2015. For 5 years I was a software engineer II. Lost job due to Covid then restarted my career as a Data Engineer but at level 1. It hurts me a lot that I am a DE I at my current job even when my job responsibilities/work is closely matching a senior data engineer job role. Well, I also cannot switch jobs currently due to work visa restrictions but looks like in about 2025-26 time range, I can make a job switch safely without hurting my visa formalities.\n\nIn my current job I am working heavily with Pyspark, Python, Airflow, Datadog, K8s, mentoring juniors (my company is hiring recent grads on level II where as I am at level I and I mentor them but still call them juniors fml), AWS, Databases (RDBMS or NoSQL), Snowflake, MSK and Confluent Kafka, Gitlab, CICD, SQL, Bash, Grafana, Opensearch, Loki etc. I have created highly scalable and robust data software applications to fulfill data requests from other teams in my company. I also do data solution architecture-ing to discuss how to make data reach from point A to point B, I join meetings with other teams and negotiate the contracts with them to fulfill their data requirements and what our infra can offer.\n\nOf course, I am not happy being titled as a Data Engineer I. But I have to deal with it till at least 2025-2026. So I have about 1.5 years to prepare for interviews, read books, be a better candidate for my next job. I need guidance on how to do a proper planning to be successful in cracking interviews.\n\nI have following books on my radar to prep for the interviews:\n\n1. Designing Data Intensive Applications\n2. The Data Warehouse Toolkit\n3. Elements of Programming Interviews\n4. Beginning Database Design Solutions\n\nOther than that, I am going to go heavy on Leetcode for Python, SQL, system design questions.\n\nI need guidance to know what am I missing in my preparations and what technologies am I missing to have hands-on in my current job. The more I know current trends of the market, the more I would be able to prep for my next interviews in upcoming years.\n\nYour guidance and suggestions are welcome. Its my time to hustle and solve the problem when its hurting me the most. Thank you!", "author_fullname": "t2_1dlki6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mid Senior Data Engineer looking for guidance in interview preparation plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k4oox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702772556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A little bit of my career background. I am a MS in CS, batch 2015. For 5 years I was a software engineer II. Lost job due to Covid then restarted my career as a Data Engineer but at level 1. It hurts me a lot that I am a DE I at my current job even when my job responsibilities/work is closely matching a senior data engineer job role. Well, I also cannot switch jobs currently due to work visa restrictions but looks like in about 2025-26 time range, I can make a job switch safely without hurting my visa formalities.&lt;/p&gt;\n\n&lt;p&gt;In my current job I am working heavily with Pyspark, Python, Airflow, Datadog, K8s, mentoring juniors (my company is hiring recent grads on level II where as I am at level I and I mentor them but still call them juniors fml), AWS, Databases (RDBMS or NoSQL), Snowflake, MSK and Confluent Kafka, Gitlab, CICD, SQL, Bash, Grafana, Opensearch, Loki etc. I have created highly scalable and robust data software applications to fulfill data requests from other teams in my company. I also do data solution architecture-ing to discuss how to make data reach from point A to point B, I join meetings with other teams and negotiate the contracts with them to fulfill their data requirements and what our infra can offer.&lt;/p&gt;\n\n&lt;p&gt;Of course, I am not happy being titled as a Data Engineer I. But I have to deal with it till at least 2025-2026. So I have about 1.5 years to prepare for interviews, read books, be a better candidate for my next job. I need guidance on how to do a proper planning to be successful in cracking interviews.&lt;/p&gt;\n\n&lt;p&gt;I have following books on my radar to prep for the interviews:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Designing Data Intensive Applications&lt;/li&gt;\n&lt;li&gt;The Data Warehouse Toolkit&lt;/li&gt;\n&lt;li&gt;Elements of Programming Interviews&lt;/li&gt;\n&lt;li&gt;Beginning Database Design Solutions&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Other than that, I am going to go heavy on Leetcode for Python, SQL, system design questions.&lt;/p&gt;\n\n&lt;p&gt;I need guidance to know what am I missing in my preparations and what technologies am I missing to have hands-on in my current job. The more I know current trends of the market, the more I would be able to prep for my next interviews in upcoming years.&lt;/p&gt;\n\n&lt;p&gt;Your guidance and suggestions are welcome. Its my time to hustle and solve the problem when its hurting me the most. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18k4oox", "is_robot_indexable": true, "report_reasons": null, "author": "musicplay313", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18k4oox/mid_senior_data_engineer_looking_for_guidance_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18k4oox/mid_senior_data_engineer_looking_for_guidance_in/", "subreddit_subscribers": 146596, "created_utc": 1702772556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey!\n\nI\u2019m a second year computer science student and I managed to secure a data analytics/dev role at a big bank. This will be my first internship and just want to prepare. Just want some insight.\n\nWhat technologies should I learn beforehand? I\u2019ll be be primarily working with Python, SQL and R. I have lots of experience with python and some with R (took statistics), should I be putting a lot of focus into SQL? \n\nLet me know!", "author_fullname": "t2_slvidrds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips prior to first data/dev internship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k35b1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702768041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;I\u2019m a second year computer science student and I managed to secure a data analytics/dev role at a big bank. This will be my first internship and just want to prepare. Just want some insight.&lt;/p&gt;\n\n&lt;p&gt;What technologies should I learn beforehand? I\u2019ll be be primarily working with Python, SQL and R. I have lots of experience with python and some with R (took statistics), should I be putting a lot of focus into SQL? &lt;/p&gt;\n\n&lt;p&gt;Let me know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18k35b1", "is_robot_indexable": true, "report_reasons": null, "author": "svahsvst", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18k35b1/tips_prior_to_first_datadev_internship/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18k35b1/tips_prior_to_first_datadev_internship/", "subreddit_subscribers": 146596, "created_utc": 1702768041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! I've put together a Docker Compose setup that includes tools like Hadoop, Hive, Spark, PySpark, Jupyter, and Airflow. It's designed to be easy for anyone to set up and start using.\n\nJust clone the [repository](https://github.com/carteakey/data-pipeline-compose) and spin up all services using \\`docker compose up -d\\`.\n\nThe purpose is to just streamline the initial configuration process without the usual setup hassles, which can often be a roadblock for someone trying to get their hands into DE.\n\nLet me know if you have any suggestions / feedback.", "author_fullname": "t2_dy2vwvki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data-pipeline-compose - Data Engineering environment setup using Docker Compose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18kqgcq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702845962.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702845018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I&amp;#39;ve put together a Docker Compose setup that includes tools like Hadoop, Hive, Spark, PySpark, Jupyter, and Airflow. It&amp;#39;s designed to be easy for anyone to set up and start using.&lt;/p&gt;\n\n&lt;p&gt;Just clone the &lt;a href=\"https://github.com/carteakey/data-pipeline-compose\"&gt;repository&lt;/a&gt; and spin up all services using `docker compose up -d`.&lt;/p&gt;\n\n&lt;p&gt;The purpose is to just streamline the initial configuration process without the usual setup hassles, which can often be a roadblock for someone trying to get their hands into DE.&lt;/p&gt;\n\n&lt;p&gt;Let me know if you have any suggestions / feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?auto=webp&amp;s=36bf0a22bba6a2a7d1d4b79a57ae8a4747d31b21", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8a38719866a3a024c5cae9c40e8d7114a86d5dc1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3863951cc22e89d14d365c1a4985954eb48df551", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0742b4d74d60f7ed3d115a3ce55842fb6ef7cfb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4067d1729385147a328e04df989a90dfb686fdf6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c616e99051a82cc5406da4f2c8f4d93a70ade7d7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/NjrBq4Hm5D3q71ZwAS7CxyBv__TwX1VE3VljmaU8FPI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3889262a6c4d922ceb30d055bc7589a95ee9aa58", "width": 1080, "height": 540}], "variants": {}, "id": "5sG3ddbTVX7UyJNxaXu479stuzePSzohwXGcYKXzae0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18kqgcq", "is_robot_indexable": true, "report_reasons": null, "author": "carteakey", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kqgcq/datapipelinecompose_data_engineering_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kqgcq/datapipelinecompose_data_engineering_environment/", "subreddit_subscribers": 146596, "created_utc": 1702845018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work with dataflow and don\u2019t use Hadoop or its ecosystem. \n\nI am pretty familiar with how  MapReduce works and I\u2019ve read on how Hadoop accesses the HDFS, YARN, worker nodes, data replication/redundancy, etc. \n\nI also have read up on Spark and understand that it can use the Hadoop software framework to run MapReduce on data stored in HDFS. \n\nBut here is a point of confusion: people refer to Hadoop and the entire software framework, the entire ecosystem, sometimes incorrectly as simply HDFS, or incorrectly as simply MapReduce computations. Can anyone verify that Hadoop can ONLY perform computations using a MapReduce algorithm? There is NO OTHER framework it can use?\n\nSecond point of confusion: spark is frequently discussed in terms of Hadoop and MapReduce. But if I understand correctly, Spark can operate without using Hadoop, HDFS, or using MapReduce. In other words that is one series of things it can do but it has additional flexibility. Is this correct?\n\nNote: not a question about what is best to use. I just am trying to understand the underlying framework.", "author_fullname": "t2_kii3tm7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hadoop vs MapReduce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kmkf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702834679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work with dataflow and don\u2019t use Hadoop or its ecosystem. &lt;/p&gt;\n\n&lt;p&gt;I am pretty familiar with how  MapReduce works and I\u2019ve read on how Hadoop accesses the HDFS, YARN, worker nodes, data replication/redundancy, etc. &lt;/p&gt;\n\n&lt;p&gt;I also have read up on Spark and understand that it can use the Hadoop software framework to run MapReduce on data stored in HDFS. &lt;/p&gt;\n\n&lt;p&gt;But here is a point of confusion: people refer to Hadoop and the entire software framework, the entire ecosystem, sometimes incorrectly as simply HDFS, or incorrectly as simply MapReduce computations. Can anyone verify that Hadoop can ONLY perform computations using a MapReduce algorithm? There is NO OTHER framework it can use?&lt;/p&gt;\n\n&lt;p&gt;Second point of confusion: spark is frequently discussed in terms of Hadoop and MapReduce. But if I understand correctly, Spark can operate without using Hadoop, HDFS, or using MapReduce. In other words that is one series of things it can do but it has additional flexibility. Is this correct?&lt;/p&gt;\n\n&lt;p&gt;Note: not a question about what is best to use. I just am trying to understand the underlying framework.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18kmkf6", "is_robot_indexable": true, "report_reasons": null, "author": "Ornery_Vanilla1902", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kmkf6/hadoop_vs_mapreduce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kmkf6/hadoop_vs_mapreduce/", "subreddit_subscribers": 146596, "created_utc": 1702834679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have tried reading through the DBT docs but finding it hard to understand how to change the target database for DBT production runs when developing in DBT cloud. I am using Redshift, and want my prod runs to output to its own 'analytics' database, and all dev runs to go to a dbt\\_dev database.  \nTIA ", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Changing DBT Cloud Target Database (Redshift)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kazlm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702794141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried reading through the DBT docs but finding it hard to understand how to change the target database for DBT production runs when developing in DBT cloud. I am using Redshift, and want my prod runs to output to its own &amp;#39;analytics&amp;#39; database, and all dev runs to go to a dbt_dev database.&lt;br/&gt;\nTIA &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18kazlm", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kazlm/changing_dbt_cloud_target_database_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kazlm/changing_dbt_cloud_target_database_redshift/", "subreddit_subscribers": 146596, "created_utc": 1702794141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm developing a data pipeline that takes data from a video game API about player's matches, transforms the data, and loads it into the database.\n\nFirst, I have to get a list of all the match IDs in order to then access their info, but in order to prevent duplicate entries to the player\\_matches table, I remove the match IDs that I already have in my database, from the list of match IDs I receive in the current call.\n\nThe problem comes when I want to do testing on any step in the pipeline, when there are no matches found. If there are no matches found, then the program rightfully exits early because all the subsequent logic is for processing new matches not already in the database.\n\nI had the idea of setting a development flag that would rollback and changes to the database if it's set. That way, I can always test the entirety of the pipeline without having to find new player that have played matches in real time since I last ran the pipeline.\n\nAfter talking about this with ChatGPT, it said this is called a \"dry run.\" I'm not seeing a whole lot of information about it online, so I'm wondering if this is normal practice when developing data pipelines.\n\nEdit: I'm fairly confident this problem has nothing to do with my environment setup or having multiple database environments either. This is a pipeline problem only", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is doing a \"dry run\" of a data pipeline - so that changes to the database are either rolled back or not committed, for testing purposes - normal practice in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18kqo3p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702847010.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702845590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m developing a data pipeline that takes data from a video game API about player&amp;#39;s matches, transforms the data, and loads it into the database.&lt;/p&gt;\n\n&lt;p&gt;First, I have to get a list of all the match IDs in order to then access their info, but in order to prevent duplicate entries to the player_matches table, I remove the match IDs that I already have in my database, from the list of match IDs I receive in the current call.&lt;/p&gt;\n\n&lt;p&gt;The problem comes when I want to do testing on any step in the pipeline, when there are no matches found. If there are no matches found, then the program rightfully exits early because all the subsequent logic is for processing new matches not already in the database.&lt;/p&gt;\n\n&lt;p&gt;I had the idea of setting a development flag that would rollback and changes to the database if it&amp;#39;s set. That way, I can always test the entirety of the pipeline without having to find new player that have played matches in real time since I last ran the pipeline.&lt;/p&gt;\n\n&lt;p&gt;After talking about this with ChatGPT, it said this is called a &amp;quot;dry run.&amp;quot; I&amp;#39;m not seeing a whole lot of information about it online, so I&amp;#39;m wondering if this is normal practice when developing data pipelines.&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;m fairly confident this problem has nothing to do with my environment setup or having multiple database environments either. This is a pipeline problem only&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18kqo3p", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kqo3p/is_doing_a_dry_run_of_a_data_pipeline_so_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kqo3p/is_doing_a_dry_run_of_a_data_pipeline_so_that/", "subreddit_subscribers": 146596, "created_utc": 1702845590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow DE \n\nI would love to hear some tips about how I should proceed with star  schema implementation for DW (Synapse will serve as serving layer, while  Databricks as transformation in PySpark).\n\nI have data with partition by market, what I plan is to enforce schema in file of every market, then in enriched layer, I'll union every  market to create cross market dataset, lets call it master\\_table\n\nNow, I need to distinguish dim tables from fact\\_table, let it be for instance \"AGE\",\"GENDER\",\"LOCATION\" for \"customer\\_profile\" and \"PRODUCT\", \"BRAND\" for \"product\\_profile\"\n\nFrom each dim table, I need to create either surrogate key  (synthetic one, like integer number, incrementally) or composite key  (for instance concatenation of all columns) - Microsoft proposes INT values for joins and groups for better performance\n\nAfter I got the key, I need to have dim table keys back into fact  table and drop original values, leaving only keys and rest columns which  belongs to fact table\n\nMy questions and concerns are:\n\nWhat is the order of tasks? Should I create a view on this cross market (lets call it master\\_table) dataset from databricks and then:\n\n&amp;#x200B;\n\n1. first create VIEW for dim table customer\\_profile like SELECT DISTINCT AGE,GENDER,LOCATION from master\\_table - but then how I can get ROW\\_NUMBER OVER() for customer\\_profile\\_id?\n2. second create VIEW for dim table product\\_profile - SELECT DISTINCT PRODUCT, BRAND from master\\_table - again, how I can get ROW\\_NUMBER OVER() to get product\\_profile\\_id?\n3. third create fact\\_table : SELECT customer\\_profile\\_id, product\\_profile\\_id, \\[some fact measure\\] FROM master\\_table mt  LEFT JOIN customer\\_profile cp ON mt.AGE = cp.AGE AND mt.LOCATION=cp.LOCATION and mt.GENDER=cp.GENDER and same for product\\_profile\n\nIs it the way to go?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Star-Schema DW implementation in real life scenario", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ko96l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702839237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DE &lt;/p&gt;\n\n&lt;p&gt;I would love to hear some tips about how I should proceed with star  schema implementation for DW (Synapse will serve as serving layer, while  Databricks as transformation in PySpark).&lt;/p&gt;\n\n&lt;p&gt;I have data with partition by market, what I plan is to enforce schema in file of every market, then in enriched layer, I&amp;#39;ll union every  market to create cross market dataset, lets call it master_table&lt;/p&gt;\n\n&lt;p&gt;Now, I need to distinguish dim tables from fact_table, let it be for instance &amp;quot;AGE&amp;quot;,&amp;quot;GENDER&amp;quot;,&amp;quot;LOCATION&amp;quot; for &amp;quot;customer_profile&amp;quot; and &amp;quot;PRODUCT&amp;quot;, &amp;quot;BRAND&amp;quot; for &amp;quot;product_profile&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;From each dim table, I need to create either surrogate key  (synthetic one, like integer number, incrementally) or composite key  (for instance concatenation of all columns) - Microsoft proposes INT values for joins and groups for better performance&lt;/p&gt;\n\n&lt;p&gt;After I got the key, I need to have dim table keys back into fact  table and drop original values, leaving only keys and rest columns which  belongs to fact table&lt;/p&gt;\n\n&lt;p&gt;My questions and concerns are:&lt;/p&gt;\n\n&lt;p&gt;What is the order of tasks? Should I create a view on this cross market (lets call it master_table) dataset from databricks and then:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;first create VIEW for dim table customer_profile like SELECT DISTINCT AGE,GENDER,LOCATION from master_table - but then how I can get ROW_NUMBER OVER() for customer_profile_id?&lt;/li&gt;\n&lt;li&gt;second create VIEW for dim table product_profile - SELECT DISTINCT PRODUCT, BRAND from master_table - again, how I can get ROW_NUMBER OVER() to get product_profile_id?&lt;/li&gt;\n&lt;li&gt;third create fact_table : SELECT customer_profile_id, product_profile_id, [some fact measure] FROM master_table mt  LEFT JOIN customer_profile cp ON mt.AGE = cp.AGE AND mt.LOCATION=cp.LOCATION and mt.GENDER=cp.GENDER and same for product_profile&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is it the way to go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ko96l", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ko96l/starschema_dw_implementation_in_real_life_scenario/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ko96l/starschema_dw_implementation_in_real_life_scenario/", "subreddit_subscribers": 146596, "created_utc": 1702839237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys\n\nSo I'm migrating some data from an old DB to our new one, I need to use a custom integrated runtime on the old DB, because of that I can't use the source directly in the data flow activity to make changes, and the new DB already has data that I can't change.\n\nI've gone with this setup:\n\n&amp;#x200B;\n\n1. Copy old DB table to file in blob storage\n2. Pass that file to the data flow\n3. Add new columns/rename as needed\n4. Insert into table\n5. Save new primary keys into a table or file that maps to the old primary keys in the old DB\n\n&amp;#x200B;\n\nI've only got to step 4 so far, but I need to keep the primary keys so step 5 may end up being multiple steps if it becomes a lot of trouble.\n\n&amp;#x200B;\n\nI'm just wondering, if there's a way to simplify my work flow? As I was working on it, it feels like I'm making it overly complicated.", "author_fullname": "t2_56o0g58i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to simplify this process (Azure Data Factory)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18km4jo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702833550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m migrating some data from an old DB to our new one, I need to use a custom integrated runtime on the old DB, because of that I can&amp;#39;t use the source directly in the data flow activity to make changes, and the new DB already has data that I can&amp;#39;t change.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve gone with this setup:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Copy old DB table to file in blob storage&lt;/li&gt;\n&lt;li&gt;Pass that file to the data flow&lt;/li&gt;\n&lt;li&gt;Add new columns/rename as needed&lt;/li&gt;\n&lt;li&gt;Insert into table&lt;/li&gt;\n&lt;li&gt;Save new primary keys into a table or file that maps to the old primary keys in the old DB&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only got to step 4 so far, but I need to keep the primary keys so step 5 may end up being multiple steps if it becomes a lot of trouble.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just wondering, if there&amp;#39;s a way to simplify my work flow? As I was working on it, it feels like I&amp;#39;m making it overly complicated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18km4jo", "is_robot_indexable": true, "report_reasons": null, "author": "IG-55", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18km4jo/is_there_a_way_to_simplify_this_process_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18km4jo/is_there_a_way_to_simplify_this_process_azure/", "subreddit_subscribers": 146596, "created_utc": 1702833550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi l , I want to build a feature pipeline for real time approximation (over 90 min) and use this feature in real time inference (90 min back). The data for the feature and the inference derived from the same events. \n\nHow would you suggest to solve those kind of problems and what technology stack do you use? Do you have some examples ? \n\nThanks", "author_fullname": "t2_j15inhpup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature pipeline in stream for approximation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18kiirt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702823459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi l , I want to build a feature pipeline for real time approximation (over 90 min) and use this feature in real time inference (90 min back). The data for the feature and the inference derived from the same events. &lt;/p&gt;\n\n&lt;p&gt;How would you suggest to solve those kind of problems and what technology stack do you use? Do you have some examples ? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18kiirt", "is_robot_indexable": true, "report_reasons": null, "author": "springRock88", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18kiirt/feature_pipeline_in_stream_for_approximation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18kiirt/feature_pipeline_in_stream_for_approximation/", "subreddit_subscribers": 146596, "created_utc": 1702823459.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}