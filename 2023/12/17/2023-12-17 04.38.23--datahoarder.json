{"kind": "Listing", "data": {"after": "t3_18jlj7e", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_cymqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tech-America has 61.44 TB Solidigm SSDs in stock for anyone who would like to pick up one (or more)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18jr7bi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 91, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 91, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JRDbdU5p-ih-TLRUkK1T_TfJH-PB6IdvyomTaYlbfCc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702733258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech-america.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tech-america.com/item/solidigm-ssd-sbfpf2bv614t001-d5-p5336-61-44tb-2-5-pcie4-0x4-3d5-qlc-retail/sbfpf2bv614t001", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S1JCBVuI14muOgLT5HGiENcuI9RdQ6PKFRra5I3Tmec.jpg?auto=webp&amp;s=ddab834e87c77f2a20110b14192274d36fa3f585", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/S1JCBVuI14muOgLT5HGiENcuI9RdQ6PKFRra5I3Tmec.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db4eed6a6ce54fd45798353a908a24e6b13f9c60", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/S1JCBVuI14muOgLT5HGiENcuI9RdQ6PKFRra5I3Tmec.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc6e307375e63311a623141b2a9b19e3108d1a77", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/S1JCBVuI14muOgLT5HGiENcuI9RdQ6PKFRra5I3Tmec.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24413b113bcef8db88734888ee9a6f0bf1b85ec8", "width": 320, "height": 320}], "variants": {}, "id": "DonADQrU4gV7WaMEXYTEwb2zitE-NnkvTPiB8yncAH4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "18jr7bi", "is_robot_indexable": true, "report_reasons": null, "author": "Torley_", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jr7bi/techamerica_has_6144_tb_solidigm_ssds_in_stock/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tech-america.com/item/solidigm-ssd-sbfpf2bv614t001-d5-p5336-61-44tb-2-5-pcie4-0x4-3d5-qlc-retail/sbfpf2bv614t001", "subreddit_subscribers": 718557, "created_utc": 1702733258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My drives get fanned 24/7, and since it's winter they get pretty chill. At what temp should I worry and do something? under 30 degrees C? Under 25? 20?\n\nMy drives are hovering around 25, some drives even lower at times, like 21/22.", "author_fullname": "t2_9ymyrd1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How cold can you keep the hard drives running at?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jmer8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702713380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My drives get fanned 24/7, and since it&amp;#39;s winter they get pretty chill. At what temp should I worry and do something? under 30 degrees C? Under 25? 20?&lt;/p&gt;\n\n&lt;p&gt;My drives are hovering around 25, some drives even lower at times, like 21/22.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "400TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jmer8", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Arugula-1592", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18jmer8/how_cold_can_you_keep_the_hard_drives_running_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jmer8/how_cold_can_you_keep_the_hard_drives_running_at/", "subreddit_subscribers": 718557, "created_utc": 1702713380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've recently acquired access to a huge number of smaller (2-3TB) HDDs. My question is, how do I connect such a massive number of HDDs (think \"we need a scalable solution that connects 100\\~ of these\") in an economically feasible way? (the main pro of the drives is they're free/extremely cheap)\n\nI've thought about USB -&gt; 8x sata breakouts, but it seems they don't really exist. Bandwidth isn't the limitation, because its very unlikely all 8 drives in a cluster would be active at once. # of ports is what I don't know how to get past, because even with 8x PCIE sata cards, it's infeasible to connect enough of them.\n\nIdeally there's USB in the link somewhere, because I don't need them all drawing power when idle. \n\nWhat's the best solution here? Is it feasible at all, or should I give up and relegate these to cold storage?", "author_fullname": "t2_aw8fvxxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feasible way to connect massive amounts of HDDs to one computer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jmli8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702714179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently acquired access to a huge number of smaller (2-3TB) HDDs. My question is, how do I connect such a massive number of HDDs (think &amp;quot;we need a scalable solution that connects 100~ of these&amp;quot;) in an economically feasible way? (the main pro of the drives is they&amp;#39;re free/extremely cheap)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve thought about USB -&amp;gt; 8x sata breakouts, but it seems they don&amp;#39;t really exist. Bandwidth isn&amp;#39;t the limitation, because its very unlikely all 8 drives in a cluster would be active at once. # of ports is what I don&amp;#39;t know how to get past, because even with 8x PCIE sata cards, it&amp;#39;s infeasible to connect enough of them.&lt;/p&gt;\n\n&lt;p&gt;Ideally there&amp;#39;s USB in the link somewhere, because I don&amp;#39;t need them all drawing power when idle. &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best solution here? Is it feasible at all, or should I give up and relegate these to cold storage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "48TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jmli8", "is_robot_indexable": true, "report_reasons": null, "author": "DrDrago-4", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18jmli8/feasible_way_to_connect_massive_amounts_of_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jmli8/feasible_way_to_connect_massive_amounts_of_hdds/", "subreddit_subscribers": 718557, "created_utc": 1702714179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I dont know a lot about servers Just had a few websites before. But can i rent a webserver without really a website on it. And just use it for me own little cloud storage i can access anywhere. Is this  valid use case or are there better ways? I dont really know the difference between servers like what a vps is etc. And it seems you always have to get a domein name with it even tho i dont use the website function just sftp.", "author_fullname": "t2_3c7nq2fk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can i use a webserver sftp as cloud storage or is that weird, dumb?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jny5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702720146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I dont know a lot about servers Just had a few websites before. But can i rent a webserver without really a website on it. And just use it for me own little cloud storage i can access anywhere. Is this  valid use case or are there better ways? I dont really know the difference between servers like what a vps is etc. And it seems you always have to get a domein name with it even tho i dont use the website function just sftp.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jny5f", "is_robot_indexable": true, "report_reasons": null, "author": "Anakhsunamon", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jny5f/can_i_use_a_webserver_sftp_as_cloud_storage_or_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jny5f/can_i_use_a_webserver_sftp_as_cloud_storage_or_is/", "subreddit_subscribers": 718557, "created_utc": 1702720146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does Synology DS1821+ lock out  (blacklist) third party NICs (or whitelist only theirs), or is that very short compatibility list just the devices that they guarantee will work with the DS1821+ NAS?\n\n&amp;#x200B;\n\nI was considering the TP-Link TX201", "author_fullname": "t2_5a3zhit0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology DS1821+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k69b0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702777465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does Synology DS1821+ lock out  (blacklist) third party NICs (or whitelist only theirs), or is that very short compatibility list just the devices that they guarantee will work with the DS1821+ NAS?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was considering the TP-Link TX201&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k69b0", "is_robot_indexable": true, "report_reasons": null, "author": "FalconSteve89", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k69b0/synology_ds1821/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k69b0/synology_ds1821/", "subreddit_subscribers": 718557, "created_utc": 1702777465.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm having a weird problem with idrive. I upload a file, and then try to share it with someone. I copy the link, share it, and the person tries to access. However, he can't. It's just the loading logo, and that's it.\n\nI tried with a different computer and ISP and no problem, but in my friends house, when I try with a different computer, but same ISP, same problem.\n\nDo you know whats the problem? I googled it, and nothing.\n\nThanks", "author_fullname": "t2_mp9c6lcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IDrive problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k66zw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702777255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a weird problem with idrive. I upload a file, and then try to share it with someone. I copy the link, share it, and the person tries to access. However, he can&amp;#39;t. It&amp;#39;s just the loading logo, and that&amp;#39;s it.&lt;/p&gt;\n\n&lt;p&gt;I tried with a different computer and ISP and no problem, but in my friends house, when I try with a different computer, but same ISP, same problem.&lt;/p&gt;\n\n&lt;p&gt;Do you know whats the problem? I googled it, and nothing.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k66zw", "is_robot_indexable": true, "report_reasons": null, "author": "JackL_88", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k66zw/idrive_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k66zw/idrive_problem/", "subreddit_subscribers": 718557, "created_utc": 1702777255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All!\n\nIs there a (safe) way to mirror my Linux boot drive to another disk? I recently had a scare where I thought my boot drive died, so now I want to have an automatic backup of my data\n\nTo clarify, I'm not looking for the mirror to be bootable (and would prefer it's not, to avoid the issues that could lead to) - I just want to have a second place the same data exists in case my drive dies. My first intuition is to do this with a ZFS mirror, but I'm not sure that's safe to do on the same drive ZFS is running from, or if it can be done after-the-fact when I didn't install Ubuntu on ZFS, as I know it now supports\n\nThanks!", "author_fullname": "t2_4id0x6ry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring Boot Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jydso", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702754240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All!&lt;/p&gt;\n\n&lt;p&gt;Is there a (safe) way to mirror my Linux boot drive to another disk? I recently had a scare where I thought my boot drive died, so now I want to have an automatic backup of my data&lt;/p&gt;\n\n&lt;p&gt;To clarify, I&amp;#39;m not looking for the mirror to be bootable (and would prefer it&amp;#39;s not, to avoid the issues that could lead to) - I just want to have a second place the same data exists in case my drive dies. My first intuition is to do this with a ZFS mirror, but I&amp;#39;m not sure that&amp;#39;s safe to do on the same drive ZFS is running from, or if it can be done after-the-fact when I didn&amp;#39;t install Ubuntu on ZFS, as I know it now supports&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jydso", "is_robot_indexable": true, "report_reasons": null, "author": "ThatFireGuy0", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jydso/mirroring_boot_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jydso/mirroring_boot_drive/", "subreddit_subscribers": 718557, "created_utc": 1702754240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nJust curious what the consensus is on saving archives of email accounts, best formats for viewing them that is somewhat 'future proof'.  What do you think about importing them into Thunderbird?  I believe Thunderbird is multi-platform.\n\nI made the mistake of using Microsoft Entourage (Mac-only) back from about 1999-2005 or so.  If I have those drives, it will be an adventure getting that database converted.", "author_fullname": "t2_6pky9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving (and future-proofing) email accounts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jua51", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702742691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Just curious what the consensus is on saving archives of email accounts, best formats for viewing them that is somewhat &amp;#39;future proof&amp;#39;.  What do you think about importing them into Thunderbird?  I believe Thunderbird is multi-platform.&lt;/p&gt;\n\n&lt;p&gt;I made the mistake of using Microsoft Entourage (Mac-only) back from about 1999-2005 or so.  If I have those drives, it will be an adventure getting that database converted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jua51", "is_robot_indexable": true, "report_reasons": null, "author": "drycounty", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jua51/archiving_and_futureproofing_email_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jua51/archiving_and_futureproofing_email_accounts/", "subreddit_subscribers": 718557, "created_utc": 1702742691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use Epson V600 Photo and \"Restore colors\" (Epson ScanSmart). I get better results than in VueScan.\n\nIs it possible to reduce scanning passes for multiple crop areas in v600 (or any another scanner)?\n\nI want to scan multiple images (e.g. 3) at once (see picture).  \nI open Epson ScanSmart:\n\n* I can scan maximum area with all photos (without auto or manual cropping). It's 1 pass for scanner.\n* Or I can crop my 3 images and scanner will do 3 passes.\n\nOf course it's waste of time and scanner resources but this is not big deals for me.  \nMain difference for me is restored colors. Restored colors are different. Because of when I scan all photos at once scanner takes into account empty space (white cushioning bit or black sheet of paper).\n\nQuestion: Can I make 3 crops and make only 1 pass (not 3) with restoring colors?\n\nIf not I will need to make 1 pass without restoring, then crop images and restore colors in Photoshop or Lightroom.\n\nWith VueSmart I also make 3\\*1 passes.\n\n[process](https://preview.redd.it/v7e44naf5o6c1.jpg?width=840&amp;format=pjpg&amp;auto=webp&amp;s=7abb27d2193e0cfe559f6ba9769847bc04bdfb0f)", "author_fullname": "t2_7i0wwc92", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Epson Perfection V600 Photo Scanner] Reduce scanning passes for multiple crop areas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"v7e44naf5o6c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 110, "x": 108, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df0278a96552236d13de1a04ae8a59ebfdac27f7"}, {"y": 221, "x": 216, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=124de0050e21f4c427f11ff7262088dd9722194e"}, {"y": 327, "x": 320, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=96758b2a9505948fdcbc73c1bb51811293999142"}, {"y": 655, "x": 640, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a457d9d7535823920cc8eec3d3da05afbb9fd34"}], "s": {"y": 860, "x": 840, "u": "https://preview.redd.it/v7e44naf5o6c1.jpg?width=840&amp;format=pjpg&amp;auto=webp&amp;s=7abb27d2193e0cfe559f6ba9769847bc04bdfb0f"}, "id": "v7e44naf5o6c1"}}, "name": "t3_18jsna1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5FPSCWV7s3UcY5c2vPQxs3_0P2CEycKhlXoYNWdERig.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702737811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Epson V600 Photo and &amp;quot;Restore colors&amp;quot; (Epson ScanSmart). I get better results than in VueScan.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to reduce scanning passes for multiple crop areas in v600 (or any another scanner)?&lt;/p&gt;\n\n&lt;p&gt;I want to scan multiple images (e.g. 3) at once (see picture).&lt;br/&gt;\nI open Epson ScanSmart:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I can scan maximum area with all photos (without auto or manual cropping). It&amp;#39;s 1 pass for scanner.&lt;/li&gt;\n&lt;li&gt;Or I can crop my 3 images and scanner will do 3 passes.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Of course it&amp;#39;s waste of time and scanner resources but this is not big deals for me.&lt;br/&gt;\nMain difference for me is restored colors. Restored colors are different. Because of when I scan all photos at once scanner takes into account empty space (white cushioning bit or black sheet of paper).&lt;/p&gt;\n\n&lt;p&gt;Question: Can I make 3 crops and make only 1 pass (not 3) with restoring colors?&lt;/p&gt;\n\n&lt;p&gt;If not I will need to make 1 pass without restoring, then crop images and restore colors in Photoshop or Lightroom.&lt;/p&gt;\n\n&lt;p&gt;With VueSmart I also make 3*1 passes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v7e44naf5o6c1.jpg?width=840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7abb27d2193e0cfe559f6ba9769847bc04bdfb0f\"&gt;process&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jsna1", "is_robot_indexable": true, "report_reasons": null, "author": "michl-pfeiff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jsna1/epson_perfection_v600_photo_scanner_reduce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jsna1/epson_perfection_v600_photo_scanner_reduce/", "subreddit_subscribers": 718557, "created_utc": 1702737811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using a ScanPro 3000 to scan a microfiche. I can see that the document is in color but all my scans are saved in black and white. I've tried to Google this and I can't find anything that verifies that scanning in color is even an option. I'm hoping someone on this forum works in a library or has experience with these scanners and can let me know.\n\nThanks!", "author_fullname": "t2_b35w3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanning Microfiche in color", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k3azt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702768505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a ScanPro 3000 to scan a microfiche. I can see that the document is in color but all my scans are saved in black and white. I&amp;#39;ve tried to Google this and I can&amp;#39;t find anything that verifies that scanning in color is even an option. I&amp;#39;m hoping someone on this forum works in a library or has experience with these scanners and can let me know.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "16TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k3azt", "is_robot_indexable": true, "report_reasons": null, "author": "kyjb70", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18k3azt/scanning_microfiche_in_color/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k3azt/scanning_microfiche_in_color/", "subreddit_subscribers": 718557, "created_utc": 1702768505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi folks, I've searched and there are several threads about UltraDMA CRC Error Count - I took the advice in those threads and swapped cables, but the error persists, and I think my value (91) seems higher than most people have.\n\nDrive is WD RED 6TB WD60EFAX (most of the other people I've seen with the issue were also using WD RED). It's out of warranty so I'm wondering if there's any other things worth trying before I have to replace it.\n\nHere's smartctl output:\n    \n    SMART Attributes Data Structure revision number: 16\n    Vendor Specific SMART Attributes with Thresholds:\n    ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n      1 Raw_Read_Error_Rate     0x002f   200   200   051    Pre-fail  Always       -       0\n      3 Spin_Up_Time            0x0027   224   224   021    Pre-fail  Always       -       3775\n      4 Start_Stop_Count        0x0032   096   096   000    Old_age   Always       -       4889\n      5 Reallocated_Sector_Ct   0x0033   200   200   140    Pre-fail  Always       -       0\n      7 Seek_Error_Rate         0x002e   200   200   000    Old_age   Always       -       0\n      9 Power_On_Hours          0x0032   088   088   000    Old_age   Always       -       9323\n     10 Spin_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0\n     11 Calibration_Retry_Count 0x0032   100   100   000    Old_age   Always       -       0\n     12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       107\n    192 Power-Off_Retract_Count 0x0032   200   200   000    Old_age   Always       -       48\n    193 Load_Cycle_Count        0x0032   199   199   000    Old_age   Always       -       4869\n    194 Temperature_Celsius     0x0022   120   102   000    Old_age   Always       -       30\n    196 Reallocated_Event_Count 0x0032   200   200   000    Old_age   Always       -       0\n    197 Current_Pending_Sector  0x0032   200   200   000    Old_age   Always       -       0\n    198 Offline_Uncorrectable   0x0030   100   253   000    Old_age   Offline      -       0\n    **199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       91**\n    200 Multi_Zone_Error_Rate   0x0008   100   253   000    Old_age   Offline      -       0\n    \nDoesn't seem like much of an issue in smartctl but scrutiny calls it a fail. It also runs a lot colder than my Ironwolf. Obviously I'm not that experienced with this hardware so this is a little open ended. Thanks in advance for any ideas.", "author_fullname": "t2_qb9m0ag7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UDMA_CRC_Error_Count persists after changing cables - anything else to try?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jxjo8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702751895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, I&amp;#39;ve searched and there are several threads about UltraDMA CRC Error Count - I took the advice in those threads and swapped cables, but the error persists, and I think my value (91) seems higher than most people have.&lt;/p&gt;\n\n&lt;p&gt;Drive is WD RED 6TB WD60EFAX (most of the other people I&amp;#39;ve seen with the issue were also using WD RED). It&amp;#39;s out of warranty so I&amp;#39;m wondering if there&amp;#39;s any other things worth trying before I have to replace it.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s smartctl output:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n  1 Raw_Read_Error_Rate     0x002f   200   200   051    Pre-fail  Always       -       0\n  3 Spin_Up_Time            0x0027   224   224   021    Pre-fail  Always       -       3775\n  4 Start_Stop_Count        0x0032   096   096   000    Old_age   Always       -       4889\n  5 Reallocated_Sector_Ct   0x0033   200   200   140    Pre-fail  Always       -       0\n  7 Seek_Error_Rate         0x002e   200   200   000    Old_age   Always       -       0\n  9 Power_On_Hours          0x0032   088   088   000    Old_age   Always       -       9323\n 10 Spin_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0\n 11 Calibration_Retry_Count 0x0032   100   100   000    Old_age   Always       -       0\n 12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       107\n192 Power-Off_Retract_Count 0x0032   200   200   000    Old_age   Always       -       48\n193 Load_Cycle_Count        0x0032   199   199   000    Old_age   Always       -       4869\n194 Temperature_Celsius     0x0022   120   102   000    Old_age   Always       -       30\n196 Reallocated_Event_Count 0x0032   200   200   000    Old_age   Always       -       0\n197 Current_Pending_Sector  0x0032   200   200   000    Old_age   Always       -       0\n198 Offline_Uncorrectable   0x0030   100   253   000    Old_age   Offline      -       0\n**199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       91**\n200 Multi_Zone_Error_Rate   0x0008   100   253   000    Old_age   Offline      -       0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Doesn&amp;#39;t seem like much of an issue in smartctl but scrutiny calls it a fail. It also runs a lot colder than my Ironwolf. Obviously I&amp;#39;m not that experienced with this hardware so this is a little open ended. Thanks in advance for any ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jxjo8", "is_robot_indexable": true, "report_reasons": null, "author": "ambiance6462", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jxjo8/udma_crc_error_count_persists_after_changing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jxjo8/udma_crc_error_count_persists_after_changing/", "subreddit_subscribers": 718557, "created_utc": 1702751895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nI'm not exactly a data hoarder... compared to others here at least, so I really hope this is the appropriate sub for this question. I have a fair amount of archival stuff for old computers such as software and HDD images, so not small quantities of data, plus several TB of personal data, mainly documents and build outputs (software dev).\n\nGot myself an LTO-5 drive (HP EH958A external SAS drive) and a box of 25 tapes, tapes are yet to arrive but drive is here. Was pretty dusty when it arrived both in and outside so I cracked the external enclosure case and cleaned it out thouroughly. Daren't open the actual drive itself for fear of destroying it.\n\nLooking through the front flap, I see a fair amount of dust. Using an electronics wipe, I gently reached in and got rid of most of the dust on the surface taking care not to touch anything past the front cavity, and not wipe away any lubricating greases.\n\nI can see the takeup reel at the back of the unit which appears to have a transparent plastic tape attached to it, which I guess is normal.\n\nThe reel itself looks pretty dusty, and knowing that's where the tape media will eventually be spooled has me concerned that its just going to rub all of that dust off, onto the side edge of the tape.\n\nAnything I can/should do about this dust?\n\nAny other first timer advice would be greatly appreciated too, as I am still seeking some open source software for forever-incremental backups to tapes for both personal and archival offsite backups (append changes to dataset to existing tape rather than rewrite entire tape every time, until tape gets full, then rewrite entire tape)\n\nThanks!", "author_fullname": "t2_1pmvzm8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO-5 Drive takeup reel dusty, cause for concern? + Forever-incremental software recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jxah2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702751158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not exactly a data hoarder... compared to others here at least, so I really hope this is the appropriate sub for this question. I have a fair amount of archival stuff for old computers such as software and HDD images, so not small quantities of data, plus several TB of personal data, mainly documents and build outputs (software dev).&lt;/p&gt;\n\n&lt;p&gt;Got myself an LTO-5 drive (HP EH958A external SAS drive) and a box of 25 tapes, tapes are yet to arrive but drive is here. Was pretty dusty when it arrived both in and outside so I cracked the external enclosure case and cleaned it out thouroughly. Daren&amp;#39;t open the actual drive itself for fear of destroying it.&lt;/p&gt;\n\n&lt;p&gt;Looking through the front flap, I see a fair amount of dust. Using an electronics wipe, I gently reached in and got rid of most of the dust on the surface taking care not to touch anything past the front cavity, and not wipe away any lubricating greases.&lt;/p&gt;\n\n&lt;p&gt;I can see the takeup reel at the back of the unit which appears to have a transparent plastic tape attached to it, which I guess is normal.&lt;/p&gt;\n\n&lt;p&gt;The reel itself looks pretty dusty, and knowing that&amp;#39;s where the tape media will eventually be spooled has me concerned that its just going to rub all of that dust off, onto the side edge of the tape.&lt;/p&gt;\n\n&lt;p&gt;Anything I can/should do about this dust?&lt;/p&gt;\n\n&lt;p&gt;Any other first timer advice would be greatly appreciated too, as I am still seeking some open source software for forever-incremental backups to tapes for both personal and archival offsite backups (append changes to dataset to existing tape rather than rewrite entire tape every time, until tape gets full, then rewrite entire tape)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jxah2", "is_robot_indexable": true, "report_reasons": null, "author": "DevelopedLogic", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jxah2/lto5_drive_takeup_reel_dusty_cause_for_concern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jxah2/lto5_drive_takeup_reel_dusty_cause_for_concern/", "subreddit_subscribers": 718557, "created_utc": 1702751158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My accountant was asking me for help in backing up her data (I'm an IT guy).    She uses a cloud-based system called Tax Dome where she stores all of her files and she is worried she is putting all of her eggs in one basket with them.   It's only about 300GB of data but it would be a big deal if she lost it all.\n\nTax Dome mounts as a drive in Windows requiring occasional authentication.   Was thinking some sort of continuous cloud backup service (backing up the Tax Dome drive) would work for her as the machine is likely not running 24/7 and she is typically authenticated into Tax Dome during the work day, at least.\n\nAny recommendations for her that are simple to use, very reliable, has good support, and do a good job alerting if the backup is failing for any reason?   Was initially thinking IDrive or pCloud but the reviews seem quite mixed.", "author_fullname": "t2_ewor6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple and Reliable Continuous Backup Options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jrixz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702734318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My accountant was asking me for help in backing up her data (I&amp;#39;m an IT guy).    She uses a cloud-based system called Tax Dome where she stores all of her files and she is worried she is putting all of her eggs in one basket with them.   It&amp;#39;s only about 300GB of data but it would be a big deal if she lost it all.&lt;/p&gt;\n\n&lt;p&gt;Tax Dome mounts as a drive in Windows requiring occasional authentication.   Was thinking some sort of continuous cloud backup service (backing up the Tax Dome drive) would work for her as the machine is likely not running 24/7 and she is typically authenticated into Tax Dome during the work day, at least.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations for her that are simple to use, very reliable, has good support, and do a good job alerting if the backup is failing for any reason?   Was initially thinking IDrive or pCloud but the reviews seem quite mixed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jrixz", "is_robot_indexable": true, "report_reasons": null, "author": "christoman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jrixz/simple_and_reliable_continuous_backup_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jrixz/simple_and_reliable_continuous_backup_options/", "subreddit_subscribers": 718557, "created_utc": 1702734318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Should one download audio files from youtube into Flac? It seems like youtube supports decently high bitrates (a file I just downloaded has a 20khz cutoff). I don't know if there is a way to download their version without converting it into another lossy transcoded file format. It seems like a waste of space to download non lossless audio into lossless but I'm a little worried about file degradation. Let me know what you think!", "author_fullname": "t2_ld10r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice when converting youtube audio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jp8qx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702725730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Should one download audio files from youtube into Flac? It seems like youtube supports decently high bitrates (a file I just downloaded has a 20khz cutoff). I don&amp;#39;t know if there is a way to download their version without converting it into another lossy transcoded file format. It seems like a waste of space to download non lossless audio into lossless but I&amp;#39;m a little worried about file degradation. Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jp8qx", "is_robot_indexable": true, "report_reasons": null, "author": "MadMax2230", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jp8qx/best_practice_when_converting_youtube_audio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jp8qx/best_practice_when_converting_youtube_audio/", "subreddit_subscribers": 718557, "created_utc": 1702725730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I feel like I'm not asking for much.\n\n1. On the PC side: just some proper syncing technologies that sync from my PC to Cloud (i.e. for my past academic work, I just want to have it properly stored and stuff); for my current work, I want to be able to just have a proper 1 to 1 sync, i.e. if I delete something, cloud side should delete as well.\n2. On my phone side (android): same old proper syncing, maybe with an option for me to keep some stuff in the cloud that I don't need on my phone (i.e. certain older albums)\n\nI have been trying to drag and drop stuff to onedrive but when folder size is too big it just freeze entirely.\n\nOneDrive sync is total crap and it just doesn't work...\n\nAre there better cloud service provider out there? I am considering stuff like pCloud but am worried if they would be around few years from now.\n\nReally really appreciate any advice!!!", "author_fullname": "t2_frutaosj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Honestly, what is the best cloud storage provider that has proper syncing capabilities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jo5if", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702721039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like I&amp;#39;m not asking for much.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;On the PC side: just some proper syncing technologies that sync from my PC to Cloud (i.e. for my past academic work, I just want to have it properly stored and stuff); for my current work, I want to be able to just have a proper 1 to 1 sync, i.e. if I delete something, cloud side should delete as well.&lt;/li&gt;\n&lt;li&gt;On my phone side (android): same old proper syncing, maybe with an option for me to keep some stuff in the cloud that I don&amp;#39;t need on my phone (i.e. certain older albums)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have been trying to drag and drop stuff to onedrive but when folder size is too big it just freeze entirely.&lt;/p&gt;\n\n&lt;p&gt;OneDrive sync is total crap and it just doesn&amp;#39;t work...&lt;/p&gt;\n\n&lt;p&gt;Are there better cloud service provider out there? I am considering stuff like pCloud but am worried if they would be around few years from now.&lt;/p&gt;\n\n&lt;p&gt;Really really appreciate any advice!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jo5if", "is_robot_indexable": true, "report_reasons": null, "author": "warrior123456781", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jo5if/honestly_what_is_the_best_cloud_storage_provider/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jo5if/honestly_what_is_the_best_cloud_storage_provider/", "subreddit_subscribers": 718557, "created_utc": 1702721039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nLong story short - I've spent recent days to recover and move files from multiple HDDs from the past 10 years. Overall about 3 TB of data.\n\n- Now I have about 5 different partitions\n- Some of the files are duplicates\n- Some of the files are sadly corrupted\n-I'm dealing with many different files from images, office files, Time Machine backups and so on\n\nNow as a next step I would like to organise everything, separate them by file types, ideally remove duplicates and corrupted files and move them to a single location. \n\nIs there any (maybe even free) software that you would recommend to do such thing?", "author_fullname": "t2_6cmljsoq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any decent software to categorise and move files based on file type?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jnj91", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702718328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Long story short - I&amp;#39;ve spent recent days to recover and move files from multiple HDDs from the past 10 years. Overall about 3 TB of data.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Now I have about 5 different partitions&lt;/li&gt;\n&lt;li&gt;Some of the files are duplicates&lt;/li&gt;\n&lt;li&gt;Some of the files are sadly corrupted\n-I&amp;#39;m dealing with many different files from images, office files, Time Machine backups and so on&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now as a next step I would like to organise everything, separate them by file types, ideally remove duplicates and corrupted files and move them to a single location. &lt;/p&gt;\n\n&lt;p&gt;Is there any (maybe even free) software that you would recommend to do such thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jnj91", "is_robot_indexable": true, "report_reasons": null, "author": "Tuttle489", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jnj91/any_decent_software_to_categorise_and_move_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jnj91/any_decent_software_to_categorise_and_move_files/", "subreddit_subscribers": 718557, "created_utc": 1702718328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I hope this doesn't violate the subs rules.\n\nI finally want to do real backups for my data, not just relying on my NAS with Raid 5 and hoping that no major thing breaks.\n\nMy data mostly consist of personal pictures, documents and a movie/music library. All's in all not more than 5TB. It's not super important stuff, but I still would like to keep it. Archive storage won't allow me to regularly update the backup, but for old pictures this wouldn't really matter.\n\nI was thinking about cloud archive storage. Something like Azure blob storage. Price per GB is only 0,0009 \u20ac per month, so pretty cheap. But what I still haven't fully understood are the extra costs to access the data and how to actually accomplish that.\n\nI'm aware that it's not a simple cloud with a pretty web interface, but even with some searching I could figure out how to setup the storage.\n\nMy question now is: Is this the best/cheapest way to backup my stuff? Is there a cheaper provider or a better alternative solution? Does this even work at all? And how would I do it?", "author_fullname": "t2_2twg4boe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best/cheapest data archive storage for personal stuff", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18k8cat", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702784522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this doesn&amp;#39;t violate the subs rules.&lt;/p&gt;\n\n&lt;p&gt;I finally want to do real backups for my data, not just relying on my NAS with Raid 5 and hoping that no major thing breaks.&lt;/p&gt;\n\n&lt;p&gt;My data mostly consist of personal pictures, documents and a movie/music library. All&amp;#39;s in all not more than 5TB. It&amp;#39;s not super important stuff, but I still would like to keep it. Archive storage won&amp;#39;t allow me to regularly update the backup, but for old pictures this wouldn&amp;#39;t really matter.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about cloud archive storage. Something like Azure blob storage. Price per GB is only 0,0009 \u20ac per month, so pretty cheap. But what I still haven&amp;#39;t fully understood are the extra costs to access the data and how to actually accomplish that.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that it&amp;#39;s not a simple cloud with a pretty web interface, but even with some searching I could figure out how to setup the storage.&lt;/p&gt;\n\n&lt;p&gt;My question now is: Is this the best/cheapest way to backup my stuff? Is there a cheaper provider or a better alternative solution? Does this even work at all? And how would I do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k8cat", "is_robot_indexable": true, "report_reasons": null, "author": "ichfrissdich", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k8cat/bestcheapest_data_archive_storage_for_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k8cat/bestcheapest_data_archive_storage_for_personal/", "subreddit_subscribers": 718557, "created_utc": 1702784522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a very humble script that calls one json endpoint a day, I can get to it from my laptop, but it stopped working from github actions.\n\nAnyone in the same boat?", "author_fullname": "t2_4uxoi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone accessing reddit .json endpoints noticed getting blocked recently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k1qw3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702763993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a very humble script that calls one json endpoint a day, I can get to it from my laptop, but it stopped working from github actions.&lt;/p&gt;\n\n&lt;p&gt;Anyone in the same boat?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k1qw3", "is_robot_indexable": true, "report_reasons": null, "author": "Madd0g", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k1qw3/anyone_accessing_reddit_json_endpoints_noticed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k1qw3/anyone_accessing_reddit_json_endpoints_noticed/", "subreddit_subscribers": 718557, "created_utc": 1702763993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I recently got my hands on a bunch of 16tb EXOS drives out of an old data center, however they are all password protected and refuse to do anything.\n\n-They will not format in windows disk manager and show as a grey volume\n\n-HDPARM shows the following info under security\n\n    Security:\n    \tMaster password revision code = 65534\n    \t\tsupported\n    \t\tenabled\n    \t\tlocked\n    \tnot\tfrozen\n    \tnot\texpired: security count\n    \t\tsupported: enhanced erase\n    \tSecurity level high\n    \t1282min for SECURITY ERASE UNIT. 1282min for ENHANCED SECURITY ERASE UNIT.\n\n-None of the SeaChest format functions will do anything\n\n-RevertSP gives this output:\n\n    ==========================================================================================\n     SeaChest_Erase - Seagate drive utilities - NVMe Enabled\n     Copyright (c) 2014-2023 Seagate Technology LLC and/or its Affiliates, All Rights Reserved\n     SeaChest_Erase Version: 4.1.0-4_1_1 X86_64\n     Build Date: Mar 27 2023\n     Today: Sat Dec 16 21:43:57 2023\tUser: root\n    ==========================================================================================\n    \n    /dev/sg6 - ST16000NM001G-2KK103 - ZL23XAGN - SN02 - ATA\n    \n    RevertSP\n    RevertSP is not supported on this device.\n\n-ataSecutityInfo looks like this:\n\n    ==========================================================================================\n     SeaChest_Security - Seagate drive utilities - NVMe Enabled\n     Copyright (c) 2014-2023 Seagate Technology LLC and/or its Affiliates, All Rights Reserved\n     SeaChest_Security Version: 3.2.1-4_1_1 X86_64\n     Build Date: Mar 27 2023\n     Today: Sat Dec 16 21:49:33 2023\tUser: root\n    ==========================================================================================\n    \n    /dev/sg6 - ST16000NM001G-2KK103 - ZL23XAGN - SN02 - ATA\n    \n        ====ATA Security Information====\n        Security State: 4\n        \tEnabled: True\n        \tLocked: True\n        \tFrozen: False\n        \tPassword Attempts Exceeded: False\n        Master Password Capability: High\n        Master Password Identifier: 65534 (may be set to manufacture master password)\n        Enhanced Erase Time Estimate:  21 hours 22 minutes \n        Security Erase Time Estimate:  21 hours 22 minutes \n        All user data is encrypted: False\n        Restricted Sanitize Overrides ATA Security: False\n        SAT security protocol supported: False\n    \n    \n\n-SeaTools detects the drive but will not perform any operations past basic tests\n\n-The drives DO have a PSID and the seagate secure labeling on them, but sedutil-cli does not detect them as compliant\n\n-The computer i have it in will not boot with the drive installed and will show a password input screen instead\n\nIs there anything I can do with these things or are they just completely bricked?", "author_fullname": "t2_c6la42qsr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Locked seagate exos drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k1m22", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702765604.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702763586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I recently got my hands on a bunch of 16tb EXOS drives out of an old data center, however they are all password protected and refuse to do anything.&lt;/p&gt;\n\n&lt;p&gt;-They will not format in windows disk manager and show as a grey volume&lt;/p&gt;\n\n&lt;p&gt;-HDPARM shows the following info under security&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Security:\n    Master password revision code = 65534\n        supported\n        enabled\n        locked\n    not frozen\n    not expired: security count\n        supported: enhanced erase\n    Security level high\n    1282min for SECURITY ERASE UNIT. 1282min for ENHANCED SECURITY ERASE UNIT.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;-None of the SeaChest format functions will do anything&lt;/p&gt;\n\n&lt;p&gt;-RevertSP gives this output:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;==========================================================================================\n SeaChest_Erase - Seagate drive utilities - NVMe Enabled\n Copyright (c) 2014-2023 Seagate Technology LLC and/or its Affiliates, All Rights Reserved\n SeaChest_Erase Version: 4.1.0-4_1_1 X86_64\n Build Date: Mar 27 2023\n Today: Sat Dec 16 21:43:57 2023    User: root\n==========================================================================================\n\n/dev/sg6 - ST16000NM001G-2KK103 - ZL23XAGN - SN02 - ATA\n\nRevertSP\nRevertSP is not supported on this device.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;-ataSecutityInfo looks like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;==========================================================================================\n SeaChest_Security - Seagate drive utilities - NVMe Enabled\n Copyright (c) 2014-2023 Seagate Technology LLC and/or its Affiliates, All Rights Reserved\n SeaChest_Security Version: 3.2.1-4_1_1 X86_64\n Build Date: Mar 27 2023\n Today: Sat Dec 16 21:49:33 2023    User: root\n==========================================================================================\n\n/dev/sg6 - ST16000NM001G-2KK103 - ZL23XAGN - SN02 - ATA\n\n    ====ATA Security Information====\n    Security State: 4\n        Enabled: True\n        Locked: True\n        Frozen: False\n        Password Attempts Exceeded: False\n    Master Password Capability: High\n    Master Password Identifier: 65534 (may be set to manufacture master password)\n    Enhanced Erase Time Estimate:  21 hours 22 minutes \n    Security Erase Time Estimate:  21 hours 22 minutes \n    All user data is encrypted: False\n    Restricted Sanitize Overrides ATA Security: False\n    SAT security protocol supported: False\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;-SeaTools detects the drive but will not perform any operations past basic tests&lt;/p&gt;\n\n&lt;p&gt;-The drives DO have a PSID and the seagate secure labeling on them, but sedutil-cli does not detect them as compliant&lt;/p&gt;\n\n&lt;p&gt;-The computer i have it in will not boot with the drive installed and will show a password input screen instead&lt;/p&gt;\n\n&lt;p&gt;Is there anything I can do with these things or are they just completely bricked?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k1m22", "is_robot_indexable": true, "report_reasons": null, "author": "Technical-Job-6641", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k1m22/locked_seagate_exos_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k1m22/locked_seagate_exos_drives/", "subreddit_subscribers": 718557, "created_utc": 1702763586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there any script or way to download all TikTok URLs from your downloaded data like videos and text files? I realize the URLs have to be clicked on first. And then you can get the actual URL for downloading those videos. Tiktok downloaders cannot download links from the text file provided by the downloaded Tiktok data. Is there a workaround to this? ", "author_fullname": "t2_2vmpw7em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "tiktok mass download from downloaded data liked videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18k1fdf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702763038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any script or way to download all TikTok URLs from your downloaded data like videos and text files? I realize the URLs have to be clicked on first. And then you can get the actual URL for downloading those videos. Tiktok downloaders cannot download links from the text file provided by the downloaded Tiktok data. Is there a workaround to this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18k1fdf", "is_robot_indexable": true, "report_reasons": null, "author": "UltraElixir", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18k1fdf/tiktok_mass_download_from_downloaded_data_liked/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18k1fdf/tiktok_mass_download_from_downloaded_data_liked/", "subreddit_subscribers": 718557, "created_utc": 1702763038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there,\n\nI know there are a bunch of posts about this topic, but I'm in the process of doing a NAS upgrade, and I'm likely going from 5 drives to 3. I currently have 4 red plus drives and 1 schucked drive. These are likely quieter than the Exos drives, but I'm going from 5 to 3. Would 3 Exos drives be a lot noisier than 5 of the drives I currently have?\n\nAlso, I do have a dedicated server closet, and I've put extra sound protection on the door. When I walk by, I still hear some fans, though, so I'm overall looking to reduce heat / noise in the closet. Curious to get your thoughts here. Thanks!", "author_fullname": "t2_bra1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exos drive noise considerations... a few questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jy8ci", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702753817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;I know there are a bunch of posts about this topic, but I&amp;#39;m in the process of doing a NAS upgrade, and I&amp;#39;m likely going from 5 drives to 3. I currently have 4 red plus drives and 1 schucked drive. These are likely quieter than the Exos drives, but I&amp;#39;m going from 5 to 3. Would 3 Exos drives be a lot noisier than 5 of the drives I currently have?&lt;/p&gt;\n\n&lt;p&gt;Also, I do have a dedicated server closet, and I&amp;#39;ve put extra sound protection on the door. When I walk by, I still hear some fans, though, so I&amp;#39;m overall looking to reduce heat / noise in the closet. Curious to get your thoughts here. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18jy8ci", "is_robot_indexable": true, "report_reasons": null, "author": "hungarianhc", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jy8ci/exos_drive_noise_considerations_a_few_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jy8ci/exos_drive_noise_considerations_a_few_questions/", "subreddit_subscribers": 718557, "created_utc": 1702753817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Reddit,\n\nI have 8 hard drives for media storage, which constitute my active drives my PC (Ryzen 7 5800x, GB x570 AORUS Elite). \n\nI use three redundant sets of backup hard drives (3x2=6 drives), which I refresh at regular intervals. All are Western Digital, either Gold or Red Pro. All are of significant sizes (16TB or larger), setup using BitLocker in identical configurations, and all drives are less than three years old.\n\nWhen re-connecting a drive or copying content to a drive for the first time, I copy contents to the drive, power down the machine, power up again to verify contents, physically disconnect the drive, and place it into storage.\n\nWhen it comes time to do a backup, I re-connect the drive, update the content, and disconnect the drive again, which I store at an alternate location until the next regularly scheduled backup. \n\nI transport that drives in padded cases inside anti-static bags when moving them.\n\nWhen I reconnect any drive, I use the same SATA cables and power cables i.e., changing out the drive to any one of the other 5 backup drives works.\n\nThe past three instances I have tried to do a backup, a Western Digital Red Pro 20TB has failed after the first use: I connected the new drive, setup BitLocker, and copied content to the drive, and then stored it. \n\nHowever, when I reconnect that drive, it does not detect: not visible in BIOS or Windows. The drive does not spin-up\u2014you cannot hear feel it begin spinning or hear it power up. The drive is simply dead. This has now happened x3 *IN A ROW*. \n\nI tried swapping SATA and power cables for the dead drive to either of the drives in the PC, and those cables work\u2014only the one drive was dead. I can also connect any other drive using same cables.\n\nThe first time this happened, I RMA\u2019d and sent it back to Western Digital, which provided a replacement, but then it happened again: I connected the new drive and copied content to it and then stored it. \n\nHowever, when I reconnected that drive, it does not detect: not visible in BIOS or Windows\u2014the drive is dead and won\u2019t detect, but every other drive will.\n\nSo I RMA\u2019d that drive and sent it back to Western Digital, received a replacement, and then it happened again\u2013I can copy content to the drive once, but after reconnecting the drive to update the content, the drive is dead.\n\nSo I RMA\u2019d that drive\u2014now I am on my fourth replacement, but it is statistically improbably that 3 drives would fail in a row. \n\nDoes Reddit have any ideas? Western Digital Support said it is \u201creally unusual\u201d. I work in a technical company, and my technical team doesn\u2019t have any ideas and agrees it is almost impossible that 3 drives would fail in a row but all of the other drives work fine using same cables.", "author_fullname": "t2_q2ju67pv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "x3 20TB Western Digital Red Pro failed... in a row!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jvj0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702746239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt;\n\n&lt;p&gt;I have 8 hard drives for media storage, which constitute my active drives my PC (Ryzen 7 5800x, GB x570 AORUS Elite). &lt;/p&gt;\n\n&lt;p&gt;I use three redundant sets of backup hard drives (3x2=6 drives), which I refresh at regular intervals. All are Western Digital, either Gold or Red Pro. All are of significant sizes (16TB or larger), setup using BitLocker in identical configurations, and all drives are less than three years old.&lt;/p&gt;\n\n&lt;p&gt;When re-connecting a drive or copying content to a drive for the first time, I copy contents to the drive, power down the machine, power up again to verify contents, physically disconnect the drive, and place it into storage.&lt;/p&gt;\n\n&lt;p&gt;When it comes time to do a backup, I re-connect the drive, update the content, and disconnect the drive again, which I store at an alternate location until the next regularly scheduled backup. &lt;/p&gt;\n\n&lt;p&gt;I transport that drives in padded cases inside anti-static bags when moving them.&lt;/p&gt;\n\n&lt;p&gt;When I reconnect any drive, I use the same SATA cables and power cables i.e., changing out the drive to any one of the other 5 backup drives works.&lt;/p&gt;\n\n&lt;p&gt;The past three instances I have tried to do a backup, a Western Digital Red Pro 20TB has failed after the first use: I connected the new drive, setup BitLocker, and copied content to the drive, and then stored it. &lt;/p&gt;\n\n&lt;p&gt;However, when I reconnect that drive, it does not detect: not visible in BIOS or Windows. The drive does not spin-up\u2014you cannot hear feel it begin spinning or hear it power up. The drive is simply dead. This has now happened x3 &lt;em&gt;IN A ROW&lt;/em&gt;. &lt;/p&gt;\n\n&lt;p&gt;I tried swapping SATA and power cables for the dead drive to either of the drives in the PC, and those cables work\u2014only the one drive was dead. I can also connect any other drive using same cables.&lt;/p&gt;\n\n&lt;p&gt;The first time this happened, I RMA\u2019d and sent it back to Western Digital, which provided a replacement, but then it happened again: I connected the new drive and copied content to it and then stored it. &lt;/p&gt;\n\n&lt;p&gt;However, when I reconnected that drive, it does not detect: not visible in BIOS or Windows\u2014the drive is dead and won\u2019t detect, but every other drive will.&lt;/p&gt;\n\n&lt;p&gt;So I RMA\u2019d that drive and sent it back to Western Digital, received a replacement, and then it happened again\u2013I can copy content to the drive once, but after reconnecting the drive to update the content, the drive is dead.&lt;/p&gt;\n\n&lt;p&gt;So I RMA\u2019d that drive\u2014now I am on my fourth replacement, but it is statistically improbably that 3 drives would fail in a row. &lt;/p&gt;\n\n&lt;p&gt;Does Reddit have any ideas? Western Digital Support said it is \u201creally unusual\u201d. I work in a technical company, and my technical team doesn\u2019t have any ideas and agrees it is almost impossible that 3 drives would fail in a row but all of the other drives work fine using same cables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jvj0v", "is_robot_indexable": true, "report_reasons": null, "author": "RecoveringDataHoard2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jvj0v/x3_20tb_western_digital_red_pro_failed_in_a_row/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jvj0v/x3_20tb_western_digital_red_pro_failed_in_a_row/", "subreddit_subscribers": 718557, "created_utc": 1702746239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello dear colleagues,  \nSince there were similar questions in the past with relatively unclear conditions, I have tried to make the subject line as clear as possible. I tried some of the previously suggested methods in older threads like Firefox add-on titled \"Reddit All Comments Viewer by Draxxx\", but it appears that one is not operational anymore - I have tried several experiments but I am not getting browser option \"Load All Comments\" with that add-on.   \n\n\nSo basically my conditions are the following:  \n\n\n\\- Tool that would allow the saving/exporting the entire Reddit thread/post - especially targeted for large threads containing over 500 comments.  \n\\- Manual options are not practical since for proper archiving the user has to manually open/expand hundreds of comments and replies to save the entire thread properly. (similar questions in the past by other users haven't explained that part well which lead to many redundant replies).  \n\\- So the tool should load all comments/replies automatically and then the entire thread could be saved properly to capture all for historical reference/offline browsing.\n\nDoes anyone know is there a working option for that currently in 2023/2024.   \n\n\nBest wishes.   \n\n\n  \n", "author_fullname": "t2_p695jcz7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best method in 2023/2024 to save/extract large Reddit threads with all comments expanded/opened for offline browsing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jo6e8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702721143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello dear colleagues,&lt;br/&gt;\nSince there were similar questions in the past with relatively unclear conditions, I have tried to make the subject line as clear as possible. I tried some of the previously suggested methods in older threads like Firefox add-on titled &amp;quot;Reddit All Comments Viewer by Draxxx&amp;quot;, but it appears that one is not operational anymore - I have tried several experiments but I am not getting browser option &amp;quot;Load All Comments&amp;quot; with that add-on.   &lt;/p&gt;\n\n&lt;p&gt;So basically my conditions are the following:  &lt;/p&gt;\n\n&lt;p&gt;- Tool that would allow the saving/exporting the entire Reddit thread/post - especially targeted for large threads containing over 500 comments.&lt;br/&gt;\n- Manual options are not practical since for proper archiving the user has to manually open/expand hundreds of comments and replies to save the entire thread properly. (similar questions in the past by other users haven&amp;#39;t explained that part well which lead to many redundant replies).&lt;br/&gt;\n- So the tool should load all comments/replies automatically and then the entire thread could be saved properly to capture all for historical reference/offline browsing.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know is there a working option for that currently in 2023/2024.   &lt;/p&gt;\n\n&lt;p&gt;Best wishes.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jo6e8", "is_robot_indexable": true, "report_reasons": null, "author": "ObservationDeck2001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jo6e8/what_is_the_best_method_in_20232024_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jo6e8/what_is_the_best_method_in_20232024_to/", "subreddit_subscribers": 718557, "created_utc": 1702721143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought a SABRENT Rocket Q 8TB NVMe M.2 with the hopes of using it as a Time Machine backup drive for mac. I also use backblaze for online backups, so quiet operation and speed is more important to me than data persistance.\n\nI found that upon the initial back up of my 2tb drive, the speed was initially fast (maxing out the usb 3.2 speeds of the enclosure), but then after about 15 mins of operation, the transfer speed grinds to a halt. The enclosure is very hot to the touch. I'm thinking its overheating, as I don't think caching is a problem in this circumstance.\n\nAny ideas for a better enclosure to use for this purpose - maybe something with a fan, and heat pads for both sides of the SSD rather than just one, and maybe a large heat sink?\n\nI've searched around and can't find anything that looks promising, and that claims it will work with an 8tb drive.", "author_fullname": "t2_czx3tdrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best enclosure for 8tb NVMe M.2 SSD for large file transfer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jmi5t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702713769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought a SABRENT Rocket Q 8TB NVMe M.2 with the hopes of using it as a Time Machine backup drive for mac. I also use backblaze for online backups, so quiet operation and speed is more important to me than data persistance.&lt;/p&gt;\n\n&lt;p&gt;I found that upon the initial back up of my 2tb drive, the speed was initially fast (maxing out the usb 3.2 speeds of the enclosure), but then after about 15 mins of operation, the transfer speed grinds to a halt. The enclosure is very hot to the touch. I&amp;#39;m thinking its overheating, as I don&amp;#39;t think caching is a problem in this circumstance.&lt;/p&gt;\n\n&lt;p&gt;Any ideas for a better enclosure to use for this purpose - maybe something with a fan, and heat pads for both sides of the SSD rather than just one, and maybe a large heat sink?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve searched around and can&amp;#39;t find anything that looks promising, and that claims it will work with an 8tb drive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jmi5t", "is_robot_indexable": true, "report_reasons": null, "author": "Many_Slices_Of_Bread", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jmi5t/best_enclosure_for_8tb_nvme_m2_ssd_for_large_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jmi5t/best_enclosure_for_8tb_nvme_m2_ssd_for_large_file/", "subreddit_subscribers": 718557, "created_utc": 1702713769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good day,\n\nLost in the forest of choice and contradicting review, time to ask the community..  \nI want to backup my NAS (or part of it as least) to HDD/SSD that i will store offsite.\n\nI'll have multiple copies (+Glacier Deep Archive as last resort), and was wondering which HDD/SSD I should (not) use.\n\nThe reviews are slightly confusing when it comes reliabilty'.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_lnyanlqi9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost efficiency reliable disks for backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18jlj7e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702709713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day,&lt;/p&gt;\n\n&lt;p&gt;Lost in the forest of choice and contradicting review, time to ask the community..&lt;br/&gt;\nI want to backup my NAS (or part of it as least) to HDD/SSD that i will store offsite.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll have multiple copies (+Glacier Deep Archive as last resort), and was wondering which HDD/SSD I should (not) use.&lt;/p&gt;\n\n&lt;p&gt;The reviews are slightly confusing when it comes reliabilty&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18jlj7e", "is_robot_indexable": true, "report_reasons": null, "author": "ChrisTheChti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18jlj7e/cost_efficiency_reliable_disks_for_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18jlj7e/cost_efficiency_reliable_disks_for_backup/", "subreddit_subscribers": 718557, "created_utc": 1702709713.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}