{"kind": "Listing", "data": {"after": "t3_18m9atw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!\n\n[https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra\\_5PGH&amp;index=6&amp;t=689s](https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;index=6&amp;t=689s)", "author_fullname": "t2_me12im5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lsb9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 63, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 63, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702957460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;amp;index=6&amp;amp;t=689s\"&gt;https://www.youtube.com/watch?v=aiHSMYvoqYE&amp;amp;list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&amp;amp;index=6&amp;amp;t=689s&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?auto=webp&amp;s=7297230f4639915c7e57af7b9a255708da17bfa9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=784a54eeca77266e76144dcc7477cdae8da54cbb", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76e755d0518dfef7107139a615d64e1857715e31", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/1PDIkye6LArTIWWNy8yF79XX41KoySrft8dUlTpOS7g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bcd0428c65436c58727868ae2eae5a9b4a6d541d", "width": 320, "height": 240}], "variants": {}, "id": "_rVKQNaxcPqL1qZYn-JkINnf7oHCvdPuQq4k6I3ej4A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18lsb9p", "is_robot_indexable": true, "report_reasons": null, "author": "onurbaltaci", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lsb9p/i_recorded_a_crash_course_on_polars_library_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lsb9p/i_recorded_a_crash_course_on_polars_library_of/", "subreddit_subscribers": 147066, "created_utc": 1702957460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Throwaway because I know they are sometime on Reddit.\n\nMy boss is constantly asking for unnecessary details and micromanaging us. I am out of ideas on how to manage them.\n\nIt ranges from anything like wanting to be in meetings where we change the name of a cluster, up to asking why we need a tool to track bugs (and since we never had one before, why bother now?). Even not being technical they are:\n\n- checking the SQL queries of my colleagues;\n- they want to know if this or that has been documented for tiny operations;\n- they are asking for ingestion of new datasets before we have even finished patching bugs or validating stuff, no unit tests because we are always short on time\n\nI am trying to handle the stress of cascading requests by having a roadmap and shielding our team, but my manager will constantly challenge the roadmap and put pressure on my reports.\n\nThis is not tenable in the long term, do you have any advice on managing stakeholders above that aren\u2019t technical?", "author_fullname": "t2_qa5f09pa8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non-technical boss, wanting to micromanage and kills our team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m4e2i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 63, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 63, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702999366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throwaway because I know they are sometime on Reddit.&lt;/p&gt;\n\n&lt;p&gt;My boss is constantly asking for unnecessary details and micromanaging us. I am out of ideas on how to manage them.&lt;/p&gt;\n\n&lt;p&gt;It ranges from anything like wanting to be in meetings where we change the name of a cluster, up to asking why we need a tool to track bugs (and since we never had one before, why bother now?). Even not being technical they are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;checking the SQL queries of my colleagues;&lt;/li&gt;\n&lt;li&gt;they want to know if this or that has been documented for tiny operations;&lt;/li&gt;\n&lt;li&gt;they are asking for ingestion of new datasets before we have even finished patching bugs or validating stuff, no unit tests because we are always short on time&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am trying to handle the stress of cascading requests by having a roadmap and shielding our team, but my manager will constantly challenge the roadmap and put pressure on my reports.&lt;/p&gt;\n\n&lt;p&gt;This is not tenable in the long term, do you have any advice on managing stakeholders above that aren\u2019t technical?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18m4e2i", "is_robot_indexable": true, "report_reasons": null, "author": "SuperMarioDataGalaxy", "discussion_type": null, "num_comments": 48, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m4e2i/nontechnical_boss_wanting_to_micromanage_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m4e2i/nontechnical_boss_wanting_to_micromanage_and/", "subreddit_subscribers": 147066, "created_utc": 1702999366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm sys-admin and fiddled around with python and just discoverd datasets.\n\nHow awesome is that stuff?! I was flashed.\n\nKnowing about datasets etc would have saved me \\_so\\_ much time back when i started my career, it is hilarious.\n\nI guess it is time to start learning again(this is not the 1000 post asking for advice, this sub seems to have pretty good resources)! \n\n&amp;#x200B;", "author_fullname": "t2_14rai9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I discoverd datasets in python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m5gxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703002123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sys-admin and fiddled around with python and just discoverd datasets.&lt;/p&gt;\n\n&lt;p&gt;How awesome is that stuff?! I was flashed.&lt;/p&gt;\n\n&lt;p&gt;Knowing about datasets etc would have saved me _so_ much time back when i started my career, it is hilarious.&lt;/p&gt;\n\n&lt;p&gt;I guess it is time to start learning again(this is not the 1000 post asking for advice, this sub seems to have pretty good resources)! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m5gxj", "is_robot_indexable": true, "report_reasons": null, "author": "toast_one", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m5gxj/i_discoverd_datasets_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m5gxj/i_discoverd_datasets_in_python/", "subreddit_subscribers": 147066, "created_utc": 1703002123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am looking to switch my career and move into DE. \n\nI've only covered basics of Python, SQL, MySQL, PostgreSQL, Linux, bash scripting, Database administration + a few rdbms tools like pgadmin4, phpmyadmin or dbeaver. \n\nThere's still a long way to go for me, but I am already looking at some job offers to see what specific tools and skills companies in my area require.\n\nWhat are some red flags that when you see in a job offer you're like \"This company has no structure / it needs three different people for that role / etc.\"?\n\nI am looking for ways to weed out those offers that I shouldn't be using as a baseline for gathering my skills.\n\nThanks.", "author_fullname": "t2_7xz6r2in", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Red flags in DE job offers (beginner)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m14z5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702990200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am looking to switch my career and move into DE. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only covered basics of Python, SQL, MySQL, PostgreSQL, Linux, bash scripting, Database administration + a few rdbms tools like pgadmin4, phpmyadmin or dbeaver. &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s still a long way to go for me, but I am already looking at some job offers to see what specific tools and skills companies in my area require.&lt;/p&gt;\n\n&lt;p&gt;What are some red flags that when you see in a job offer you&amp;#39;re like &amp;quot;This company has no structure / it needs three different people for that role / etc.&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;I am looking for ways to weed out those offers that I shouldn&amp;#39;t be using as a baseline for gathering my skills.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18m14z5", "is_robot_indexable": true, "report_reasons": null, "author": "Consistent-Drink-235", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m14z5/red_flags_in_de_job_offers_beginner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m14z5/red_flags_in_de_job_offers_beginner/", "subreddit_subscribers": 147066, "created_utc": 1702990200.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New video \ud83e\udd73 What's new in Apache Airflow 2.8?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18m30x1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/M9qyj5Dszks?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.8?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What's new in Apache Airflow 2.8?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/M9qyj5Dszks?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.8?\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/M9qyj5Dszks/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/M9qyj5Dszks?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.8?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18m30x1", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/W9gW-x75UmKzyC9I0b799PZ2OnmHhIBK7_zD9y5N684.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702995743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/M9qyj5Dszks", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QXKmfKRtkK5dfP9afHt8uOZ1FSxHfDvKOTGJVXdJMEQ.jpg?auto=webp&amp;s=14111aa6adfeb2c33ce6f5e60e93b26adf640e3d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/QXKmfKRtkK5dfP9afHt8uOZ1FSxHfDvKOTGJVXdJMEQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4cfe605ca8eda8a8d053c6cebb0db4699467a79", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QXKmfKRtkK5dfP9afHt8uOZ1FSxHfDvKOTGJVXdJMEQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2161187bfb9511953e9b866e0bcbd6fd0319ee77", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QXKmfKRtkK5dfP9afHt8uOZ1FSxHfDvKOTGJVXdJMEQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=99e1acd7c824946fbb87246bb66219af8fd5f6d8", "width": 320, "height": 240}], "variants": {}, "id": "Sn7V-8gShoAirVVII0LH7BwleF0omtwue3ruXwusF2M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18m30x1", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m30x1/new_video_whats_new_in_apache_airflow_28/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/M9qyj5Dszks", "subreddit_subscribers": 147066, "created_utc": 1702995743.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What's new in Apache Airflow 2.8?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/M9qyj5Dszks?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.8?\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/M9qyj5Dszks/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recap of 2023's Transformative Data Landscape", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_18luokv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QC-u87qQqiWYhhkIC7zp7h8awQeZx1F8z1WVloZ5my4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702965072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/recap-of-2023s-transformative-data", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?auto=webp&amp;s=05cf70f29567b297be9005cc1fe6c52634c548d1", "width": 852, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8235b4ab19fc2ff32de42dcad115918f2e142ea", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac56b2d2c72cfcf52f1d78a8b06cc0971bef6d20", "width": 216, "height": 152}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d507577ef1336581efee9304ad98a549d20d63fe", "width": 320, "height": 225}, {"url": "https://external-preview.redd.it/McBDtjackmJxipnxmo2grRRcVo8lpGsP1S5KQmF4E2g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e89944c98598044365a90f48cd565f877fa7541c", "width": 640, "height": 450}], "variants": {}, "id": "bU0KynVYqdS53QNQHO7TTIr2Kuty1FCLaOXHnUOQcI0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18luokv", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18luokv/recap_of_2023s_transformative_data_landscape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/recap-of-2023s-transformative-data", "subreddit_subscribers": 147066, "created_utc": 1702965072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5ud8qz3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Netflix Data Engineering Summit videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "name": "t3_18mactq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sSDLo57eZMNIp1wzW5IoGguFiQ6_Ua87WD5lDt3x1Es.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703014490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "netflixtechblog.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?auto=webp&amp;s=5c559c3b17160025d56491777d567987248931ef", "width": 1111, "height": 571}, "resolutions": [{"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da24254e4bb038691cfdd4c156a5d5244c08c4bf", "width": 108, "height": 55}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=052f735a1ffa64a0e1d6d51c639439521a6f2004", "width": 216, "height": 111}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d0fc2cebd67c1385fae0b1324dc39348b3217f8", "width": 320, "height": 164}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc32a3e086f0217c8aa2eab757773f562dea42d3", "width": 640, "height": 328}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=756a90ecc71cbca89abb9212b906435a9bd267bd", "width": 960, "height": 493}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b00bb639fe14e1acfb87b0939a9785a7a7e3c46", "width": 1080, "height": 555}], "variants": {}, "id": "TMo7-NZlAEt-Oth2MSgWHW_-6WCPBJ0hDscjjgNPtxU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18mactq", "is_robot_indexable": true, "report_reasons": null, "author": "limeslice2020", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mactq/netflix_data_engineering_summit_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102", "subreddit_subscribers": 147066, "created_utc": 1703014490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the holidays approach, why not use some of your downtime to dive into a quick and practical data engineering project? This [GitHub repo](https://github.com/airbytehq/quickstarts/tree/main/airbyte_dbt_airflow_bigquery) offers a chance to see Airbyte, Airflow and dbt in action, working together!\n\nIn less than an hour, you can have a full-fledged data stack to extract e-commerce data and transform it in BigQuery (or any other data warehouse, with a few tweaks). All instructions are in the README. \n\nDesigned for busy professionals and curious learners, this project offers a straightforward but extendable structure.\n\nI hope you will enjoy it! Let me know how it goes and where it can be improved. \n\nDisclaimer: I\u2019m part of the Airbyte team, and it\u2019s my personal interest to help my fellow engineers experiment and learn.\n\n[End to end DAG](https://preview.redd.it/w1tbxns9xa7c1.png?width=1354&amp;format=png&amp;auto=webp&amp;s=aee24208c9e649211d422ae12e5eeffae2f4abd5)\n\n[dbt DAG](https://preview.redd.it/tcs1jwcfxa7c1.png?width=1880&amp;format=png&amp;auto=webp&amp;s=bdb19c1aedfeb8469ac52beea683ff03246f1aa3)", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrating Airbyte, Airflow &amp; dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"tcs1jwcfxa7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4585fe4ef43dcf2fa137993774b3cb9e720c6ba0"}, {"y": 110, "x": 216, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cabddcb7e42993c86645282df39f9066d8b929c6"}, {"y": 163, "x": 320, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f88310d9cdda18f566eb7130b55e4f0ab9ef94a6"}, {"y": 326, "x": 640, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f0a419d349035775e01c7d76249bb0d2b0db8a5"}, {"y": 489, "x": 960, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8a7223b8f338131b21d49cc04d832e1b01291cd6"}, {"y": 550, "x": 1080, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a5ddf6342374db4be5144b2abdffd5a3eb57f1c"}], "s": {"y": 958, "x": 1880, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=1880&amp;format=png&amp;auto=webp&amp;s=bdb19c1aedfeb8469ac52beea683ff03246f1aa3"}, "id": "tcs1jwcfxa7c1"}, "w1tbxns9xa7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d1c10ac0a94930899a0a889cdbbe6061d33e325"}, {"y": 64, "x": 216, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=50e1cf2e63a1a49effb7d64e19897b9a6d81abba"}, {"y": 95, "x": 320, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8bf1aba786f9c8d4d29c95149a4db6572e64bfd4"}, {"y": 190, "x": 640, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cd409b30e35cefac56645a0c71d9b8c01120d92"}, {"y": 285, "x": 960, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6142356bda8e7b2549dbabdd8ee9439cfc66aadd"}, {"y": 320, "x": 1080, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5690b2f5a07fdd12cfc7ffbd5ba1b9eba22e5259"}], "s": {"y": 402, "x": 1354, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=1354&amp;format=png&amp;auto=webp&amp;s=aee24208c9e649211d422ae12e5eeffae2f4abd5"}, "id": "w1tbxns9xa7c1"}}, "name": "t3_18m9yto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pouf7O2bHswW7FoCfoyalXWgbHN4PalGFB7Cl00vhmU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1703013525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the holidays approach, why not use some of your downtime to dive into a quick and practical data engineering project? This &lt;a href=\"https://github.com/airbytehq/quickstarts/tree/main/airbyte_dbt_airflow_bigquery\"&gt;GitHub repo&lt;/a&gt; offers a chance to see Airbyte, Airflow and dbt in action, working together!&lt;/p&gt;\n\n&lt;p&gt;In less than an hour, you can have a full-fledged data stack to extract e-commerce data and transform it in BigQuery (or any other data warehouse, with a few tweaks). All instructions are in the README. &lt;/p&gt;\n\n&lt;p&gt;Designed for busy professionals and curious learners, this project offers a straightforward but extendable structure.&lt;/p&gt;\n\n&lt;p&gt;I hope you will enjoy it! Let me know how it goes and where it can be improved. &lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I\u2019m part of the Airbyte team, and it\u2019s my personal interest to help my fellow engineers experiment and learn.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w1tbxns9xa7c1.png?width=1354&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aee24208c9e649211d422ae12e5eeffae2f4abd5\"&gt;End to end DAG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tcs1jwcfxa7c1.png?width=1880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdb19c1aedfeb8469ac52beea683ff03246f1aa3\"&gt;dbt DAG&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?auto=webp&amp;s=2d1bd7ee7e1dacb5ae59ebe4b6a52937118c7eba", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4cb3cab32bb4c0377c3bcf465b03a307fae1425", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba542763a1b4351acd1002befba883881db3e31d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=31c1718d22be0d9ff790a87a2afd179b8bfb5e5c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b0294daefa0204451abb6e8f3a147d06c940bf2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6cc981f415ac4f15bdb8e3e431a9d34156ab5824", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0bea4b583e75edfc9b379e33c725391509e474d0", "width": 1080, "height": 540}], "variants": {}, "id": "TcgpYk8BJNd4uAgmGeO3-7znAtRUmIi5C-Abf1OarDE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18m9yto", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18m9yto/integrating_airbyte_airflow_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m9yto/integrating_airbyte_airflow_dbt/", "subreddit_subscribers": 147066, "created_utc": 1703013525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the company I am currently contracting in they have a central data warehouse running on Postgres with about 30 terabytes of data. Users of the warehouse are expected to pull data out of the database into their own environments. \nAs many of the tables are too large to do a full sync, we need to do incremental sync on new/updated data. The person that created the central warehouse has created a system where every record is tagged with the id of the batch of data that it was ingested from. These are not necessarily in incrementing order. This is done per table. Thus, every system working with the data warehouse has to maintain their own logic for every table that they use of which transactions they have already processed. When I look at their ETL scripts about half of the code is spent on this logic. \nTo me this seems overly convoluted compared to a database generated timestamp that I can query for changed data. It also means that I cannot use default functionality for incremental loads in dbt and meltano. Am I overlooking something here?", "author_fullname": "t2_dxt8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting new records/changes: timestamp vs ingest I\u2019d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m00kd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702986364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the company I am currently contracting in they have a central data warehouse running on Postgres with about 30 terabytes of data. Users of the warehouse are expected to pull data out of the database into their own environments. \nAs many of the tables are too large to do a full sync, we need to do incremental sync on new/updated data. The person that created the central warehouse has created a system where every record is tagged with the id of the batch of data that it was ingested from. These are not necessarily in incrementing order. This is done per table. Thus, every system working with the data warehouse has to maintain their own logic for every table that they use of which transactions they have already processed. When I look at their ETL scripts about half of the code is spent on this logic. \nTo me this seems overly convoluted compared to a database generated timestamp that I can query for changed data. It also means that I cannot use default functionality for incremental loads in dbt and meltano. Am I overlooking something here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m00kd", "is_robot_indexable": true, "report_reasons": null, "author": "pokepip", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m00kd/detecting_new_recordschanges_timestamp_vs_ingest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m00kd/detecting_new_recordschanges_timestamp_vs_ingest/", "subreddit_subscribers": 147066, "created_utc": 1702986364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Why storage and compute can't be separated for an OLTP database?", "author_fullname": "t2_fulplt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OLTP vs OLAP databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lyljw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702980821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why storage and compute can&amp;#39;t be separated for an OLTP database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18lyljw", "is_robot_indexable": true, "report_reasons": null, "author": "ankit_goyal", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lyljw/oltp_vs_olap_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lyljw/oltp_vs_olap_databases/", "subreddit_subscribers": 147066, "created_utc": 1702980821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_qvzmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expedia Uses WebSockets and Kafka to Query Near Real-Time Streaming Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18lz0me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_-95beDt0eTU61sjw3MyytCoW7zyk-9mjFOz5cqSkbM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702982547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoq.com/news/2023/12/expedia-websockets-kafka-query/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?auto=webp&amp;s=22be1127b3a032c0428d4bee24b8d2cb2a2233c0", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74b528758f8dac132781b7ba52cec3194953941b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b5018e5a478380623b37087dbf9dc4b43bbf70c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1b372ee2d8ab5031fb9f92e22e20238371990e25", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec9e2d3c657a70106b4608c93841f75070af8d79", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3de9be364444c886057dff0f382a0ea14630cb01", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oDquMrZue4nfq8ELf06QbeFTrQhOEwsl9KHtE8Dgca4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7820fff738d4f7d7dcf3a548b2e8c36161176b6", "width": 1080, "height": 567}], "variants": {}, "id": "BmdzDPEbzLW9dI8d-7eePz_SJf3gyF91hmqD-LM6Od4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18lz0me", "is_robot_indexable": true, "report_reasons": null, "author": "rgancarz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lz0me/expedia_uses_websockets_and_kafka_to_query_near/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoq.com/news/2023/12/expedia-websockets-kafka-query/", "subreddit_subscribers": 147066, "created_utc": 1702982547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've worked at a company for 4 years after 6 months I stopped opening outlook.\n\nSimply put if it doesn't happen in jira it never happened, I've been so hard on this the rest of my team followed. PM SM and PO were all but hurt now they look at my tickets and are like oh ok I'll check tomorrow.\n\nI put everything in my tickets from git commits, call recordings, results, error codes. I've done this to the point I'm smooth brained when asked questions on the ticket my answer is always what do the comments say?\n\nAnyone else make their communication funnelled through on channel?", "author_fullname": "t2_vtm8z2o0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you read emails?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18migud", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703036186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve worked at a company for 4 years after 6 months I stopped opening outlook.&lt;/p&gt;\n\n&lt;p&gt;Simply put if it doesn&amp;#39;t happen in jira it never happened, I&amp;#39;ve been so hard on this the rest of my team followed. PM SM and PO were all but hurt now they look at my tickets and are like oh ok I&amp;#39;ll check tomorrow.&lt;/p&gt;\n\n&lt;p&gt;I put everything in my tickets from git commits, call recordings, results, error codes. I&amp;#39;ve done this to the point I&amp;#39;m smooth brained when asked questions on the ticket my answer is always what do the comments say?&lt;/p&gt;\n\n&lt;p&gt;Anyone else make their communication funnelled through on channel?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18migud", "is_robot_indexable": true, "report_reasons": null, "author": "Action_Maxim", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18migud/do_you_read_emails/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18migud/do_you_read_emails/", "subreddit_subscribers": 147066, "created_utc": 1703036186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I do all the ETL for my BI team. I'd like to incorporate Python into my ETL process for learning purposes, but I'm not sure where to start. I have a lot of SQL experience but no programming experience.\n\nCurrent process:\n\n* All of our data comes from Azure SQL dbs and federal government APIs. \n\n\n* The data sourced from SQL dbs is ingested via SSIS packages. It is loaded into an on-prem staging database. I then transform and import the data into our reporting warehouse using stored procs and scheduled jobs.\n\n* The data sourced from APIs is ingested via ADF pipelines. Everything else is done just like the SQL pipeline -- I load it to a staging db and complete the transformation and loading with stored procs.\n\nIf you inherited this pipeline, how would you improve it using Python?\n\nThanks!", "author_fullname": "t2_emnkl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm a BI Developer wanting to transition to DE and have accepted it's time to learn Python. Based on my current workflow, where can I integrate it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18madbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703014522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I do all the ETL for my BI team. I&amp;#39;d like to incorporate Python into my ETL process for learning purposes, but I&amp;#39;m not sure where to start. I have a lot of SQL experience but no programming experience.&lt;/p&gt;\n\n&lt;p&gt;Current process:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;All of our data comes from Azure SQL dbs and federal government APIs. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The data sourced from SQL dbs is ingested via SSIS packages. It is loaded into an on-prem staging database. I then transform and import the data into our reporting warehouse using stored procs and scheduled jobs.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The data sourced from APIs is ingested via ADF pipelines. Everything else is done just like the SQL pipeline -- I load it to a staging db and complete the transformation and loading with stored procs.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you inherited this pipeline, how would you improve it using Python?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18madbk", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward_Tick0", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18madbk/im_a_bi_developer_wanting_to_transition_to_de_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18madbk/im_a_bi_developer_wanting_to_transition_to_de_and/", "subreddit_subscribers": 147066, "created_utc": 1703014522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need some advice on best approach/ practise.\n\nBackground :\n\nIm a data analyst working in a team. Breakdown of our reporting flows are :\n\n1. We remote into VM and pull data using SQL. Then do the necessary conditioning or merging or etc and automate these stuffs using the task scheduler. We need python because we interact with google sheet a lot.\n\nProblem i have with this is approach is that currently we dont have a proper Git framework whatsoever. Everybody just remote and change the code directly on the VM. \n\n2. We have internal data tools that connects to the datawarehouse. Mainly the workflows are being done on SQL which has proper logging, code versioning etc as they were built and maintained in house. \n\nThis tool is actually quite solid, it even supports pyspark and spark sql. The only problem is that our team have never leveraged spark on this internal data platform. Just SQL. \n\nAnd when we need massive update on google sheets reporting flows we just use the no.1 approach, do everything on the VM.\n\nSo the advice i need is, i want to understand if approach no.1 is bad practise and should we bring everything over on approach no.2? \n\nMy concern for keeping no.1 is that there is no proper Git framework to it. Everything is just so messy. Should i introduce Git and keep it around?\n\nAs for to just use no.2 approach, im not sure if pyspark can do exactly what we're doing in no.1 since we lack of the exposure. \n\nSorry for lack of knowledge, im new to these kinds of infra. I dont know much other than just writing good SQL queries.", "author_fullname": "t2_7nk0x7fc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice Needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m0x3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703027106.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702989497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need some advice on best approach/ practise.&lt;/p&gt;\n\n&lt;p&gt;Background :&lt;/p&gt;\n\n&lt;p&gt;Im a data analyst working in a team. Breakdown of our reporting flows are :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We remote into VM and pull data using SQL. Then do the necessary conditioning or merging or etc and automate these stuffs using the task scheduler. We need python because we interact with google sheet a lot.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Problem i have with this is approach is that currently we dont have a proper Git framework whatsoever. Everybody just remote and change the code directly on the VM. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We have internal data tools that connects to the datawarehouse. Mainly the workflows are being done on SQL which has proper logging, code versioning etc as they were built and maintained in house. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This tool is actually quite solid, it even supports pyspark and spark sql. The only problem is that our team have never leveraged spark on this internal data platform. Just SQL. &lt;/p&gt;\n\n&lt;p&gt;And when we need massive update on google sheets reporting flows we just use the no.1 approach, do everything on the VM.&lt;/p&gt;\n\n&lt;p&gt;So the advice i need is, i want to understand if approach no.1 is bad practise and should we bring everything over on approach no.2? &lt;/p&gt;\n\n&lt;p&gt;My concern for keeping no.1 is that there is no proper Git framework to it. Everything is just so messy. Should i introduce Git and keep it around?&lt;/p&gt;\n\n&lt;p&gt;As for to just use no.2 approach, im not sure if pyspark can do exactly what we&amp;#39;re doing in no.1 since we lack of the exposure. &lt;/p&gt;\n\n&lt;p&gt;Sorry for lack of knowledge, im new to these kinds of infra. I dont know much other than just writing good SQL queries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m0x3c", "is_robot_indexable": true, "report_reasons": null, "author": "Nopal97", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m0x3c/advice_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m0x3c/advice_needed/", "subreddit_subscribers": 147066, "created_utc": 1702989497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp; Jesse Anderson \u2022 GOTO 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18mdpui", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/P-9FwZxO1zE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp;amp; Jesse Anderson \u2022 GOTO 2023\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp; Jesse Anderson \u2022 GOTO 2023", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/P-9FwZxO1zE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp;amp; Jesse Anderson \u2022 GOTO 2023\"&gt;&lt;/iframe&gt;", "author_name": "GOTO Conferences", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/P-9FwZxO1zE/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@GOTO-"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/P-9FwZxO1zE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp;amp; Jesse Anderson \u2022 GOTO 2023\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18mdpui", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EG6dIjYfo7koB7YwHcHVp8yD3RXY4TPnkQbuGBMMe60.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703023012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/P-9FwZxO1zE?si=UxntLaaWAtmKBiY7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HmAUe332NFaRs9nHi_AaIW-hxIAsSR-MYRhh_jQSE5Y.jpg?auto=webp&amp;s=c893c1d8ac73658e95fcfa89af76635829a5f0a1", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/HmAUe332NFaRs9nHi_AaIW-hxIAsSR-MYRhh_jQSE5Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99c4a68cc33ce73559ee7a00f579d49bdb5bf269", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/HmAUe332NFaRs9nHi_AaIW-hxIAsSR-MYRhh_jQSE5Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4857927413c84daab0812d5b736443b5d601f28", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/HmAUe332NFaRs9nHi_AaIW-hxIAsSR-MYRhh_jQSE5Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1cb2927d72da9e6d6353b49959f7508ba4c618c", "width": 320, "height": 240}], "variants": {}, "id": "tc09KFreOE8Z89GIDoJawxTXj4KExCL8ftOn_LgGEU8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18mdpui", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mdpui/designing_a_dataintensive_future_expert_talk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/P-9FwZxO1zE?si=UxntLaaWAtmKBiY7", "subreddit_subscribers": 147066, "created_utc": 1703023012.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp; Jesse Anderson \u2022 GOTO 2023", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/P-9FwZxO1zE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp;amp; Jesse Anderson \u2022 GOTO 2023\"&gt;&lt;/iframe&gt;", "author_name": "GOTO Conferences", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/P-9FwZxO1zE/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@GOTO-"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title is pretty self-explanatory. \n\nAre y'all noticing recruiter emails coming to your work email ? I don't know how they got my work email into their HR or emailing systems ?! \n\nI'm interested but obvi can't reply from work email lol", "author_fullname": "t2_roct4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recruiters reaching out to you on your work email ? are these emails automated or hand-written ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m6ey9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703013326.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703004561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title is pretty self-explanatory. &lt;/p&gt;\n\n&lt;p&gt;Are y&amp;#39;all noticing recruiter emails coming to your work email ? I don&amp;#39;t know how they got my work email into their HR or emailing systems ?! &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested but obvi can&amp;#39;t reply from work email lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m6ey9", "is_robot_indexable": true, "report_reasons": null, "author": "dronedesigner", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m6ey9/recruiters_reaching_out_to_you_on_your_work_email/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m6ey9/recruiters_reaching_out_to_you_on_your_work_email/", "subreddit_subscribers": 147066, "created_utc": 1703004561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIam writing a snowflake stored procedure in python language.\n\nBelow is the sample code:\n\n    CREATE OR REPLACE PROCEDURE edw_dev.master.SampleSP(TABLENAMES ARRAY, ORDERBYCOLUMNS ARRAY DEFAULT NULL)\n    RETURNS STRING\n    LANGUAGE PYTHON\n    RUNTIME_VERSION = '3.8'\n    PACKAGES = ('snowflake-snowpark-python')\n    HANDLER = 'process_tables'\n    as\n    $$\n    def process_tables(snowpark_session, TABLENAMES, ORDERBYCOLUMNS):\n        \n                createTableSql = f'CREATE OR REPLACE TABLE {newTableName} LIKE {fullTableName}'\n    \n                snowpark_session.sql(createTableSql).collect()\n    \n    \n    \n    $$;\n\nI need to implement a lock using 'begin transaction' and 'commit' for a section inside my stored procedure. How can we do this in python snowflake stored proc?\n\nI don't see much detail about this in the documentation as it contains details for Snowflake scripting alone.\n\nHas anyone come across this? Appreciate your help in advance.!", "author_fullname": "t2_9fr6if3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help: Snowflake Python Stored procedure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m2rg5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702995008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Iam writing a snowflake stored procedure in python language.&lt;/p&gt;\n\n&lt;p&gt;Below is the sample code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REPLACE PROCEDURE edw_dev.master.SampleSP(TABLENAMES ARRAY, ORDERBYCOLUMNS ARRAY DEFAULT NULL)\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = &amp;#39;3.8&amp;#39;\nPACKAGES = (&amp;#39;snowflake-snowpark-python&amp;#39;)\nHANDLER = &amp;#39;process_tables&amp;#39;\nas\n$$\ndef process_tables(snowpark_session, TABLENAMES, ORDERBYCOLUMNS):\n\n            createTableSql = f&amp;#39;CREATE OR REPLACE TABLE {newTableName} LIKE {fullTableName}&amp;#39;\n\n            snowpark_session.sql(createTableSql).collect()\n\n\n\n$$;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I need to implement a lock using &amp;#39;begin transaction&amp;#39; and &amp;#39;commit&amp;#39; for a section inside my stored procedure. How can we do this in python snowflake stored proc?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t see much detail about this in the documentation as it contains details for Snowflake scripting alone.&lt;/p&gt;\n\n&lt;p&gt;Has anyone come across this? Appreciate your help in advance.!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18m2rg5", "is_robot_indexable": true, "report_reasons": null, "author": "aj_here_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m2rg5/help_snowflake_python_stored_procedure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m2rg5/help_snowflake_python_stored_procedure/", "subreddit_subscribers": 147066, "created_utc": 1702995008.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m not a data engineer, am a BI analyst whos interested in becoming a DE, so currently I\u2019m trying to observe, ask questions, search\u2026 but I noticed it\u2019s really stressful job and there are a lot of things i need to learn and i feel overwhelmed. \n\nHow you guys dealing with this pressure and what strategies are you using to balance between doing work and continue learning and sharpening your skills?\n\nI also want to ask about how you learned DE and if you know a good source teaching the basics of DE for beginners. \nnote that I\u2019m a BI analyst for almost 3 years :)", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Work pressure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18lzawl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703013614.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702983685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not a data engineer, am a BI analyst whos interested in becoming a DE, so currently I\u2019m trying to observe, ask questions, search\u2026 but I noticed it\u2019s really stressful job and there are a lot of things i need to learn and i feel overwhelmed. &lt;/p&gt;\n\n&lt;p&gt;How you guys dealing with this pressure and what strategies are you using to balance between doing work and continue learning and sharpening your skills?&lt;/p&gt;\n\n&lt;p&gt;I also want to ask about how you learned DE and if you know a good source teaching the basics of DE for beginners. \nnote that I\u2019m a BI analyst for almost 3 years :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18lzawl", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18lzawl/work_pressure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18lzawl/work_pressure/", "subreddit_subscribers": 147066, "created_utc": 1702983685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dose hosting the system on your environment will change the source you\u2019re reading from? \n\nFor example System x is hosted now on the provider side, if the IT said we need to host it on our environment, dose that means the data source in fivetran connector have to be changed? I\u2019m I gonna establish a connector to my own environment rather that to x system? \n\ni\u2019m not familiar to cloud however I assume no need to do that it sounds weird.. however I heard people here say they have to \ud83e\udd14", "author_fullname": "t2_fludc35u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting the tool on your environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ltbyp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702960586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dose hosting the system on your environment will change the source you\u2019re reading from? &lt;/p&gt;\n\n&lt;p&gt;For example System x is hosted now on the provider side, if the IT said we need to host it on our environment, dose that means the data source in fivetran connector have to be changed? I\u2019m I gonna establish a connector to my own environment rather that to x system? &lt;/p&gt;\n\n&lt;p&gt;i\u2019m not familiar to cloud however I assume no need to do that it sounds weird.. however I heard people here say they have to \ud83e\udd14&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ltbyp", "is_robot_indexable": true, "report_reasons": null, "author": "OddElk1083", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ltbyp/hosting_the_tool_on_your_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ltbyp/hosting_the_tool_on_your_environment/", "subreddit_subscribers": 147066, "created_utc": 1702960586.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry if this is an amateur question, I\u2019m not a data engineer, just an analyst that wears a lot of hats except analyzing data. \n\nCurrently we are evaluating how to develop better data processes as a lot of our work is done manually in Excel. We have a SAAS system we produce which does a lot of the calculations (industry specific data transformation software), but before ingesting the data into the system, the analyst must perform a bunch of transformations in Excel and perform a bunch of validations. We want to automate this.\n\nAt first I was leaning towards using SQL and Python if necessary. One of my managers is leading towards using Jedox, a BI tool (really a FP&amp;A tool). Using Jedox would have a lot of benefits, it will most likely be on me to build this, and I\u2019ve automated our reporting in Jedox, but haven\u2019t really built anything with Python or SQL before (at work), so I should be able to move quicker with Jedox. There is also the benefit that we could store aggregated data before processing, after processing, and then while reporting to make sure everything was processed correctly all in the same system. We also have all the security stuff figured out in Jedox, and could give analysts a front end where they could run their own data pipelines and get exports. Jedox also allows us to use Groovy scripting for more complex transformations, running jobs, setting variables, and all that stuff.\n\nThe validations are stuff like making sure no prices were sold below a certain listed price, specific ID\u2019s for specific types of sales are still active in another database, finding any value not contained various reference tables that needs to be added, validating certain amounts are calculated correctly, and so on. The transformations are basic and can be done in Excel. With some rare clients, we have to do weird stuff like consolidate pieces of data from PDF\u2019s to produce a file, but that\u2019s very rare. We are constantly adding new clients though, so this system will need to be able to work for all our clients.\n\nTo me it seems like Jedox could actually be really useful for this, but I know it generally goes against what the type of advice I see here where people generally don\u2019t recommend to use a BI tool for work like this. Is there anything wrong with taking this approach? ", "author_fullname": "t2_8e28mn79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it bad to use a BI tool for Data pre-processing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18mits6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703037237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is an amateur question, I\u2019m not a data engineer, just an analyst that wears a lot of hats except analyzing data. &lt;/p&gt;\n\n&lt;p&gt;Currently we are evaluating how to develop better data processes as a lot of our work is done manually in Excel. We have a SAAS system we produce which does a lot of the calculations (industry specific data transformation software), but before ingesting the data into the system, the analyst must perform a bunch of transformations in Excel and perform a bunch of validations. We want to automate this.&lt;/p&gt;\n\n&lt;p&gt;At first I was leaning towards using SQL and Python if necessary. One of my managers is leading towards using Jedox, a BI tool (really a FP&amp;amp;A tool). Using Jedox would have a lot of benefits, it will most likely be on me to build this, and I\u2019ve automated our reporting in Jedox, but haven\u2019t really built anything with Python or SQL before (at work), so I should be able to move quicker with Jedox. There is also the benefit that we could store aggregated data before processing, after processing, and then while reporting to make sure everything was processed correctly all in the same system. We also have all the security stuff figured out in Jedox, and could give analysts a front end where they could run their own data pipelines and get exports. Jedox also allows us to use Groovy scripting for more complex transformations, running jobs, setting variables, and all that stuff.&lt;/p&gt;\n\n&lt;p&gt;The validations are stuff like making sure no prices were sold below a certain listed price, specific ID\u2019s for specific types of sales are still active in another database, finding any value not contained various reference tables that needs to be added, validating certain amounts are calculated correctly, and so on. The transformations are basic and can be done in Excel. With some rare clients, we have to do weird stuff like consolidate pieces of data from PDF\u2019s to produce a file, but that\u2019s very rare. We are constantly adding new clients though, so this system will need to be able to work for all our clients.&lt;/p&gt;\n\n&lt;p&gt;To me it seems like Jedox could actually be really useful for this, but I know it generally goes against what the type of advice I see here where people generally don\u2019t recommend to use a BI tool for work like this. Is there anything wrong with taking this approach? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mits6", "is_robot_indexable": true, "report_reasons": null, "author": "Icy-Big2472", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mits6/is_it_bad_to_use_a_bi_tool_for_data_preprocessing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mits6/is_it_bad_to_use_a_bi_tool_for_data_preprocessing/", "subreddit_subscribers": 147066, "created_utc": 1703037237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I created a YouTube channel focused on Python and mid to senior level software/data engineering topics. Please let me know what you think, I will be uploading frequently. Have a great day!!\n\n[https://www.youtube.com/watch?v=4crPt0OROQ0&amp;t=57s](https://www.youtube.com/watch?v=4crPt0OROQ0&amp;t=57s)", "author_fullname": "t2_qb2z2bsq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I started a Python YouTube channel, will be teaching mid/senior software &amp; data engineering topics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18mhebk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703033023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I created a YouTube channel focused on Python and mid to senior level software/data engineering topics. Please let me know what you think, I will be uploading frequently. Have a great day!!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=4crPt0OROQ0&amp;amp;t=57s\"&gt;https://www.youtube.com/watch?v=4crPt0OROQ0&amp;amp;t=57s&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5knoksiiHMrNwtkz3IzxnSr_JLA4z_HAlNDTc2pkZ2c.jpg?auto=webp&amp;s=631d80fe92f9e1c0e091e2478039eb86c3f8ad5d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/5knoksiiHMrNwtkz3IzxnSr_JLA4z_HAlNDTc2pkZ2c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1cc48e79c291a5b77832b964020bd1c92ed01619", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/5knoksiiHMrNwtkz3IzxnSr_JLA4z_HAlNDTc2pkZ2c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=424f2fad6de4ecff29b6876fb43f08ee357e21a0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/5knoksiiHMrNwtkz3IzxnSr_JLA4z_HAlNDTc2pkZ2c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f9bc875c0840d7e319eac399b21156a531eb5c6", "width": 320, "height": 240}], "variants": {}, "id": "XVh1Qgn094PME8eAQXx7fcAj5ulwtvi-zLveL4B13q8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18mhebk", "is_robot_indexable": true, "report_reasons": null, "author": "EngineeringMug", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mhebk/i_started_a_python_youtube_channel_will_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mhebk/i_started_a_python_youtube_channel_will_be/", "subreddit_subscribers": 147066, "created_utc": 1703033023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have a source table updated from GCS files uploaded daily.\n\nFrom this source table i have another table for my client, cleansed &amp; transformed.\n\n&amp;#x200B;\n\nBut now i would like to only add new incoming data daily from the source table without having a unique key.\n\n  \nHow should i do ?  \n\n\nRegards.", "author_fullname": "t2_4lzse6nu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "update BQ with GCS using DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18me2df", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703023929.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have a source table updated from GCS files uploaded daily.&lt;/p&gt;\n\n&lt;p&gt;From this source table i have another table for my client, cleansed &amp;amp; transformed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But now i would like to only add new incoming data daily from the source table without having a unique key.&lt;/p&gt;\n\n&lt;p&gt;How should i do ?  &lt;/p&gt;\n\n&lt;p&gt;Regards.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18me2df", "is_robot_indexable": true, "report_reasons": null, "author": "Resident_Set204", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18me2df/update_bq_with_gcs_using_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18me2df/update_bq_with_gcs_using_dbt/", "subreddit_subscribers": 147066, "created_utc": 1703023929.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm a Product Manager for a data platform company (think DWH, Lake House, or DB) and I'm working on redesigning the product onboarding experience. I want this experience to serve Data Engineers the best it can, so that's the spirit of these questions:\n\nQ1: When trying a new data platform, what are you looking for in the first 30-60 minutes? \n\nQ2: Can you think of a product you recently tried for the first time and loved the onboarding? If so, what product was it?\n\nThanks!", "author_fullname": "t2_qaafk4p40", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2-Question Survey", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mdomd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703022915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m a Product Manager for a data platform company (think DWH, Lake House, or DB) and I&amp;#39;m working on redesigning the product onboarding experience. I want this experience to serve Data Engineers the best it can, so that&amp;#39;s the spirit of these questions:&lt;/p&gt;\n\n&lt;p&gt;Q1: When trying a new data platform, what are you looking for in the first 30-60 minutes? &lt;/p&gt;\n\n&lt;p&gt;Q2: Can you think of a product you recently tried for the first time and loved the onboarding? If so, what product was it?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mdomd", "is_robot_indexable": true, "report_reasons": null, "author": "IEatDataForLunch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mdomd/2question_survey/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mdomd/2question_survey/", "subreddit_subscribers": 147066, "created_utc": 1703022915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is building a data lake from a number of different ERPs (we have several subsidiaries). Unfortunately, we cannot seem to find any meaningful backend support for integrating the data from Unleashed and Xero (one of our companies uses them in tandem).\n\nDoes anyone here have experience querying and working with the backend of either of these platforms? Do you know someone who does? We basically just need a Q&amp;A session for the team to ask questions about where to find specific dimensions.\n\nThanks in advance!\n\nPS - Sorry if this is too close to a job post, I'm just running out of logistical resources trying to find an SME to consult on this.", "author_fullname": "t2_o1ln6yad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here worked with Xero/Unleashed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mbe32", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703017100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is building a data lake from a number of different ERPs (we have several subsidiaries). Unfortunately, we cannot seem to find any meaningful backend support for integrating the data from Unleashed and Xero (one of our companies uses them in tandem).&lt;/p&gt;\n\n&lt;p&gt;Does anyone here have experience querying and working with the backend of either of these platforms? Do you know someone who does? We basically just need a Q&amp;amp;A session for the team to ask questions about where to find specific dimensions.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;PS - Sorry if this is too close to a job post, I&amp;#39;m just running out of logistical resources trying to find an SME to consult on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mbe32", "is_robot_indexable": true, "report_reasons": null, "author": "_tr9800a_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mbe32/anyone_here_worked_with_xerounleashed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mbe32/anyone_here_worked_with_xerounleashed/", "subreddit_subscribers": 147066, "created_utc": 1703017100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am Heading Product at a FinTech. Currently I use Zendesk for ticketing. This flows to snowflake\u00a0+ looker for insights. I need a better way to link feedback to Churn. I am prioritizing Feedback and potential impact on churn is important. Right now, we just do it manually but its getting out of hand. How can I automate this? What all variables do I need to account for here? ", "author_fullname": "t2_nvnticro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Link Customer Feedback to Churn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m9atw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703011857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am Heading Product at a FinTech. Currently I use Zendesk for ticketing. This flows to snowflake\u00a0+ looker for insights. I need a better way to link feedback to Churn. I am prioritizing Feedback and potential impact on churn is important. Right now, we just do it manually but its getting out of hand. How can I automate this? What all variables do I need to account for here? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18m9atw", "is_robot_indexable": true, "report_reasons": null, "author": "Scary-Swing2852", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m9atw/link_customer_feedback_to_churn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m9atw/link_customer_feedback_to_churn/", "subreddit_subscribers": 147066, "created_utc": 1703011857.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}