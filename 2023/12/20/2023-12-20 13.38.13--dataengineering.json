{"kind": "Listing", "data": {"after": "t3_18m2rg5", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Throwaway because I know they are sometime on Reddit.\n\nMy boss is constantly asking for unnecessary details and micromanaging us. I am out of ideas on how to manage them.\n\nIt ranges from anything like wanting to be in meetings where we change the name of a cluster, up to asking why we need a tool to track bugs (and since we never had one before, why bother now?). Even not being technical they are:\n\n- checking the SQL queries of my colleagues;\n- they want to know if this or that has been documented for tiny operations;\n- they are asking for ingestion of new datasets before we have even finished patching bugs or validating stuff, no unit tests because we are always short on time\n\nI am trying to handle the stress of cascading requests by having a roadmap and shielding our team, but my manager will constantly challenge the roadmap and put pressure on my reports.\n\nThis is not tenable in the long term, do you have any advice on managing stakeholders above that aren\u2019t technical?", "author_fullname": "t2_qa5f09pa8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non-technical boss, wanting to micromanage and kills our team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m4e2i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702999366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throwaway because I know they are sometime on Reddit.&lt;/p&gt;\n\n&lt;p&gt;My boss is constantly asking for unnecessary details and micromanaging us. I am out of ideas on how to manage them.&lt;/p&gt;\n\n&lt;p&gt;It ranges from anything like wanting to be in meetings where we change the name of a cluster, up to asking why we need a tool to track bugs (and since we never had one before, why bother now?). Even not being technical they are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;checking the SQL queries of my colleagues;&lt;/li&gt;\n&lt;li&gt;they want to know if this or that has been documented for tiny operations;&lt;/li&gt;\n&lt;li&gt;they are asking for ingestion of new datasets before we have even finished patching bugs or validating stuff, no unit tests because we are always short on time&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am trying to handle the stress of cascading requests by having a roadmap and shielding our team, but my manager will constantly challenge the roadmap and put pressure on my reports.&lt;/p&gt;\n\n&lt;p&gt;This is not tenable in the long term, do you have any advice on managing stakeholders above that aren\u2019t technical?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18m4e2i", "is_robot_indexable": true, "report_reasons": null, "author": "SuperMarioDataGalaxy", "discussion_type": null, "num_comments": 55, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m4e2i/nontechnical_boss_wanting_to_micromanage_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m4e2i/nontechnical_boss_wanting_to_micromanage_and/", "subreddit_subscribers": 147146, "created_utc": 1702999366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm sys-admin and fiddled around with python and just discoverd datasets.\n\nHow awesome is that stuff?! I was flashed.\n\nKnowing about datasets etc would have saved me \\_so\\_ much time back when i started my career, it is hilarious.\n\nI guess it is time to start learning again(this is not the 1000 post asking for advice, this sub seems to have pretty good resources)! \n\n&amp;#x200B;", "author_fullname": "t2_14rai9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I discoverd datasets in python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m5gxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703002123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sys-admin and fiddled around with python and just discoverd datasets.&lt;/p&gt;\n\n&lt;p&gt;How awesome is that stuff?! I was flashed.&lt;/p&gt;\n\n&lt;p&gt;Knowing about datasets etc would have saved me _so_ much time back when i started my career, it is hilarious.&lt;/p&gt;\n\n&lt;p&gt;I guess it is time to start learning again(this is not the 1000 post asking for advice, this sub seems to have pretty good resources)! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m5gxj", "is_robot_indexable": true, "report_reasons": null, "author": "toast_one", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m5gxj/i_discoverd_datasets_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m5gxj/i_discoverd_datasets_in_python/", "subreddit_subscribers": 147146, "created_utc": 1703002123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've worked at a company for 4 years after 6 months I stopped opening outlook.\n\nSimply put if it doesn't happen in jira it never happened, I've been so hard on this the rest of my team followed. PM SM and PO were all but hurt now they look at my tickets and are like oh ok I'll check tomorrow.\n\nI put everything in my tickets from git commits, call recordings, results, error codes. I've done this to the point I'm smooth brained when asked questions on the ticket my answer is always what do the comments say?\n\nAnyone else make their communication funnelled through on channel?", "author_fullname": "t2_vtm8z2o0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you read emails?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18migud", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703036186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve worked at a company for 4 years after 6 months I stopped opening outlook.&lt;/p&gt;\n\n&lt;p&gt;Simply put if it doesn&amp;#39;t happen in jira it never happened, I&amp;#39;ve been so hard on this the rest of my team followed. PM SM and PO were all but hurt now they look at my tickets and are like oh ok I&amp;#39;ll check tomorrow.&lt;/p&gt;\n\n&lt;p&gt;I put everything in my tickets from git commits, call recordings, results, error codes. I&amp;#39;ve done this to the point I&amp;#39;m smooth brained when asked questions on the ticket my answer is always what do the comments say?&lt;/p&gt;\n\n&lt;p&gt;Anyone else make their communication funnelled through on channel?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18migud", "is_robot_indexable": true, "report_reasons": null, "author": "Action_Maxim", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18migud/do_you_read_emails/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18migud/do_you_read_emails/", "subreddit_subscribers": 147146, "created_utc": 1703036186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the holidays approach, why not use some of your downtime to dive into a quick and practical data engineering project? This [GitHub repo](https://github.com/airbytehq/quickstarts/tree/main/airbyte_dbt_airflow_bigquery) offers a chance to see Airbyte, Airflow and dbt in action, working together!\n\nIn less than an hour, you can have a full-fledged data stack to extract e-commerce data and transform it in BigQuery (or any other data warehouse, with a few tweaks). All instructions are in the README. \n\nDesigned for busy professionals and curious learners, this project offers a straightforward but extendable structure.\n\nI hope you will enjoy it! Let me know how it goes and where it can be improved. \n\nDisclaimer: I\u2019m part of the Airbyte team, and it\u2019s my personal interest to help my fellow engineers experiment and learn.\n\n[End to end DAG](https://preview.redd.it/w1tbxns9xa7c1.png?width=1354&amp;format=png&amp;auto=webp&amp;s=aee24208c9e649211d422ae12e5eeffae2f4abd5)\n\n[dbt DAG](https://preview.redd.it/tcs1jwcfxa7c1.png?width=1880&amp;format=png&amp;auto=webp&amp;s=bdb19c1aedfeb8469ac52beea683ff03246f1aa3)", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrating Airbyte, Airflow &amp; dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"tcs1jwcfxa7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4585fe4ef43dcf2fa137993774b3cb9e720c6ba0"}, {"y": 110, "x": 216, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cabddcb7e42993c86645282df39f9066d8b929c6"}, {"y": 163, "x": 320, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f88310d9cdda18f566eb7130b55e4f0ab9ef94a6"}, {"y": 326, "x": 640, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f0a419d349035775e01c7d76249bb0d2b0db8a5"}, {"y": 489, "x": 960, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8a7223b8f338131b21d49cc04d832e1b01291cd6"}, {"y": 550, "x": 1080, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a5ddf6342374db4be5144b2abdffd5a3eb57f1c"}], "s": {"y": 958, "x": 1880, "u": "https://preview.redd.it/tcs1jwcfxa7c1.png?width=1880&amp;format=png&amp;auto=webp&amp;s=bdb19c1aedfeb8469ac52beea683ff03246f1aa3"}, "id": "tcs1jwcfxa7c1"}, "w1tbxns9xa7c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d1c10ac0a94930899a0a889cdbbe6061d33e325"}, {"y": 64, "x": 216, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=50e1cf2e63a1a49effb7d64e19897b9a6d81abba"}, {"y": 95, "x": 320, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8bf1aba786f9c8d4d29c95149a4db6572e64bfd4"}, {"y": 190, "x": 640, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cd409b30e35cefac56645a0c71d9b8c01120d92"}, {"y": 285, "x": 960, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6142356bda8e7b2549dbabdd8ee9439cfc66aadd"}, {"y": 320, "x": 1080, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5690b2f5a07fdd12cfc7ffbd5ba1b9eba22e5259"}], "s": {"y": 402, "x": 1354, "u": "https://preview.redd.it/w1tbxns9xa7c1.png?width=1354&amp;format=png&amp;auto=webp&amp;s=aee24208c9e649211d422ae12e5eeffae2f4abd5"}, "id": "w1tbxns9xa7c1"}}, "name": "t3_18m9yto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pouf7O2bHswW7FoCfoyalXWgbHN4PalGFB7Cl00vhmU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1703013525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the holidays approach, why not use some of your downtime to dive into a quick and practical data engineering project? This &lt;a href=\"https://github.com/airbytehq/quickstarts/tree/main/airbyte_dbt_airflow_bigquery\"&gt;GitHub repo&lt;/a&gt; offers a chance to see Airbyte, Airflow and dbt in action, working together!&lt;/p&gt;\n\n&lt;p&gt;In less than an hour, you can have a full-fledged data stack to extract e-commerce data and transform it in BigQuery (or any other data warehouse, with a few tweaks). All instructions are in the README. &lt;/p&gt;\n\n&lt;p&gt;Designed for busy professionals and curious learners, this project offers a straightforward but extendable structure.&lt;/p&gt;\n\n&lt;p&gt;I hope you will enjoy it! Let me know how it goes and where it can be improved. &lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I\u2019m part of the Airbyte team, and it\u2019s my personal interest to help my fellow engineers experiment and learn.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w1tbxns9xa7c1.png?width=1354&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aee24208c9e649211d422ae12e5eeffae2f4abd5\"&gt;End to end DAG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tcs1jwcfxa7c1.png?width=1880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdb19c1aedfeb8469ac52beea683ff03246f1aa3\"&gt;dbt DAG&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?auto=webp&amp;s=2d1bd7ee7e1dacb5ae59ebe4b6a52937118c7eba", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4cb3cab32bb4c0377c3bcf465b03a307fae1425", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba542763a1b4351acd1002befba883881db3e31d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=31c1718d22be0d9ff790a87a2afd179b8bfb5e5c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b0294daefa0204451abb6e8f3a147d06c940bf2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6cc981f415ac4f15bdb8e3e431a9d34156ab5824", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/j1aTRlxBce-WIwvNzjW33AqeMOSCAOuMmv_8bYGymf4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0bea4b583e75edfc9b379e33c725391509e474d0", "width": 1080, "height": 540}], "variants": {}, "id": "TcgpYk8BJNd4uAgmGeO3-7znAtRUmIi5C-Abf1OarDE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18m9yto", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18m9yto/integrating_airbyte_airflow_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m9yto/integrating_airbyte_airflow_dbt/", "subreddit_subscribers": 147146, "created_utc": 1703013525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New video \ud83e\udd73 What's new in Apache Airflow 2.8?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18m30x1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/M9qyj5Dszks?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.8?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What's new in Apache Airflow 2.8?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/M9qyj5Dszks?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.8?\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/M9qyj5Dszks/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/M9qyj5Dszks?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.8?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18m30x1", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/W9gW-x75UmKzyC9I0b799PZ2OnmHhIBK7_zD9y5N684.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702995743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/M9qyj5Dszks", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QXKmfKRtkK5dfP9afHt8uOZ1FSxHfDvKOTGJVXdJMEQ.jpg?auto=webp&amp;s=14111aa6adfeb2c33ce6f5e60e93b26adf640e3d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/QXKmfKRtkK5dfP9afHt8uOZ1FSxHfDvKOTGJVXdJMEQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4cfe605ca8eda8a8d053c6cebb0db4699467a79", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QXKmfKRtkK5dfP9afHt8uOZ1FSxHfDvKOTGJVXdJMEQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2161187bfb9511953e9b866e0bcbd6fd0319ee77", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QXKmfKRtkK5dfP9afHt8uOZ1FSxHfDvKOTGJVXdJMEQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=99e1acd7c824946fbb87246bb66219af8fd5f6d8", "width": 320, "height": 240}], "variants": {}, "id": "Sn7V-8gShoAirVVII0LH7BwleF0omtwue3ruXwusF2M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18m30x1", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m30x1/new_video_whats_new_in_apache_airflow_28/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/M9qyj5Dszks", "subreddit_subscribers": 147146, "created_utc": 1702995743.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What's new in Apache Airflow 2.8?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/M9qyj5Dszks?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.8?\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/M9qyj5Dszks/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Last year my New year resolution was to get into Data field from banking to IT and I did it.  \n Now I'm a BI Developer (Power BI, SSRS, SQL) but I'm unsure what should be my next year's resolution. I'm more inclined towards Microsoft products so maybe Azure, Databricks (I'm unsure what to do).   \n So anyone please help me out, I know if I set my goals I could do it.   \n\n\nAnd thank you everyone in this sub who've helped me this past year.", "author_fullname": "t2_3k9gevl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should be New Year's Resolution ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18movto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703057131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last year my New year resolution was to get into Data field from banking to IT and I did it.&lt;br/&gt;\n Now I&amp;#39;m a BI Developer (Power BI, SSRS, SQL) but I&amp;#39;m unsure what should be my next year&amp;#39;s resolution. I&amp;#39;m more inclined towards Microsoft products so maybe Azure, Databricks (I&amp;#39;m unsure what to do).&lt;br/&gt;\n So anyone please help me out, I know if I set my goals I could do it.   &lt;/p&gt;\n\n&lt;p&gt;And thank you everyone in this sub who&amp;#39;ve helped me this past year.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18movto", "is_robot_indexable": true, "report_reasons": null, "author": "Pillstyr", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18movto/what_should_be_new_years_resolution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18movto/what_should_be_new_years_resolution/", "subreddit_subscribers": 147146, "created_utc": 1703057131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im a Data/Software developer. My boss recently told me we are switching our data warehouse to Snowflake from Sql Server.\n\nAnyone have experience with snowflake and share your thoughts?\n\nOur current DWH has a lot of fact and dimensional tables. We also run a lot of batch processes prior to having this data land in the warehouse. Im not fully sure how this will work with snowflake.\n\nLooking for opinions,thoughts, etc.", "author_fullname": "t2_5epqry7a5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ml6ay", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703044315.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a Data/Software developer. My boss recently told me we are switching our data warehouse to Snowflake from Sql Server.&lt;/p&gt;\n\n&lt;p&gt;Anyone have experience with snowflake and share your thoughts?&lt;/p&gt;\n\n&lt;p&gt;Our current DWH has a lot of fact and dimensional tables. We also run a lot of batch processes prior to having this data land in the warehouse. Im not fully sure how this will work with snowflake.&lt;/p&gt;\n\n&lt;p&gt;Looking for opinions,thoughts, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ml6ay", "is_robot_indexable": true, "report_reasons": null, "author": "liskeeksil", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ml6ay/thoughts_on_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ml6ay/thoughts_on_snowflake/", "subreddit_subscribers": 147146, "created_utc": 1703044315.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5ud8qz3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Netflix Data Engineering Summit videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "name": "t3_18mactq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sSDLo57eZMNIp1wzW5IoGguFiQ6_Ua87WD5lDt3x1Es.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703014490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "netflixtechblog.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?auto=webp&amp;s=5c559c3b17160025d56491777d567987248931ef", "width": 1111, "height": 571}, "resolutions": [{"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da24254e4bb038691cfdd4c156a5d5244c08c4bf", "width": 108, "height": 55}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=052f735a1ffa64a0e1d6d51c639439521a6f2004", "width": 216, "height": 111}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d0fc2cebd67c1385fae0b1324dc39348b3217f8", "width": 320, "height": 164}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc32a3e086f0217c8aa2eab757773f562dea42d3", "width": 640, "height": 328}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=756a90ecc71cbca89abb9212b906435a9bd267bd", "width": 960, "height": 493}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b00bb639fe14e1acfb87b0939a9785a7a7e3c46", "width": 1080, "height": 555}], "variants": {}, "id": "TMo7-NZlAEt-Oth2MSgWHW_-6WCPBJ0hDscjjgNPtxU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18mactq", "is_robot_indexable": true, "report_reasons": null, "author": "limeslice2020", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mactq/netflix_data_engineering_summit_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102", "subreddit_subscribers": 147146, "created_utc": 1703014490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,I've come across a test failure in one of my dbt tests, which examines null values in a column. Null values have appeared in this column due to a gap in the load time for clone/source tables. Specifically, one table has more recent entries that are missing from another table, and these tables are essential for creating the final dataset.Example:\n\n    Table A: Loaded at 10:00 AM\n    \n    | ID | Name   | Gender |\n    |----|--------|--------|\n    | 1  | Alice  | Female |\n    | 2  | Bob    | Male   |\n    \n    \n    Table B: Loaded at 11:00 AM\n    | ID | Value |\n    |----|-------|\n    | 1  | 100   |  \n    | 2  | 200   |  \n    | 3  | 300   |  &lt;-- New entry\n    \n    Final Table (Table B left join Table A on ID)\n    | ID | Name   | Value |\n    |----|--------|-------|\n    | 1  | Alice  | 100   |  &lt;-- From Table A\n    | 2  | Bob    | 200  |  &lt;-- From Table A\n    | 3  | Carol  | null  |  &lt;-- New entry from Table B, but missing in Table A\n\nPlease suggest the best way to fix this issue. A couple of ideas off the top of my head are:\n\n1. Rearrange the clone table loading pipeline. This might not work because we run these pipelines in parallel.\n2. Set up a process to filter table entries based on a cutoff time in the staging schema right after clone/source schema (keep only those entries where the timestamp is less than 10 AM on that day).\n3. Update the dbt test to exclude entries where the timestamp is from today.\n\nAny insights would be greatly appreciated!", "author_fullname": "t2_g2837t6l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Null values in final table due to source table load time difference", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mpw03", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703073777.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703061328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,I&amp;#39;ve come across a test failure in one of my dbt tests, which examines null values in a column. Null values have appeared in this column due to a gap in the load time for clone/source tables. Specifically, one table has more recent entries that are missing from another table, and these tables are essential for creating the final dataset.Example:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Table A: Loaded at 10:00 AM\n\n| ID | Name   | Gender |\n|----|--------|--------|\n| 1  | Alice  | Female |\n| 2  | Bob    | Male   |\n\n\nTable B: Loaded at 11:00 AM\n| ID | Value |\n|----|-------|\n| 1  | 100   |  \n| 2  | 200   |  \n| 3  | 300   |  &amp;lt;-- New entry\n\nFinal Table (Table B left join Table A on ID)\n| ID | Name   | Value |\n|----|--------|-------|\n| 1  | Alice  | 100   |  &amp;lt;-- From Table A\n| 2  | Bob    | 200  |  &amp;lt;-- From Table A\n| 3  | Carol  | null  |  &amp;lt;-- New entry from Table B, but missing in Table A\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Please suggest the best way to fix this issue. A couple of ideas off the top of my head are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Rearrange the clone table loading pipeline. This might not work because we run these pipelines in parallel.&lt;/li&gt;\n&lt;li&gt;Set up a process to filter table entries based on a cutoff time in the staging schema right after clone/source schema (keep only those entries where the timestamp is less than 10 AM on that day).&lt;/li&gt;\n&lt;li&gt;Update the dbt test to exclude entries where the timestamp is from today.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any insights would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mpw03", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting-Fun8932", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mpw03/null_values_in_final_table_due_to_source_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mpw03/null_values_in_final_table_due_to_source_table/", "subreddit_subscribers": 147146, "created_utc": 1703061328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI have six years of experience as a data professional but have a mix of skills that includes data analytics and science (R, Python and SQL) and full stack dev skills (HTML, CSS and JavaScript). I started out initially as a data analyst conducting ad-hoc analyses for studies in clinical research. Later, I worked in government and I started doing a lot of data automation work. The only tools we had at my agency were R and SQL, so I basically wrote scripts to extract the data, clean and add variables, merge data from different sources and feed the final dataset into other scripts to do the respective summary analysis and organization on excel tables that were eventually automatically e-mailed out every month on the necessary date.\n\nThe work made me realize enjoy programming a lot more than I like analyzing data and have been trying to transition into data engineering. The problem is that unfortunately I've worked in places where the technology is very behind and I don't know if somebody would consider my automation work experience counting towards the type of data engineering where people use Python, EC2, Kafka, Airflow, Spark, Snowflake, etc. I've looked through this sub and also look at job descriptions that seem to vary greatly (some data engineering roles are vague and just seem to want people with strong analytical backgrounds, which I comfortably have, but other tools I unfortunately lack experience with).\n\nWhat would be the best place to start to fill the gaps? Would reading [this book](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/) be a good start? Any other suggestions?", "author_fullname": "t2_2prckadt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What things should I learn or do to fill my gaps in knowledge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mn3ce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703050984.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703050587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I have six years of experience as a data professional but have a mix of skills that includes data analytics and science (R, Python and SQL) and full stack dev skills (HTML, CSS and JavaScript). I started out initially as a data analyst conducting ad-hoc analyses for studies in clinical research. Later, I worked in government and I started doing a lot of data automation work. The only tools we had at my agency were R and SQL, so I basically wrote scripts to extract the data, clean and add variables, merge data from different sources and feed the final dataset into other scripts to do the respective summary analysis and organization on excel tables that were eventually automatically e-mailed out every month on the necessary date.&lt;/p&gt;\n\n&lt;p&gt;The work made me realize enjoy programming a lot more than I like analyzing data and have been trying to transition into data engineering. The problem is that unfortunately I&amp;#39;ve worked in places where the technology is very behind and I don&amp;#39;t know if somebody would consider my automation work experience counting towards the type of data engineering where people use Python, EC2, Kafka, Airflow, Spark, Snowflake, etc. I&amp;#39;ve looked through this sub and also look at job descriptions that seem to vary greatly (some data engineering roles are vague and just seem to want people with strong analytical backgrounds, which I comfortably have, but other tools I unfortunately lack experience with).&lt;/p&gt;\n\n&lt;p&gt;What would be the best place to start to fill the gaps? Would reading &lt;a href=\"https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/\"&gt;this book&lt;/a&gt; be a good start? Any other suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zHSaoEDYHu6eqtSmoc4Unrb86gYnuwJnLVhx80Veckw.jpg?auto=webp&amp;s=57782f29029ef996ccaa726a3d0e8a314bd32412", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/zHSaoEDYHu6eqtSmoc4Unrb86gYnuwJnLVhx80Veckw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=165371ce811dab187b9739e6a1883bced59a39da", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/zHSaoEDYHu6eqtSmoc4Unrb86gYnuwJnLVhx80Veckw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=adfc93eea39ab42f4940e53982ec546ba57b04fb", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/zHSaoEDYHu6eqtSmoc4Unrb86gYnuwJnLVhx80Veckw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5e55caf080e2f183883064a06d430925706761a", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/zHSaoEDYHu6eqtSmoc4Unrb86gYnuwJnLVhx80Veckw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2fe64bbfc0bbc48f50970b2d9f39fbd76d2a78ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/zHSaoEDYHu6eqtSmoc4Unrb86gYnuwJnLVhx80Veckw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb65d6477ec9eb17d79c0920d2a994c9fc340f79", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/zHSaoEDYHu6eqtSmoc4Unrb86gYnuwJnLVhx80Veckw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=917ea202aecb9836d08dcc3138bd8474e08f1d3a", "width": 1080, "height": 567}], "variants": {}, "id": "W5kos2T_Zxibm0tVKkrkGXgcaF96beVGU9ErKoxzTxU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mn3ce", "is_robot_indexable": true, "report_reasons": null, "author": "thro0away12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mn3ce/what_things_should_i_learn_or_do_to_fill_my_gaps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mn3ce/what_things_should_i_learn_or_do_to_fill_my_gaps/", "subreddit_subscribers": 147146, "created_utc": 1703050587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm eager to dive into data science and build my skillset, but unfortunately, my PC is unavailable for a month. While Youtube offers resources, I'm seeking a more interactive learning platform like DataCamp. However, its pricing doesn't currently align with my budget. Are there any free alternatives that offer similar hands-on learning experiences, allowing me to apply concepts as I learn and are also available on phone?", "author_fullname": "t2_jd72erbca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best learning path for a complete beginner?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ms95j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703070880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m eager to dive into data science and build my skillset, but unfortunately, my PC is unavailable for a month. While Youtube offers resources, I&amp;#39;m seeking a more interactive learning platform like DataCamp. However, its pricing doesn&amp;#39;t currently align with my budget. Are there any free alternatives that offer similar hands-on learning experiences, allowing me to apply concepts as I learn and are also available on phone?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ms95j", "is_robot_indexable": true, "report_reasons": null, "author": "Netroseige101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ms95j/what_are_the_best_learning_path_for_a_complete/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ms95j/what_are_the_best_learning_path_for_a_complete/", "subreddit_subscribers": 147146, "created_utc": 1703070880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey y'all! \n\nMy coworkers worked at Apple on the ML compute platform team and constantly found themselves supporting ML engineers with their large, distributed ML training jobs. ML engineers had to either use less data or they had to rewrite the training jobs to weave in more complicated *data chunking*. They also struggled to keep GPU utilization above 80% because so much time was spent waiting for data to just load: [https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609](https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609)\n\nInspired by the pains of that experience, they created an open source library for mounting large datasets inside Kubernetes.\n\nThis way, you can just:\n\n\\- Write &amp; iterate on ML code locally\n\n\\- Deploy the ML job in Kubernetes, mounting the relevant data repo / bucket in seconds\n\n\\- Watch the relevant rows &amp; columns get streamed into different pods just-in-time on an as-needed basis\n\nHere's a link to the short post, which includes a quick tutorial: [https://about.xethub.com/blog/mount-big-data-kubernetes-faster-ml](https://about.xethub.com/blog/mount-big-data-kubernetes-faster-ml)", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kubernetes plugin for mounting datasets to speed up model training", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mjeeg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703038930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all! &lt;/p&gt;\n\n&lt;p&gt;My coworkers worked at Apple on the ML compute platform team and constantly found themselves supporting ML engineers with their large, distributed ML training jobs. ML engineers had to either use less data or they had to rewrite the training jobs to weave in more complicated &lt;em&gt;data chunking&lt;/em&gt;. They also struggled to keep GPU utilization above 80% because so much time was spent waiting for data to just load: &lt;a href=\"https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609\"&gt;https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Inspired by the pains of that experience, they created an open source library for mounting large datasets inside Kubernetes.&lt;/p&gt;\n\n&lt;p&gt;This way, you can just:&lt;/p&gt;\n\n&lt;p&gt;- Write &amp;amp; iterate on ML code locally&lt;/p&gt;\n\n&lt;p&gt;- Deploy the ML job in Kubernetes, mounting the relevant data repo / bucket in seconds&lt;/p&gt;\n\n&lt;p&gt;- Watch the relevant rows &amp;amp; columns get streamed into different pods just-in-time on an as-needed basis&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a link to the short post, which includes a quick tutorial: &lt;a href=\"https://about.xethub.com/blog/mount-big-data-kubernetes-faster-ml\"&gt;https://about.xethub.com/blog/mount-big-data-kubernetes-faster-ml&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KHj2qGPVOhORlWPtHBDXZOjIHZM34SUuxSeNZiO9S3w.jpg?auto=webp&amp;s=b51fbd9f71e54307e3bfafa18d55e4b68ec0bc73", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/KHj2qGPVOhORlWPtHBDXZOjIHZM34SUuxSeNZiO9S3w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=861db8795c84a790d30b3d83b8f7ddab69350f28", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/KHj2qGPVOhORlWPtHBDXZOjIHZM34SUuxSeNZiO9S3w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb24c82e2ebf94027915fc8df913769080417931", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/KHj2qGPVOhORlWPtHBDXZOjIHZM34SUuxSeNZiO9S3w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=940ff4a5a82304edbcfea1666d0cb6c7f35e45ac", "width": 320, "height": 320}], "variants": {}, "id": "0Ikyen85Ad70GrjpN1XA6-Ndv6dFXhI3YkPZAMyxafQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18mjeeg", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mjeeg/kubernetes_plugin_for_mounting_datasets_to_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mjeeg/kubernetes_plugin_for_mounting_datasets_to_speed/", "subreddit_subscribers": 147146, "created_utc": 1703038930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I do all the ETL for my BI team. I'd like to incorporate Python into my ETL process for learning purposes, but I'm not sure where to start. I have a lot of SQL experience but no programming experience.\n\nCurrent process:\n\n* All of our data comes from Azure SQL dbs and federal government APIs. \n\n\n* The data sourced from SQL dbs is ingested via SSIS packages. It is loaded into an on-prem staging database. I then transform and import the data into our reporting warehouse using stored procs and scheduled jobs.\n\n* The data sourced from APIs is ingested via ADF pipelines. Everything else is done just like the SQL pipeline -- I load it to a staging db and complete the transformation and loading with stored procs.\n\nIf you inherited this pipeline, how would you improve it using Python?\n\nThanks!", "author_fullname": "t2_emnkl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm a BI Developer wanting to transition to DE and have accepted it's time to learn Python. Based on my current workflow, where can I integrate it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18madbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703014522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I do all the ETL for my BI team. I&amp;#39;d like to incorporate Python into my ETL process for learning purposes, but I&amp;#39;m not sure where to start. I have a lot of SQL experience but no programming experience.&lt;/p&gt;\n\n&lt;p&gt;Current process:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;All of our data comes from Azure SQL dbs and federal government APIs. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The data sourced from SQL dbs is ingested via SSIS packages. It is loaded into an on-prem staging database. I then transform and import the data into our reporting warehouse using stored procs and scheduled jobs.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The data sourced from APIs is ingested via ADF pipelines. Everything else is done just like the SQL pipeline -- I load it to a staging db and complete the transformation and loading with stored procs.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you inherited this pipeline, how would you improve it using Python?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18madbk", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward_Tick0", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18madbk/im_a_bi_developer_wanting_to_transition_to_de_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18madbk/im_a_bi_developer_wanting_to_transition_to_de_and/", "subreddit_subscribers": 147146, "created_utc": 1703014522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello experts,\n\nI am relatively new to creating data pipelines and have started working with the solution Informatica. \n\nWhat I want to check is, what relative advantage do I have if Im moving my data from SQL to Snowflake using Informatica in between rather than connecting Snowflake directly to SQL database.\n\nAnybody could help me figure this out.", "author_fullname": "t2_9elue9qa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Integration/Migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mq368", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703062148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello experts,&lt;/p&gt;\n\n&lt;p&gt;I am relatively new to creating data pipelines and have started working with the solution Informatica. &lt;/p&gt;\n\n&lt;p&gt;What I want to check is, what relative advantage do I have if Im moving my data from SQL to Snowflake using Informatica in between rather than connecting Snowflake directly to SQL database.&lt;/p&gt;\n\n&lt;p&gt;Anybody could help me figure this out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mq368", "is_robot_indexable": true, "report_reasons": null, "author": "Can-Lazy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mq368/data_integrationmigration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mq368/data_integrationmigration/", "subreddit_subscribers": 147146, "created_utc": 1703062148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Bear with me, I have not yet started using dbt.\n\ndbt seems to be introduced to almost every stack out there. And I get it. It solves a lot of the issues with reproducibility and testing that we've had historically in data management.\n\n\nHOWEVER, it just seems terribly inefficient!\nI'm of the thought, that large scheduled loads are a thing of the past. Small incremental batches that run as data becomes available are much cheaper in hardware requirements, there is no issue if a load fails, because they can just rerun in seconds, and users get a much better experience with live data.\n\n\nI mean, the above doesn't matter if a full load finish in minutes, but what what if your facts consist of billions or even 100 of millions of rows?\n\nI can read that dbt support incremental loads, but it seems like a bit of an afterthought.\n\n\nAm I wrong here? Is anyone succesfully running dbt in setups with large datasets? Do you provide \"live\" data?", "author_fullname": "t2_ekunr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone actually use dbt for large datasets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mo0u9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703053800.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bear with me, I have not yet started using dbt.&lt;/p&gt;\n\n&lt;p&gt;dbt seems to be introduced to almost every stack out there. And I get it. It solves a lot of the issues with reproducibility and testing that we&amp;#39;ve had historically in data management.&lt;/p&gt;\n\n&lt;p&gt;HOWEVER, it just seems terribly inefficient!\nI&amp;#39;m of the thought, that large scheduled loads are a thing of the past. Small incremental batches that run as data becomes available are much cheaper in hardware requirements, there is no issue if a load fails, because they can just rerun in seconds, and users get a much better experience with live data.&lt;/p&gt;\n\n&lt;p&gt;I mean, the above doesn&amp;#39;t matter if a full load finish in minutes, but what what if your facts consist of billions or even 100 of millions of rows?&lt;/p&gt;\n\n&lt;p&gt;I can read that dbt support incremental loads, but it seems like a bit of an afterthought.&lt;/p&gt;\n\n&lt;p&gt;Am I wrong here? Is anyone succesfully running dbt in setups with large datasets? Do you provide &amp;quot;live&amp;quot; data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18mo0u9", "is_robot_indexable": true, "report_reasons": null, "author": "Justbehind", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mo0u9/does_anyone_actually_use_dbt_for_large_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mo0u9/does_anyone_actually_use_dbt_for_large_datasets/", "subreddit_subscribers": 147146, "created_utc": 1703053800.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nHi guys\n\n&amp;#x200B;\n\nI'm moving some customer data into a new database, I'm getting the customer data from the old database and writing it to the new one using the code below.\n\n&amp;#x200B;\n\nMy problem is, I can't bring the old table's primary key across in case it's used in the new table, but there are other tables that I need to bring across that reference customerId, so I need a way to map the old customerid to the new one.\n\n&amp;#x200B;\n\nAfter writing the dataframe to the table, is there a way to access the new id and the original id?\n\n&amp;#x200B;\n\n    (df.write\n      .format(\"jdbc\")\n      .option(\"url\", \"jdbc:sqlserver://&lt;server_name&gt;;database=&lt;db-name&gt;;user=&lt;user-id&gt;;password=&lt;password&gt;;\")\n      .option(\"dbtable\", \"dbo.customers\")\n      .option(\"user\", \"xyz\")\n      .option(\"password\", \"xyz\")\n      .mode(\"append\")\n      .save()\n    )\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_56o0g58i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pyspark write dataframe to table, mapping new primary key", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mnnh8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703052469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hi guys&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m moving some customer data into a new database, I&amp;#39;m getting the customer data from the old database and writing it to the new one using the code below.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My problem is, I can&amp;#39;t bring the old table&amp;#39;s primary key across in case it&amp;#39;s used in the new table, but there are other tables that I need to bring across that reference customerId, so I need a way to map the old customerid to the new one.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;After writing the dataframe to the table, is there a way to access the new id and the original id?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;(df.write\n  .format(&amp;quot;jdbc&amp;quot;)\n  .option(&amp;quot;url&amp;quot;, &amp;quot;jdbc:sqlserver://&amp;lt;server_name&amp;gt;;database=&amp;lt;db-name&amp;gt;;user=&amp;lt;user-id&amp;gt;;password=&amp;lt;password&amp;gt;;&amp;quot;)\n  .option(&amp;quot;dbtable&amp;quot;, &amp;quot;dbo.customers&amp;quot;)\n  .option(&amp;quot;user&amp;quot;, &amp;quot;xyz&amp;quot;)\n  .option(&amp;quot;password&amp;quot;, &amp;quot;xyz&amp;quot;)\n  .mode(&amp;quot;append&amp;quot;)\n  .save()\n)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mnnh8", "is_robot_indexable": true, "report_reasons": null, "author": "IG-55", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mnnh8/pyspark_write_dataframe_to_table_mapping_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mnnh8/pyspark_write_dataframe_to_table_mapping_new/", "subreddit_subscribers": 147146, "created_utc": 1703052469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I finished bootcamp last week and started applying for jobs but i noticed it is so hard to find entry job for data engineering and they ask for experience like a year sometimes. The question do you think the bootcamp i did was 3 month and the portfolio is enough to get data engineer job or i should try to go for something lower and if so i don't even know what is lower then make my way up to DE maybe like DE internship or data analyst \n\nMy portfolio now have \n*data pipeline on aws and connect to grafana\n*Have another cli python program that takes order, records couriers as well, recorder on file or mysql database either as you want.\n*And other projects such as like blackjack game\nSoduko solver\n*And machine learning project did in engineering degree used knime so i didn't use code to build it just interactive thingy.", "author_fullname": "t2_a3acnwor", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on job hunt please", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mjey4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703038975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finished bootcamp last week and started applying for jobs but i noticed it is so hard to find entry job for data engineering and they ask for experience like a year sometimes. The question do you think the bootcamp i did was 3 month and the portfolio is enough to get data engineer job or i should try to go for something lower and if so i don&amp;#39;t even know what is lower then make my way up to DE maybe like DE internship or data analyst &lt;/p&gt;\n\n&lt;p&gt;My portfolio now have \n*data pipeline on aws and connect to grafana\n*Have another cli python program that takes order, records couriers as well, recorder on file or mysql database either as you want.\n*And other projects such as like blackjack game\nSoduko solver\n*And machine learning project did in engineering degree used knime so i didn&amp;#39;t use code to build it just interactive thingy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mjey4", "is_robot_indexable": true, "report_reasons": null, "author": "mahdy_mahmoud", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mjey4/advice_on_job_hunt_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mjey4/advice_on_job_hunt_please/", "subreddit_subscribers": 147146, "created_utc": 1703038975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI've been exploring tech choices for ad-hoc, user-facing/online query engines over delta lake tables stored in s3. Target latency isn't very strict but should be roughly up to 10 seconds. TBs of data stored over multiple tables where joins are expected. AWS tech is preferred but not mandatory. Queries should be SQL.\n\nThe primary motivation is managing costs while offering, as aforementioned, consistent latency up to 10 seconds.\n\nWe've been looking at:\n\n* EMR -- queries take up to 1 minute &amp; generally don't seem directed towards online querying. Out of that 1 minute it seems like most of the time is overhead rather than the actual query\n* Athena -- looked promising, but then I read queries might be queued &amp; in general, concurrency might be an issue.\n* Redshift (Spectrum) -- s3 support is experimental &amp; it seems concurrency might be an issue too\n\n&amp;#x200B;\n\nIs there an adequate &amp; proven engine for online, highly concurrent queries, or should I consider introducing a dedicated data tier?\n\nThanks!", "author_fullname": "t2_5aqdehqj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "query engine for a highly concurrent, user-facing data layer over delta lake tables in s3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18molzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703056027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been exploring tech choices for ad-hoc, user-facing/online query engines over delta lake tables stored in s3. Target latency isn&amp;#39;t very strict but should be roughly up to 10 seconds. TBs of data stored over multiple tables where joins are expected. AWS tech is preferred but not mandatory. Queries should be SQL.&lt;/p&gt;\n\n&lt;p&gt;The primary motivation is managing costs while offering, as aforementioned, consistent latency up to 10 seconds.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been looking at:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;EMR -- queries take up to 1 minute &amp;amp; generally don&amp;#39;t seem directed towards online querying. Out of that 1 minute it seems like most of the time is overhead rather than the actual query&lt;/li&gt;\n&lt;li&gt;Athena -- looked promising, but then I read queries might be queued &amp;amp; in general, concurrency might be an issue.&lt;/li&gt;\n&lt;li&gt;Redshift (Spectrum) -- s3 support is experimental &amp;amp; it seems concurrency might be an issue too&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there an adequate &amp;amp; proven engine for online, highly concurrent queries, or should I consider introducing a dedicated data tier?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18molzy", "is_robot_indexable": true, "report_reasons": null, "author": "Adventurous_Ad3893", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18molzy/query_engine_for_a_highly_concurrent_userfacing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18molzy/query_engine_for_a_highly_concurrent_userfacing/", "subreddit_subscribers": 147146, "created_utc": 1703056027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know any good data modelling resources?\n\nBesides, the Kimball book or blog posts with super basic sales facts and dimensions, I'm struggling to find good resources to upskill my data modelling skills. I'm looking for resources that deep-dive into real-world use cases.\n\nI basically would like to be better at answering the questions: \"What is the best way to model those OLTP tables to answer those business analytical use cases?\"", "author_fullname": "t2_2joko4jg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good data modelling resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mmdlx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703048190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know any good data modelling resources?&lt;/p&gt;\n\n&lt;p&gt;Besides, the Kimball book or blog posts with super basic sales facts and dimensions, I&amp;#39;m struggling to find good resources to upskill my data modelling skills. I&amp;#39;m looking for resources that deep-dive into real-world use cases.&lt;/p&gt;\n\n&lt;p&gt;I basically would like to be better at answering the questions: &amp;quot;What is the best way to model those OLTP tables to answer those business analytical use cases?&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18mmdlx", "is_robot_indexable": true, "report_reasons": null, "author": "Alert_Dragonfly", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mmdlx/any_good_data_modelling_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mmdlx/any_good_data_modelling_resources/", "subreddit_subscribers": 147146, "created_utc": 1703048190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking to move from a Microsoft SQL Analysis Service (Excel + SSRS) data stack to a modern data stack using Snowflake and Tableau. My data models are all star schemas and modeled directly in the cube(s).\n\nAs I reviewing how the modern data stack is typically deployed, I see that the data warehouse, like Snowflake, does not maintain a physical data model. It seems as though the data can be orchestrated in a normalized model and live in Snowflake, but my end user connecting Tableau to Snowflake will have to do their own modeling.\n\nIt seems as though a set of denormalized tables might be easier for end users to work with, but I fear the tables will be so wide and so large that they will not perform well.\n\nPerhaps I am missing something in terms of the structure, but how would an end user who isn't a savvy with SQL or truly understand JOINS go about building consistent and clear dashboards? \n\nDoes Tableau allow me to build an enterprise data model that all users can use so they can just drag and drop dimension attributes and metrics onto the page?", "author_fullname": "t2_nuco2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling in Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mkxxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703043590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to move from a Microsoft SQL Analysis Service (Excel + SSRS) data stack to a modern data stack using Snowflake and Tableau. My data models are all star schemas and modeled directly in the cube(s).&lt;/p&gt;\n\n&lt;p&gt;As I reviewing how the modern data stack is typically deployed, I see that the data warehouse, like Snowflake, does not maintain a physical data model. It seems as though the data can be orchestrated in a normalized model and live in Snowflake, but my end user connecting Tableau to Snowflake will have to do their own modeling.&lt;/p&gt;\n\n&lt;p&gt;It seems as though a set of denormalized tables might be easier for end users to work with, but I fear the tables will be so wide and so large that they will not perform well.&lt;/p&gt;\n\n&lt;p&gt;Perhaps I am missing something in terms of the structure, but how would an end user who isn&amp;#39;t a savvy with SQL or truly understand JOINS go about building consistent and clear dashboards? &lt;/p&gt;\n\n&lt;p&gt;Does Tableau allow me to build an enterprise data model that all users can use so they can just drag and drop dimension attributes and metrics onto the page?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18mkxxj", "is_robot_indexable": true, "report_reasons": null, "author": "2000gt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mkxxj/data_modeling_in_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mkxxj/data_modeling_in_snowflake/", "subreddit_subscribers": 147146, "created_utc": 1703043590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry if this is an amateur question, I\u2019m not a data engineer, just an analyst that wears a lot of hats except analyzing data. \n\nCurrently we are evaluating how to develop better data processes as a lot of our work is done manually in Excel. We have a SAAS system we produce which does a lot of the calculations (industry specific data transformation software), but before ingesting the data into the system, the analyst must perform a bunch of transformations in Excel and perform a bunch of validations. We want to automate this.\n\nAt first I was leaning towards using SQL and Python if necessary. One of my managers is leading towards using Jedox, a BI tool (really a FP&amp;A tool). Using Jedox would have a lot of benefits, it will most likely be on me to build this, and I\u2019ve automated our reporting in Jedox, but haven\u2019t really built anything with Python or SQL before (at work), so I should be able to move quicker with Jedox. There is also the benefit that we could store aggregated data before processing, after processing, and then while reporting to make sure everything was processed correctly all in the same system. We also have all the security stuff figured out in Jedox, and could give analysts a front end where they could run their own data pipelines and get exports. Jedox also allows us to use Groovy scripting for more complex transformations, running jobs, setting variables, and all that stuff.\n\nThe validations are stuff like making sure no prices were sold below a certain listed price, specific ID\u2019s for specific types of sales are still active in another database, finding any value not contained various reference tables that needs to be added, validating certain amounts are calculated correctly, and so on. The transformations are basic and can be done in Excel. With some rare clients, we have to do weird stuff like consolidate pieces of data from PDF\u2019s to produce a file, but that\u2019s very rare. We are constantly adding new clients though, so this system will need to be able to work for all our clients.\n\nTo me it seems like Jedox could actually be really useful for this, but I know it generally goes against what the type of advice I see here where people generally don\u2019t recommend to use a BI tool for work like this. Is there anything wrong with taking this approach? ", "author_fullname": "t2_8e28mn79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it bad to use a BI tool for Data pre-processing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18mits6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703037237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is an amateur question, I\u2019m not a data engineer, just an analyst that wears a lot of hats except analyzing data. &lt;/p&gt;\n\n&lt;p&gt;Currently we are evaluating how to develop better data processes as a lot of our work is done manually in Excel. We have a SAAS system we produce which does a lot of the calculations (industry specific data transformation software), but before ingesting the data into the system, the analyst must perform a bunch of transformations in Excel and perform a bunch of validations. We want to automate this.&lt;/p&gt;\n\n&lt;p&gt;At first I was leaning towards using SQL and Python if necessary. One of my managers is leading towards using Jedox, a BI tool (really a FP&amp;amp;A tool). Using Jedox would have a lot of benefits, it will most likely be on me to build this, and I\u2019ve automated our reporting in Jedox, but haven\u2019t really built anything with Python or SQL before (at work), so I should be able to move quicker with Jedox. There is also the benefit that we could store aggregated data before processing, after processing, and then while reporting to make sure everything was processed correctly all in the same system. We also have all the security stuff figured out in Jedox, and could give analysts a front end where they could run their own data pipelines and get exports. Jedox also allows us to use Groovy scripting for more complex transformations, running jobs, setting variables, and all that stuff.&lt;/p&gt;\n\n&lt;p&gt;The validations are stuff like making sure no prices were sold below a certain listed price, specific ID\u2019s for specific types of sales are still active in another database, finding any value not contained various reference tables that needs to be added, validating certain amounts are calculated correctly, and so on. The transformations are basic and can be done in Excel. With some rare clients, we have to do weird stuff like consolidate pieces of data from PDF\u2019s to produce a file, but that\u2019s very rare. We are constantly adding new clients though, so this system will need to be able to work for all our clients.&lt;/p&gt;\n\n&lt;p&gt;To me it seems like Jedox could actually be really useful for this, but I know it generally goes against what the type of advice I see here where people generally don\u2019t recommend to use a BI tool for work like this. Is there anything wrong with taking this approach? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18mits6", "is_robot_indexable": true, "report_reasons": null, "author": "Icy-Big2472", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mits6/is_it_bad_to_use_a_bi_tool_for_data_preprocessing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18mits6/is_it_bad_to_use_a_bi_tool_for_data_preprocessing/", "subreddit_subscribers": 147146, "created_utc": 1703037237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have a source table updated from GCS files uploaded daily.\n\nFrom this source table i have another table for my client, cleansed &amp; transformed.\n\n&amp;#x200B;\n\nBut now i would like to only add new incoming data daily from the source table without having a unique key.\n\n  \nHow should i do ?  \n\n\nRegards.", "author_fullname": "t2_4lzse6nu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "update BQ with GCS using DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18me2df", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703023929.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have a source table updated from GCS files uploaded daily.&lt;/p&gt;\n\n&lt;p&gt;From this source table i have another table for my client, cleansed &amp;amp; transformed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But now i would like to only add new incoming data daily from the source table without having a unique key.&lt;/p&gt;\n\n&lt;p&gt;How should i do ?  &lt;/p&gt;\n\n&lt;p&gt;Regards.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18me2df", "is_robot_indexable": true, "report_reasons": null, "author": "Resident_Set204", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18me2df/update_bq_with_gcs_using_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18me2df/update_bq_with_gcs_using_dbt/", "subreddit_subscribers": 147146, "created_utc": 1703023929.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp; Jesse Anderson \u2022 GOTO 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18mdpui", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/P-9FwZxO1zE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp;amp; Jesse Anderson \u2022 GOTO 2023\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp; Jesse Anderson \u2022 GOTO 2023", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/P-9FwZxO1zE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp;amp; Jesse Anderson \u2022 GOTO 2023\"&gt;&lt;/iframe&gt;", "author_name": "GOTO Conferences", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/P-9FwZxO1zE/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@GOTO-"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/P-9FwZxO1zE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp;amp; Jesse Anderson \u2022 GOTO 2023\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18mdpui", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EG6dIjYfo7koB7YwHcHVp8yD3RXY4TPnkQbuGBMMe60.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703023012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/P-9FwZxO1zE?si=UxntLaaWAtmKBiY7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HmAUe332NFaRs9nHi_AaIW-hxIAsSR-MYRhh_jQSE5Y.jpg?auto=webp&amp;s=c893c1d8ac73658e95fcfa89af76635829a5f0a1", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/HmAUe332NFaRs9nHi_AaIW-hxIAsSR-MYRhh_jQSE5Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99c4a68cc33ce73559ee7a00f579d49bdb5bf269", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/HmAUe332NFaRs9nHi_AaIW-hxIAsSR-MYRhh_jQSE5Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4857927413c84daab0812d5b736443b5d601f28", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/HmAUe332NFaRs9nHi_AaIW-hxIAsSR-MYRhh_jQSE5Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1cb2927d72da9e6d6353b49959f7508ba4c618c", "width": 320, "height": 240}], "variants": {}, "id": "tc09KFreOE8Z89GIDoJawxTXj4KExCL8ftOn_LgGEU8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18mdpui", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18mdpui/designing_a_dataintensive_future_expert_talk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/P-9FwZxO1zE?si=UxntLaaWAtmKBiY7", "subreddit_subscribers": 147146, "created_utc": 1703023012.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp; Jesse Anderson \u2022 GOTO 2023", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/P-9FwZxO1zE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Designing A Data-Intensive Future: Expert Talk \u2022 Martin Kleppmann &amp;amp; Jesse Anderson \u2022 GOTO 2023\"&gt;&lt;/iframe&gt;", "author_name": "GOTO Conferences", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/P-9FwZxO1zE/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@GOTO-"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title is pretty self-explanatory. \n\nAre y'all noticing recruiter emails coming to your work email ? I don't know how they got my work email into their HR or emailing systems ?! \n\nI'm interested but obvi can't reply from work email lol", "author_fullname": "t2_roct4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recruiters reaching out to you on your work email ? are these emails automated or hand-written ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m6ey9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703013326.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703004561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title is pretty self-explanatory. &lt;/p&gt;\n\n&lt;p&gt;Are y&amp;#39;all noticing recruiter emails coming to your work email ? I don&amp;#39;t know how they got my work email into their HR or emailing systems ?! &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested but obvi can&amp;#39;t reply from work email lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18m6ey9", "is_robot_indexable": true, "report_reasons": null, "author": "dronedesigner", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m6ey9/recruiters_reaching_out_to_you_on_your_work_email/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m6ey9/recruiters_reaching_out_to_you_on_your_work_email/", "subreddit_subscribers": 147146, "created_utc": 1703004561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIam writing a snowflake stored procedure in python language.\n\nBelow is the sample code:\n\n    CREATE OR REPLACE PROCEDURE edw_dev.master.SampleSP(TABLENAMES ARRAY, ORDERBYCOLUMNS ARRAY DEFAULT NULL)\n    RETURNS STRING\n    LANGUAGE PYTHON\n    RUNTIME_VERSION = '3.8'\n    PACKAGES = ('snowflake-snowpark-python')\n    HANDLER = 'process_tables'\n    as\n    $$\n    def process_tables(snowpark_session, TABLENAMES, ORDERBYCOLUMNS):\n        \n                createTableSql = f'CREATE OR REPLACE TABLE {newTableName} LIKE {fullTableName}'\n    \n                snowpark_session.sql(createTableSql).collect()\n    \n    \n    \n    $$;\n\nI need to implement a lock using 'begin transaction' and 'commit' for a section inside my stored procedure. How can we do this in python snowflake stored proc?\n\nI don't see much detail about this in the documentation as it contains details for Snowflake scripting alone.\n\nHas anyone come across this? Appreciate your help in advance.!", "author_fullname": "t2_9fr6if3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help: Snowflake Python Stored procedure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18m2rg5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702995008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Iam writing a snowflake stored procedure in python language.&lt;/p&gt;\n\n&lt;p&gt;Below is the sample code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REPLACE PROCEDURE edw_dev.master.SampleSP(TABLENAMES ARRAY, ORDERBYCOLUMNS ARRAY DEFAULT NULL)\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = &amp;#39;3.8&amp;#39;\nPACKAGES = (&amp;#39;snowflake-snowpark-python&amp;#39;)\nHANDLER = &amp;#39;process_tables&amp;#39;\nas\n$$\ndef process_tables(snowpark_session, TABLENAMES, ORDERBYCOLUMNS):\n\n            createTableSql = f&amp;#39;CREATE OR REPLACE TABLE {newTableName} LIKE {fullTableName}&amp;#39;\n\n            snowpark_session.sql(createTableSql).collect()\n\n\n\n$$;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I need to implement a lock using &amp;#39;begin transaction&amp;#39; and &amp;#39;commit&amp;#39; for a section inside my stored procedure. How can we do this in python snowflake stored proc?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t see much detail about this in the documentation as it contains details for Snowflake scripting alone.&lt;/p&gt;\n\n&lt;p&gt;Has anyone come across this? Appreciate your help in advance.!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18m2rg5", "is_robot_indexable": true, "report_reasons": null, "author": "aj_here_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18m2rg5/help_snowflake_python_stored_procedure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18m2rg5/help_snowflake_python_stored_procedure/", "subreddit_subscribers": 147146, "created_utc": 1702995008.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}