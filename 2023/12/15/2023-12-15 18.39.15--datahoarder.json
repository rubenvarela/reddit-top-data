{"kind": "Listing", "data": {"after": "t3_18iewzm", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ev065", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Come on Kingston... Do Better!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18j43eo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 93, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 93, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c0NwfLMauQ7HGfy9jt-wWmeZ_oR2ULC4_qLvazjGWAA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702658017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8taijqobkh6c1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?auto=webp&amp;s=c559e876d037d78e2827367a07269a4edef22942", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71f68c29717332a2a186d09ea132d80b400145a3", "width": 108, "height": 81}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ce2df53814e50b9650050ce6268eddb3f287b52", "width": 216, "height": 162}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9246c0e08e429aee0122accdfbe4fcedee6fa03e", "width": 320, "height": 240}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=56cefba73e05779a5e51f6dc104bf7e0da91817c", "width": 640, "height": 480}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e3464a282d0e36342aa9d5fb3a78d7e51d10c6b", "width": 960, "height": 720}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=969a77d6c9ca3371a70627016739b5f5596849f4", "width": 1080, "height": 810}], "variants": {}, "id": "_CJouJmRgGzuxL0A-uh26EL2IKXlNQyA6ymyZOk2Z8c"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j43eo", "is_robot_indexable": true, "report_reasons": null, "author": "zaca21", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j43eo/come_on_kingston_do_better/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8taijqobkh6c1.jpeg", "subreddit_subscribers": 718264, "created_utc": 1702658017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure where else to post this. I purchased a few orders from WD's online store. Opened and used all the orders except one which had a 4TB SSD in it. I did not even open this package. Wanted to send it back as they claim they offer free returns and refunds. Price dropped on the SSDs, so I purchased more drives at the discount price, so I wanted to send this order back because customer service said they could not price adjust, but I could always return it. So I contact a WD rep on live chat, they give me a UPS label. \n\nI take their UPS label, apply it on the package it arrived in and sent it off. \n\nA few days after they received the package they claim there is no WD product in my return, and claim I sent back a Samsung drive. I have owned a lot of WD drives over the years, but one brand I never owned was a Samsung drive. \n\nTheir responses seem to say they will scrap the item, or they can send me back what I assume is another customer's Samsung drive. In either case, I will not get a refund, nor the item I paid for. \n\nThis is more frustrating than usual because that Samsung drive likely belongs to someone else and has their personal info on it. \n\nAre there any tactics to force WD to do a refund? Their Customer Service is giving me 1 more week to decide on no refund, or no refund and some other customer's mystery Samsung drive, which I would have to purchase a shipping label for. Any advice for people that dealt with WD customer service appreciated. \n\nOnly other thing I can say is don't buy from WD.com.", "author_fullname": "t2_10oeg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital denying return, claims I sent a Samsung drive back", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iozmo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702605616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure where else to post this. I purchased a few orders from WD&amp;#39;s online store. Opened and used all the orders except one which had a 4TB SSD in it. I did not even open this package. Wanted to send it back as they claim they offer free returns and refunds. Price dropped on the SSDs, so I purchased more drives at the discount price, so I wanted to send this order back because customer service said they could not price adjust, but I could always return it. So I contact a WD rep on live chat, they give me a UPS label. &lt;/p&gt;\n\n&lt;p&gt;I take their UPS label, apply it on the package it arrived in and sent it off. &lt;/p&gt;\n\n&lt;p&gt;A few days after they received the package they claim there is no WD product in my return, and claim I sent back a Samsung drive. I have owned a lot of WD drives over the years, but one brand I never owned was a Samsung drive. &lt;/p&gt;\n\n&lt;p&gt;Their responses seem to say they will scrap the item, or they can send me back what I assume is another customer&amp;#39;s Samsung drive. In either case, I will not get a refund, nor the item I paid for. &lt;/p&gt;\n\n&lt;p&gt;This is more frustrating than usual because that Samsung drive likely belongs to someone else and has their personal info on it. &lt;/p&gt;\n\n&lt;p&gt;Are there any tactics to force WD to do a refund? Their Customer Service is giving me 1 more week to decide on no refund, or no refund and some other customer&amp;#39;s mystery Samsung drive, which I would have to purchase a shipping label for. Any advice for people that dealt with WD customer service appreciated. &lt;/p&gt;\n\n&lt;p&gt;Only other thing I can say is don&amp;#39;t buy from WD.com.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18iozmo", "is_robot_indexable": true, "report_reasons": null, "author": "abcalt", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iozmo/western_digital_denying_return_claims_i_sent_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iozmo/western_digital_denying_return_claims_i_sent_a/", "subreddit_subscribers": 718264, "created_utc": 1702605616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I give you the [TNASR1](https://imgur.com/a/fduKt1N) \"Tiny NAS with RAID1\" - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5\" SATA HDDs in  RAID1\n\n\n##Table of content\n\n1. Intro\n\n2. Why\n\n3. Goals\n\n4. BOM\n\n5. My Setup \n\n6. Cost\n\n7. Performance\n\n8. Power consumption\n\n9. Setup\n\n10. Scripting with scheduled wakeup to save power\n\n###Intro\n\nSo I have an Unraid Server wich is great and I am really happy with it. It stores all my files including media but also my very important documents as well as my maybe even more important pictures and videos of my family. So I always felt uneasy thinking about its safety. It's a consumer grade tower PC located in my garage. I use two parity drives so on a system level I have some redundancy. Nothing wrong with that but if something happens to my server as a whole (fire/flooding/kids) all my data is gone.\n\nOf course there is the possibility if using a cloud storage provider. Google drive is $12 per month or $144 per year. First of all I would be trusting a third party with the safety of my data and secondly I am cheap bastard. \n\nThis is the genesis of the TNASR1.\n\nIt lives in my garden shed in a watertight container and does everything a google drive would.\n\n\n###Why\n\n- Data Backup needs - common sense i.e. the [3 2 1 Rule](https://en.wikipedia.org/wiki/Glossary_of_backup_terms#Terms_and_definitions) demands a remote copy.\n\n- Control - cloud providers are a third party that require a level of trust and cost money above certain storage needs\n\n- Cost - cheaper in the long run\n\n\n###Goals\n\n- Cheap\n\n- Low Power\n\n- enough storage for the most important files. At least 500gb+\n\n- RAID1 for some fault resilience\n\n###BOM\n\n- Raspberry Pi Zero 2 W\n\n- a small heat sink\n\n- thermal paste\n\n- microSD card with at least 2gb\n\n- USB OTG Hub Host Cable\n\n- A 2A/**5.3V** - Power supply\n\n- USB A to Micro USB B cable\n\n- 2x 2.5inch SATA Case \n\n- 2x SATA drives\n\n###My Setup\n\n\n- Raspberry Pi Zero 2 W  - \u20ac19.80\n\n- The heat sink and thermal paste i had laying around. But I guess \u20ac1 is fair.\n\n- INTENSO 3413460 - MicroSDHC-Card with 8GB, Intenso Class 10 - \u20ac3.40\n\n- USB OTG Hub Host Cable. Ali express item:3256805033322631 \u20ac1.40\n\n- Samsung EP-TA10EWE Power supply.\n\n- 2x 2.5inch SATA Case. Ali express item: 3256805261700001  - \u20ac1.00\n\n- For storage I used  2x 2TB Toshiba L200 bulk HDWL120UZSVA - \u20ac65.00 each\n\n###Cost\n\nNAS cost before storage: \u20ac34.60 or around $37.20\n\nIn total ~ \u20ac164 or $178 for 2 Terabyte of remote RAID1 storage. Not bad if I say so myself. ROI vs. google Drive ($144 per year for 2 terabyte) in 15 months. \n\nCost for electricity (more down below) is about $/\u20ac20 a year if you let it run continuously with $/\u20ac0.3 per kWh. ROI vs Google Drive under 17 months.\n\n###Performance\n\nSo the two SATA drives in RAID1 are connected over a single USB 2.0 interface. Suffice to say you won't get SSD speeds. But I am Happy to report that it is quite fast enough for our needs. I get around 2MB/s write and 4MB/s of read. That means I can sync 1 gigabyte of data in under 9 Minutes. Now this is nothing to write home about but we have to consider context. This for a remote backup that uses less than 10w. Also after initial setup this is for syncing the diff only. I tend to sync less than 5 gigabyte of new data a week. That means backup takes under an hour.\n\nFor my initial sync of my files it took 8 hours for 48 Gigabytes with a transfer speed of about  1.8 MB/s\n\n    Transferred:   \t   51.312 GiB / 51.312 GiB, 100%, 1,852.5 KiB/s, ETA 0s\n    Checks:             71697 / 71697, 100%\n    Deleted:               11 (files), 0 (dirs)\n    Transferred:        70846 / 70846, 100%\n    Elapsed time:    8h4m34.3s\n\nMy initial sync of my pictures and videos of 435 Gigabytes took 2 days and 9 hours with an average transfer speed of 2.2 MB/s\n\n    Transferred:   \t  455.358 GiB / 455.358 GiB, 100%, 2.316 MiB/s, ETA 0s\n    Checks:            111442 / 111442, 100%\n    Deleted:               11 (files), 0 (dirs)\n    Transferred:       111478 / 111478, 100%\n    Elapsed time:  2d9h13m13.9s\n\nA sync run without anything to sync takes under 2 Minutes.\n\n    Rclone sync completed in 0 hours, 1 minutes, and 45 seconds 390 milliseconds.\n\n###Power Consumption\n\nThe star of the show is undoubtedly the 5.3V power supply. Any power supply with just 5V, even ones that can deliver 100W, failed the boot up during the high ramp up power spike from the HDDs. Te whole system is teetering on being power starved. But it was up and running and syncing my 500 gigabytes without any issues.\n\n[The whole setup draws continuous 5.3V 1.5A or 8W max during a sync.](https://imgur.com/PANKpo5) \n\nI am sure when the drives spin up the power draw spikes up above 1.5A but as the supply has more than 5V it does not cause any issues.\n\n[At Idle the power draw is about 0.6A or about 3W.](https://imgur.com/c1CnIv7)\n\nThere are some Samsung Power Supplies that have 5.3V. \n\n[Alternatively there are AC/DC adapters like these.](https://www.voc-electronics.com/a-42425080/power-batteries-adapters/5-3v-2a-power-supply-eu/#description)\n\n\n\n###Setup\n\n1. Setup the MicroSD with Raspberry Pi Imager. \n\n  1.1 Choose Raspberry Pi OS (other)\n\n  1.2 Chose Raspberry Pi OS lite (64-bit)\n\n  1.3 Set Up WiFi and turn on SSH\n\n  1.4 Burn Image\n\n2. Check if everything is running\n\n  2.1 Insert microSD \n\n  2.2 Connect Power Supply\n\n  2.3 Ping device\n\n  2.4 If successful try to SSH into the pi with : SSH user@nameOfPi or: SSH user@IPaddrOfPi\n\n  2.5 Power down and disconnect power supply\n\n3. [Setup the drives](https://www.computernetworkingnotes.com/linux-tutorials/how-to-configure-raid-in-linux-step-by-step-guide.html)\n\n  3.1 Connect drives via the USB hub dongle\n\n  3.2 Reconnect Power Supply\n\n  3.3 SSH back into the Pi\n\n  3.4 Type in:  \n\n        lsblk  \n\n  This should confirm two things. The drives are connected and have the right size. You should have an output like this:\n\n\n    \n        sda           8:16   0  1.8T  0 disk \n        sdb           8:16   0  1.8T  0 disk\n        mmcblk0     179:0    0  7.5G  0 disk \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n\n  3.5 Type in: \n\n        sudo fdisk /dev/sda\n\n  new drive\n\n        n\n\n  primary\n\n        p\n  \n  list\n\n        l\n\n  type\n\n        t\n\n  Linux raid auto\n\n        fd\n\n\n  write\n\n        w\n\n  3.6 repeat for sdb\n\n  3.7 [Partprobe](https://www.computerhope.com/unix/partprob.htm)\n\n        partprobe\n\n  3.8 Check if sda1 and sdb1 are listed\n\n        lsblk\n\n  output should look like this\n\n        NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n        sda           8:0    0  1.8T  0 disk \n        \u2514\u2500sda1        8:1    0  1.8T  0 part \n        sdb           8:16   0  1.8T  0 disk \n        \u2514\u2500sdb1        8:17   0  1.8T  0 part \n        mmcblk0     179:0    0  7.5G  0 disk \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n        \n  3.9 Check if the file system is correct\n\n        sudo fdisk -l\n\n  the output should include the type \"Linux raid auto\"\n\n        Device     Boot Start        End    Sectors  Size Id Type\n        /dev/sda1        2048 3907029167 3907027120  1.8T fd Linux raid autodetect\n \n\n4. [Setup raid](https://www.thetechedvocate.org/how-to-set-up-raid-1-on-the-raspberry-pi-the-easy-way/)\n\n  4.1 Install mdadm\n\n        sudo apt-get install mdadm\n\n  4.2 create raid array\n\n        sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1\n\n  Continue creating array?\n\n        y\n\n  4.3 check if md0 was created\n\n        lsblk\n\n  the output should look like this\n\n        NAME        MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT\n        sda           8:0    0  1.8T  0 disk  \n        \u2514\u2500sda1        8:1    0  1.8T  0 part  \n          \u2514\u2500md0       9:0    0  1.8T  0 raid1 \n        sdb           8:16   0  1.8T  0 disk  \n        \u2514\u2500sdb1        8:17   0  1.8T  0 part  \n          \u2514\u2500md0       9:0    0  1.8T  0 raid1 \n        mmcblk0     179:0    0  7.5G  0 disk  \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part  /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part  /\n\n5. Format the raid array / make filesystem\n\n  5.1 Type in\n\n        sudo mkfs.ext4 /dev/md0\n\n6. [Mount to folder](https://www.youtube.com/watch?v=Ff96FPJHq5o)\n\n  6.1 Make folder under mnt\n\n        sudo mkdir /mnt/storage\n\n  6.2 Mount md0 to storage\n\n        sudo mount /dev/md0 /mnt/storage\n\n\n  6.6. mount on startup\n\n        sudo nano /etc/fstab\n\n  6.7 add line\n\n        /dev/md0 /mnt/storage ext4 defaults 0 0\n\n  6.8 Save and exit\n\n  6.9 reboot\n\n        reboot\n\n\n  6.4 check if mount is successfull\n        \n        df -alh\n\n        \n  output should look like this\n\n        Filesystem      Size  Used Avail Use% Mounted on\n        /dev/root       7.1G  1.7G  5.2G  25% /\n        devtmpfs         80M     0   80M   0% /dev\n        proc               0     0     0    - /proc\n        sysfs              0     0     0    - /sys\n        securityfs         0     0     0    - /sys/kernel/security\n        tmpfs           210M     0  210M   0% /dev/shm\n        devpts             0     0     0    - /dev/pts\n        tmpfs            84M  3.0M   81M   4% /run\n        tmpfs           5.0M  4.0K  5.0M   1% /run/lock\n        cgroup2            0     0     0    - /sys/fs/cgroup\n        pstore             0     0     0    - /sys/fs/pstore\n        bpf                0     0     0    - /sys/fs/bpf\n        systemd-1          0     0     0    - /proc/sys/fs/binfmt_misc\n        mqueue             0     0     0    - /dev/mqueue\n        debugfs            0     0     0    - /sys/kernel/debug\n        sunrpc             0     0     0    - /run/rpc_pipefs\n        tracefs            0     0     0    - /sys/kernel/tracing\n        configfs           0     0     0    - /sys/kernel/config\n        fusectl            0     0     0    - /sys/fs/fuse/connections\n        /dev/mmcblk0p1  255M   31M  225M  13% /boot\n        tmpfs            42M     0   42M   0% /run/user/1000\n        /dev/md0        1.8T   28K  1.7T   1% /mnt/storage\n        \n7. enable SMB sharing\n\n  7.1 install samba\n\n        sudo apt-get samba\n\n  7.2 edit samba config\n\n        sudo nano /etc/samba/smb.conf\n\n  7.3 add this at the end\n\n        [storage]\n        path=/mnt/storage\n        writeable=yes\n        reate mask=0666\n        directorty mask=0666\n        public=yes\n\n  7.4 Save end exit\n\n  7.5 restart samba service\n\n        sudo systemctl restart smbd\n\n  7.6 ad a user to samba\n\n        sudo smbpasswd -a yourDesiredUsername\n\n  7.7 Set a user password\n\nThat is it you are done. See if you can find your folder in the network on your windows machine.\n\n\n###Scripting\n\nOn my unraid server I have this neat little script where I use mosquitto and nodered to turn on and off a shelly socket to save power. The script is on a weekly schedule. In Nodered I also send myself a telegram message once the script starts, When the PI is found, and when its done including the time elapsed and the amount of data synced.\n\n\n\n    #!/bin/bash\n    \n    \n    \n    # Record the start time\n    start_timeTotal=$(date +%s%3N)\n    \n    # Set your Raspberry Pi's IP address\n    RASPBERRY_PI_IP=\"192.168.8.107\"\n    \n    \n    # Set your Mosquitto container name or ID\n    CONTAINER_NAME=\"mosquitto\"\n    \n    \n    \n    # Set MQTT details\n    HOST=\"localhost\"  # Use localhost because we're inside the container\n    PORT=1883         # Specify the Mosquitto broker port\n    TOPIC=\"Backup\"    # Specify the topic you want to publish to\n    \n    \n    # Execute mosquitto_pub inside the container\n    #docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Starting\"\n    \n    \n    \n    \n    for _ in {1..12}; do\n        if ping -c 1 \"$RASPBERRY_PI_IP\" &amp;&gt; /dev/null; then\n            echo \"Raspberry Pi is up!\"\n            break\n        else\n            sleep 5  # Wait 5 seconds before checking again\n        fi\n    done\n    \n    # If Raspberry Pi is still not up after 1 minute, exit\n    if ! ping -c 1 \"$RASPBERRY_PI_IP\" &amp;&gt; /dev/null; then\n        docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Raspberry Pi did not start up within 1 minute. Exiting.\"\n        exit 1\n    fi\n    \n    \n    # Record the start time\n    start_timeFiles=$(date +%s%3N)\n    \n    # Start Rclone sync (adjust paths and remote as needed)\n    rclone sync /mnt/user/files backupPI:storage/files -v\n    \n    # Record the end time of files\n    end_timeFiles=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timeFiles - start_timeFiles))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Rclone sync files completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    \n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Sync Files done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n    # Record the start time pictures\n    start_timePictures=$(date +%s%3N)\n    \n    \n    # Start Rclone sync (adjust paths and remote as needed)\n    rclone sync /mnt/user/pictures backupPI:storage/pictures -v\n    \n    \n    \n    # Record the end time\n    end_timePictures=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timePictures - start_timePictures))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Rclone sync Pictures completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    \n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Sync Pictures done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n    # Record the end time\n    end_timeTotal=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timeTotal - start_timeTotal))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Complete sync completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Complete sync completed and was done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n\n\n\nI hope this is useful to some of you. I will amend and edit this post with your feedback and keep it alive as long as I can. Have a great weekend!", "author_fullname": "t2_fbw2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[TNASR1] - Tiny NAS with RAID1 - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5\" SATA HDDs in RAID1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j130b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup | Guide/How-to | Scripts/Software", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702649808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I give you the &lt;a href=\"https://imgur.com/a/fduKt1N\"&gt;TNASR1&lt;/a&gt; &amp;quot;Tiny NAS with RAID1&amp;quot; - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5&amp;quot; SATA HDDs in  RAID1&lt;/p&gt;\n\n&lt;h2&gt;Table of content&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Intro&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Why&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Goals&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;BOM&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;My Setup &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cost&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Performance&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Power consumption&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Setup&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scripting with scheduled wakeup to save power&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Intro&lt;/h3&gt;\n\n&lt;p&gt;So I have an Unraid Server wich is great and I am really happy with it. It stores all my files including media but also my very important documents as well as my maybe even more important pictures and videos of my family. So I always felt uneasy thinking about its safety. It&amp;#39;s a consumer grade tower PC located in my garage. I use two parity drives so on a system level I have some redundancy. Nothing wrong with that but if something happens to my server as a whole (fire/flooding/kids) all my data is gone.&lt;/p&gt;\n\n&lt;p&gt;Of course there is the possibility if using a cloud storage provider. Google drive is $12 per month or $144 per year. First of all I would be trusting a third party with the safety of my data and secondly I am cheap bastard. &lt;/p&gt;\n\n&lt;p&gt;This is the genesis of the TNASR1.&lt;/p&gt;\n\n&lt;p&gt;It lives in my garden shed in a watertight container and does everything a google drive would.&lt;/p&gt;\n\n&lt;h3&gt;Why&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Data Backup needs - common sense i.e. the &lt;a href=\"https://en.wikipedia.org/wiki/Glossary_of_backup_terms#Terms_and_definitions\"&gt;3 2 1 Rule&lt;/a&gt; demands a remote copy.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Control - cloud providers are a third party that require a level of trust and cost money above certain storage needs&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cost - cheaper in the long run&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Goals&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Cheap&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Low Power&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;enough storage for the most important files. At least 500gb+&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;RAID1 for some fault resilience&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;BOM&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Raspberry Pi Zero 2 W&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;a small heat sink&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;thermal paste&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;microSD card with at least 2gb&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB OTG Hub Host Cable&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A 2A/&lt;strong&gt;5.3V&lt;/strong&gt; - Power supply&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB A to Micro USB B cable&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x 2.5inch SATA Case &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x SATA drives&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;My Setup&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Raspberry Pi Zero 2 W  - \u20ac19.80&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The heat sink and thermal paste i had laying around. But I guess \u20ac1 is fair.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;INTENSO 3413460 - MicroSDHC-Card with 8GB, Intenso Class 10 - \u20ac3.40&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB OTG Hub Host Cable. Ali express item:3256805033322631 \u20ac1.40&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Samsung EP-TA10EWE Power supply.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x 2.5inch SATA Case. Ali express item: 3256805261700001  - \u20ac1.00&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For storage I used  2x 2TB Toshiba L200 bulk HDWL120UZSVA - \u20ac65.00 each&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Cost&lt;/h3&gt;\n\n&lt;p&gt;NAS cost before storage: \u20ac34.60 or around $37.20&lt;/p&gt;\n\n&lt;p&gt;In total ~ \u20ac164 or $178 for 2 Terabyte of remote RAID1 storage. Not bad if I say so myself. ROI vs. google Drive ($144 per year for 2 terabyte) in 15 months. &lt;/p&gt;\n\n&lt;p&gt;Cost for electricity (more down below) is about $/\u20ac20 a year if you let it run continuously with $/\u20ac0.3 per kWh. ROI vs Google Drive under 17 months.&lt;/p&gt;\n\n&lt;h3&gt;Performance&lt;/h3&gt;\n\n&lt;p&gt;So the two SATA drives in RAID1 are connected over a single USB 2.0 interface. Suffice to say you won&amp;#39;t get SSD speeds. But I am Happy to report that it is quite fast enough for our needs. I get around 2MB/s write and 4MB/s of read. That means I can sync 1 gigabyte of data in under 9 Minutes. Now this is nothing to write home about but we have to consider context. This for a remote backup that uses less than 10w. Also after initial setup this is for syncing the diff only. I tend to sync less than 5 gigabyte of new data a week. That means backup takes under an hour.&lt;/p&gt;\n\n&lt;p&gt;For my initial sync of my files it took 8 hours for 48 Gigabytes with a transfer speed of about  1.8 MB/s&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Transferred:       51.312 GiB / 51.312 GiB, 100%, 1,852.5 KiB/s, ETA 0s\nChecks:             71697 / 71697, 100%\nDeleted:               11 (files), 0 (dirs)\nTransferred:        70846 / 70846, 100%\nElapsed time:    8h4m34.3s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My initial sync of my pictures and videos of 435 Gigabytes took 2 days and 9 hours with an average transfer speed of 2.2 MB/s&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Transferred:      455.358 GiB / 455.358 GiB, 100%, 2.316 MiB/s, ETA 0s\nChecks:            111442 / 111442, 100%\nDeleted:               11 (files), 0 (dirs)\nTransferred:       111478 / 111478, 100%\nElapsed time:  2d9h13m13.9s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;A sync run without anything to sync takes under 2 Minutes.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Rclone sync completed in 0 hours, 1 minutes, and 45 seconds 390 milliseconds.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Power Consumption&lt;/h3&gt;\n\n&lt;p&gt;The star of the show is undoubtedly the 5.3V power supply. Any power supply with just 5V, even ones that can deliver 100W, failed the boot up during the high ramp up power spike from the HDDs. Te whole system is teetering on being power starved. But it was up and running and syncing my 500 gigabytes without any issues.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/PANKpo5\"&gt;The whole setup draws continuous 5.3V 1.5A or 8W max during a sync.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I am sure when the drives spin up the power draw spikes up above 1.5A but as the supply has more than 5V it does not cause any issues.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/c1CnIv7\"&gt;At Idle the power draw is about 0.6A or about 3W.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are some Samsung Power Supplies that have 5.3V. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.voc-electronics.com/a-42425080/power-batteries-adapters/5-3v-2a-power-supply-eu/#description\"&gt;Alternatively there are AC/DC adapters like these.&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Setup&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Setup the MicroSD with Raspberry Pi Imager. &lt;/p&gt;\n\n&lt;p&gt;1.1 Choose Raspberry Pi OS (other)&lt;/p&gt;\n\n&lt;p&gt;1.2 Chose Raspberry Pi OS lite (64-bit)&lt;/p&gt;\n\n&lt;p&gt;1.3 Set Up WiFi and turn on SSH&lt;/p&gt;\n\n&lt;p&gt;1.4 Burn Image&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Check if everything is running&lt;/p&gt;\n\n&lt;p&gt;2.1 Insert microSD &lt;/p&gt;\n\n&lt;p&gt;2.2 Connect Power Supply&lt;/p&gt;\n\n&lt;p&gt;2.3 Ping device&lt;/p&gt;\n\n&lt;p&gt;2.4 If successful try to SSH into the pi with : SSH user@nameOfPi or: SSH user@IPaddrOfPi&lt;/p&gt;\n\n&lt;p&gt;2.5 Power down and disconnect power supply&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.computernetworkingnotes.com/linux-tutorials/how-to-configure-raid-in-linux-step-by-step-guide.html\"&gt;Setup the drives&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;3.1 Connect drives via the USB hub dongle&lt;/p&gt;\n\n&lt;p&gt;3.2 Reconnect Power Supply&lt;/p&gt;\n\n&lt;p&gt;3.3 SSH back into the Pi&lt;/p&gt;\n\n&lt;p&gt;3.4 Type in:  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This should confirm two things. The drives are connected and have the right size. You should have an output like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sda           8:16   0  1.8T  0 disk \nsdb           8:16   0  1.8T  0 disk\nmmcblk0     179:0    0  7.5G  0 disk \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.5 Type in: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo fdisk /dev/sda\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;new drive&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;n\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;primary&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;p\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;list&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;l\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;type&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;t\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Linux raid auto&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;fd\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;write&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;w\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.6 repeat for sdb&lt;/p&gt;\n\n&lt;p&gt;3.7 &lt;a href=\"https://www.computerhope.com/unix/partprob.htm\"&gt;Partprobe&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;partprobe\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.8 Check if sda1 and sdb1 are listed&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsda           8:0    0  1.8T  0 disk \n\u2514\u2500sda1        8:1    0  1.8T  0 part \nsdb           8:16   0  1.8T  0 disk \n\u2514\u2500sdb1        8:17   0  1.8T  0 part \nmmcblk0     179:0    0  7.5G  0 disk \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.9 Check if the file system is correct&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo fdisk -l\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the output should include the type &amp;quot;Linux raid auto&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Device     Boot Start        End    Sectors  Size Id Type\n/dev/sda1        2048 3907029167 3907027120  1.8T fd Linux raid autodetect\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.thetechedvocate.org/how-to-set-up-raid-1-on-the-raspberry-pi-the-easy-way/\"&gt;Setup raid&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;4.1 Install mdadm&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo apt-get install mdadm\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;4.2 create raid array&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Continue creating array?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;y\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;4.3 check if md0 was created&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;NAME        MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT\nsda           8:0    0  1.8T  0 disk  \n\u2514\u2500sda1        8:1    0  1.8T  0 part  \n  \u2514\u2500md0       9:0    0  1.8T  0 raid1 \nsdb           8:16   0  1.8T  0 disk  \n\u2514\u2500sdb1        8:17   0  1.8T  0 part  \n  \u2514\u2500md0       9:0    0  1.8T  0 raid1 \nmmcblk0     179:0    0  7.5G  0 disk  \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part  /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part  /\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Format the raid array / make filesystem&lt;/p&gt;\n\n&lt;p&gt;5.1 Type in&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mkfs.ext4 /dev/md0\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=Ff96FPJHq5o\"&gt;Mount to folder&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;6.1 Make folder under mnt&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mkdir /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.2 Mount md0 to storage&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mount /dev/md0 /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.6. mount on startup&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo nano /etc/fstab\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.7 add line&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/dev/md0 /mnt/storage ext4 defaults 0 0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.8 Save and exit&lt;/p&gt;\n\n&lt;p&gt;6.9 reboot&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;reboot\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.4 check if mount is successfull&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df -alh\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Filesystem      Size  Used Avail Use% Mounted on\n/dev/root       7.1G  1.7G  5.2G  25% /\ndevtmpfs         80M     0   80M   0% /dev\nproc               0     0     0    - /proc\nsysfs              0     0     0    - /sys\nsecurityfs         0     0     0    - /sys/kernel/security\ntmpfs           210M     0  210M   0% /dev/shm\ndevpts             0     0     0    - /dev/pts\ntmpfs            84M  3.0M   81M   4% /run\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ncgroup2            0     0     0    - /sys/fs/cgroup\npstore             0     0     0    - /sys/fs/pstore\nbpf                0     0     0    - /sys/fs/bpf\nsystemd-1          0     0     0    - /proc/sys/fs/binfmt_misc\nmqueue             0     0     0    - /dev/mqueue\ndebugfs            0     0     0    - /sys/kernel/debug\nsunrpc             0     0     0    - /run/rpc_pipefs\ntracefs            0     0     0    - /sys/kernel/tracing\nconfigfs           0     0     0    - /sys/kernel/config\nfusectl            0     0     0    - /sys/fs/fuse/connections\n/dev/mmcblk0p1  255M   31M  225M  13% /boot\ntmpfs            42M     0   42M   0% /run/user/1000\n/dev/md0        1.8T   28K  1.7T   1% /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;enable SMB sharing&lt;/p&gt;\n\n&lt;p&gt;7.1 install samba&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo apt-get samba\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.2 edit samba config&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo nano /etc/samba/smb.conf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.3 add this at the end&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[storage]\npath=/mnt/storage\nwriteable=yes\nreate mask=0666\ndirectorty mask=0666\npublic=yes\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.4 Save end exit&lt;/p&gt;\n\n&lt;p&gt;7.5 restart samba service&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo systemctl restart smbd\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.6 ad a user to samba&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo smbpasswd -a yourDesiredUsername\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.7 Set a user password&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That is it you are done. See if you can find your folder in the network on your windows machine.&lt;/p&gt;\n\n&lt;h3&gt;Scripting&lt;/h3&gt;\n\n&lt;p&gt;On my unraid server I have this neat little script where I use mosquitto and nodered to turn on and off a shelly socket to save power. The script is on a weekly schedule. In Nodered I also send myself a telegram message once the script starts, When the PI is found, and when its done including the time elapsed and the amount of data synced.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\n\n\n\n# Record the start time\nstart_timeTotal=$(date +%s%3N)\n\n# Set your Raspberry Pi&amp;#39;s IP address\nRASPBERRY_PI_IP=&amp;quot;192.168.8.107&amp;quot;\n\n\n# Set your Mosquitto container name or ID\nCONTAINER_NAME=&amp;quot;mosquitto&amp;quot;\n\n\n\n# Set MQTT details\nHOST=&amp;quot;localhost&amp;quot;  # Use localhost because we&amp;#39;re inside the container\nPORT=1883         # Specify the Mosquitto broker port\nTOPIC=&amp;quot;Backup&amp;quot;    # Specify the topic you want to publish to\n\n\n# Execute mosquitto_pub inside the container\n#docker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Starting&amp;quot;\n\n\n\n\nfor _ in {1..12}; do\n    if ping -c 1 &amp;quot;$RASPBERRY_PI_IP&amp;quot; &amp;amp;&amp;gt; /dev/null; then\n        echo &amp;quot;Raspberry Pi is up!&amp;quot;\n        break\n    else\n        sleep 5  # Wait 5 seconds before checking again\n    fi\ndone\n\n# If Raspberry Pi is still not up after 1 minute, exit\nif ! ping -c 1 &amp;quot;$RASPBERRY_PI_IP&amp;quot; &amp;amp;&amp;gt; /dev/null; then\n    docker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Raspberry Pi did not start up within 1 minute. Exiting.&amp;quot;\n    exit 1\nfi\n\n\n# Record the start time\nstart_timeFiles=$(date +%s%3N)\n\n# Start Rclone sync (adjust paths and remote as needed)\nrclone sync /mnt/user/files backupPI:storage/files -v\n\n# Record the end time of files\nend_timeFiles=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timeFiles - start_timeFiles))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Rclone sync files completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Sync Files done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n\n# Record the start time pictures\nstart_timePictures=$(date +%s%3N)\n\n\n# Start Rclone sync (adjust paths and remote as needed)\nrclone sync /mnt/user/pictures backupPI:storage/pictures -v\n\n\n\n# Record the end time\nend_timePictures=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timePictures - start_timePictures))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Rclone sync Pictures completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Sync Pictures done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n\n# Record the end time\nend_timeTotal=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timeTotal - start_timeTotal))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Complete sync completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Complete sync completed and was done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I hope this is useful to some of you. I will amend and edit this post with your feedback and keep it alive as long as I can. Have a great weekend!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?auto=webp&amp;s=1b8a86e76338b6c56926fad15933014e95c3842f", "width": 1500, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89bdfc939cafb8391d5e61a9567b4d4824284fcc", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86e77b7a1e8adf26f55b29177d43a792eaf05079", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=79eb0d4e1427b28536b1113953431a110410e80f", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=815fb17aab37a32dcd7821a0b443af7e57d6e5e9", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=240c63ed78cf374d220221b681b49faf1cf9a1e2", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2df389b2db307451f66469500dbffb7465972360", "width": 1080, "height": 1440}], "variants": {}, "id": "bMThnKntPA0wspAl37RZB5eagDAmk4PAnyYetytPxlU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j130b", "is_robot_indexable": true, "report_reasons": null, "author": "ElementII5", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j130b/tnasr1_tiny_nas_with_raid1_the_cheapest_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j130b/tnasr1_tiny_nas_with_raid1_the_cheapest_and/", "subreddit_subscribers": 718264, "created_utc": 1702649808.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ran across a pretty good deal on one at 225 renewed. \nWas going to get the X20 20TB until I saw this around the same price. \nBelieve it has double the cache of the X20 model. \n\nReviews are scarce on this though. \n\nIs the X22 just a newer model than the X20?? Assume it\u2019s a little faster too?\n\nAny info before I pull the trigger would be great!", "author_fullname": "t2_8msflw3m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos X22 20TB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iuhgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702624608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ran across a pretty good deal on one at 225 renewed. \nWas going to get the X20 20TB until I saw this around the same price. \nBelieve it has double the cache of the X20 model. &lt;/p&gt;\n\n&lt;p&gt;Reviews are scarce on this though. &lt;/p&gt;\n\n&lt;p&gt;Is the X22 just a newer model than the X20?? Assume it\u2019s a little faster too?&lt;/p&gt;\n\n&lt;p&gt;Any info before I pull the trigger would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iuhgm", "is_robot_indexable": true, "report_reasons": null, "author": "hellcatpekes", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iuhgm/seagate_exos_x22_20tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iuhgm/seagate_exos_x22_20tb/", "subreddit_subscribers": 718264, "created_utc": 1702624608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My company currently has a rather large (maybe not too large by some of your standards) dataset of images of handwritten documents totalling just over 10TB, with more to come in future. These images are very high resolution, but we don't want to reduce their size as we want to retain all of the information. Due to the nature of the images there is a large reduction in file size when converting from BMP to PNG, and so my question is whether there are any drawbacks to using PNG over BMP in this scenario, or more generally why would one want to use BMP over PNG for anything? ", "author_fullname": "t2_65dpfzvg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reason to use BMP over PNG?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j3gmq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702656294.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company currently has a rather large (maybe not too large by some of your standards) dataset of images of handwritten documents totalling just over 10TB, with more to come in future. These images are very high resolution, but we don&amp;#39;t want to reduce their size as we want to retain all of the information. Due to the nature of the images there is a large reduction in file size when converting from BMP to PNG, and so my question is whether there are any drawbacks to using PNG over BMP in this scenario, or more generally why would one want to use BMP over PNG for anything? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j3gmq", "is_robot_indexable": true, "report_reasons": null, "author": "ClearlyCylindrical", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j3gmq/any_reason_to_use_bmp_over_png/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j3gmq/any_reason_to_use_bmp_over_png/", "subreddit_subscribers": 718264, "created_utc": 1702656294.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a bunch of old VHS videos I'm wanting to digitise. I purchased this used [VHS player](https://www.ebay.com.au/itm/266552319384) (an LG V181 as the price was good) and [this capture card](https://www.ebay.com.au/itm/175982263794). I understand the card is cheap and may not be the best quality (the same could be said for the VHS player). I thought I'd trial first and see how we go. \n\nI've made a rookie error, because even with the card I have no additional cord to connect the capture card to the VHS player. The VHS player also doesn't have a scart plug (I don't know if that's already ended my chances of this not working). \n\nI'm hoping to get some advice to rectify the situation. In order to connect the capture card to the VHS player correctly I'm looking at the cord option below. It might connect to the two, but don't know if that will technically do anything. \n\n [3 x RCA Piggyback Plugs to 3 RCA Plugs - 1.5m | Jaycar Electronics](https://www.jaycar.com.au/3-x-rca-piggyback-plugs-to-3-rca-plugs-1-5m/p/WV7324) \n\nIf a cable to connect to the two is all I need and the above one is wrong, if there's any other cables [in their store](https://www.jaycar.com.au/cables-connectors/audio-video-cables/rca-av-cables/c/1AD?q=%3Apopularity-desc%3AsshomeAVLEAD-avleadTYPESIG%3AComposite%2BVideo%2Band%2BAudio&amp;text=#) that would work, please let me know. If this is the wrong approach and I need another piece of equipment, please also let me know.\n\nAny help would be greatly appreciated. \n\n&amp;#x200B;", "author_fullname": "t2_ezv1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Purchased used VCR and VHS converter card - Need advice on next steps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iwyx0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702635496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of old VHS videos I&amp;#39;m wanting to digitise. I purchased this used &lt;a href=\"https://www.ebay.com.au/itm/266552319384\"&gt;VHS player&lt;/a&gt; (an LG V181 as the price was good) and &lt;a href=\"https://www.ebay.com.au/itm/175982263794\"&gt;this capture card&lt;/a&gt;. I understand the card is cheap and may not be the best quality (the same could be said for the VHS player). I thought I&amp;#39;d trial first and see how we go. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made a rookie error, because even with the card I have no additional cord to connect the capture card to the VHS player. The VHS player also doesn&amp;#39;t have a scart plug (I don&amp;#39;t know if that&amp;#39;s already ended my chances of this not working). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping to get some advice to rectify the situation. In order to connect the capture card to the VHS player correctly I&amp;#39;m looking at the cord option below. It might connect to the two, but don&amp;#39;t know if that will technically do anything. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.jaycar.com.au/3-x-rca-piggyback-plugs-to-3-rca-plugs-1-5m/p/WV7324\"&gt;3 x RCA Piggyback Plugs to 3 RCA Plugs - 1.5m | Jaycar Electronics&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If a cable to connect to the two is all I need and the above one is wrong, if there&amp;#39;s any other cables &lt;a href=\"https://www.jaycar.com.au/cables-connectors/audio-video-cables/rca-av-cables/c/1AD?q=%3Apopularity-desc%3AsshomeAVLEAD-avleadTYPESIG%3AComposite%2BVideo%2Band%2BAudio&amp;amp;text=#\"&gt;in their store&lt;/a&gt; that would work, please let me know. If this is the wrong approach and I need another piece of equipment, please also let me know.&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dK-vPSUxnCDa9AQ3CJrlLRyGaKeHHI_zrKRuGo6GBIA.jpg?auto=webp&amp;s=5057ef2f3a93cc0fa1374b2daff196167248ae9e", "width": 160, "height": 159}, "resolutions": [{"url": "https://external-preview.redd.it/dK-vPSUxnCDa9AQ3CJrlLRyGaKeHHI_zrKRuGo6GBIA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e032bf4c46820210d06c55050551f54ecf7dfb7a", "width": 108, "height": 107}], "variants": {}, "id": "sTudcUIITL1ED_TEpskZf2L45uLN0C0Oir49ptIzTrE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iwyx0", "is_robot_indexable": true, "report_reasons": null, "author": "jtoml3", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iwyx0/purchased_used_vcr_and_vhs_converter_card_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iwyx0/purchased_used_vcr_and_vhs_converter_card_need/", "subreddit_subscribers": 718264, "created_utc": 1702635496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking at buying some extra refurbished drives, actually a specific model of Exos drives since they have performed so well for me (and for Backblaze apparently).  I found one person on eBay selling them and it says that it's a ST16000NM001G but actually when you get one it appears to be some off-brand version with \"OS\" written on it.  The smartctl reports the model number as \"OOS16000G\" and the stats look okay, I am running a battery of tests on it now.\n\nAnyone have any idea what this \"OS\" brand of Seagate drives is about?  I tried searching but all the posts that come back are about which drive to install your operating system...", "author_fullname": "t2_5iywr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"OS\" brand drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18isuql", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702618341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at buying some extra refurbished drives, actually a specific model of Exos drives since they have performed so well for me (and for Backblaze apparently).  I found one person on eBay selling them and it says that it&amp;#39;s a ST16000NM001G but actually when you get one it appears to be some off-brand version with &amp;quot;OS&amp;quot; written on it.  The smartctl reports the model number as &amp;quot;OOS16000G&amp;quot; and the stats look okay, I am running a battery of tests on it now.&lt;/p&gt;\n\n&lt;p&gt;Anyone have any idea what this &amp;quot;OS&amp;quot; brand of Seagate drives is about?  I tried searching but all the posts that come back are about which drive to install your operating system...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18isuql", "is_robot_indexable": true, "report_reasons": null, "author": "butters1337", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18isuql/os_brand_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18isuql/os_brand_drives/", "subreddit_subscribers": 718264, "created_utc": 1702618341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently got about 12 old dvrs each with a 500 GB segate pipeline drive. Looks to be about a decade old and 5900 RPM. Any advice on what to do with them? I don't think they were that heavily used and some of the dvrs looked brand new", "author_fullname": "t2_e1bhrebh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with 6 TBs of old HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iradw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702612907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got about 12 old dvrs each with a 500 GB segate pipeline drive. Looks to be about a decade old and 5900 RPM. Any advice on what to do with them? I don&amp;#39;t think they were that heavily used and some of the dvrs looked brand new&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iradw", "is_robot_indexable": true, "report_reasons": null, "author": "baseballandpcs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iradw/what_to_do_with_6_tbs_of_old_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iradw/what_to_do_with_6_tbs_of_old_hdd/", "subreddit_subscribers": 718264, "created_utc": 1702612907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download some shows off [archive.org](https://archive.org). Is there a way I can do this quickly? I want to download an entire subject, but I'm having issues. I tried using the IA CLI, but after a while it started to download videos that weren't in the subject. Any ideas?", "author_fullname": "t2_i1tjr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch Dowwnload Archive.org Subject?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iqncx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702610832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download some shows off &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;. Is there a way I can do this quickly? I want to download an entire subject, but I&amp;#39;m having issues. I tried using the IA CLI, but after a while it started to download videos that weren&amp;#39;t in the subject. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iqncx", "is_robot_indexable": true, "report_reasons": null, "author": "_ENunn_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iqncx/batch_dowwnload_archiveorg_subject/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iqncx/batch_dowwnload_archiveorg_subject/", "subreddit_subscribers": 718264, "created_utc": 1702610832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm getting  into data hoarding and have programming skills(python, JavaScript) that I've used to work on small freelance projects. Just through fiverr really.\n\nNow I'm wondering if there is any way I can use these more storage based computer skills to make money freelance.\n\nI'm specifying freelance because I already have a full time job in a different industry (sales) but I'd love to do more freelance on the side and see where it goes.", "author_fullname": "t2_istwnl540", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Can I Work Freelance Using These Skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iezjg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702578048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting  into data hoarding and have programming skills(python, JavaScript) that I&amp;#39;ve used to work on small freelance projects. Just through fiverr really.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m wondering if there is any way I can use these more storage based computer skills to make money freelance.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m specifying freelance because I already have a full time job in a different industry (sales) but I&amp;#39;d love to do more freelance on the side and see where it goes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iezjg", "is_robot_indexable": true, "report_reasons": null, "author": "Worldly_Cook_5449", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iezjg/how_can_i_work_freelance_using_these_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iezjg/how_can_i_work_freelance_using_these_skills/", "subreddit_subscribers": 718264, "created_utc": 1702578048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi :D  \nI found a good offer (I think) on leboncoin (a french website).  \n6x 16Tb HDD Seagate Exos x16 for 1175 Euros (12.5E/Tb).  \nThis is the description:  \n\"Corporate hard disks. Little used about 5000h only, very little booting. Never refursbished/recercified/renewed. So you have the actual disk usage in the diskinfo statistics.  \nThey're all \"OK\" on diskinfo so their SMART are good.  \nThey don't have the Seagate warrant as they're corporate disks\"  \n\n\nShould I go for it?  \nThanks in advance.", "author_fullname": "t2_4f1rf3oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I buy this disks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18j57v9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702660952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi :D&lt;br/&gt;\nI found a good offer (I think) on leboncoin (a french website).&lt;br/&gt;\n6x 16Tb HDD Seagate Exos x16 for 1175 Euros (12.5E/Tb).&lt;br/&gt;\nThis is the description:&lt;br/&gt;\n&amp;quot;Corporate hard disks. Little used about 5000h only, very little booting. Never refursbished/recercified/renewed. So you have the actual disk usage in the diskinfo statistics.&lt;br/&gt;\nThey&amp;#39;re all &amp;quot;OK&amp;quot; on diskinfo so their SMART are good.&lt;br/&gt;\nThey don&amp;#39;t have the Seagate warrant as they&amp;#39;re corporate disks&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;Should I go for it?&lt;br/&gt;\nThanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j57v9", "is_robot_indexable": true, "report_reasons": null, "author": "momokinou", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j57v9/should_i_buy_this_disks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j57v9/should_i_buy_this_disks/", "subreddit_subscribers": 718264, "created_utc": 1702660952.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_g3wnhn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Worth a try for $80?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"xp58hy41sh6c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/xp58hy41sh6c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e80532193ddc59b855bb1567491f789217b08188"}, {"y": 167, "x": 216, "u": "https://preview.redd.it/xp58hy41sh6c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44baa1bb5b95d519be8cf75c915ae35027a80deb"}, {"y": 247, "x": 320, "u": "https://preview.redd.it/xp58hy41sh6c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=78136b97af4c063793d0cab1e4cd5a96a8043991"}, {"y": 495, "x": 640, "u": "https://preview.redd.it/xp58hy41sh6c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=482972ba577c8b8e88d1bd7bda2c1c0df3f44b07"}, {"y": 743, "x": 960, "u": "https://preview.redd.it/xp58hy41sh6c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b7833fdc02b2e7342de697bc29080685856f05d"}, {"y": 836, "x": 1080, "u": "https://preview.redd.it/xp58hy41sh6c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=121bdca78aaa28eb4c25dd99265c09ee8b587d18"}], "s": {"y": 995, "x": 1284, "u": "https://preview.redd.it/xp58hy41sh6c1.jpg?width=1284&amp;format=pjpg&amp;auto=webp&amp;s=84e9fe7612775cc577ca213d2a9d699824741201"}, "id": "xp58hy41sh6c1"}, "kyc40z41sh6c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 143, "x": 108, "u": "https://preview.redd.it/kyc40z41sh6c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2aff2b738df401bb2e333cc9e1bf725c5e23d434"}, {"y": 287, "x": 216, "u": "https://preview.redd.it/kyc40z41sh6c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=81d73a78aa61a079ec88fdd4e2f5e10dd0971eb9"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/kyc40z41sh6c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6215b8b8c4b0b1de79c48a45cec7eb5ddcbbf28f"}, {"y": 852, "x": 640, "u": "https://preview.redd.it/kyc40z41sh6c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17184e7be4647dbfcfe36506f5f00064082f3631"}, {"y": 1278, "x": 960, "u": "https://preview.redd.it/kyc40z41sh6c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=adf2f588cb9889fe9909499ee1c8ce40c3b6bc6c"}, {"y": 1438, "x": 1080, "u": "https://preview.redd.it/kyc40z41sh6c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f013b236b74e69f8210226603db9c614fdacf99"}], "s": {"y": 1710, "x": 1284, "u": "https://preview.redd.it/kyc40z41sh6c1.jpg?width=1284&amp;format=pjpg&amp;auto=webp&amp;s=6da0ac810c1e3069ad3450b70414fe687f361948"}, "id": "kyc40z41sh6c1"}, "horoey41sh6c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 176, "x": 108, "u": "https://preview.redd.it/horoey41sh6c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e7fafcac1b5f2e1411cbc046e2bf37f828c31997"}, {"y": 352, "x": 216, "u": "https://preview.redd.it/horoey41sh6c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c986c0417f028ace536664255102589746957b8"}, {"y": 522, "x": 320, "u": "https://preview.redd.it/horoey41sh6c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a057469138e33b71653e7bb1e2846f7b0c4d1d9b"}, {"y": 1045, "x": 640, "u": "https://preview.redd.it/horoey41sh6c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aeadde073bfc384eb2fc5cd38aeb56588452c557"}, {"y": 1568, "x": 960, "u": "https://preview.redd.it/horoey41sh6c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=01b2f5bbbb349f5c6cee2ea4c4c6aa9cff2b646d"}, {"y": 1764, "x": 1080, "u": "https://preview.redd.it/horoey41sh6c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=685095771c01e5e276acd9a8f48371e6b11c4194"}], "s": {"y": 2098, "x": 1284, "u": "https://preview.redd.it/horoey41sh6c1.jpg?width=1284&amp;format=pjpg&amp;auto=webp&amp;s=ce3918fbbe0ca04fecc36079cb0ffd504f17245a"}, "id": "horoey41sh6c1"}}, "name": "t3_18j52kt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "horoey41sh6c1", "id": 374314303}, {"media_id": "kyc40z41sh6c1", "id": 374314304}, {"media_id": "xp58hy41sh6c1", "id": 374314305}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/8xqj3NBrHdFMYzxcK8n1NjGDzCvDdeiqg12HnjyySk0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702660565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/18j52kt", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j52kt", "is_robot_indexable": true, "report_reasons": null, "author": "jfarm47", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j52kt/worth_a_try_for_80/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/18j52kt", "subreddit_subscribers": 718264, "created_utc": 1702660565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings fellow hoarders.  \n\nSeveral years ago I had a drive with about 3-4k movies on it.  At some point I screwed up and \"broke\" the file system or wiped the partition table.  Can't remember exactly what happened but I was able to use photorec to carve out all the files and they're all on a new drive with generic file names f12345678 and so on.  The files are all playable but I can't tell what is what.  They have just been sitting on that drive since I recovered everything, waiting for me to make time to deal with the mess of identifying the files and renaming them.  Occasionally I'll open up a few movies, check for a title credit or recognizable scene and manually rename them but it's a painful process.  \n\nThis may be a long shot but I'm looking for a tool that I can point to the directory of files that will \"view\" each one and identify the name.  I don't really care how long it takes.  I have hardware available to dedicate to this job if needed and can easily spin up a VM to work on this in the background. \n\nDon't worry, I've since gotten smarter about backups, setting up offsite copies, etc.  This is essentially the last big effort to get things fully recovered.   Thanks in advance!", "author_fullname": "t2_58o36", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for determining movie title with broken file names.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j36h8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702655547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings fellow hoarders.  &lt;/p&gt;\n\n&lt;p&gt;Several years ago I had a drive with about 3-4k movies on it.  At some point I screwed up and &amp;quot;broke&amp;quot; the file system or wiped the partition table.  Can&amp;#39;t remember exactly what happened but I was able to use photorec to carve out all the files and they&amp;#39;re all on a new drive with generic file names f12345678 and so on.  The files are all playable but I can&amp;#39;t tell what is what.  They have just been sitting on that drive since I recovered everything, waiting for me to make time to deal with the mess of identifying the files and renaming them.  Occasionally I&amp;#39;ll open up a few movies, check for a title credit or recognizable scene and manually rename them but it&amp;#39;s a painful process.  &lt;/p&gt;\n\n&lt;p&gt;This may be a long shot but I&amp;#39;m looking for a tool that I can point to the directory of files that will &amp;quot;view&amp;quot; each one and identify the name.  I don&amp;#39;t really care how long it takes.  I have hardware available to dedicate to this job if needed and can easily spin up a VM to work on this in the background. &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t worry, I&amp;#39;ve since gotten smarter about backups, setting up offsite copies, etc.  This is essentially the last big effort to get things fully recovered.   Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j36h8", "is_robot_indexable": true, "report_reasons": null, "author": "lobstahcookah", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j36h8/software_for_determining_movie_title_with_broken/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j36h8/software_for_determining_movie_title_with_broken/", "subreddit_subscribers": 718264, "created_utc": 1702655547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I wanna build a pool made from 6 drives. 2 drives were bought refurbished so I consider them less safe. But they could be perfect for holding duplicated files. Can I \"mark\" these drives so they would be the only drives to hold duplicated files ?   \nI read some article where it talked about how you could build nested pools for exactly this purpose, but it was not very clear for me. ", "author_fullname": "t2_4uf8g8z9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DrivePool, can I designate only certain drives to be used for duplicated files ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j2ln3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702654012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna build a pool made from 6 drives. 2 drives were bought refurbished so I consider them less safe. But they could be perfect for holding duplicated files. Can I &amp;quot;mark&amp;quot; these drives so they would be the only drives to hold duplicated files ?&lt;br/&gt;\nI read some article where it talked about how you could build nested pools for exactly this purpose, but it was not very clear for me. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j2ln3", "is_robot_indexable": true, "report_reasons": null, "author": "sebastian___111", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j2ln3/drivepool_can_i_designate_only_certain_drives_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j2ln3/drivepool_can_i_designate_only_certain_drives_to/", "subreddit_subscribers": 718264, "created_utc": 1702654012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many videos collections and i have been looking since a while for a way on windows to have an animated preview thumbnail of parts of the video when you are hovering the mouse on it, does anyone knows if it is somehow possible? I haven't found anything good, i tried one paid software recommended on reddit but i forgot the name and it was bad anyways, it did not had that specific feature", "author_fullname": "t2_eneyhc4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Animated thumbnails for video collections?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j0v0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702649150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many videos collections and i have been looking since a while for a way on windows to have an animated preview thumbnail of parts of the video when you are hovering the mouse on it, does anyone knows if it is somehow possible? I haven&amp;#39;t found anything good, i tried one paid software recommended on reddit but i forgot the name and it was bad anyways, it did not had that specific feature&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j0v0g", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Gate6899", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j0v0g/animated_thumbnails_for_video_collections/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j0v0g/animated_thumbnails_for_video_collections/", "subreddit_subscribers": 718264, "created_utc": 1702649150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there anything I can set up and leave running on one of my servers that will automatically crawl and rip a specific site or pages, and check for new versions?\n\nI'm quite new to this stuff, so I wouldn't even know where to look.", "author_fullname": "t2_dljiq4aip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Continuous automated site ripping?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iytnl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702642744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there anything I can set up and leave running on one of my servers that will automatically crawl and rip a specific site or pages, and check for new versions?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite new to this stuff, so I wouldn&amp;#39;t even know where to look.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iytnl", "is_robot_indexable": true, "report_reasons": null, "author": "Constant_Shine_2367", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iytnl/continuous_automated_site_ripping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iytnl/continuous_automated_site_ripping/", "subreddit_subscribers": 718264, "created_utc": 1702642744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\n[what are all of these? i cleared them out and they came back. most have a folder called en\\_us in them with nothing inside](https://preview.redd.it/q33ujsn3ef6c1.png?width=539&amp;format=png&amp;auto=webp&amp;s=7e6c1ce7a24382e757c376cff932fb181eca1dd8)\n\nhttps://preview.redd.it/2b7wdq8aef6c1.png?width=575&amp;format=png&amp;auto=webp&amp;s=fbb60fcc7154794f14681e326961db45a8e7d4df\n\n[wtf is neat office?](https://preview.redd.it/s9bbk38kef6c1.png?width=774&amp;format=png&amp;auto=webp&amp;s=b2ed352678f663ff20a00d81040468e7ef64800e)", "author_fullname": "t2_3dj4aknb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Random files on SSD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2b7wdq8aef6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/2b7wdq8aef6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f682c18e2bed31c751768b4846ea782455b60aee"}, {"y": 131, "x": 216, "u": "https://preview.redd.it/2b7wdq8aef6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=97651e0c9af8474315758c5e107fdfdc64433877"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/2b7wdq8aef6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=96390e9fb47f785a8d895755ce8c5ca78aabde2e"}], "s": {"y": 351, "x": 575, "u": "https://preview.redd.it/2b7wdq8aef6c1.png?width=575&amp;format=png&amp;auto=webp&amp;s=fbb60fcc7154794f14681e326961db45a8e7d4df"}, "id": "2b7wdq8aef6c1"}, "q33ujsn3ef6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 78, "x": 108, "u": "https://preview.redd.it/q33ujsn3ef6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=167ab006d6b7719957eeb6c3ff530a18acac0a01"}, {"y": 157, "x": 216, "u": "https://preview.redd.it/q33ujsn3ef6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=990a31724ea6ca92316e63339ff8814e0c72c058"}, {"y": 233, "x": 320, "u": "https://preview.redd.it/q33ujsn3ef6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03a39cc1635dcb2b53423f3d1919a11326ef9b06"}], "s": {"y": 393, "x": 539, "u": "https://preview.redd.it/q33ujsn3ef6c1.png?width=539&amp;format=png&amp;auto=webp&amp;s=7e6c1ce7a24382e757c376cff932fb181eca1dd8"}, "id": "q33ujsn3ef6c1"}, "s9bbk38kef6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=de0e9e3a7d7591ef05e3fb851c697d7c6df01b39"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df31793f24f06cb88c5fd5d369079fb2a700c732"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=677d86ddc25e6cd2fc7b4134f25b82572db2526a"}, {"y": 308, "x": 640, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6c06d1f9852b840049e36449fbb8843cd247757"}], "s": {"y": 373, "x": 774, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=774&amp;format=png&amp;auto=webp&amp;s=b2ed352678f663ff20a00d81040468e7ef64800e"}, "id": "s9bbk38kef6c1"}}, "name": "t3_18iw4er", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Hmk9POVoVh0nyxCxqZ4KD2_9NNayVJOpfTwG-MsRakI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702631731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q33ujsn3ef6c1.png?width=539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e6c1ce7a24382e757c376cff932fb181eca1dd8\"&gt;what are all of these? i cleared them out and they came back. most have a folder called en_us in them with nothing inside&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2b7wdq8aef6c1.png?width=575&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbb60fcc7154794f14681e326961db45a8e7d4df\"&gt;https://preview.redd.it/2b7wdq8aef6c1.png?width=575&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbb60fcc7154794f14681e326961db45a8e7d4df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s9bbk38kef6c1.png?width=774&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2ed352678f663ff20a00d81040468e7ef64800e\"&gt;wtf is neat office?&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iw4er", "is_robot_indexable": true, "report_reasons": null, "author": "hazardtm", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iw4er/random_files_on_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iw4er/random_files_on_ssd/", "subreddit_subscribers": 718264, "created_utc": 1702631731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm new to this data hoarding scene and now that I've started my archive, I've quickly realized that 6 TB is so little space it's blowing my mind. Coming from the ages of 40gb hdd's where the biggest they got, 6TB was like infinite to me, till now. \n\n&amp;#x200B;\n\nMy question for advice is where do I search for the best deals on HDD's as they come up? I have the [shucks.top](https://shucks.top) open constantly but I'm wondering if there's some other website that does what this angel does? \n\n&amp;#x200B;\n\nOr where to start on drive knowledge anyways, what I'm storing PROBABLY won't move from drive to drive. It'll stay on the HDD and then maybe copied to other drives occasionally but for the most part it'll end up sitting in the drives in a protective case. \n\n&amp;#x200B;\n\nAny help would be GREATLY appreciated. I'm not concerned about speed and what not really, I like going slow and taking my time when transferring data and what not so I'm not concerned about some NAS set up or anything unless there's some MASSIVE benefit to investing in it vs just putting in a new 16TGB HDD everytime one fills up. \n\n&amp;#x200B;\n\nIf I sound like a noob, it's cause I am when it comes to this. Please don't rip me to bits. haha", "author_fullname": "t2_bg3zcbnm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to archiving and need some advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ig11u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702580837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this data hoarding scene and now that I&amp;#39;ve started my archive, I&amp;#39;ve quickly realized that 6 TB is so little space it&amp;#39;s blowing my mind. Coming from the ages of 40gb hdd&amp;#39;s where the biggest they got, 6TB was like infinite to me, till now. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question for advice is where do I search for the best deals on HDD&amp;#39;s as they come up? I have the &lt;a href=\"https://shucks.top\"&gt;shucks.top&lt;/a&gt; open constantly but I&amp;#39;m wondering if there&amp;#39;s some other website that does what this angel does? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Or where to start on drive knowledge anyways, what I&amp;#39;m storing PROBABLY won&amp;#39;t move from drive to drive. It&amp;#39;ll stay on the HDD and then maybe copied to other drives occasionally but for the most part it&amp;#39;ll end up sitting in the drives in a protective case. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any help would be GREATLY appreciated. I&amp;#39;m not concerned about speed and what not really, I like going slow and taking my time when transferring data and what not so I&amp;#39;m not concerned about some NAS set up or anything unless there&amp;#39;s some MASSIVE benefit to investing in it vs just putting in a new 16TGB HDD everytime one fills up. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I sound like a noob, it&amp;#39;s cause I am when it comes to this. Please don&amp;#39;t rip me to bits. haha&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zRLFtn-6oVv6rEQ7IOONhFbHxXDHwJjmbwnN7D_thZk.jpg?auto=webp&amp;s=67dae02de5f2f5bbdab22337b1d614ff6d511092", "width": 512, "height": 256}, "resolutions": [{"url": "https://external-preview.redd.it/zRLFtn-6oVv6rEQ7IOONhFbHxXDHwJjmbwnN7D_thZk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf43cb28471bc4327923afe88dfcf6aa750f51b1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/zRLFtn-6oVv6rEQ7IOONhFbHxXDHwJjmbwnN7D_thZk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a0aaef1fdd5bff6b7b06aae95a598c72f5e99fb", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/zRLFtn-6oVv6rEQ7IOONhFbHxXDHwJjmbwnN7D_thZk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d43e8f9a57710c82ade7bb10618ffb01a3b7da57", "width": 320, "height": 160}], "variants": {}, "id": "btzbLKPxXvroz0VXZS1fg-sPzgV5mm-5c3WebLrqOBw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ig11u", "is_robot_indexable": true, "report_reasons": null, "author": "ImHereForGameboys", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ig11u/new_to_archiving_and_need_some_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ig11u/new_to_archiving_and_need_some_advice/", "subreddit_subscribers": 718264, "created_utc": 1702580837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if this the right forum, if not, point me towards it please\n\ntl:dr - When trying to download a file from a certain website, the file size shows as 1.9gig at the start but file would finish downloading some lesser size in that making it corrupt.\n\nHello guys, I have been having trouble downloading a large course from a certain website. The course is divided into 17 archive files. Whenever i try to download them from browser, a certain file would give me a size at start but then it would finish downloading some random size shorter than that. I have been able to download some files successfully from that site repeatedly, but sometimes other files would show size let's say \"downloading 1.9gig\" and finish around 700meg. I am not sure what's the issue here.\n\nI cannot even resume the \"actually incomplete\" downloads sinces the software, be it browser or download manager, never shows an error but says that the download is complete which infact is much lesser size than it showed when download started.\n\nI need to have all 17 parts in order to have them successfully unzipped. I tried downloading them multiple times but some file or another would be incomplete.\n\nI tried switching browser, Chrono download manager and even download manager programs all seem to have the same issue.\n\nI am guessing that the site may have some sort of rate limiting on downloads or is it some sort of technical glitch from server?\n\nThe site also has a TG channel where it states the download limit is 3 Mbps per client but I have been able to download on much higher rate than that so I guess there's else at play here.\n\nCould the rate limit be a \"per day\" thing or \"you can download a certain size in certain time'. I really could not figure it out what the issue is here.\n\nCan I use some utility to limit the download speed to make it really slow to ensure that I download the file in its entirety. Is there's a software which can help in ensuring this - like it ensure that the file downloaded is actually is of the size which was communicated by the server when download started?", "author_fullname": "t2_1g8c19m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trouble with downloading from a certain site", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iutv8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702626066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if this the right forum, if not, point me towards it please&lt;/p&gt;\n\n&lt;p&gt;tl:dr - When trying to download a file from a certain website, the file size shows as 1.9gig at the start but file would finish downloading some lesser size in that making it corrupt.&lt;/p&gt;\n\n&lt;p&gt;Hello guys, I have been having trouble downloading a large course from a certain website. The course is divided into 17 archive files. Whenever i try to download them from browser, a certain file would give me a size at start but then it would finish downloading some random size shorter than that. I have been able to download some files successfully from that site repeatedly, but sometimes other files would show size let&amp;#39;s say &amp;quot;downloading 1.9gig&amp;quot; and finish around 700meg. I am not sure what&amp;#39;s the issue here.&lt;/p&gt;\n\n&lt;p&gt;I cannot even resume the &amp;quot;actually incomplete&amp;quot; downloads sinces the software, be it browser or download manager, never shows an error but says that the download is complete which infact is much lesser size than it showed when download started.&lt;/p&gt;\n\n&lt;p&gt;I need to have all 17 parts in order to have them successfully unzipped. I tried downloading them multiple times but some file or another would be incomplete.&lt;/p&gt;\n\n&lt;p&gt;I tried switching browser, Chrono download manager and even download manager programs all seem to have the same issue.&lt;/p&gt;\n\n&lt;p&gt;I am guessing that the site may have some sort of rate limiting on downloads or is it some sort of technical glitch from server?&lt;/p&gt;\n\n&lt;p&gt;The site also has a TG channel where it states the download limit is 3 Mbps per client but I have been able to download on much higher rate than that so I guess there&amp;#39;s else at play here.&lt;/p&gt;\n\n&lt;p&gt;Could the rate limit be a &amp;quot;per day&amp;quot; thing or &amp;quot;you can download a certain size in certain time&amp;#39;. I really could not figure it out what the issue is here.&lt;/p&gt;\n\n&lt;p&gt;Can I use some utility to limit the download speed to make it really slow to ensure that I download the file in its entirety. Is there&amp;#39;s a software which can help in ensuring this - like it ensure that the file downloaded is actually is of the size which was communicated by the server when download started?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iutv8", "is_robot_indexable": true, "report_reasons": null, "author": "bawlachora", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iutv8/trouble_with_downloading_from_a_certain_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iutv8/trouble_with_downloading_from_a_certain_site/", "subreddit_subscribers": 718264, "created_utc": 1702626066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Although I have seen this post [https://www.reddit.com/r/DataHoarder/comments/12qjefj/how\\_can\\_i\\_download\\_videos\\_from\\_a\\_private\\_telegram/](https://www.reddit.com/r/DataHoarder/comments/12qjefj/how_can_i_download_videos_from_a_private_telegram/) , in a comment it uses tampermonkey, it is effective but when using it you have to download one by one, is there a way to download everything in \"batch\" from a private Telegram group?", "author_fullname": "t2_kovxb0k3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I download videos/photos from a private Telegram channel in batch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18is5ht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702615875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Although I have seen this post &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12qjefj/how_can_i_download_videos_from_a_private_telegram/\"&gt;https://www.reddit.com/r/DataHoarder/comments/12qjefj/how_can_i_download_videos_from_a_private_telegram/&lt;/a&gt; , in a comment it uses tampermonkey, it is effective but when using it you have to download one by one, is there a way to download everything in &amp;quot;batch&amp;quot; from a private Telegram group?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18is5ht", "is_robot_indexable": true, "report_reasons": null, "author": "Background_130", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18is5ht/how_can_i_download_videosphotos_from_a_private/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18is5ht/how_can_i_download_videosphotos_from_a_private/", "subreddit_subscribers": 718264, "created_utc": 1702615875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! \n\nI currently am trying to archive some mini dv tapes I have. I have an iMac G5 with a FireWire port and have been using iMovie at the moment but it is a bit finicky, and can sometimes randomly stop, but I do get raw DV files out of it from my camera. \n\nI also have a few other MacBooks (2016 MacBook Air, 2023 M3 MacBook Pro) if it would somehow be easier using one of those. \n\nWhatever is recommended please let me know! Thanks", "author_fullname": "t2_ky83pub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitizing Mini DV Tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ihrvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702585514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;I currently am trying to archive some mini dv tapes I have. I have an iMac G5 with a FireWire port and have been using iMovie at the moment but it is a bit finicky, and can sometimes randomly stop, but I do get raw DV files out of it from my camera. &lt;/p&gt;\n\n&lt;p&gt;I also have a few other MacBooks (2016 MacBook Air, 2023 M3 MacBook Pro) if it would somehow be easier using one of those. &lt;/p&gt;\n\n&lt;p&gt;Whatever is recommended please let me know! Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ihrvg", "is_robot_indexable": true, "report_reasons": null, "author": "TheOnlyWonGames", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ihrvg/digitizing_mini_dv_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ihrvg/digitizing_mini_dv_tapes/", "subreddit_subscribers": 718264, "created_utc": 1702585514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm having a bit of trouble locating a piece of hardware I figured would be fairly common and cheap.\n\nBasically, here's my dilemma. I have a TrueNAS box, small form factor, and wanted to throw a few more drives at it for an experiment. The problem is, the SATA power connectors aren't convenient to move with the existing array in there, and I'm out of SATA ports on the motherboard. I bought an LSI HBA (flashed to IT mode, recognized and ready for the OS) to add extra drives. But I am also missing the mounting hanger pieces for these drives, so I'm looking for alternative ways to mount them. The SATA/SAS cables are plenty long so I can drag them outside the case, but I still have a power issue. Most of the cheap solutions are just boxes with trays you can open and close with a row of SATA male connectors on the back along with a few SATA power or molex power connectors. I'd like to find something small and modest that can hold a few drives, power them, and pass through or allow the SATA data cables to connect to each drive (or to the box but not get \"in the way\" with some goofy RAID stuff). If it has a few cooling fans, that'd be nice too.  \n\n**TLDR;** I need a **powered** box, preferably with a fan, to stack a few extra hard drives in. It should be external, I'll provide the SATA/SAS connectors and the drives. Don't want any backplane or raid jazz getting in the way, this thing should be completely dumb. ", "author_fullname": "t2_n56ppn9d8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a cheap, special box to hold and power a few 3.5\" SATA drives.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iklud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702593047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m having a bit of trouble locating a piece of hardware I figured would be fairly common and cheap.&lt;/p&gt;\n\n&lt;p&gt;Basically, here&amp;#39;s my dilemma. I have a TrueNAS box, small form factor, and wanted to throw a few more drives at it for an experiment. The problem is, the SATA power connectors aren&amp;#39;t convenient to move with the existing array in there, and I&amp;#39;m out of SATA ports on the motherboard. I bought an LSI HBA (flashed to IT mode, recognized and ready for the OS) to add extra drives. But I am also missing the mounting hanger pieces for these drives, so I&amp;#39;m looking for alternative ways to mount them. The SATA/SAS cables are plenty long so I can drag them outside the case, but I still have a power issue. Most of the cheap solutions are just boxes with trays you can open and close with a row of SATA male connectors on the back along with a few SATA power or molex power connectors. I&amp;#39;d like to find something small and modest that can hold a few drives, power them, and pass through or allow the SATA data cables to connect to each drive (or to the box but not get &amp;quot;in the way&amp;quot; with some goofy RAID stuff). If it has a few cooling fans, that&amp;#39;d be nice too.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt; I need a &lt;strong&gt;powered&lt;/strong&gt; box, preferably with a fan, to stack a few extra hard drives in. It should be external, I&amp;#39;ll provide the SATA/SAS connectors and the drives. Don&amp;#39;t want any backplane or raid jazz getting in the way, this thing should be completely dumb. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iklud", "is_robot_indexable": true, "report_reasons": null, "author": "franksandbeans911", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iklud/looking_for_a_cheap_special_box_to_hold_and_power/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iklud/looking_for_a_cheap_special_box_to_hold_and_power/", "subreddit_subscribers": 718264, "created_utc": 1702593047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have 2 separate 2TB subscriptions for 2 different GMail/Google accounts that are 100% both mine. I would like to be able to chain/link them together but I don't know if this is actually possible or not. Does anyone know if this is even possible?\n\nIn essence what I am looking to do is set up a folder with sub folders on my main GD account which I will nickname \"GOT\" to the secondary GD account which I will nickname \"APD\" (nicknames are only for purposes of this post to keep things straight). I would like the ability to have specific folders and subfolders on my GOT seamlessly chain/link to the APD but I don't know if that is possible or how to set that up. I want to do this to maximize my storage amount.\n\nThank you..........", "author_fullname": "t2_4i81zeh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linking/Chaining 2 separate 2TB GD Subscriptions - is it possible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ih8w3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702584106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have 2 separate 2TB subscriptions for 2 different GMail/Google accounts that are 100% both mine. I would like to be able to chain/link them together but I don&amp;#39;t know if this is actually possible or not. Does anyone know if this is even possible?&lt;/p&gt;\n\n&lt;p&gt;In essence what I am looking to do is set up a folder with sub folders on my main GD account which I will nickname &amp;quot;GOT&amp;quot; to the secondary GD account which I will nickname &amp;quot;APD&amp;quot; (nicknames are only for purposes of this post to keep things straight). I would like the ability to have specific folders and subfolders on my GOT seamlessly chain/link to the APD but I don&amp;#39;t know if that is possible or how to set that up. I want to do this to maximize my storage amount.&lt;/p&gt;\n\n&lt;p&gt;Thank you..........&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ih8w3", "is_robot_indexable": true, "report_reasons": null, "author": "WndrWmn77", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ih8w3/linkingchaining_2_separate_2tb_gd_subscriptions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ih8w3/linkingchaining_2_separate_2tb_gd_subscriptions/", "subreddit_subscribers": 718264, "created_utc": 1702584106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I did something stupid and deleted a years worth of backups from my NAS. Every recovery tool I've found wants to use SSH to connect to perform the recovery but it doesn't appear that Thecus has native support for SSH.   I do see mention of a HiSSH module to provide this but I cannot find any working download links.  Would anyone have this module?   Thanks", "author_fullname": "t2_8c7fp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help recovering files from Thecus N3200PRO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18inlxq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702601387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did something stupid and deleted a years worth of backups from my NAS. Every recovery tool I&amp;#39;ve found wants to use SSH to connect to perform the recovery but it doesn&amp;#39;t appear that Thecus has native support for SSH.   I do see mention of a HiSSH module to provide this but I cannot find any working download links.  Would anyone have this module?   Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18inlxq", "is_robot_indexable": true, "report_reasons": null, "author": "cacchip", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18inlxq/help_recovering_files_from_thecus_n3200pro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18inlxq/help_recovering_files_from_thecus_n3200pro/", "subreddit_subscribers": 718264, "created_utc": 1702601387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello.\n\nI have a Seagate Expansion drive and as we all know the industry sorta kinda design these drives to fail after a given number of hours because they need to sell you another one. Mine simply deleted the NTFS partition with the files inside and that was it.\n\nI was wondering if there is any way/hack to force these back? Is there any video or other resource I could research in order to learn how to do this properly? It would be nice if I could recover the data in it but if lost permanently it wouldn't be the end of the world.\n\nThanks.", "author_fullname": "t2_v5xai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bringing External HD back to life", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iewzm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.22, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702577862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;I have a Seagate Expansion drive and as we all know the industry sorta kinda design these drives to fail after a given number of hours because they need to sell you another one. Mine simply deleted the NTFS partition with the files inside and that was it.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there is any way/hack to force these back? Is there any video or other resource I could research in order to learn how to do this properly? It would be nice if I could recover the data in it but if lost permanently it wouldn&amp;#39;t be the end of the world.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iewzm", "is_robot_indexable": true, "report_reasons": null, "author": "Yuri_TxM", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iewzm/bringing_external_hd_back_to_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iewzm/bringing_external_hd_back_to_life/", "subreddit_subscribers": 718264, "created_utc": 1702577862.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}