{"kind": "Listing", "data": {"after": "t3_18iutv8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure where else to post this. I purchased a few orders from WD's online store. Opened and used all the orders except one which had a 4TB SSD in it. I did not even open this package. Wanted to send it back as they claim they offer free returns and refunds. Price dropped on the SSDs, so I purchased more drives at the discount price, so I wanted to send this order back because customer service said they could not price adjust, but I could always return it. So I contact a WD rep on live chat, they give me a UPS label. \n\nI take their UPS label, apply it on the package it arrived in and sent it off. \n\nA few days after they received the package they claim there is no WD product in my return, and claim I sent back a Samsung drive. I have owned a lot of WD drives over the years, but one brand I never owned was a Samsung drive. \n\nTheir responses seem to say they will scrap the item, or they can send me back what I assume is another customer's Samsung drive. In either case, I will not get a refund, nor the item I paid for. \n\nThis is more frustrating than usual because that Samsung drive likely belongs to someone else and has their personal info on it. \n\nAre there any tactics to force WD to do a refund? Their Customer Service is giving me 1 more week to decide on no refund, or no refund and some other customer's mystery Samsung drive, which I would have to purchase a shipping label for. Any advice for people that dealt with WD customer service appreciated. \n\nOnly other thing I can say is don't buy from WD.com.", "author_fullname": "t2_10oeg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital denying return, claims I sent a Samsung drive back", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iozmo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702605616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure where else to post this. I purchased a few orders from WD&amp;#39;s online store. Opened and used all the orders except one which had a 4TB SSD in it. I did not even open this package. Wanted to send it back as they claim they offer free returns and refunds. Price dropped on the SSDs, so I purchased more drives at the discount price, so I wanted to send this order back because customer service said they could not price adjust, but I could always return it. So I contact a WD rep on live chat, they give me a UPS label. &lt;/p&gt;\n\n&lt;p&gt;I take their UPS label, apply it on the package it arrived in and sent it off. &lt;/p&gt;\n\n&lt;p&gt;A few days after they received the package they claim there is no WD product in my return, and claim I sent back a Samsung drive. I have owned a lot of WD drives over the years, but one brand I never owned was a Samsung drive. &lt;/p&gt;\n\n&lt;p&gt;Their responses seem to say they will scrap the item, or they can send me back what I assume is another customer&amp;#39;s Samsung drive. In either case, I will not get a refund, nor the item I paid for. &lt;/p&gt;\n\n&lt;p&gt;This is more frustrating than usual because that Samsung drive likely belongs to someone else and has their personal info on it. &lt;/p&gt;\n\n&lt;p&gt;Are there any tactics to force WD to do a refund? Their Customer Service is giving me 1 more week to decide on no refund, or no refund and some other customer&amp;#39;s mystery Samsung drive, which I would have to purchase a shipping label for. Any advice for people that dealt with WD customer service appreciated. &lt;/p&gt;\n\n&lt;p&gt;Only other thing I can say is don&amp;#39;t buy from WD.com.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18iozmo", "is_robot_indexable": true, "report_reasons": null, "author": "abcalt", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iozmo/western_digital_denying_return_claims_i_sent_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iozmo/western_digital_denying_return_claims_i_sent_a/", "subreddit_subscribers": 718251, "created_utc": 1702605616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.westerndigital.com/products/internal-drives/wd-red-pro-sata-hdd?sku=WD2002FFSX\n\nNot a screaming deal but a good one if you\u2019re looking for max capacity drives.", "author_fullname": "t2_39a2j1um", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Two 22TB WD Red\u2019s for $669", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18idj1z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702574273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-red-pro-sata-hdd?sku=WD2002FFSX\"&gt;https://www.westerndigital.com/products/internal-drives/wd-red-pro-sata-hdd?sku=WD2002FFSX&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Not a screaming deal but a good one if you\u2019re looking for max capacity drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CWTSWMZ5xNrQhAgoLQhTXP7M9-Ei_Wo5DI4CG-AAYOY.jpg?auto=webp&amp;s=0e32b92dd1b60d1547d38c45c7cb5c9a7f89d91d", "width": 1680, "height": 1680}, "resolutions": [{"url": "https://external-preview.redd.it/CWTSWMZ5xNrQhAgoLQhTXP7M9-Ei_Wo5DI4CG-AAYOY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bce420228f6b9830b62fe89c114ebe9cb735d83b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/CWTSWMZ5xNrQhAgoLQhTXP7M9-Ei_Wo5DI4CG-AAYOY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f68b9af2f4cf77496812d99ea86beb318156a938", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/CWTSWMZ5xNrQhAgoLQhTXP7M9-Ei_Wo5DI4CG-AAYOY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76cf1885eec435de40e5f7022425b9385d763f9b", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/CWTSWMZ5xNrQhAgoLQhTXP7M9-Ei_Wo5DI4CG-AAYOY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6794ee69628dcd6e0cc457e719e09d367ad5285", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/CWTSWMZ5xNrQhAgoLQhTXP7M9-Ei_Wo5DI4CG-AAYOY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=74a9e4b54971ed27914b742a1adeef436eaa703a", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/CWTSWMZ5xNrQhAgoLQhTXP7M9-Ei_Wo5DI4CG-AAYOY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=390dfbad882c7e7917a19c15b8c3e263e6ce427b", "width": 1080, "height": 1080}], "variants": {}, "id": "6keymE9zpOEX9OFD9sJvRPtVBVor6KjTRLVm8MbwRlA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "80TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "18idj1z", "is_robot_indexable": true, "report_reasons": null, "author": "30rdsIsStandardCap", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18idj1z/two_22tb_wd_reds_for_669/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18idj1z/two_22tb_wd_reds_for_669/", "subreddit_subscribers": 718251, "created_utc": 1702574273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ev065", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Come on Kingston... Do Better!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_18j43eo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c0NwfLMauQ7HGfy9jt-wWmeZ_oR2ULC4_qLvazjGWAA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702658017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8taijqobkh6c1.jpeg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?auto=webp&amp;s=c559e876d037d78e2827367a07269a4edef22942", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71f68c29717332a2a186d09ea132d80b400145a3", "width": 108, "height": 81}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ce2df53814e50b9650050ce6268eddb3f287b52", "width": 216, "height": 162}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9246c0e08e429aee0122accdfbe4fcedee6fa03e", "width": 320, "height": 240}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=56cefba73e05779a5e51f6dc104bf7e0da91817c", "width": 640, "height": 480}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e3464a282d0e36342aa9d5fb3a78d7e51d10c6b", "width": 960, "height": 720}, {"url": "https://preview.redd.it/8taijqobkh6c1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=969a77d6c9ca3371a70627016739b5f5596849f4", "width": 1080, "height": 810}], "variants": {}, "id": "_CJouJmRgGzuxL0A-uh26EL2IKXlNQyA6ymyZOk2Z8c"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j43eo", "is_robot_indexable": true, "report_reasons": null, "author": "zaca21", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j43eo/come_on_kingston_do_better/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8taijqobkh6c1.jpeg", "subreddit_subscribers": 718251, "created_utc": 1702658017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I give you the [TNASR1](https://imgur.com/a/fduKt1N) \"Tiny NAS with RAID1\" - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5\" SATA HDDs in  RAID1\n\n\n##Table of content\n\n1. Intro\n\n2. Why\n\n3. Goals\n\n4. BOM\n\n5. My Setup \n\n6. Cost\n\n7. Performance\n\n8. Power consumption\n\n9. Setup\n\n10. Scripting with scheduled wakeup to save power\n\n###Intro\n\nSo I have an Unraid Server wich is great and I am really happy with it. It stores all my files including media but also my very important documents as well as my maybe even more important pictures and videos of my family. So I always felt uneasy thinking about its safety. It's a consumer grade tower PC located in my garage. I use two parity drives so on a system level I have some redundancy. Nothing wrong with that but if something happens to my server as a whole (fire/flooding/kids) all my data is gone.\n\nOf course there is the possibility if using a cloud storage provider. Google drive is $12 per month or $144 per year. First of all I would be trusting a third party with the safety of my data and secondly I am cheap bastard. \n\nThis is the genesis of the TNASR1.\n\nIt lives in my garden shed in a watertight container and does everything a google drive would.\n\n\n###Why\n\n- Data Backup needs - common sense i.e. the [3 2 1 Rule](https://en.wikipedia.org/wiki/Glossary_of_backup_terms#Terms_and_definitions) demands a remote copy.\n\n- Control - cloud providers are a third party that require a level of trust and cost money above certain storage needs\n\n- Cost - cheaper in the long run\n\n\n###Goals\n\n- Cheap\n\n- Low Power\n\n- enough storage for the most important files. At least 500gb+\n\n- RAID1 for some fault resilience\n\n###BOM\n\n- Raspberry Pi Zero 2 W\n\n- a small heat sink\n\n- thermal paste\n\n- microSD card with at least 2gb\n\n- USB OTG Hub Host Cable\n\n- A 2A/**5.3V** - Power supply\n\n- USB A to Micro USB B cable\n\n- 2x 2.5inch SATA Case \n\n- 2x SATA drives\n\n###My Setup\n\n\n- Raspberry Pi Zero 2 W  - \u20ac19.80\n\n- The heat sink and thermal paste i had laying around. But I guess \u20ac1 is fair.\n\n- INTENSO 3413460 - MicroSDHC-Card with 8GB, Intenso Class 10 - \u20ac3.40\n\n- USB OTG Hub Host Cable. Ali express item:3256805033322631 \u20ac1.40\n\n- Samsung EP-TA10EWE Power supply.\n\n- 2x 2.5inch SATA Case. Ali express item: 3256805261700001  - \u20ac1.00\n\n- For storage I used  2x 2TB Toshiba L200 bulk HDWL120UZSVA - \u20ac65.00 each\n\n###Cost\n\nNAS cost before storage: \u20ac34.60 or around $37.20\n\nIn total ~ \u20ac164 or $178 for 2 Terabyte of remote RAID1 storage. Not bad if I say so myself. ROI vs. google Drive ($144 per year for 2 terabyte) in 15 months. \n\nCost for electricity (more down below) is about $/\u20ac20 a year if you let it run continuously with $/\u20ac0.3 per kWh. ROI vs Google Drive under 17 months.\n\n###Performance\n\nSo the two SATA drives in RAID1 are connected over a single USB 2.0 interface. Suffice to say you won't get SSD speeds. But I am Happy to report that it is quite fast enough for our needs. I get around 2MB/s write and 4MB/s of read. That means I can sync 1 gigabyte of data in under 9 Minutes. Now this is nothing to write home about but we have to consider context. This for a remote backup that uses less than 10w. Also after initial setup this is for syncing the diff only. I tend to sync less than 5 gigabyte of new data a week. That means backup takes under an hour.\n\nFor my initial sync of my files it took 8 hours for 48 Gigabytes with a transfer speed of about  1.8 MB/s\n\n    Transferred:   \t   51.312 GiB / 51.312 GiB, 100%, 1,852.5 KiB/s, ETA 0s\n    Checks:             71697 / 71697, 100%\n    Deleted:               11 (files), 0 (dirs)\n    Transferred:        70846 / 70846, 100%\n    Elapsed time:    8h4m34.3s\n\nMy initial sync of my pictures and videos of 435 Gigabytes took 2 days and 9 hours with an average transfer speed of 2.2 MB/s\n\n    Transferred:   \t  455.358 GiB / 455.358 GiB, 100%, 2.316 MiB/s, ETA 0s\n    Checks:            111442 / 111442, 100%\n    Deleted:               11 (files), 0 (dirs)\n    Transferred:       111478 / 111478, 100%\n    Elapsed time:  2d9h13m13.9s\n\nA sync run without anything to sync takes under 2 Minutes.\n\n    Rclone sync completed in 0 hours, 1 minutes, and 45 seconds 390 milliseconds.\n\n###Power Consumption\n\nThe star of the show is undoubtedly the 5.3V power supply. Any power supply with just 5V, even ones that can deliver 100W, failed the boot up during the high ramp up power spike from the HDDs. Te whole system is teetering on being power starved. But it was up and running and syncing my 500 gigabytes without any issues.\n\n[The whole setup draws continuous 5.3V 1.5A or 8W max during a sync.](https://imgur.com/PANKpo5) \n\nI am sure when the drives spin up the power draw spikes up above 1.5A but as the supply has more than 5V it does not cause any issues.\n\n[At Idle the power draw is about 0.6A or about 3W.](https://imgur.com/c1CnIv7)\n\nThere are some Samsung Power Supplies that have 5.3V. \n\n[Alternatively there are AC/DC adapters like these.](https://www.voc-electronics.com/a-42425080/power-batteries-adapters/5-3v-2a-power-supply-eu/#description)\n\n\n\n###Setup\n\n1. Setup the MicroSD with Raspberry Pi Imager. \n\n  1.1 Choose Raspberry Pi OS (other)\n\n  1.2 Chose Raspberry Pi OS lite (64-bit)\n\n  1.3 Set Up WiFi and turn on SSH\n\n  1.4 Burn Image\n\n2. Check if everything is running\n\n  2.1 Insert microSD \n\n  2.2 Connect Power Supply\n\n  2.3 Ping device\n\n  2.4 If successful try to SSH into the pi with : SSH user@nameOfPi or: SSH user@IPaddrOfPi\n\n  2.5 Power down and disconnect power supply\n\n3. [Setup the drives](https://www.computernetworkingnotes.com/linux-tutorials/how-to-configure-raid-in-linux-step-by-step-guide.html)\n\n  3.1 Connect drives via the USB hub dongle\n\n  3.2 Reconnect Power Supply\n\n  3.3 SSH back into the Pi\n\n  3.4 Type in:  \n\n        lsblk  \n\n  This should confirm two things. The drives are connected and have the right size. You should have an output like this:\n\n\n    \n        sda           8:16   0  1.8T  0 disk \n        sdb           8:16   0  1.8T  0 disk\n        mmcblk0     179:0    0  7.5G  0 disk \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n\n  3.5 Type in: \n\n        sudo fdisk /dev/sda\n\n  new drive\n\n        n\n\n  primary\n\n        p\n  \n  list\n\n        l\n\n  type\n\n        t\n\n  Linux raid auto\n\n        fd\n\n\n  write\n\n        w\n\n  3.6 repeat for sdb\n\n  3.7 [Partprobe](https://www.computerhope.com/unix/partprob.htm)\n\n        partprobe\n\n  3.8 Check if sda1 and sdb1 are listed\n\n        lsblk\n\n  output should look like this\n\n        NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n        sda           8:0    0  1.8T  0 disk \n        \u2514\u2500sda1        8:1    0  1.8T  0 part \n        sdb           8:16   0  1.8T  0 disk \n        \u2514\u2500sdb1        8:17   0  1.8T  0 part \n        mmcblk0     179:0    0  7.5G  0 disk \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n        \n  3.9 Check if the file system is correct\n\n        sudo fdisk -l\n\n  the output should include the type \"Linux raid auto\"\n\n        Device     Boot Start        End    Sectors  Size Id Type\n        /dev/sda1        2048 3907029167 3907027120  1.8T fd Linux raid autodetect\n \n\n4. [Setup raid](https://www.thetechedvocate.org/how-to-set-up-raid-1-on-the-raspberry-pi-the-easy-way/)\n\n  4.1 Install mdadm\n\n        sudo apt-get install mdadm\n\n  4.2 create raid array\n\n        sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1\n\n  Continue creating array?\n\n        y\n\n  4.3 check if md0 was created\n\n        lsblk\n\n  the output should look like this\n\n        NAME        MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT\n        sda           8:0    0  1.8T  0 disk  \n        \u2514\u2500sda1        8:1    0  1.8T  0 part  \n          \u2514\u2500md0       9:0    0  1.8T  0 raid1 \n        sdb           8:16   0  1.8T  0 disk  \n        \u2514\u2500sdb1        8:17   0  1.8T  0 part  \n          \u2514\u2500md0       9:0    0  1.8T  0 raid1 \n        mmcblk0     179:0    0  7.5G  0 disk  \n        \u251c\u2500mmcblk0p1 179:1    0  256M  0 part  /boot\n        \u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part  /\n\n5. Format the raid array / make filesystem\n\n  5.1 Type in\n\n        sudo mkfs.ext4 /dev/md0\n\n6. [Mount to folder](https://www.youtube.com/watch?v=Ff96FPJHq5o)\n\n  6.1 Make folder under mnt\n\n        sudo mkdir /mnt/storage\n\n  6.2 Mount md0 to storage\n\n        sudo mount /dev/md0 /mnt/storage\n\n\n  6.6. mount on startup\n\n        sudo nano /etc/fstab\n\n  6.7 add line\n\n        /dev/md0 /mnt/storage ext4 defaults 0 0\n\n  6.8 Save and exit\n\n  6.9 reboot\n\n        reboot\n\n\n  6.4 check if mount is successfull\n        \n        df -alh\n\n        \n  output should look like this\n\n        Filesystem      Size  Used Avail Use% Mounted on\n        /dev/root       7.1G  1.7G  5.2G  25% /\n        devtmpfs         80M     0   80M   0% /dev\n        proc               0     0     0    - /proc\n        sysfs              0     0     0    - /sys\n        securityfs         0     0     0    - /sys/kernel/security\n        tmpfs           210M     0  210M   0% /dev/shm\n        devpts             0     0     0    - /dev/pts\n        tmpfs            84M  3.0M   81M   4% /run\n        tmpfs           5.0M  4.0K  5.0M   1% /run/lock\n        cgroup2            0     0     0    - /sys/fs/cgroup\n        pstore             0     0     0    - /sys/fs/pstore\n        bpf                0     0     0    - /sys/fs/bpf\n        systemd-1          0     0     0    - /proc/sys/fs/binfmt_misc\n        mqueue             0     0     0    - /dev/mqueue\n        debugfs            0     0     0    - /sys/kernel/debug\n        sunrpc             0     0     0    - /run/rpc_pipefs\n        tracefs            0     0     0    - /sys/kernel/tracing\n        configfs           0     0     0    - /sys/kernel/config\n        fusectl            0     0     0    - /sys/fs/fuse/connections\n        /dev/mmcblk0p1  255M   31M  225M  13% /boot\n        tmpfs            42M     0   42M   0% /run/user/1000\n        /dev/md0        1.8T   28K  1.7T   1% /mnt/storage\n        \n7. enable SMB sharing\n\n  7.1 install samba\n\n        sudo apt-get samba\n\n  7.2 edit samba config\n\n        sudo nano /etc/samba/smb.conf\n\n  7.3 add this at the end\n\n        [storage]\n        path=/mnt/storage\n        writeable=yes\n        reate mask=0666\n        directorty mask=0666\n        public=yes\n\n  7.4 Save end exit\n\n  7.5 restart samba service\n\n        sudo systemctl restart smbd\n\n  7.6 ad a user to samba\n\n        sudo smbpasswd -a yourDesiredUsername\n\n  7.7 Set a user password\n\nThat is it you are done. See if you can find your folder in the network on your windows machine.\n\n\n###Scripting\n\nOn my unraid server I have this neat little script where I use mosquitto and nodered to turn on and off a shelly socket to save power. The script is on a weekly schedule. In Nodered I also send myself a telegram message once the script starts, When the PI is found, and when its done including the time elapsed and the amount of data synced.\n\n\n\n    #!/bin/bash\n    \n    \n    \n    # Record the start time\n    start_timeTotal=$(date +%s%3N)\n    \n    # Set your Raspberry Pi's IP address\n    RASPBERRY_PI_IP=\"192.168.8.107\"\n    \n    \n    # Set your Mosquitto container name or ID\n    CONTAINER_NAME=\"mosquitto\"\n    \n    \n    \n    # Set MQTT details\n    HOST=\"localhost\"  # Use localhost because we're inside the container\n    PORT=1883         # Specify the Mosquitto broker port\n    TOPIC=\"Backup\"    # Specify the topic you want to publish to\n    \n    \n    # Execute mosquitto_pub inside the container\n    #docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Starting\"\n    \n    \n    \n    \n    for _ in {1..12}; do\n        if ping -c 1 \"$RASPBERRY_PI_IP\" &amp;&gt; /dev/null; then\n            echo \"Raspberry Pi is up!\"\n            break\n        else\n            sleep 5  # Wait 5 seconds before checking again\n        fi\n    done\n    \n    # If Raspberry Pi is still not up after 1 minute, exit\n    if ! ping -c 1 \"$RASPBERRY_PI_IP\" &amp;&gt; /dev/null; then\n        docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Raspberry Pi did not start up within 1 minute. Exiting.\"\n        exit 1\n    fi\n    \n    \n    # Record the start time\n    start_timeFiles=$(date +%s%3N)\n    \n    # Start Rclone sync (adjust paths and remote as needed)\n    rclone sync /mnt/user/files backupPI:storage/files -v\n    \n    # Record the end time of files\n    end_timeFiles=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timeFiles - start_timeFiles))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Rclone sync files completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    \n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Sync Files done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n    # Record the start time pictures\n    start_timePictures=$(date +%s%3N)\n    \n    \n    # Start Rclone sync (adjust paths and remote as needed)\n    rclone sync /mnt/user/pictures backupPI:storage/pictures -v\n    \n    \n    \n    # Record the end time\n    end_timePictures=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timePictures - start_timePictures))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Rclone sync Pictures completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    \n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Sync Pictures done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n    # Record the end time\n    end_timeTotal=$(date +%s%3N)\n    \n    # Calculate elapsed time\n    elapsed_time=$((end_timeTotal - start_timeTotal))\n    hours=$((elapsed_time / 3600000))\n    minutes=$(( (elapsed_time % 3600000) / 60000 ))\n    seconds=$(( (elapsed_time % 60000) / 1000))\n    milliseconds=$((elapsed_time % 1000 ))\n    \n    # Display elapsed time\n    echo \"Complete sync completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.\"\n    \n    # Execute mosquitto_pub inside the container\n    docker exec \"$CONTAINER_NAME\" mosquitto_pub -h \"$HOST\" -p \"$PORT\" -t \"$TOPIC\" -m \"Complete sync completed and was done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.\"\n    \n\n\n\nI hope this is useful to some of you. I will amend and edit this post with your feedback and keep it alive as long as I can. Have a great weekend!", "author_fullname": "t2_fbw2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[TNASR1] - Tiny NAS with RAID1 - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5\" SATA HDDs in RAID1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j130b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup | Guide/How-to | Scripts/Software", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702649808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I give you the &lt;a href=\"https://imgur.com/a/fduKt1N\"&gt;TNASR1&lt;/a&gt; &amp;quot;Tiny NAS with RAID1&amp;quot; - the cheapest and smallest NAS for your backup storage needs based on the Raspberry Pi Zero 2 W with two 2.5&amp;quot; SATA HDDs in  RAID1&lt;/p&gt;\n\n&lt;h2&gt;Table of content&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Intro&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Why&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Goals&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;BOM&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;My Setup &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cost&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Performance&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Power consumption&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Setup&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scripting with scheduled wakeup to save power&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Intro&lt;/h3&gt;\n\n&lt;p&gt;So I have an Unraid Server wich is great and I am really happy with it. It stores all my files including media but also my very important documents as well as my maybe even more important pictures and videos of my family. So I always felt uneasy thinking about its safety. It&amp;#39;s a consumer grade tower PC located in my garage. I use two parity drives so on a system level I have some redundancy. Nothing wrong with that but if something happens to my server as a whole (fire/flooding/kids) all my data is gone.&lt;/p&gt;\n\n&lt;p&gt;Of course there is the possibility if using a cloud storage provider. Google drive is $12 per month or $144 per year. First of all I would be trusting a third party with the safety of my data and secondly I am cheap bastard. &lt;/p&gt;\n\n&lt;p&gt;This is the genesis of the TNASR1.&lt;/p&gt;\n\n&lt;p&gt;It lives in my garden shed in a watertight container and does everything a google drive would.&lt;/p&gt;\n\n&lt;h3&gt;Why&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Data Backup needs - common sense i.e. the &lt;a href=\"https://en.wikipedia.org/wiki/Glossary_of_backup_terms#Terms_and_definitions\"&gt;3 2 1 Rule&lt;/a&gt; demands a remote copy.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Control - cloud providers are a third party that require a level of trust and cost money above certain storage needs&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cost - cheaper in the long run&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Goals&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Cheap&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Low Power&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;enough storage for the most important files. At least 500gb+&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;RAID1 for some fault resilience&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;BOM&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Raspberry Pi Zero 2 W&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;a small heat sink&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;thermal paste&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;microSD card with at least 2gb&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB OTG Hub Host Cable&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A 2A/&lt;strong&gt;5.3V&lt;/strong&gt; - Power supply&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB A to Micro USB B cable&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x 2.5inch SATA Case &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x SATA drives&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;My Setup&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Raspberry Pi Zero 2 W  - \u20ac19.80&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The heat sink and thermal paste i had laying around. But I guess \u20ac1 is fair.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;INTENSO 3413460 - MicroSDHC-Card with 8GB, Intenso Class 10 - \u20ac3.40&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;USB OTG Hub Host Cable. Ali express item:3256805033322631 \u20ac1.40&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Samsung EP-TA10EWE Power supply.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;2x 2.5inch SATA Case. Ali express item: 3256805261700001  - \u20ac1.00&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For storage I used  2x 2TB Toshiba L200 bulk HDWL120UZSVA - \u20ac65.00 each&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Cost&lt;/h3&gt;\n\n&lt;p&gt;NAS cost before storage: \u20ac34.60 or around $37.20&lt;/p&gt;\n\n&lt;p&gt;In total ~ \u20ac164 or $178 for 2 Terabyte of remote RAID1 storage. Not bad if I say so myself. ROI vs. google Drive ($144 per year for 2 terabyte) in 15 months. &lt;/p&gt;\n\n&lt;p&gt;Cost for electricity (more down below) is about $/\u20ac20 a year if you let it run continuously with $/\u20ac0.3 per kWh. ROI vs Google Drive under 17 months.&lt;/p&gt;\n\n&lt;h3&gt;Performance&lt;/h3&gt;\n\n&lt;p&gt;So the two SATA drives in RAID1 are connected over a single USB 2.0 interface. Suffice to say you won&amp;#39;t get SSD speeds. But I am Happy to report that it is quite fast enough for our needs. I get around 2MB/s write and 4MB/s of read. That means I can sync 1 gigabyte of data in under 9 Minutes. Now this is nothing to write home about but we have to consider context. This for a remote backup that uses less than 10w. Also after initial setup this is for syncing the diff only. I tend to sync less than 5 gigabyte of new data a week. That means backup takes under an hour.&lt;/p&gt;\n\n&lt;p&gt;For my initial sync of my files it took 8 hours for 48 Gigabytes with a transfer speed of about  1.8 MB/s&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Transferred:       51.312 GiB / 51.312 GiB, 100%, 1,852.5 KiB/s, ETA 0s\nChecks:             71697 / 71697, 100%\nDeleted:               11 (files), 0 (dirs)\nTransferred:        70846 / 70846, 100%\nElapsed time:    8h4m34.3s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My initial sync of my pictures and videos of 435 Gigabytes took 2 days and 9 hours with an average transfer speed of 2.2 MB/s&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Transferred:      455.358 GiB / 455.358 GiB, 100%, 2.316 MiB/s, ETA 0s\nChecks:            111442 / 111442, 100%\nDeleted:               11 (files), 0 (dirs)\nTransferred:       111478 / 111478, 100%\nElapsed time:  2d9h13m13.9s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;A sync run without anything to sync takes under 2 Minutes.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Rclone sync completed in 0 hours, 1 minutes, and 45 seconds 390 milliseconds.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h3&gt;Power Consumption&lt;/h3&gt;\n\n&lt;p&gt;The star of the show is undoubtedly the 5.3V power supply. Any power supply with just 5V, even ones that can deliver 100W, failed the boot up during the high ramp up power spike from the HDDs. Te whole system is teetering on being power starved. But it was up and running and syncing my 500 gigabytes without any issues.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/PANKpo5\"&gt;The whole setup draws continuous 5.3V 1.5A or 8W max during a sync.&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I am sure when the drives spin up the power draw spikes up above 1.5A but as the supply has more than 5V it does not cause any issues.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/c1CnIv7\"&gt;At Idle the power draw is about 0.6A or about 3W.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are some Samsung Power Supplies that have 5.3V. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.voc-electronics.com/a-42425080/power-batteries-adapters/5-3v-2a-power-supply-eu/#description\"&gt;Alternatively there are AC/DC adapters like these.&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;Setup&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Setup the MicroSD with Raspberry Pi Imager. &lt;/p&gt;\n\n&lt;p&gt;1.1 Choose Raspberry Pi OS (other)&lt;/p&gt;\n\n&lt;p&gt;1.2 Chose Raspberry Pi OS lite (64-bit)&lt;/p&gt;\n\n&lt;p&gt;1.3 Set Up WiFi and turn on SSH&lt;/p&gt;\n\n&lt;p&gt;1.4 Burn Image&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Check if everything is running&lt;/p&gt;\n\n&lt;p&gt;2.1 Insert microSD &lt;/p&gt;\n\n&lt;p&gt;2.2 Connect Power Supply&lt;/p&gt;\n\n&lt;p&gt;2.3 Ping device&lt;/p&gt;\n\n&lt;p&gt;2.4 If successful try to SSH into the pi with : SSH user@nameOfPi or: SSH user@IPaddrOfPi&lt;/p&gt;\n\n&lt;p&gt;2.5 Power down and disconnect power supply&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.computernetworkingnotes.com/linux-tutorials/how-to-configure-raid-in-linux-step-by-step-guide.html\"&gt;Setup the drives&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;3.1 Connect drives via the USB hub dongle&lt;/p&gt;\n\n&lt;p&gt;3.2 Reconnect Power Supply&lt;/p&gt;\n\n&lt;p&gt;3.3 SSH back into the Pi&lt;/p&gt;\n\n&lt;p&gt;3.4 Type in:  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This should confirm two things. The drives are connected and have the right size. You should have an output like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sda           8:16   0  1.8T  0 disk \nsdb           8:16   0  1.8T  0 disk\nmmcblk0     179:0    0  7.5G  0 disk \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.5 Type in: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo fdisk /dev/sda\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;new drive&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;n\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;primary&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;p\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;list&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;l\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;type&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;t\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Linux raid auto&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;fd\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;write&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;w\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.6 repeat for sdb&lt;/p&gt;\n\n&lt;p&gt;3.7 &lt;a href=\"https://www.computerhope.com/unix/partprob.htm\"&gt;Partprobe&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;partprobe\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.8 Check if sda1 and sdb1 are listed&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsda           8:0    0  1.8T  0 disk \n\u2514\u2500sda1        8:1    0  1.8T  0 part \nsdb           8:16   0  1.8T  0 disk \n\u2514\u2500sdb1        8:17   0  1.8T  0 part \nmmcblk0     179:0    0  7.5G  0 disk \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part /\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;3.9 Check if the file system is correct&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo fdisk -l\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the output should include the type &amp;quot;Linux raid auto&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Device     Boot Start        End    Sectors  Size Id Type\n/dev/sda1        2048 3907029167 3907027120  1.8T fd Linux raid autodetect\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.thetechedvocate.org/how-to-set-up-raid-1-on-the-raspberry-pi-the-easy-way/\"&gt;Setup raid&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;4.1 Install mdadm&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo apt-get install mdadm\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;4.2 create raid array&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Continue creating array?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;y\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;4.3 check if md0 was created&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lsblk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;NAME        MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT\nsda           8:0    0  1.8T  0 disk  \n\u2514\u2500sda1        8:1    0  1.8T  0 part  \n  \u2514\u2500md0       9:0    0  1.8T  0 raid1 \nsdb           8:16   0  1.8T  0 disk  \n\u2514\u2500sdb1        8:17   0  1.8T  0 part  \n  \u2514\u2500md0       9:0    0  1.8T  0 raid1 \nmmcblk0     179:0    0  7.5G  0 disk  \n\u251c\u2500mmcblk0p1 179:1    0  256M  0 part  /boot\n\u2514\u2500mmcblk0p2 179:2    0  7.2G  0 part  /\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Format the raid array / make filesystem&lt;/p&gt;\n\n&lt;p&gt;5.1 Type in&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mkfs.ext4 /dev/md0\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=Ff96FPJHq5o\"&gt;Mount to folder&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;6.1 Make folder under mnt&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mkdir /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.2 Mount md0 to storage&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mount /dev/md0 /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.6. mount on startup&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo nano /etc/fstab\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.7 add line&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/dev/md0 /mnt/storage ext4 defaults 0 0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.8 Save and exit&lt;/p&gt;\n\n&lt;p&gt;6.9 reboot&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;reboot\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;6.4 check if mount is successfull&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df -alh\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;output should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Filesystem      Size  Used Avail Use% Mounted on\n/dev/root       7.1G  1.7G  5.2G  25% /\ndevtmpfs         80M     0   80M   0% /dev\nproc               0     0     0    - /proc\nsysfs              0     0     0    - /sys\nsecurityfs         0     0     0    - /sys/kernel/security\ntmpfs           210M     0  210M   0% /dev/shm\ndevpts             0     0     0    - /dev/pts\ntmpfs            84M  3.0M   81M   4% /run\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ncgroup2            0     0     0    - /sys/fs/cgroup\npstore             0     0     0    - /sys/fs/pstore\nbpf                0     0     0    - /sys/fs/bpf\nsystemd-1          0     0     0    - /proc/sys/fs/binfmt_misc\nmqueue             0     0     0    - /dev/mqueue\ndebugfs            0     0     0    - /sys/kernel/debug\nsunrpc             0     0     0    - /run/rpc_pipefs\ntracefs            0     0     0    - /sys/kernel/tracing\nconfigfs           0     0     0    - /sys/kernel/config\nfusectl            0     0     0    - /sys/fs/fuse/connections\n/dev/mmcblk0p1  255M   31M  225M  13% /boot\ntmpfs            42M     0   42M   0% /run/user/1000\n/dev/md0        1.8T   28K  1.7T   1% /mnt/storage\n&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;enable SMB sharing&lt;/p&gt;\n\n&lt;p&gt;7.1 install samba&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo apt-get samba\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.2 edit samba config&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo nano /etc/samba/smb.conf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.3 add this at the end&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[storage]\npath=/mnt/storage\nwriteable=yes\nreate mask=0666\ndirectorty mask=0666\npublic=yes\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.4 Save end exit&lt;/p&gt;\n\n&lt;p&gt;7.5 restart samba service&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo systemctl restart smbd\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.6 ad a user to samba&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo smbpasswd -a yourDesiredUsername\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;7.7 Set a user password&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That is it you are done. See if you can find your folder in the network on your windows machine.&lt;/p&gt;\n\n&lt;h3&gt;Scripting&lt;/h3&gt;\n\n&lt;p&gt;On my unraid server I have this neat little script where I use mosquitto and nodered to turn on and off a shelly socket to save power. The script is on a weekly schedule. In Nodered I also send myself a telegram message once the script starts, When the PI is found, and when its done including the time elapsed and the amount of data synced.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\n\n\n\n# Record the start time\nstart_timeTotal=$(date +%s%3N)\n\n# Set your Raspberry Pi&amp;#39;s IP address\nRASPBERRY_PI_IP=&amp;quot;192.168.8.107&amp;quot;\n\n\n# Set your Mosquitto container name or ID\nCONTAINER_NAME=&amp;quot;mosquitto&amp;quot;\n\n\n\n# Set MQTT details\nHOST=&amp;quot;localhost&amp;quot;  # Use localhost because we&amp;#39;re inside the container\nPORT=1883         # Specify the Mosquitto broker port\nTOPIC=&amp;quot;Backup&amp;quot;    # Specify the topic you want to publish to\n\n\n# Execute mosquitto_pub inside the container\n#docker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Starting&amp;quot;\n\n\n\n\nfor _ in {1..12}; do\n    if ping -c 1 &amp;quot;$RASPBERRY_PI_IP&amp;quot; &amp;amp;&amp;gt; /dev/null; then\n        echo &amp;quot;Raspberry Pi is up!&amp;quot;\n        break\n    else\n        sleep 5  # Wait 5 seconds before checking again\n    fi\ndone\n\n# If Raspberry Pi is still not up after 1 minute, exit\nif ! ping -c 1 &amp;quot;$RASPBERRY_PI_IP&amp;quot; &amp;amp;&amp;gt; /dev/null; then\n    docker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Raspberry Pi did not start up within 1 minute. Exiting.&amp;quot;\n    exit 1\nfi\n\n\n# Record the start time\nstart_timeFiles=$(date +%s%3N)\n\n# Start Rclone sync (adjust paths and remote as needed)\nrclone sync /mnt/user/files backupPI:storage/files -v\n\n# Record the end time of files\nend_timeFiles=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timeFiles - start_timeFiles))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Rclone sync files completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Sync Files done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n\n# Record the start time pictures\nstart_timePictures=$(date +%s%3N)\n\n\n# Start Rclone sync (adjust paths and remote as needed)\nrclone sync /mnt/user/pictures backupPI:storage/pictures -v\n\n\n\n# Record the end time\nend_timePictures=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timePictures - start_timePictures))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Rclone sync Pictures completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Sync Pictures done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n\n# Record the end time\nend_timeTotal=$(date +%s%3N)\n\n# Calculate elapsed time\nelapsed_time=$((end_timeTotal - start_timeTotal))\nhours=$((elapsed_time / 3600000))\nminutes=$(( (elapsed_time % 3600000) / 60000 ))\nseconds=$(( (elapsed_time % 60000) / 1000))\nmilliseconds=$((elapsed_time % 1000 ))\n\n# Display elapsed time\necho &amp;quot;Complete sync completed in $hours hours, $minutes minutes, and $seconds seconds $milliseconds milliseconds.&amp;quot;\n\n# Execute mosquitto_pub inside the container\ndocker exec &amp;quot;$CONTAINER_NAME&amp;quot; mosquitto_pub -h &amp;quot;$HOST&amp;quot; -p &amp;quot;$PORT&amp;quot; -t &amp;quot;$TOPIC&amp;quot; -m &amp;quot;Complete sync completed and was done in $hours hours, $minutes minutes, and $seconds seconds $milliseconds ms.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I hope this is useful to some of you. I will amend and edit this post with your feedback and keep it alive as long as I can. Have a great weekend!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?auto=webp&amp;s=1b8a86e76338b6c56926fad15933014e95c3842f", "width": 1500, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89bdfc939cafb8391d5e61a9567b4d4824284fcc", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86e77b7a1e8adf26f55b29177d43a792eaf05079", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=79eb0d4e1427b28536b1113953431a110410e80f", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=815fb17aab37a32dcd7821a0b443af7e57d6e5e9", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=240c63ed78cf374d220221b681b49faf1cf9a1e2", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/4zo7u24_6idtWGVPukNNAUpfp2lFY5Lp-Snp8qNtMOo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2df389b2db307451f66469500dbffb7465972360", "width": 1080, "height": 1440}], "variants": {}, "id": "bMThnKntPA0wspAl37RZB5eagDAmk4PAnyYetytPxlU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j130b", "is_robot_indexable": true, "report_reasons": null, "author": "ElementII5", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j130b/tnasr1_tiny_nas_with_raid1_the_cheapest_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j130b/tnasr1_tiny_nas_with_raid1_the_cheapest_and/", "subreddit_subscribers": 718251, "created_utc": 1702649808.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ran across a pretty good deal on one at 225 renewed. \nWas going to get the X20 20TB until I saw this around the same price. \nBelieve it has double the cache of the X20 model. \n\nReviews are scarce on this though. \n\nIs the X22 just a newer model than the X20?? Assume it\u2019s a little faster too?\n\nAny info before I pull the trigger would be great!", "author_fullname": "t2_8msflw3m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos X22 20TB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iuhgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702624608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ran across a pretty good deal on one at 225 renewed. \nWas going to get the X20 20TB until I saw this around the same price. \nBelieve it has double the cache of the X20 model. &lt;/p&gt;\n\n&lt;p&gt;Reviews are scarce on this though. &lt;/p&gt;\n\n&lt;p&gt;Is the X22 just a newer model than the X20?? Assume it\u2019s a little faster too?&lt;/p&gt;\n\n&lt;p&gt;Any info before I pull the trigger would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iuhgm", "is_robot_indexable": true, "report_reasons": null, "author": "hellcatpekes", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iuhgm/seagate_exos_x22_20tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iuhgm/seagate_exos_x22_20tb/", "subreddit_subscribers": 718251, "created_utc": 1702624608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a bunch of old VHS videos I'm wanting to digitise. I purchased this used [VHS player](https://www.ebay.com.au/itm/266552319384) (an LG V181 as the price was good) and [this capture card](https://www.ebay.com.au/itm/175982263794). I understand the card is cheap and may not be the best quality (the same could be said for the VHS player). I thought I'd trial first and see how we go. \n\nI've made a rookie error, because even with the card I have no additional cord to connect the capture card to the VHS player. The VHS player also doesn't have a scart plug (I don't know if that's already ended my chances of this not working). \n\nI'm hoping to get some advice to rectify the situation. In order to connect the capture card to the VHS player correctly I'm looking at the cord option below. It might connect to the two, but don't know if that will technically do anything. \n\n [3 x RCA Piggyback Plugs to 3 RCA Plugs - 1.5m | Jaycar Electronics](https://www.jaycar.com.au/3-x-rca-piggyback-plugs-to-3-rca-plugs-1-5m/p/WV7324) \n\nIf a cable to connect to the two is all I need and the above one is wrong, if there's any other cables [in their store](https://www.jaycar.com.au/cables-connectors/audio-video-cables/rca-av-cables/c/1AD?q=%3Apopularity-desc%3AsshomeAVLEAD-avleadTYPESIG%3AComposite%2BVideo%2Band%2BAudio&amp;text=#) that would work, please let me know. If this is the wrong approach and I need another piece of equipment, please also let me know.\n\nAny help would be greatly appreciated. \n\n&amp;#x200B;", "author_fullname": "t2_ezv1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Purchased used VCR and VHS converter card - Need advice on next steps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iwyx0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702635496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of old VHS videos I&amp;#39;m wanting to digitise. I purchased this used &lt;a href=\"https://www.ebay.com.au/itm/266552319384\"&gt;VHS player&lt;/a&gt; (an LG V181 as the price was good) and &lt;a href=\"https://www.ebay.com.au/itm/175982263794\"&gt;this capture card&lt;/a&gt;. I understand the card is cheap and may not be the best quality (the same could be said for the VHS player). I thought I&amp;#39;d trial first and see how we go. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made a rookie error, because even with the card I have no additional cord to connect the capture card to the VHS player. The VHS player also doesn&amp;#39;t have a scart plug (I don&amp;#39;t know if that&amp;#39;s already ended my chances of this not working). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping to get some advice to rectify the situation. In order to connect the capture card to the VHS player correctly I&amp;#39;m looking at the cord option below. It might connect to the two, but don&amp;#39;t know if that will technically do anything. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.jaycar.com.au/3-x-rca-piggyback-plugs-to-3-rca-plugs-1-5m/p/WV7324\"&gt;3 x RCA Piggyback Plugs to 3 RCA Plugs - 1.5m | Jaycar Electronics&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;If a cable to connect to the two is all I need and the above one is wrong, if there&amp;#39;s any other cables &lt;a href=\"https://www.jaycar.com.au/cables-connectors/audio-video-cables/rca-av-cables/c/1AD?q=%3Apopularity-desc%3AsshomeAVLEAD-avleadTYPESIG%3AComposite%2BVideo%2Band%2BAudio&amp;amp;text=#\"&gt;in their store&lt;/a&gt; that would work, please let me know. If this is the wrong approach and I need another piece of equipment, please also let me know.&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dK-vPSUxnCDa9AQ3CJrlLRyGaKeHHI_zrKRuGo6GBIA.jpg?auto=webp&amp;s=5057ef2f3a93cc0fa1374b2daff196167248ae9e", "width": 160, "height": 159}, "resolutions": [{"url": "https://external-preview.redd.it/dK-vPSUxnCDa9AQ3CJrlLRyGaKeHHI_zrKRuGo6GBIA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e032bf4c46820210d06c55050551f54ecf7dfb7a", "width": 108, "height": 107}], "variants": {}, "id": "sTudcUIITL1ED_TEpskZf2L45uLN0C0Oir49ptIzTrE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iwyx0", "is_robot_indexable": true, "report_reasons": null, "author": "jtoml3", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iwyx0/purchased_used_vcr_and_vhs_converter_card_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iwyx0/purchased_used_vcr_and_vhs_converter_card_need/", "subreddit_subscribers": 718251, "created_utc": 1702635496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Excuse the NOOB question. But I\u2019m trying to find storage for my files and videos etc. I want them to last for a long time. \n\nI\u2019m new to this whole stuff and I\u2019m curious as to what\u2019s the best kind of storage to get, from what I\u2019ve read online, some of external HDD units can corrupt after 3-5 years. SSD can break and be unrecoverable, USB has data corrosion. \n\nI would prefer it to be offline and not in a cloud or servers. \n\nI\u2019m looking for suggestions or advice as I have no first hand experience with this subject so please excuse my ignorance. \n\nWould I be ok with external HHD drives? \nIs SSD the most reliable option? \nShall I bug 400USB sticks and back up the backups?\ud83d\ude05", "author_fullname": "t2_bhw3u2sss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best type of storage for long term use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j2bab", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702653211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excuse the NOOB question. But I\u2019m trying to find storage for my files and videos etc. I want them to last for a long time. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m new to this whole stuff and I\u2019m curious as to what\u2019s the best kind of storage to get, from what I\u2019ve read online, some of external HDD units can corrupt after 3-5 years. SSD can break and be unrecoverable, USB has data corrosion. &lt;/p&gt;\n\n&lt;p&gt;I would prefer it to be offline and not in a cloud or servers. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking for suggestions or advice as I have no first hand experience with this subject so please excuse my ignorance. &lt;/p&gt;\n\n&lt;p&gt;Would I be ok with external HHD drives? \nIs SSD the most reliable option? \nShall I bug 400USB sticks and back up the backups?\ud83d\ude05&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j2bab", "is_robot_indexable": true, "report_reasons": null, "author": "No-Papaya9956", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j2bab/whats_the_best_type_of_storage_for_long_term_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j2bab/whats_the_best_type_of_storage_for_long_term_use/", "subreddit_subscribers": 718251, "created_utc": 1702653211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My company currently has a rather large (maybe not too large by some of your standards) dataset of images of handwritten documents totalling just over 10TB, with more to come in future. These images are very high resolution, but we don't want to reduce their size as we want to retain all of the information. Due to the nature of the images there is a large reduction in file size when converting from BMP to PNG, and so my question is whether there are any drawbacks to using PNG over BMP in this scenario, or more generally why would one want to use BMP over PNG for anything? ", "author_fullname": "t2_65dpfzvg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reason to use BMP over PNG?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18j3gmq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702656294.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company currently has a rather large (maybe not too large by some of your standards) dataset of images of handwritten documents totalling just over 10TB, with more to come in future. These images are very high resolution, but we don&amp;#39;t want to reduce their size as we want to retain all of the information. Due to the nature of the images there is a large reduction in file size when converting from BMP to PNG, and so my question is whether there are any drawbacks to using PNG over BMP in this scenario, or more generally why would one want to use BMP over PNG for anything? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j3gmq", "is_robot_indexable": true, "report_reasons": null, "author": "ClearlyCylindrical", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j3gmq/any_reason_to_use_bmp_over_png/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j3gmq/any_reason_to_use_bmp_over_png/", "subreddit_subscribers": 718251, "created_utc": 1702656294.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys, need your lights.\n\nPurchased a 4TB WD blue from a well respected eshop in my EU Country, not labelled as authorized retailer clearly by WD but handling huge loads of e-commerce and generally having great reputation.\n\nOverthinking as usual and reading various stories, I have checked the warranty of the part with SN. It checks out as 2 years (minus 2 months but guessing registering receipt will fix that ) , the part is manufactured september 2023 and basically in the end of the warranty and SN it says this:\n\nSecond tier OEM. \n\n&amp;#x200B;\n\nHow that directly translates to my usage? Is it like a returned/refurb or something of lower quality drive? \n\nI cannot find any specific information on the WEB to 2nd tier OEM. Only about OEM statuses. The warranty says limited warranty so it is probably covered normally but the tag Second tier OEM this I can't understand.\n\n&amp;#x200B;\n\nThanks.\n\nhttps://preview.redd.it/a8yry1z7oa6c1.png?width=197&amp;format=png&amp;auto=webp&amp;s=3206709252a6422c7ab56737a7801d2c17209347", "author_fullname": "t2_9dmr41h6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question HDD Second tier OEM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 38, "top_awarded_type": null, "hide_score": false, "media_metadata": {"a8yry1z7oa6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 29, "x": 108, "u": "https://preview.redd.it/a8yry1z7oa6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f41a7ef2c2fc2c3950f4c99dcb3c5e9c9af6959e"}], "s": {"y": 54, "x": 197, "u": "https://preview.redd.it/a8yry1z7oa6c1.png?width=197&amp;format=png&amp;auto=webp&amp;s=3206709252a6422c7ab56737a7801d2c17209347"}, "id": "a8yry1z7oa6c1"}}, "name": "t3_18ido1m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AgXjIQLzHo18kbXdax-HkwB7Pr6_NZ0F61VQ9JS-hWs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702574632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, need your lights.&lt;/p&gt;\n\n&lt;p&gt;Purchased a 4TB WD blue from a well respected eshop in my EU Country, not labelled as authorized retailer clearly by WD but handling huge loads of e-commerce and generally having great reputation.&lt;/p&gt;\n\n&lt;p&gt;Overthinking as usual and reading various stories, I have checked the warranty of the part with SN. It checks out as 2 years (minus 2 months but guessing registering receipt will fix that ) , the part is manufactured september 2023 and basically in the end of the warranty and SN it says this:&lt;/p&gt;\n\n&lt;p&gt;Second tier OEM. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How that directly translates to my usage? Is it like a returned/refurb or something of lower quality drive? &lt;/p&gt;\n\n&lt;p&gt;I cannot find any specific information on the WEB to 2nd tier OEM. Only about OEM statuses. The warranty says limited warranty so it is probably covered normally but the tag Second tier OEM this I can&amp;#39;t understand.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/a8yry1z7oa6c1.png?width=197&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3206709252a6422c7ab56737a7801d2c17209347\"&gt;https://preview.redd.it/a8yry1z7oa6c1.png?width=197&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3206709252a6422c7ab56737a7801d2c17209347&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ido1m", "is_robot_indexable": true, "report_reasons": null, "author": "Long_Distribution_89", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ido1m/question_hdd_second_tier_oem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ido1m/question_hdd_second_tier_oem/", "subreddit_subscribers": 718251, "created_utc": 1702574632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are only cracktros for Commodore 64 on the Internet Archive but seems like no single of Microsoft Windows. Has anyone managed to backup all NFOs/cracktros/installers from Defacto2? Other sites include Demozoo and Pouet.", "author_fullname": "t2_lru1al2t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Defacto2 backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ivh8y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702628909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are only cracktros for Commodore 64 on the Internet Archive but seems like no single of Microsoft Windows. Has anyone managed to backup all NFOs/cracktros/installers from Defacto2? Other sites include Demozoo and Pouet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ivh8y", "is_robot_indexable": true, "report_reasons": null, "author": "miller11568", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ivh8y/defacto2_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ivh8y/defacto2_backup/", "subreddit_subscribers": 718251, "created_utc": 1702628909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking at buying some extra refurbished drives, actually a specific model of Exos drives since they have performed so well for me (and for Backblaze apparently).  I found one person on eBay selling them and it says that it's a ST16000NM001G but actually when you get one it appears to be some off-brand version with \"OS\" written on it.  The smartctl reports the model number as \"OOS16000G\" and the stats look okay, I am running a battery of tests on it now.\n\nAnyone have any idea what this \"OS\" brand of Seagate drives is about?  I tried searching but all the posts that come back are about which drive to install your operating system...", "author_fullname": "t2_5iywr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"OS\" brand drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18isuql", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702618341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at buying some extra refurbished drives, actually a specific model of Exos drives since they have performed so well for me (and for Backblaze apparently).  I found one person on eBay selling them and it says that it&amp;#39;s a ST16000NM001G but actually when you get one it appears to be some off-brand version with &amp;quot;OS&amp;quot; written on it.  The smartctl reports the model number as &amp;quot;OOS16000G&amp;quot; and the stats look okay, I am running a battery of tests on it now.&lt;/p&gt;\n\n&lt;p&gt;Anyone have any idea what this &amp;quot;OS&amp;quot; brand of Seagate drives is about?  I tried searching but all the posts that come back are about which drive to install your operating system...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18isuql", "is_robot_indexable": true, "report_reasons": null, "author": "butters1337", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18isuql/os_brand_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18isuql/os_brand_drives/", "subreddit_subscribers": 718251, "created_utc": 1702618341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently got about 12 old dvrs each with a 500 GB segate pipeline drive. Looks to be about a decade old and 5900 RPM. Any advice on what to do with them? I don't think they were that heavily used and some of the dvrs looked brand new", "author_fullname": "t2_e1bhrebh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with 6 TBs of old HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iradw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702612907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got about 12 old dvrs each with a 500 GB segate pipeline drive. Looks to be about a decade old and 5900 RPM. Any advice on what to do with them? I don&amp;#39;t think they were that heavily used and some of the dvrs looked brand new&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iradw", "is_robot_indexable": true, "report_reasons": null, "author": "baseballandpcs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iradw/what_to_do_with_6_tbs_of_old_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iradw/what_to_do_with_6_tbs_of_old_hdd/", "subreddit_subscribers": 718251, "created_utc": 1702612907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "To avoid redundancy besides Seagate seatools  long self test what else would you run ? Do you run benchmarks to check the speed is within tolerance of other similar drives  ? Just got a supposedly x22 20tb from server parts but I\u2019m getting 220mb instead of the benched 285. Since windows doesn\u2019t have zfs is there any good alternative?", "author_fullname": "t2_cbmylwsx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to run on a new hard drive new or refurbished before start using it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18idqh1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702574812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To avoid redundancy besides Seagate seatools  long self test what else would you run ? Do you run benchmarks to check the speed is within tolerance of other similar drives  ? Just got a supposedly x22 20tb from server parts but I\u2019m getting 220mb instead of the benched 285. Since windows doesn\u2019t have zfs is there any good alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18idqh1", "is_robot_indexable": true, "report_reasons": null, "author": "notnotrkghr", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18idqh1/what_to_run_on_a_new_hard_drive_new_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18idqh1/what_to_run_on_a_new_hard_drive_new_or/", "subreddit_subscribers": 718251, "created_utc": 1702574812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings fellow hoarders.  \n\nSeveral years ago I had a drive with about 3-4k movies on it.  At some point I screwed up and \"broke\" the file system or wiped the partition table.  Can't remember exactly what happened but I was able to use photorec to carve out all the files and they're all on a new drive with generic file names f12345678 and so on.  The files are all playable but I can't tell what is what.  They have just been sitting on that drive since I recovered everything, waiting for me to make time to deal with the mess of identifying the files and renaming them.  Occasionally I'll open up a few movies, check for a title credit or recognizable scene and manually rename them but it's a painful process.  \n\nThis may be a long shot but I'm looking for a tool that I can point to the directory of files that will \"view\" each one and identify the name.  I don't really care how long it takes.  I have hardware available to dedicate to this job if needed and can easily spin up a VM to work on this in the background. \n\nDon't worry, I've since gotten smarter about backups, setting up offsite copies, etc.  This is essentially the last big effort to get things fully recovered.   Thanks in advance!", "author_fullname": "t2_58o36", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for determining movie title with broken file names.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18j36h8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702655547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings fellow hoarders.  &lt;/p&gt;\n\n&lt;p&gt;Several years ago I had a drive with about 3-4k movies on it.  At some point I screwed up and &amp;quot;broke&amp;quot; the file system or wiped the partition table.  Can&amp;#39;t remember exactly what happened but I was able to use photorec to carve out all the files and they&amp;#39;re all on a new drive with generic file names f12345678 and so on.  The files are all playable but I can&amp;#39;t tell what is what.  They have just been sitting on that drive since I recovered everything, waiting for me to make time to deal with the mess of identifying the files and renaming them.  Occasionally I&amp;#39;ll open up a few movies, check for a title credit or recognizable scene and manually rename them but it&amp;#39;s a painful process.  &lt;/p&gt;\n\n&lt;p&gt;This may be a long shot but I&amp;#39;m looking for a tool that I can point to the directory of files that will &amp;quot;view&amp;quot; each one and identify the name.  I don&amp;#39;t really care how long it takes.  I have hardware available to dedicate to this job if needed and can easily spin up a VM to work on this in the background. &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t worry, I&amp;#39;ve since gotten smarter about backups, setting up offsite copies, etc.  This is essentially the last big effort to get things fully recovered.   Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j36h8", "is_robot_indexable": true, "report_reasons": null, "author": "lobstahcookah", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j36h8/software_for_determining_movie_title_with_broken/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j36h8/software_for_determining_movie_title_with_broken/", "subreddit_subscribers": 718251, "created_utc": 1702655547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\n[what are all of these? i cleared them out and they came back. most have a folder called en\\_us in them with nothing inside](https://preview.redd.it/q33ujsn3ef6c1.png?width=539&amp;format=png&amp;auto=webp&amp;s=7e6c1ce7a24382e757c376cff932fb181eca1dd8)\n\nhttps://preview.redd.it/2b7wdq8aef6c1.png?width=575&amp;format=png&amp;auto=webp&amp;s=fbb60fcc7154794f14681e326961db45a8e7d4df\n\n[wtf is neat office?](https://preview.redd.it/s9bbk38kef6c1.png?width=774&amp;format=png&amp;auto=webp&amp;s=b2ed352678f663ff20a00d81040468e7ef64800e)", "author_fullname": "t2_3dj4aknb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Random files on SSD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2b7wdq8aef6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/2b7wdq8aef6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f682c18e2bed31c751768b4846ea782455b60aee"}, {"y": 131, "x": 216, "u": "https://preview.redd.it/2b7wdq8aef6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=97651e0c9af8474315758c5e107fdfdc64433877"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/2b7wdq8aef6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=96390e9fb47f785a8d895755ce8c5ca78aabde2e"}], "s": {"y": 351, "x": 575, "u": "https://preview.redd.it/2b7wdq8aef6c1.png?width=575&amp;format=png&amp;auto=webp&amp;s=fbb60fcc7154794f14681e326961db45a8e7d4df"}, "id": "2b7wdq8aef6c1"}, "q33ujsn3ef6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 78, "x": 108, "u": "https://preview.redd.it/q33ujsn3ef6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=167ab006d6b7719957eeb6c3ff530a18acac0a01"}, {"y": 157, "x": 216, "u": "https://preview.redd.it/q33ujsn3ef6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=990a31724ea6ca92316e63339ff8814e0c72c058"}, {"y": 233, "x": 320, "u": "https://preview.redd.it/q33ujsn3ef6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03a39cc1635dcb2b53423f3d1919a11326ef9b06"}], "s": {"y": 393, "x": 539, "u": "https://preview.redd.it/q33ujsn3ef6c1.png?width=539&amp;format=png&amp;auto=webp&amp;s=7e6c1ce7a24382e757c376cff932fb181eca1dd8"}, "id": "q33ujsn3ef6c1"}, "s9bbk38kef6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=de0e9e3a7d7591ef05e3fb851c697d7c6df01b39"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df31793f24f06cb88c5fd5d369079fb2a700c732"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=677d86ddc25e6cd2fc7b4134f25b82572db2526a"}, {"y": 308, "x": 640, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6c06d1f9852b840049e36449fbb8843cd247757"}], "s": {"y": 373, "x": 774, "u": "https://preview.redd.it/s9bbk38kef6c1.png?width=774&amp;format=png&amp;auto=webp&amp;s=b2ed352678f663ff20a00d81040468e7ef64800e"}, "id": "s9bbk38kef6c1"}}, "name": "t3_18iw4er", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Hmk9POVoVh0nyxCxqZ4KD2_9NNayVJOpfTwG-MsRakI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702631731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q33ujsn3ef6c1.png?width=539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e6c1ce7a24382e757c376cff932fb181eca1dd8\"&gt;what are all of these? i cleared them out and they came back. most have a folder called en_us in them with nothing inside&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2b7wdq8aef6c1.png?width=575&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbb60fcc7154794f14681e326961db45a8e7d4df\"&gt;https://preview.redd.it/2b7wdq8aef6c1.png?width=575&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbb60fcc7154794f14681e326961db45a8e7d4df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s9bbk38kef6c1.png?width=774&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2ed352678f663ff20a00d81040468e7ef64800e\"&gt;wtf is neat office?&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iw4er", "is_robot_indexable": true, "report_reasons": null, "author": "hazardtm", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iw4er/random_files_on_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iw4er/random_files_on_ssd/", "subreddit_subscribers": 718251, "created_utc": 1702631731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I attempted to record it after it kept going on but was only able to catch it once before it stopped. I was playing a game in the foreground but was not accessing data in any way from the DAS as far as I know. The sound is relatively loud. Thanks.", "author_fullname": "t2_32ywoabe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just recorded this audio from my DAS, only 1 drive in currently. I don't know if I'm worrying about nothing but the DAS is on but not in use and it made this same sound periodically for about a minute, is this a cause for concern?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18itppj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/sl9bhut3ke6c1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 1280, "width": 666, "scrubber_media_url": "https://v.redd.it/sl9bhut3ke6c1/DASH_96.mp4", "dash_url": "https://v.redd.it/sl9bhut3ke6c1/DASHPlaylist.mpd?a=1705253910%2CNWQ1NzRhNTQ2NzA3NjBhZDI5YmM3YWZmZDJiZjg3MTIyZjM3ZTQ4ZDA2YTJmMzQ2Y2I0OTlkZTE3MmJlMGNmOA%3D%3D&amp;v=1&amp;f=sd", "duration": 4, "hls_url": "https://v.redd.it/sl9bhut3ke6c1/HLSPlaylist.m3u8?a=1705253910%2CNTA1NmIyNTFmM2Q5YjVmZThjYmRmNjE4YTg5NjJkNzhhMGIzMjllMTRiMTY4MjY1N2NjNGM3YzlhYTM0MDk0NA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/MDkwc256cDNrZTZjMaYk2_Zlrv3-qF35FH9e7lUEZGDFuVy8h1wPdOh1cDBi.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0472e774351d085b22715d0012dbf4a313f0a8fa", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702621579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I attempted to record it after it kept going on but was only able to catch it once before it stopped. I was playing a game in the foreground but was not accessing data in any way from the DAS as far as I know. The sound is relatively loud. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/sl9bhut3ke6c1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MDkwc256cDNrZTZjMaYk2_Zlrv3-qF35FH9e7lUEZGDFuVy8h1wPdOh1cDBi.png?format=pjpg&amp;auto=webp&amp;s=54cdf5707fadfdae7ad76d576f5aa08f0c0e5484", "width": 881, "height": 1693}, "resolutions": [{"url": "https://external-preview.redd.it/MDkwc256cDNrZTZjMaYk2_Zlrv3-qF35FH9e7lUEZGDFuVy8h1wPdOh1cDBi.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=00161606b5aaa9465dbc85a01e9d9e2d012a6d9c", "width": 108, "height": 207}, {"url": "https://external-preview.redd.it/MDkwc256cDNrZTZjMaYk2_Zlrv3-qF35FH9e7lUEZGDFuVy8h1wPdOh1cDBi.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=00569bcf44bdcd06f8e9467dc51f1fcda18b1dd8", "width": 216, "height": 415}, {"url": "https://external-preview.redd.it/MDkwc256cDNrZTZjMaYk2_Zlrv3-qF35FH9e7lUEZGDFuVy8h1wPdOh1cDBi.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e0bb9b2957003aedd27dffc1dbd67b7e4520da23", "width": 320, "height": 614}, {"url": "https://external-preview.redd.it/MDkwc256cDNrZTZjMaYk2_Zlrv3-qF35FH9e7lUEZGDFuVy8h1wPdOh1cDBi.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=55c92d1118e8fedd12731a76a0c500d17cffa316", "width": 640, "height": 1229}], "variants": {}, "id": "MDkwc256cDNrZTZjMaYk2_Zlrv3-qF35FH9e7lUEZGDFuVy8h1wPdOh1cDBi"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "14TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18itppj", "is_robot_indexable": true, "report_reasons": null, "author": "Beanconscriptog", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18itppj/just_recorded_this_audio_from_my_das_only_1_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/sl9bhut3ke6c1", "subreddit_subscribers": 718251, "created_utc": 1702621579.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/sl9bhut3ke6c1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 1280, "width": 666, "scrubber_media_url": "https://v.redd.it/sl9bhut3ke6c1/DASH_96.mp4", "dash_url": "https://v.redd.it/sl9bhut3ke6c1/DASHPlaylist.mpd?a=1705253910%2CNWQ1NzRhNTQ2NzA3NjBhZDI5YmM3YWZmZDJiZjg3MTIyZjM3ZTQ4ZDA2YTJmMzQ2Y2I0OTlkZTE3MmJlMGNmOA%3D%3D&amp;v=1&amp;f=sd", "duration": 4, "hls_url": "https://v.redd.it/sl9bhut3ke6c1/HLSPlaylist.m3u8?a=1705253910%2CNTA1NmIyNTFmM2Q5YjVmZThjYmRmNjE4YTg5NjJkNzhhMGIzMjllMTRiMTY4MjY1N2NjNGM3YzlhYTM0MDk0NA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download some shows off [archive.org](https://archive.org). Is there a way I can do this quickly? I want to download an entire subject, but I'm having issues. I tried using the IA CLI, but after a while it started to download videos that weren't in the subject. Any ideas?", "author_fullname": "t2_i1tjr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch Dowwnload Archive.org Subject?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iqncx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702610832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download some shows off &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;. Is there a way I can do this quickly? I want to download an entire subject, but I&amp;#39;m having issues. I tried using the IA CLI, but after a while it started to download videos that weren&amp;#39;t in the subject. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iqncx", "is_robot_indexable": true, "report_reasons": null, "author": "_ENunn_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iqncx/batch_dowwnload_archiveorg_subject/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iqncx/batch_dowwnload_archiveorg_subject/", "subreddit_subscribers": 718251, "created_utc": 1702610832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm getting  into data hoarding and have programming skills(python, JavaScript) that I've used to work on small freelance projects. Just through fiverr really.\n\nNow I'm wondering if there is any way I can use these more storage based computer skills to make money freelance.\n\nI'm specifying freelance because I already have a full time job in a different industry (sales) but I'd love to do more freelance on the side and see where it goes.", "author_fullname": "t2_istwnl540", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Can I Work Freelance Using These Skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iezjg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702578048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting  into data hoarding and have programming skills(python, JavaScript) that I&amp;#39;ve used to work on small freelance projects. Just through fiverr really.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m wondering if there is any way I can use these more storage based computer skills to make money freelance.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m specifying freelance because I already have a full time job in a different industry (sales) but I&amp;#39;d love to do more freelance on the side and see where it goes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iezjg", "is_robot_indexable": true, "report_reasons": null, "author": "Worldly_Cook_5449", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iezjg/how_can_i_work_freelance_using_these_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iezjg/how_can_i_work_freelance_using_these_skills/", "subreddit_subscribers": 718251, "created_utc": 1702578048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to find a somewhat decently sized collection of old videos, for a personal project, ideally, they would be videos of people recording themselves while walking around European cities at night, but realistically if I can't find a good collection like that I would be happy with just a big stash of old videos \n\nMy main goal here is to find videos that have that early 2000s feel so going to YouTube would be a waste since most videos that happen to be like this nowadays have too much production and video quality, I want something more janky and raw \n\nDo you guys have any tips on places where I could go to find such a collection of old stuff?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_2u8id0kq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where can I find archives of old videos ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j2sqt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702654533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to find a somewhat decently sized collection of old videos, for a personal project, ideally, they would be videos of people recording themselves while walking around European cities at night, but realistically if I can&amp;#39;t find a good collection like that I would be happy with just a big stash of old videos &lt;/p&gt;\n\n&lt;p&gt;My main goal here is to find videos that have that early 2000s feel so going to YouTube would be a waste since most videos that happen to be like this nowadays have too much production and video quality, I want something more janky and raw &lt;/p&gt;\n\n&lt;p&gt;Do you guys have any tips on places where I could go to find such a collection of old stuff?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j2sqt", "is_robot_indexable": true, "report_reasons": null, "author": "finix6958", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j2sqt/where_can_i_find_archives_of_old_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j2sqt/where_can_i_find_archives_of_old_videos/", "subreddit_subscribers": 718251, "created_utc": 1702654533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I wanna build a pool made from 6 drives. 2 drives were bought refurbished so I consider them less safe. But they could be perfect for holding duplicated files. Can I \"mark\" these drives so they would be the only drives to hold duplicated files ?   \nI read some article where it talked about how you could build nested pools for exactly this purpose, but it was not very clear for me. ", "author_fullname": "t2_4uf8g8z9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DrivePool, can I designate only certain drives to be used for duplicated files ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j2ln3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702654012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna build a pool made from 6 drives. 2 drives were bought refurbished so I consider them less safe. But they could be perfect for holding duplicated files. Can I &amp;quot;mark&amp;quot; these drives so they would be the only drives to hold duplicated files ?&lt;br/&gt;\nI read some article where it talked about how you could build nested pools for exactly this purpose, but it was not very clear for me. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18j2ln3", "is_robot_indexable": true, "report_reasons": null, "author": "sebastian___111", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j2ln3/drivepool_can_i_designate_only_certain_drives_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j2ln3/drivepool_can_i_designate_only_certain_drives_to/", "subreddit_subscribers": 718251, "created_utc": 1702654012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many videos collections and i have been looking since a while for a way on windows to have an animated preview thumbnail of parts of the video when you are hovering the mouse on it, does anyone knows if it is somehow possible? I haven't found anything good, i tried one paid software recommended on reddit but i forgot the name and it was bad anyways, it did not had that specific feature", "author_fullname": "t2_eneyhc4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Animated thumbnails for video collections?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j0v0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702649150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many videos collections and i have been looking since a while for a way on windows to have an animated preview thumbnail of parts of the video when you are hovering the mouse on it, does anyone knows if it is somehow possible? I haven&amp;#39;t found anything good, i tried one paid software recommended on reddit but i forgot the name and it was bad anyways, it did not had that specific feature&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18j0v0g", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Gate6899", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18j0v0g/animated_thumbnails_for_video_collections/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18j0v0g/animated_thumbnails_for_video_collections/", "subreddit_subscribers": 718251, "created_utc": 1702649150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there anything I can set up and leave running on one of my servers that will automatically crawl and rip a specific site or pages, and check for new versions?\n\nI'm quite new to this stuff, so I wouldn't even know where to look.", "author_fullname": "t2_dljiq4aip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Continuous automated site ripping?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iytnl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702642744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there anything I can set up and leave running on one of my servers that will automatically crawl and rip a specific site or pages, and check for new versions?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite new to this stuff, so I wouldn&amp;#39;t even know where to look.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iytnl", "is_robot_indexable": true, "report_reasons": null, "author": "Constant_Shine_2367", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iytnl/continuous_automated_site_ripping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iytnl/continuous_automated_site_ripping/", "subreddit_subscribers": 718251, "created_utc": 1702642744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm new to this data hoarding scene and now that I've started my archive, I've quickly realized that 6 TB is so little space it's blowing my mind. Coming from the ages of 40gb hdd's where the biggest they got, 6TB was like infinite to me, till now. \n\n&amp;#x200B;\n\nMy question for advice is where do I search for the best deals on HDD's as they come up? I have the [shucks.top](https://shucks.top) open constantly but I'm wondering if there's some other website that does what this angel does? \n\n&amp;#x200B;\n\nOr where to start on drive knowledge anyways, what I'm storing PROBABLY won't move from drive to drive. It'll stay on the HDD and then maybe copied to other drives occasionally but for the most part it'll end up sitting in the drives in a protective case. \n\n&amp;#x200B;\n\nAny help would be GREATLY appreciated. I'm not concerned about speed and what not really, I like going slow and taking my time when transferring data and what not so I'm not concerned about some NAS set up or anything unless there's some MASSIVE benefit to investing in it vs just putting in a new 16TGB HDD everytime one fills up. \n\n&amp;#x200B;\n\nIf I sound like a noob, it's cause I am when it comes to this. Please don't rip me to bits. haha", "author_fullname": "t2_bg3zcbnm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to archiving and need some advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ig11u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702580837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this data hoarding scene and now that I&amp;#39;ve started my archive, I&amp;#39;ve quickly realized that 6 TB is so little space it&amp;#39;s blowing my mind. Coming from the ages of 40gb hdd&amp;#39;s where the biggest they got, 6TB was like infinite to me, till now. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question for advice is where do I search for the best deals on HDD&amp;#39;s as they come up? I have the &lt;a href=\"https://shucks.top\"&gt;shucks.top&lt;/a&gt; open constantly but I&amp;#39;m wondering if there&amp;#39;s some other website that does what this angel does? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Or where to start on drive knowledge anyways, what I&amp;#39;m storing PROBABLY won&amp;#39;t move from drive to drive. It&amp;#39;ll stay on the HDD and then maybe copied to other drives occasionally but for the most part it&amp;#39;ll end up sitting in the drives in a protective case. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any help would be GREATLY appreciated. I&amp;#39;m not concerned about speed and what not really, I like going slow and taking my time when transferring data and what not so I&amp;#39;m not concerned about some NAS set up or anything unless there&amp;#39;s some MASSIVE benefit to investing in it vs just putting in a new 16TGB HDD everytime one fills up. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I sound like a noob, it&amp;#39;s cause I am when it comes to this. Please don&amp;#39;t rip me to bits. haha&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zRLFtn-6oVv6rEQ7IOONhFbHxXDHwJjmbwnN7D_thZk.jpg?auto=webp&amp;s=67dae02de5f2f5bbdab22337b1d614ff6d511092", "width": 512, "height": 256}, "resolutions": [{"url": "https://external-preview.redd.it/zRLFtn-6oVv6rEQ7IOONhFbHxXDHwJjmbwnN7D_thZk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf43cb28471bc4327923afe88dfcf6aa750f51b1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/zRLFtn-6oVv6rEQ7IOONhFbHxXDHwJjmbwnN7D_thZk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a0aaef1fdd5bff6b7b06aae95a598c72f5e99fb", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/zRLFtn-6oVv6rEQ7IOONhFbHxXDHwJjmbwnN7D_thZk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d43e8f9a57710c82ade7bb10618ffb01a3b7da57", "width": 320, "height": 160}], "variants": {}, "id": "btzbLKPxXvroz0VXZS1fg-sPzgV5mm-5c3WebLrqOBw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ig11u", "is_robot_indexable": true, "report_reasons": null, "author": "ImHereForGameboys", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ig11u/new_to_archiving_and_need_some_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ig11u/new_to_archiving_and_need_some_advice/", "subreddit_subscribers": 718251, "created_utc": 1702580837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Someone know how to use this disk in Windows 10?\n\nThe disk is recognized as USB device (in the task bar) but does not apear in Windows Explorer nor in the Device disks.", "author_fullname": "t2_7klstuxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LaCie P9220", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ifz1x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702580680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Someone know how to use this disk in Windows 10?&lt;/p&gt;\n\n&lt;p&gt;The disk is recognized as USB device (in the task bar) but does not apear in Windows Explorer nor in the Device disks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ifz1x", "is_robot_indexable": true, "report_reasons": null, "author": "Zealousideal-Talk-84", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ifz1x/lacie_p9220/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ifz1x/lacie_p9220/", "subreddit_subscribers": 718251, "created_utc": 1702580680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if this the right forum, if not, point me towards it please\n\ntl:dr - When trying to download a file from a certain website, the file size shows as 1.9gig at the start but file would finish downloading some lesser size in that making it corrupt.\n\nHello guys, I have been having trouble downloading a large course from a certain website. The course is divided into 17 archive files. Whenever i try to download them from browser, a certain file would give me a size at start but then it would finish downloading some random size shorter than that. I have been able to download some files successfully from that site repeatedly, but sometimes other files would show size let's say \"downloading 1.9gig\" and finish around 700meg. I am not sure what's the issue here.\n\nI cannot even resume the \"actually incomplete\" downloads sinces the software, be it browser or download manager, never shows an error but says that the download is complete which infact is much lesser size than it showed when download started.\n\nI need to have all 17 parts in order to have them successfully unzipped. I tried downloading them multiple times but some file or another would be incomplete.\n\nI tried switching browser, Chrono download manager and even download manager programs all seem to have the same issue.\n\nI am guessing that the site may have some sort of rate limiting on downloads or is it some sort of technical glitch from server?\n\nThe site also has a TG channel where it states the download limit is 3 Mbps per client but I have been able to download on much higher rate than that so I guess there's else at play here.\n\nCould the rate limit be a \"per day\" thing or \"you can download a certain size in certain time'. I really could not figure it out what the issue is here.\n\nCan I use some utility to limit the download speed to make it really slow to ensure that I download the file in its entirety. Is there's a software which can help in ensuring this - like it ensure that the file downloaded is actually is of the size which was communicated by the server when download started?", "author_fullname": "t2_1g8c19m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trouble with downloading from a certain site", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iutv8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702626066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if this the right forum, if not, point me towards it please&lt;/p&gt;\n\n&lt;p&gt;tl:dr - When trying to download a file from a certain website, the file size shows as 1.9gig at the start but file would finish downloading some lesser size in that making it corrupt.&lt;/p&gt;\n\n&lt;p&gt;Hello guys, I have been having trouble downloading a large course from a certain website. The course is divided into 17 archive files. Whenever i try to download them from browser, a certain file would give me a size at start but then it would finish downloading some random size shorter than that. I have been able to download some files successfully from that site repeatedly, but sometimes other files would show size let&amp;#39;s say &amp;quot;downloading 1.9gig&amp;quot; and finish around 700meg. I am not sure what&amp;#39;s the issue here.&lt;/p&gt;\n\n&lt;p&gt;I cannot even resume the &amp;quot;actually incomplete&amp;quot; downloads sinces the software, be it browser or download manager, never shows an error but says that the download is complete which infact is much lesser size than it showed when download started.&lt;/p&gt;\n\n&lt;p&gt;I need to have all 17 parts in order to have them successfully unzipped. I tried downloading them multiple times but some file or another would be incomplete.&lt;/p&gt;\n\n&lt;p&gt;I tried switching browser, Chrono download manager and even download manager programs all seem to have the same issue.&lt;/p&gt;\n\n&lt;p&gt;I am guessing that the site may have some sort of rate limiting on downloads or is it some sort of technical glitch from server?&lt;/p&gt;\n\n&lt;p&gt;The site also has a TG channel where it states the download limit is 3 Mbps per client but I have been able to download on much higher rate than that so I guess there&amp;#39;s else at play here.&lt;/p&gt;\n\n&lt;p&gt;Could the rate limit be a &amp;quot;per day&amp;quot; thing or &amp;quot;you can download a certain size in certain time&amp;#39;. I really could not figure it out what the issue is here.&lt;/p&gt;\n\n&lt;p&gt;Can I use some utility to limit the download speed to make it really slow to ensure that I download the file in its entirety. Is there&amp;#39;s a software which can help in ensuring this - like it ensure that the file downloaded is actually is of the size which was communicated by the server when download started?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18iutv8", "is_robot_indexable": true, "report_reasons": null, "author": "bawlachora", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18iutv8/trouble_with_downloading_from_a_certain_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18iutv8/trouble_with_downloading_from_a_certain_site/", "subreddit_subscribers": 718251, "created_utc": 1702626066.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}